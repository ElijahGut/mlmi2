Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.773233618736267
  batch 100 loss: 3.2097266483306885
  batch 150 loss: 3.034425067901611
  batch 200 loss: 2.7748702239990233
  batch 250 loss: 2.612421360015869
  batch 300 loss: 2.4542616510391237
  batch 350 loss: 2.3738819789886474
  batch 400 loss: 2.3153184771537783
  batch 450 loss: 2.2512386322021483
  batch 500 loss: 2.1509750986099245
  batch 550 loss: 2.126170349121094
  batch 600 loss: 2.065784802436829
  batch 650 loss: 1.9737531781196593
  batch 700 loss: 1.9777314066886902
  batch 750 loss: 1.9160353302955628
  batch 800 loss: 1.9223227548599242
  batch 850 loss: 1.8742271733283997
  batch 900 loss: 1.868059401512146
LOSS train 1.86806 valid 1.84162, valid PER 71.14%
EPOCH 2:
  batch 50 loss: 1.8130150866508483
  batch 100 loss: 1.7432574725151062
  batch 150 loss: 1.740632588863373
  batch 200 loss: 1.7675302863121032
  batch 250 loss: 1.7441303277015685
  batch 300 loss: 1.7147977089881896
  batch 350 loss: 1.628375244140625
  batch 400 loss: 1.6591128373146058
  batch 450 loss: 1.608218913078308
  batch 500 loss: 1.6387489748001098
  batch 550 loss: 1.649827778339386
  batch 600 loss: 1.5820766854286195
  batch 650 loss: 1.6343154048919677
  batch 700 loss: 1.5921292281150818
  batch 750 loss: 1.5780668568611145
  batch 800 loss: 1.513403329849243
  batch 850 loss: 1.5382303142547606
  batch 900 loss: 1.5499196434020996
LOSS train 1.54992 valid 1.45706, valid PER 56.90%
EPOCH 3:
  batch 50 loss: 1.5199486303329468
  batch 100 loss: 1.478742277622223
  batch 150 loss: 1.4765703344345094
  batch 200 loss: 1.4584185814857482
  batch 250 loss: 1.459892408847809
  batch 300 loss: 1.4608456921577453
  batch 350 loss: 1.493270115852356
  batch 400 loss: 1.4770455718040467
  batch 450 loss: 1.4368677425384522
  batch 500 loss: 1.4482297253608705
  batch 550 loss: 1.4372818350791932
  batch 600 loss: 1.3999126720428468
  batch 650 loss: 1.411483051776886
  batch 700 loss: 1.4056314706802369
  batch 750 loss: 1.486294128894806
  batch 800 loss: 1.3707835745811463
  batch 850 loss: 1.4314120411872864
  batch 900 loss: 1.3613799047470092
LOSS train 1.36138 valid 1.32865, valid PER 49.30%
EPOCH 4:
  batch 50 loss: 1.3610084533691407
  batch 100 loss: 1.38228830575943
  batch 150 loss: 1.3384854221343994
  batch 200 loss: 1.363199224472046
  batch 250 loss: 1.3644135427474975
  batch 300 loss: 1.368808581829071
  batch 350 loss: 1.2985951972007752
  batch 400 loss: 1.360665340423584
  batch 450 loss: 1.3302396976947783
  batch 500 loss: 1.3009893798828125
  batch 550 loss: 1.332932550907135
  batch 600 loss: 1.3690860438346864
  batch 650 loss: 1.3254556155204773
  batch 700 loss: 1.2973346614837646
  batch 750 loss: 1.2883310830593109
  batch 800 loss: 1.2601576471328735
  batch 850 loss: 1.2988554060459137
  batch 900 loss: 1.3281147503852844
LOSS train 1.32811 valid 1.23845, valid PER 43.29%
EPOCH 5:
  batch 50 loss: 1.2595742619037629
  batch 100 loss: 1.2559286046028137
  batch 150 loss: 1.2948252773284912
  batch 200 loss: 1.2281557309627533
  batch 250 loss: 1.242292127609253
  batch 300 loss: 1.259336223602295
  batch 350 loss: 1.2721774578094482
  batch 400 loss: 1.249692451953888
  batch 450 loss: 1.231973363161087
  batch 500 loss: 1.2361416840553283
  batch 550 loss: 1.1935263085365295
  batch 600 loss: 1.271955553293228
  batch 650 loss: 1.2260484731197356
  batch 700 loss: 1.2707750272750855
  batch 750 loss: 1.1885544204711913
  batch 800 loss: 1.2541392028331757
  batch 850 loss: 1.225227154493332
  batch 900 loss: 1.2469729220867156
LOSS train 1.24697 valid 1.13725, valid PER 38.54%
EPOCH 6:
  batch 50 loss: 1.2249940407276154
  batch 100 loss: 1.1796658384799956
  batch 150 loss: 1.1613136756420135
  batch 200 loss: 1.1840781962871552
  batch 250 loss: 1.2135387086868286
  batch 300 loss: 1.1922054886817932
  batch 350 loss: 1.183703362941742
  batch 400 loss: 1.1684034156799317
  batch 450 loss: 1.2004276883602143
  batch 500 loss: 1.175475345849991
  batch 550 loss: 1.1948710310459136
  batch 600 loss: 1.1676725506782533
  batch 650 loss: 1.1843018651008606
  batch 700 loss: 1.168782535791397
  batch 750 loss: 1.1559537959098816
  batch 800 loss: 1.1364119744300842
  batch 850 loss: 1.1289279508590697
  batch 900 loss: 1.1632170724868773
LOSS train 1.16322 valid 1.12722, valid PER 36.56%
EPOCH 7:
  batch 50 loss: 1.135983531475067
  batch 100 loss: 1.1487741029262544
  batch 150 loss: 1.127407079935074
  batch 200 loss: 1.1111803507804872
  batch 250 loss: 1.13016997218132
  batch 300 loss: 1.1037998855113984
  batch 350 loss: 1.1283893048763276
  batch 400 loss: 1.1138624548912048
  batch 450 loss: 1.1181924092769622
  batch 500 loss: 1.091924374103546
  batch 550 loss: 1.1043689227104188
  batch 600 loss: 1.1338706827163696
  batch 650 loss: 1.0988672649860383
  batch 700 loss: 1.1246081709861755
  batch 750 loss: 1.1018412184715272
  batch 800 loss: 1.0908467388153076
  batch 850 loss: 1.127650669813156
  batch 900 loss: 1.147018530368805
LOSS train 1.14702 valid 1.09042, valid PER 36.50%
EPOCH 8:
  batch 50 loss: 1.0759584403038025
  batch 100 loss: 1.0613696253299714
  batch 150 loss: 1.0622024488449098
  batch 200 loss: 1.0366110038757324
  batch 250 loss: 1.0858552050590515
  batch 300 loss: 1.0171289837360382
  batch 350 loss: 1.1136613953113557
  batch 400 loss: 1.0568711686134338
  batch 450 loss: 1.0732232451438903
  batch 500 loss: 1.1120125198364257
  batch 550 loss: 1.036624332666397
  batch 600 loss: 1.0876840257644653
  batch 650 loss: 1.12203705906868
  batch 700 loss: 1.0436874902248383
  batch 750 loss: 1.0593052196502686
  batch 800 loss: 1.0740657329559327
  batch 850 loss: 1.083696665763855
  batch 900 loss: 1.0701373505592346
LOSS train 1.07014 valid 1.06377, valid PER 34.24%
EPOCH 9:
  batch 50 loss: 1.0180218601226807
  batch 100 loss: 1.0429249536991119
  batch 150 loss: 1.0395455849170685
  batch 200 loss: 1.0120207536220551
  batch 250 loss: 1.0471426057815552
  batch 300 loss: 1.0520370519161224
  batch 350 loss: 1.0839269757270813
  batch 400 loss: 1.0541184294223784
  batch 450 loss: 1.0414001142978668
  batch 500 loss: 1.0373286998271942
  batch 550 loss: 1.0456603789329528
  batch 600 loss: 1.047927360534668
  batch 650 loss: 1.0253265178203583
  batch 700 loss: 1.0136251986026763
  batch 750 loss: 1.029652863740921
  batch 800 loss: 1.0553757548332214
  batch 850 loss: 1.0623949038982392
  batch 900 loss: 1.017878452539444
LOSS train 1.01788 valid 1.03609, valid PER 32.80%
EPOCH 10:
  batch 50 loss: 0.9763485085964203
  batch 100 loss: 0.9901644718647004
  batch 150 loss: 1.0197132623195648
  batch 200 loss: 1.012553689479828
  batch 250 loss: 1.019170550107956
  batch 300 loss: 0.9865462267398835
  batch 350 loss: 1.0173319506645202
  batch 400 loss: 0.9844550824165345
  batch 450 loss: 0.9878422689437866
  batch 500 loss: 1.0164563298225402
  batch 550 loss: 1.0290615046024323
  batch 600 loss: 1.0260794162750244
  batch 650 loss: 0.9917168140411377
  batch 700 loss: 0.9957879436016083
  batch 750 loss: 0.9868830609321594
  batch 800 loss: 1.012601684331894
  batch 850 loss: 1.016812069416046
  batch 900 loss: 1.0307649314403533
LOSS train 1.03076 valid 1.03158, valid PER 33.91%
EPOCH 11:
  batch 50 loss: 0.9728786432743073
  batch 100 loss: 0.9603612816333771
  batch 150 loss: 0.9684421765804291
  batch 200 loss: 1.0050728833675384
  batch 250 loss: 0.9910150456428528
  batch 300 loss: 0.9597695994377137
  batch 350 loss: 0.9882394385337829
  batch 400 loss: 0.9884337091445923
  batch 450 loss: 0.9893420171737671
  batch 500 loss: 0.9893681192398072
  batch 550 loss: 0.9709696626663208
  batch 600 loss: 0.9728625965118408
  batch 650 loss: 1.0087821555137635
  batch 700 loss: 0.9558275139331818
  batch 750 loss: 0.9673060762882233
  batch 800 loss: 0.9846456062793731
  batch 850 loss: 1.0094034659862519
  batch 900 loss: 0.9889994490146637
LOSS train 0.98900 valid 0.97763, valid PER 31.40%
EPOCH 12:
  batch 50 loss: 0.9566398310661316
  batch 100 loss: 0.9419489431381226
  batch 150 loss: 0.9340000677108765
  batch 200 loss: 0.9345483994483947
  batch 250 loss: 0.9593756830692292
  batch 300 loss: 0.9425324511528015
  batch 350 loss: 0.9618980157375335
  batch 400 loss: 0.9674573695659637
  batch 450 loss: 0.9562415289878845
  batch 500 loss: 0.9801809287071228
  batch 550 loss: 0.924005583524704
  batch 600 loss: 0.9262424123287201
  batch 650 loss: 0.9777990782260895
  batch 700 loss: 0.9651082181930541
  batch 750 loss: 0.9676169443130493
  batch 800 loss: 0.9241128730773925
  batch 850 loss: 0.9908130085468292
  batch 900 loss: 0.9608915472030639
LOSS train 0.96089 valid 0.96543, valid PER 31.12%
EPOCH 13:
  batch 50 loss: 0.9030348551273346
  batch 100 loss: 0.935996116399765
  batch 150 loss: 0.906864492893219
  batch 200 loss: 0.9545237803459168
  batch 250 loss: 0.9511239862442017
  batch 300 loss: 0.9117953598499298
  batch 350 loss: 0.9101868224143982
  batch 400 loss: 0.9516920506954193
  batch 450 loss: 0.9486254107952118
  batch 500 loss: 0.9074093222618103
  batch 550 loss: 0.9460306429862976
  batch 600 loss: 0.9184108221530914
  batch 650 loss: 0.9659091222286225
  batch 700 loss: 0.9704303538799286
  batch 750 loss: 0.9154309594631195
  batch 800 loss: 0.9302720189094543
  batch 850 loss: 0.9620302748680115
  batch 900 loss: 0.9391797769069672
LOSS train 0.93918 valid 0.98913, valid PER 31.01%
EPOCH 14:
  batch 50 loss: 0.9172436726093293
  batch 100 loss: 0.9493826353549957
  batch 150 loss: 0.9140852165222167
  batch 200 loss: 0.9156660759449005
  batch 250 loss: 0.9087289130687713
  batch 300 loss: 0.9413231432437896
  batch 350 loss: 0.9042553460597992
  batch 400 loss: 0.9122515654563904
  batch 450 loss: 0.9028850209712982
  batch 500 loss: 0.8991261076927185
  batch 550 loss: 0.9317944777011872
  batch 600 loss: 0.9017340934276581
  batch 650 loss: 0.9370111656188965
  batch 700 loss: 0.9465530347824097
  batch 750 loss: 0.9203112721443176
  batch 800 loss: 0.8846886599063873
  batch 850 loss: 0.9584302949905396
  batch 900 loss: 0.9300417602062225
LOSS train 0.93004 valid 0.95963, valid PER 30.46%
EPOCH 15:
  batch 50 loss: 0.9106258893013001
  batch 100 loss: 0.8780991649627685
  batch 150 loss: 0.8860489976406097
  batch 200 loss: 0.9120322775840759
  batch 250 loss: 0.9313234376907349
  batch 300 loss: 0.8853923583030701
  batch 350 loss: 0.8957196760177613
  batch 400 loss: 0.8853682947158813
  batch 450 loss: 0.8912232983112335
  batch 500 loss: 0.8621697187423706
  batch 550 loss: 0.9162382376194
  batch 600 loss: 0.9252428603172302
  batch 650 loss: 0.8987506604194642
  batch 700 loss: 0.9232520461082458
  batch 750 loss: 0.9129293310642242
  batch 800 loss: 0.8996730589866638
  batch 850 loss: 0.8852601325511933
  batch 900 loss: 0.8859345304965973
LOSS train 0.88593 valid 0.96386, valid PER 30.72%
EPOCH 16:
  batch 50 loss: 0.9092360460758209
  batch 100 loss: 0.8445074081420898
  batch 150 loss: 0.8506136393547058
  batch 200 loss: 0.86191596865654
  batch 250 loss: 0.9034647071361541
  batch 300 loss: 0.8684850823879242
  batch 350 loss: 0.9017389118671417
  batch 400 loss: 0.8941531097888946
  batch 450 loss: 0.9106269228458405
  batch 500 loss: 0.8579920434951782
  batch 550 loss: 0.9013124406337738
  batch 600 loss: 0.8862203085422515
  batch 650 loss: 0.8982600891590118
  batch 700 loss: 0.8922777736186981
  batch 750 loss: 0.8910041964054107
  batch 800 loss: 0.9194836521148682
  batch 850 loss: 0.8815434014797211
  batch 900 loss: 0.8840393137931823
LOSS train 0.88404 valid 0.94364, valid PER 29.37%
EPOCH 17:
  batch 50 loss: 0.8728354465961456
  batch 100 loss: 0.8596435499191284
  batch 150 loss: 0.8652383828163147
  batch 200 loss: 0.8682699310779571
  batch 250 loss: 0.8763616907596589
  batch 300 loss: 0.8865551900863647
  batch 350 loss: 0.8536545717716217
  batch 400 loss: 0.9119491183757782
  batch 450 loss: 0.8966643178462982
  batch 500 loss: 0.8507802379131317
  batch 550 loss: 0.8706910216808319
  batch 600 loss: 0.9175493800640107
  batch 650 loss: 0.8624016189575195
  batch 700 loss: 0.8740375101566314
  batch 750 loss: 0.8590932059288025
  batch 800 loss: 0.8833890998363495
  batch 850 loss: 0.8856045651435852
  batch 900 loss: 0.8577503955364227
LOSS train 0.85775 valid 0.92868, valid PER 29.28%
EPOCH 18:
  batch 50 loss: 0.8464050281047821
  batch 100 loss: 0.8517324841022491
  batch 150 loss: 0.8688257730007172
  batch 200 loss: 0.8789781069755555
  batch 250 loss: 0.8480205583572388
  batch 300 loss: 0.8409228944778442
  batch 350 loss: 0.8755268657207489
  batch 400 loss: 0.8435547542572022
  batch 450 loss: 0.8939747893810273
  batch 500 loss: 0.8749878537654877
  batch 550 loss: 0.8734171736240387
  batch 600 loss: 0.8453980171680451
  batch 650 loss: 0.8506504940986633
  batch 700 loss: 0.8948840498924255
  batch 750 loss: 0.8529746615886689
  batch 800 loss: 0.8602036809921265
  batch 850 loss: 0.8419786286354065
  batch 900 loss: 0.9017404460906983
LOSS train 0.90174 valid 0.94733, valid PER 30.00%
EPOCH 19:
  batch 50 loss: 0.8081080853939057
  batch 100 loss: 0.8085269045829773
  batch 150 loss: 0.8294094264507293
  batch 200 loss: 0.8531522667407989
  batch 250 loss: 0.8602995228767395
  batch 300 loss: 0.846113851070404
  batch 350 loss: 0.8469162690639496
  batch 400 loss: 0.8755152606964112
  batch 450 loss: 0.8663655769824982
  batch 500 loss: 0.8594636404514313
  batch 550 loss: 0.8409120547771454
  batch 600 loss: 0.8446549916267395
  batch 650 loss: 0.895140792131424
  batch 700 loss: 0.8183742427825927
  batch 750 loss: 0.8163052093982697
  batch 800 loss: 0.8558121240139007
  batch 850 loss: 0.8582773804664612
  batch 900 loss: 0.8672696566581726
LOSS train 0.86727 valid 0.93793, valid PER 29.50%
EPOCH 20:
  batch 50 loss: 0.8204509723186493
  batch 100 loss: 0.7965534746646881
  batch 150 loss: 0.8046860694885254
  batch 200 loss: 0.8385751724243165
  batch 250 loss: 0.8238973736763
  batch 300 loss: 0.8597963225841522
  batch 350 loss: 0.8151901799440384
  batch 400 loss: 0.8246527361869812
  batch 450 loss: 0.8223350363969802
  batch 500 loss: 0.8203232920169831
  batch 550 loss: 0.8742003381252289
  batch 600 loss: 0.8260409653186798
  batch 650 loss: 0.8629052805900573
  batch 700 loss: 0.8545576226711273
  batch 750 loss: 0.8367301499843598
  batch 800 loss: 0.8772295093536377
  batch 850 loss: 0.8739346575736999
  batch 900 loss: 0.8551536762714386
LOSS train 0.85515 valid 0.92420, valid PER 28.88%
Training finished in 4.0 minutes.
Model saved to checkpoints/20231206_213416/model_20
Loading model from checkpoints/20231206_213416/model_20
SUB: 15.28%, DEL: 13.39%, INS: 1.70%, COR: 71.33%, PER: 30.36%
