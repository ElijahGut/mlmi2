Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=0.5)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 6.162901148796082
  batch 100 loss: 3.251706953048706
  batch 150 loss: 3.173088402748108
  batch 200 loss: 2.9912546634674073
  batch 250 loss: 2.7874000310897826
  batch 300 loss: 2.616499538421631
  batch 350 loss: 2.4959244441986086
  batch 400 loss: 2.4428410959243774
  batch 450 loss: 2.367891345024109
  batch 500 loss: 2.2687375569343566
  batch 550 loss: 2.22126499414444
  batch 600 loss: 2.1894733119010925
  batch 650 loss: 2.109441571235657
  batch 700 loss: 2.113018753528595
  batch 750 loss: 2.0575561928749084
  batch 800 loss: 2.0283130908012392
  batch 850 loss: 2.0028141951560974
  batch 900 loss: 1.988902609348297
LOSS train 1.98890 valid 1.91471, valid PER 74.22%
EPOCH 2:
  batch 50 loss: 1.942670419216156
  batch 100 loss: 1.893410198688507
  batch 150 loss: 1.8486807942390442
  batch 200 loss: 1.8511078238487244
  batch 250 loss: 1.8574970555305481
  batch 300 loss: 1.8335710120201112
  batch 350 loss: 1.7446707725524901
  batch 400 loss: 1.7668948936462403
  batch 450 loss: 1.7347732973098755
  batch 500 loss: 1.7463892769813538
  batch 550 loss: 1.7575861144065856
  batch 600 loss: 1.7028326797485351
  batch 650 loss: 1.7300981235504151
  batch 700 loss: 1.6944143772125244
  batch 750 loss: 1.685852711200714
  batch 800 loss: 1.6466860389709472
  batch 850 loss: 1.637897801399231
  batch 900 loss: 1.6675757002830505
LOSS train 1.66758 valid 1.60505, valid PER 62.74%
EPOCH 3:
  batch 50 loss: 1.6348378586769103
  batch 100 loss: 1.5891054892539977
  batch 150 loss: 1.5972258019447327
  batch 200 loss: 1.5789104175567628
  batch 250 loss: 1.5642219662666321
  batch 300 loss: 1.5693411993980408
  batch 350 loss: 1.5990310978889466
  batch 400 loss: 1.5808884739875793
  batch 450 loss: 1.5304941749572754
  batch 500 loss: 1.5440509486198426
  batch 550 loss: 1.5334743070602417
  batch 600 loss: 1.5022338128089905
  batch 650 loss: 1.4713945007324218
  batch 700 loss: 1.5028157019615174
  batch 750 loss: 1.5558655524253846
  batch 800 loss: 1.4832681798934937
  batch 850 loss: 1.523672604560852
  batch 900 loss: 1.447024893760681
LOSS train 1.44702 valid 1.41982, valid PER 52.78%
EPOCH 4:
  batch 50 loss: 1.4517075777053834
  batch 100 loss: 1.4691308188438414
  batch 150 loss: 1.4277382206916809
  batch 200 loss: 1.4744049668312074
  batch 250 loss: 1.4652808117866516
  batch 300 loss: 1.4621424508094787
  batch 350 loss: 1.3910679149627685
  batch 400 loss: 1.4443054914474487
  batch 450 loss: 1.4266240763664246
  batch 500 loss: 1.3877628946304321
  batch 550 loss: 1.4163856029510498
  batch 600 loss: 1.440063169002533
  batch 650 loss: 1.4211376571655274
  batch 700 loss: 1.3793680238723756
  batch 750 loss: 1.3754912161827086
  batch 800 loss: 1.3424013018608094
  batch 850 loss: 1.388750832080841
  batch 900 loss: 1.4111811184883118
LOSS train 1.41118 valid 1.34715, valid PER 48.58%
EPOCH 5:
  batch 50 loss: 1.3583658194541932
  batch 100 loss: 1.3426661944389344
  batch 150 loss: 1.3904987049102784
  batch 200 loss: 1.3266672337055205
  batch 250 loss: 1.3318126511573791
  batch 300 loss: 1.3545351004600525
  batch 350 loss: 1.340520634651184
  batch 400 loss: 1.334910249710083
  batch 450 loss: 1.3199474358558654
  batch 500 loss: 1.3390808844566344
  batch 550 loss: 1.2850229382514953
  batch 600 loss: 1.3544584536552429
  batch 650 loss: 1.316241750717163
  batch 700 loss: 1.3504661750793456
  batch 750 loss: 1.275906949043274
  batch 800 loss: 1.3146145844459534
  batch 850 loss: 1.3145602750778198
  batch 900 loss: 1.3237152802944183
LOSS train 1.32372 valid 1.24696, valid PER 42.62%
EPOCH 6:
  batch 50 loss: 1.3137957239151001
  batch 100 loss: 1.2690550422668456
  batch 150 loss: 1.2559762680530548
  batch 200 loss: 1.2725340461730956
  batch 250 loss: 1.2899849128723144
  batch 300 loss: 1.2746030843257905
  batch 350 loss: 1.2580402326583862
  batch 400 loss: 1.2669152212142945
  batch 450 loss: 1.2593225264549255
  batch 500 loss: 1.2564089798927307
  batch 550 loss: 1.2769674134254456
  batch 600 loss: 1.2560556411743165
  batch 650 loss: 1.2546040654182433
  batch 700 loss: 1.2407767868041992
  batch 750 loss: 1.2279798936843873
  batch 800 loss: 1.226775152683258
  batch 850 loss: 1.2228782498836517
  batch 900 loss: 1.2368300235271454
LOSS train 1.23683 valid 1.21027, valid PER 41.17%
EPOCH 7:
  batch 50 loss: 1.215335429906845
  batch 100 loss: 1.2370258450508118
  batch 150 loss: 1.225044071674347
  batch 200 loss: 1.1995516216754913
  batch 250 loss: 1.2162798762321472
  batch 300 loss: 1.1907842564582825
  batch 350 loss: 1.1919909358024596
  batch 400 loss: 1.2120610177516937
  batch 450 loss: 1.1921666359901428
  batch 500 loss: 1.1875354933738709
  batch 550 loss: 1.194368646144867
  batch 600 loss: 1.2044740557670592
  batch 650 loss: 1.2026527261734008
  batch 700 loss: 1.2073145306110382
  batch 750 loss: 1.1819923973083497
  batch 800 loss: 1.179134987592697
  batch 850 loss: 1.2094655871391295
  batch 900 loss: 1.2395267021656036
LOSS train 1.23953 valid 1.15929, valid PER 38.78%
EPOCH 8:
  batch 50 loss: 1.1880253100395202
  batch 100 loss: 1.1701660943031311
  batch 150 loss: 1.160998762845993
  batch 200 loss: 1.1444066035747529
  batch 250 loss: 1.1662361824512482
  batch 300 loss: 1.0968106138706206
  batch 350 loss: 1.1843397760391234
  batch 400 loss: 1.1529655754566193
  batch 450 loss: 1.1609158909320831
  batch 500 loss: 1.1972721242904663
  batch 550 loss: 1.1159856569767
  batch 600 loss: 1.1682781314849853
  batch 650 loss: 1.1895885169506073
  batch 700 loss: 1.1398932981491088
  batch 750 loss: 1.1420517933368683
  batch 800 loss: 1.1482094407081604
  batch 850 loss: 1.1749845623970032
  batch 900 loss: 1.15418581366539
LOSS train 1.15419 valid 1.12981, valid PER 36.76%
EPOCH 9:
  batch 50 loss: 1.0824658048152924
  batch 100 loss: 1.1359544289112091
  batch 150 loss: 1.130865252017975
  batch 200 loss: 1.1053813767433167
  batch 250 loss: 1.1329050517082215
  batch 300 loss: 1.1332755839824677
  batch 350 loss: 1.1470166110992432
  batch 400 loss: 1.1520183277130127
  batch 450 loss: 1.1281608176231384
  batch 500 loss: 1.0994717597961425
  batch 550 loss: 1.1299964916706084
  batch 600 loss: 1.1418047797679902
  batch 650 loss: 1.107199298143387
  batch 700 loss: 1.0892656826972962
  batch 750 loss: 1.1092037785053253
  batch 800 loss: 1.1214435231685638
  batch 850 loss: 1.1410155141353606
  batch 900 loss: 1.0967510330677033
LOSS train 1.09675 valid 1.08178, valid PER 34.52%
EPOCH 10:
  batch 50 loss: 1.0778419768810272
  batch 100 loss: 1.0835529386997222
  batch 150 loss: 1.1286407005786896
  batch 200 loss: 1.1087911248207092
  batch 250 loss: 1.0998782372474671
  batch 300 loss: 1.0667342591285705
  batch 350 loss: 1.0965388321876526
  batch 400 loss: 1.066867731809616
  batch 450 loss: 1.0545365810394287
  batch 500 loss: 1.1077045261859895
  batch 550 loss: 1.1083072650432586
  batch 600 loss: 1.0833529090881349
  batch 650 loss: 1.0690437400341033
  batch 700 loss: 1.0863244867324828
  batch 750 loss: 1.0774743580818176
  batch 800 loss: 1.0868298196792603
  batch 850 loss: 1.0825136590003968
  batch 900 loss: 1.112861989736557
LOSS train 1.11286 valid 1.07343, valid PER 36.05%
EPOCH 11:
  batch 50 loss: 1.058161928653717
  batch 100 loss: 1.0465654277801513
  batch 150 loss: 1.0397396850585938
  batch 200 loss: 1.0936262202262879
  batch 250 loss: 1.0791945564746857
  batch 300 loss: 1.038476926088333
  batch 350 loss: 1.0645477604866027
  batch 400 loss: 1.0661208617687226
  batch 450 loss: 1.0550116169452668
  batch 500 loss: 1.0410365045070649
  batch 550 loss: 1.0556017577648162
  batch 600 loss: 1.0418132793903352
  batch 650 loss: 1.1050557017326355
  batch 700 loss: 1.0390957510471344
  batch 750 loss: 1.0285209560394286
  batch 800 loss: 1.0855023491382598
  batch 850 loss: 1.0849702525138856
  batch 900 loss: 1.0662816548347473
LOSS train 1.06628 valid 1.04161, valid PER 34.01%
EPOCH 12:
  batch 50 loss: 1.035637251138687
  batch 100 loss: 1.0263012003898622
  batch 150 loss: 1.0081174683570862
  batch 200 loss: 1.029759224653244
  batch 250 loss: 1.060219099521637
  batch 300 loss: 1.0350462436676025
  batch 350 loss: 1.0282448399066926
  batch 400 loss: 1.052573583126068
  batch 450 loss: 1.0420155811309815
  batch 500 loss: 1.0644426596164704
  batch 550 loss: 0.9938506627082825
  batch 600 loss: 1.00618159532547
  batch 650 loss: 1.0575977396965026
  batch 700 loss: 1.0484107065200805
  batch 750 loss: 1.0298281478881837
  batch 800 loss: 1.0073222374916078
  batch 850 loss: 1.0671309196949006
  batch 900 loss: 1.0524957978725433
LOSS train 1.05250 valid 1.02742, valid PER 33.53%
EPOCH 13:
  batch 50 loss: 0.9929959118366242
  batch 100 loss: 1.0298088777065277
  batch 150 loss: 0.9888388967514038
  batch 200 loss: 1.00195463180542
  batch 250 loss: 1.0141258764266967
  batch 300 loss: 0.9984682643413544
  batch 350 loss: 1.0057197034358978
  batch 400 loss: 1.0323022663593293
  batch 450 loss: 1.036990110874176
  batch 500 loss: 0.9963710105419159
  batch 550 loss: 1.0104547238349915
  batch 600 loss: 1.017254204750061
  batch 650 loss: 1.0230675733089447
  batch 700 loss: 1.0197672510147096
  batch 750 loss: 0.9757984340190887
  batch 800 loss: 0.9920741391181945
  batch 850 loss: 1.0457101202011108
  batch 900 loss: 1.001748766899109
LOSS train 1.00175 valid 1.01763, valid PER 32.44%
EPOCH 14:
  batch 50 loss: 0.9833454036712647
  batch 100 loss: 0.9940747249126435
  batch 150 loss: 0.9795755100250244
  batch 200 loss: 0.9987087607383728
  batch 250 loss: 0.9846182298660279
  batch 300 loss: 1.023028436899185
  batch 350 loss: 0.9739305329322815
  batch 400 loss: 0.9890635943412781
  batch 450 loss: 0.992293108701706
  batch 500 loss: 1.008941378593445
  batch 550 loss: 1.0123689663410187
  batch 600 loss: 0.9703523123264313
  batch 650 loss: 0.9962266969680786
  batch 700 loss: 1.018953231573105
  batch 750 loss: 0.984266517162323
  batch 800 loss: 0.9554736006259918
  batch 850 loss: 1.0142239439487457
  batch 900 loss: 0.9851085472106934
LOSS train 0.98511 valid 1.02701, valid PER 33.58%
EPOCH 15:
  batch 50 loss: 0.9881236863136291
  batch 100 loss: 0.9699496698379516
  batch 150 loss: 0.9654767858982086
  batch 200 loss: 1.002985692024231
  batch 250 loss: 0.9894597411155701
  batch 300 loss: 0.9687801837921143
  batch 350 loss: 0.9547275185585022
  batch 400 loss: 0.9630031311511993
  batch 450 loss: 0.9626242780685424
  batch 500 loss: 0.9246495795249939
  batch 550 loss: 0.9772182285785675
  batch 600 loss: 0.99768364071846
  batch 650 loss: 1.003188923597336
  batch 700 loss: 0.9938643765449524
  batch 750 loss: 0.9840462625026702
  batch 800 loss: 0.9587796151638031
  batch 850 loss: 0.9477518153190613
  batch 900 loss: 0.9735281026363373
LOSS train 0.97353 valid 0.99957, valid PER 32.67%
EPOCH 16:
  batch 50 loss: 0.9833104395866394
  batch 100 loss: 0.9242160439491272
  batch 150 loss: 0.9308110368251801
  batch 200 loss: 0.963195503950119
  batch 250 loss: 0.9673070657253265
  batch 300 loss: 0.9597670578956604
  batch 350 loss: 0.9650897181034088
  batch 400 loss: 0.9807099151611328
  batch 450 loss: 0.9780733573436737
  batch 500 loss: 0.9217196822166442
  batch 550 loss: 0.9737774312496186
  batch 600 loss: 0.938162407875061
  batch 650 loss: 0.9531762862205505
  batch 700 loss: 0.9342285668849946
  batch 750 loss: 0.9483620059490204
  batch 800 loss: 0.9576907551288605
  batch 850 loss: 0.9518297111988068
  batch 900 loss: 0.9418946707248688
LOSS train 0.94189 valid 0.98085, valid PER 30.75%
EPOCH 17:
  batch 50 loss: 0.9475972127914428
  batch 100 loss: 0.9444842898845672
  batch 150 loss: 0.9248533141613007
  batch 200 loss: 0.9085973143577576
  batch 250 loss: 0.9284381687641143
  batch 300 loss: 0.9478478288650513
  batch 350 loss: 0.9057667851448059
  batch 400 loss: 0.9691528379917145
  batch 450 loss: 0.9579539132118225
  batch 500 loss: 0.9313859426975251
  batch 550 loss: 0.9498650598526001
  batch 600 loss: 0.9950707340240479
  batch 650 loss: 0.9305370819568634
  batch 700 loss: 0.9328227972984314
  batch 750 loss: 0.9107905435562134
  batch 800 loss: 0.9234633183479309
  batch 850 loss: 0.9303002882003785
  batch 900 loss: 0.9182131540775299
LOSS train 0.91821 valid 0.96154, valid PER 30.48%
EPOCH 18:
  batch 50 loss: 0.9252713823318481
  batch 100 loss: 0.9326852583885192
  batch 150 loss: 0.929452508687973
  batch 200 loss: 0.9186555790901184
  batch 250 loss: 0.9139255011081695
  batch 300 loss: 0.9120760023593902
  batch 350 loss: 0.9315502905845642
  batch 400 loss: 0.8894169092178345
  batch 450 loss: 0.94282062292099
  batch 500 loss: 0.9360632181167603
  batch 550 loss: 0.9279288959503174
  batch 600 loss: 0.9084725511074067
  batch 650 loss: 0.9226772379875183
  batch 700 loss: 0.9425825536251068
  batch 750 loss: 0.9116507399082184
  batch 800 loss: 0.9124905288219451
  batch 850 loss: 0.8942903971672058
  batch 900 loss: 0.941726313829422
LOSS train 0.94173 valid 0.98405, valid PER 31.56%
EPOCH 19:
  batch 50 loss: 0.876310864686966
  batch 100 loss: 0.8781145930290222
  batch 150 loss: 0.9007917308807373
  batch 200 loss: 0.902438907623291
  batch 250 loss: 0.9156494879722595
  batch 300 loss: 0.9138664674758911
  batch 350 loss: 0.9149515151977539
  batch 400 loss: 0.9215432345867157
  batch 450 loss: 0.9180071151256561
  batch 500 loss: 0.9103323793411255
  batch 550 loss: 0.8852222192287446
  batch 600 loss: 0.9063563513755798
  batch 650 loss: 0.9575540506839753
  batch 700 loss: 0.8901188516616821
  batch 750 loss: 0.881496057510376
  batch 800 loss: 0.9091598761081695
  batch 850 loss: 0.9173475503921509
  batch 900 loss: 0.9145026874542236
LOSS train 0.91450 valid 0.96228, valid PER 30.34%
EPOCH 20:
  batch 50 loss: 0.8841664421558381
  batch 100 loss: 0.8771745014190674
  batch 150 loss: 0.8654377353191376
  batch 200 loss: 0.9096002566814423
  batch 250 loss: 0.8922999846935272
  batch 300 loss: 0.90701211810112
  batch 350 loss: 0.8599750697612762
  batch 400 loss: 0.8884003365039825
  batch 450 loss: 0.8934299635887146
  batch 500 loss: 0.8662796247005463
  batch 550 loss: 0.9376095187664032
  batch 600 loss: 0.8723369491100311
  batch 650 loss: 0.8938667297363281
  batch 700 loss: 0.9012714374065399
  batch 750 loss: 0.872986558675766
  batch 800 loss: 0.9356032657623291
  batch 850 loss: 0.9003863501548767
  batch 900 loss: 0.9268121480941772
LOSS train 0.92681 valid 0.94058, valid PER 29.88%
Training finished in 3.0 minutes.
Model saved to checkpoints/20231207_064616/model_20
Loading model from checkpoints/20231207_064616/model_20
SUB: 15.08%, DEL: 14.65%, INS: 1.58%, COR: 70.26%, PER: 31.32%
