Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 166952
EPOCH 1, Learning Rate: 0.5
  batch 50 loss: 4.773231959342956
  batch 100 loss: 3.2097291564941406
  batch 150 loss: 3.0344232749938964
  batch 200 loss: 2.7748689031600953
  batch 250 loss: 2.612427258491516
  batch 300 loss: 2.454261145591736
  batch 350 loss: 2.37388162612915
  batch 400 loss: 2.3153166151046753
  batch 450 loss: 2.251237740516663
  batch 500 loss: 2.1509796881675722
  batch 550 loss: 2.1261849927902223
  batch 600 loss: 2.065768346786499
  batch 650 loss: 1.9737800359725952
  batch 700 loss: 1.9778371500968932
  batch 750 loss: 1.9151213264465332
  batch 800 loss: 1.9228748607635497
  batch 850 loss: 1.8711911916732789
  batch 900 loss: 1.8694777607917785
LOSS train 1.86948 valid 1.83637, valid PER 71.17%
EPOCH 2, Learning Rate: 0.5
  batch 50 loss: 1.8131246256828308
  batch 100 loss: 1.7409282612800598
  batch 150 loss: 1.740335350036621
  batch 200 loss: 1.767395269870758
  batch 250 loss: 1.740202238559723
  batch 300 loss: 1.7133205485343934
  batch 350 loss: 1.625032114982605
  batch 400 loss: 1.6586236381530761
  batch 450 loss: 1.6076861310005188
  batch 500 loss: 1.6349356865882874
  batch 550 loss: 1.6485258030891419
  batch 600 loss: 1.583320801258087
  batch 650 loss: 1.6333968663215637
  batch 700 loss: 1.5925557351112365
  batch 750 loss: 1.574564962387085
  batch 800 loss: 1.5152172374725341
  batch 850 loss: 1.5385188841819764
  batch 900 loss: 1.549763412475586
LOSS train 1.54976 valid 1.45043, valid PER 57.00%
EPOCH 3, Learning Rate: 0.5
  batch 50 loss: 1.5176277828216553
  batch 100 loss: 1.475714294910431
  batch 150 loss: 1.4758313846588136
  batch 200 loss: 1.4587156200408935
  batch 250 loss: 1.4605267024040223
  batch 300 loss: 1.4593799376487733
  batch 350 loss: 1.4937857675552368
  batch 400 loss: 1.4788510370254517
  batch 450 loss: 1.4358548641204834
  batch 500 loss: 1.4472331166267396
  batch 550 loss: 1.4358436059951782
  batch 600 loss: 1.3936623764038085
  batch 650 loss: 1.408770534992218
  batch 700 loss: 1.4021515917778016
  batch 750 loss: 1.4873652291297912
  batch 800 loss: 1.3695561003684997
  batch 850 loss: 1.428600549697876
  batch 900 loss: 1.3627357006072998
LOSS train 1.36274 valid 1.33034, valid PER 49.23%
EPOCH 4, Learning Rate: 0.5
  batch 50 loss: 1.3629683613777162
  batch 100 loss: 1.3807878851890565
  batch 150 loss: 1.3340297937393188
  batch 200 loss: 1.3605268263816834
  batch 250 loss: 1.3630779147148133
  batch 300 loss: 1.3636563396453858
  batch 350 loss: 1.296284668445587
  batch 400 loss: 1.3636731910705566
  batch 450 loss: 1.3249080049991608
  batch 500 loss: 1.3001503324508668
  batch 550 loss: 1.33089421749115
  batch 600 loss: 1.3644542360305787
  batch 650 loss: 1.3213925337791443
  batch 700 loss: 1.2972110509872437
  batch 750 loss: 1.286804517507553
  batch 800 loss: 1.262242878675461
  batch 850 loss: 1.297384008169174
  batch 900 loss: 1.326756272315979
LOSS train 1.32676 valid 1.22278, valid PER 42.52%
EPOCH 5, Learning Rate: 0.5
  batch 50 loss: 1.2643421602249145
  batch 100 loss: 1.254579335451126
  batch 150 loss: 1.293089497089386
  batch 200 loss: 1.2318525004386902
  batch 250 loss: 1.242492538690567
  batch 300 loss: 1.2600686872005462
  batch 350 loss: 1.2650681114196778
  batch 400 loss: 1.2466708397865296
  batch 450 loss: 1.2275851964950562
  batch 500 loss: 1.2356363868713378
  batch 550 loss: 1.1911549425125123
  batch 600 loss: 1.275614743232727
  batch 650 loss: 1.21612322807312
  batch 700 loss: 1.2630276358127595
  batch 750 loss: 1.1927042734622955
  batch 800 loss: 1.244137599468231
  batch 850 loss: 1.2292585170269013
  batch 900 loss: 1.2367376053333283
LOSS train 1.23674 valid 1.14709, valid PER 38.12%
EPOCH 6, Learning Rate: 0.5
  batch 50 loss: 1.2275891125202179
  batch 100 loss: 1.170219680070877
  batch 150 loss: 1.1663184726238252
  batch 200 loss: 1.1872695755958558
  batch 250 loss: 1.2141384184360504
  batch 300 loss: 1.188533706665039
  batch 350 loss: 1.1779357028007507
  batch 400 loss: 1.1721665954589844
  batch 450 loss: 1.1980920398235322
  batch 500 loss: 1.1801337015628814
  batch 550 loss: 1.1876447212696075
  batch 600 loss: 1.1638010609149934
  batch 650 loss: 1.1834853780269623
  batch 700 loss: 1.168173429965973
  batch 750 loss: 1.1509701478481293
  batch 800 loss: 1.137845174074173
  batch 850 loss: 1.1197028863430023
  batch 900 loss: 1.1631544828414917
LOSS train 1.16315 valid 1.13420, valid PER 37.22%
EPOCH 7, Learning Rate: 0.5
  batch 50 loss: 1.1416524386405944
  batch 100 loss: 1.1483666539192199
  batch 150 loss: 1.1252752721309662
  batch 200 loss: 1.1062871599197388
  batch 250 loss: 1.1386698639392854
  batch 300 loss: 1.1034539568424224
  batch 350 loss: 1.1297458708286285
  batch 400 loss: 1.1208623230457306
  batch 450 loss: 1.1095209074020387
  batch 500 loss: 1.1061202216148376
  batch 550 loss: 1.1166222870349884
  batch 600 loss: 1.1255220139026643
  batch 650 loss: 1.1029613029956817
  batch 700 loss: 1.1232313513755798
  batch 750 loss: 1.0967041730880738
  batch 800 loss: 1.0950671076774596
  batch 850 loss: 1.1354169607162476
  batch 900 loss: 1.1538301038742065
LOSS train 1.15383 valid 1.08098, valid PER 35.88%
EPOCH 8, Learning Rate: 0.5
  batch 50 loss: 1.0772197258472442
  batch 100 loss: 1.0756812036037444
  batch 150 loss: 1.0939164280891418
  batch 200 loss: 1.052068042755127
  batch 250 loss: 1.0986842906475067
  batch 300 loss: 1.0222521066665649
  batch 350 loss: 1.116255785226822
  batch 400 loss: 1.0702124321460724
  batch 450 loss: 1.084812204837799
  batch 500 loss: 1.1171085464954376
  batch 550 loss: 1.0458285021781921
  batch 600 loss: 1.0893560576438903
  batch 650 loss: 1.1203337466716767
  batch 700 loss: 1.055289659500122
  batch 750 loss: 1.0632518887519837
  batch 800 loss: 1.089612854719162
  batch 850 loss: 1.0881613159179688
  batch 900 loss: 1.0877477538585663
LOSS train 1.08775 valid 1.05272, valid PER 33.69%
EPOCH 9, Learning Rate: 0.5
  batch 50 loss: 1.0134689831733703
  batch 100 loss: 1.0466905665397643
  batch 150 loss: 1.0322732877731324
  batch 200 loss: 1.017885640859604
  batch 250 loss: 1.0587470102310181
  batch 300 loss: 1.069556300640106
  batch 350 loss: 1.0696545326709748
  batch 400 loss: 1.0555207192897798
  batch 450 loss: 1.0514096415042877
  batch 500 loss: 1.0264498281478882
  batch 550 loss: 1.0387105691432952
  batch 600 loss: 1.051881458759308
  batch 650 loss: 1.037888489961624
  batch 700 loss: 1.0231750869750977
  batch 750 loss: 1.0517418086528778
  batch 800 loss: 1.0648469007015229
  batch 850 loss: 1.0684793615341186
  batch 900 loss: 1.013508802652359
LOSS train 1.01351 valid 1.03482, valid PER 32.36%
EPOCH 10, Learning Rate: 0.5
  batch 50 loss: 0.972023229598999
  batch 100 loss: 1.0041759979724885
  batch 150 loss: 1.0300076949596404
  batch 200 loss: 1.0101485753059387
  batch 250 loss: 1.020982095003128
  batch 300 loss: 0.990527286529541
  batch 350 loss: 1.0086731719970703
  batch 400 loss: 0.9762841987609864
  batch 450 loss: 0.9833746969699859
  batch 500 loss: 1.0212703764438629
  batch 550 loss: 1.0419534707069398
  batch 600 loss: 1.0240427446365357
  batch 650 loss: 0.9979455399513245
  batch 700 loss: 0.9988697981834411
  batch 750 loss: 0.9942838764190673
  batch 800 loss: 1.0153309237957
  batch 850 loss: 1.016682653427124
  batch 900 loss: 1.0223051524162292
LOSS train 1.02231 valid 1.02430, valid PER 34.12%
EPOCH 11, Learning Rate: 0.5
  batch 50 loss: 0.9567911005020142
  batch 100 loss: 0.950670737028122
  batch 150 loss: 0.9607332479953766
  batch 200 loss: 1.0151135587692262
  batch 250 loss: 0.9839617764949798
  batch 300 loss: 0.956428861618042
  batch 350 loss: 0.9948968648910522
  batch 400 loss: 0.9840465402603149
  batch 450 loss: 0.9981995928287506
  batch 500 loss: 0.9876237487792969
  batch 550 loss: 0.9924123954772949
  batch 600 loss: 0.9708875918388367
  batch 650 loss: 1.0359595239162445
  batch 700 loss: 0.9609961974620819
  batch 750 loss: 0.9642765367031098
  batch 800 loss: 1.0036343264579772
  batch 850 loss: 1.0115466654300689
  batch 900 loss: 0.9984037387371063
LOSS train 0.99840 valid 0.98417, valid PER 31.62%
EPOCH 12, Learning Rate: 0.5
  batch 50 loss: 0.96649982213974
  batch 100 loss: 0.9399927008152008
  batch 150 loss: 0.9374978697299957
  batch 200 loss: 0.9486388492584229
  batch 250 loss: 0.9674099826812744
  batch 300 loss: 0.9550742816925049
  batch 350 loss: 0.9787599110603332
  batch 400 loss: 0.986314777135849
  batch 450 loss: 0.97201131939888
  batch 500 loss: 0.985265258550644
  batch 550 loss: 0.92742928981781
  batch 600 loss: 0.9349295151233673
  batch 650 loss: 0.9963561272621155
  batch 700 loss: 0.9799731492996215
  batch 750 loss: 0.9720330142974853
  batch 800 loss: 0.9472882676124573
  batch 850 loss: 1.0061717629432678
  batch 900 loss: 0.9927276182174682
LOSS train 0.99273 valid 0.98984, valid PER 32.04%
EPOCH 13, Learning Rate: 0.5
  batch 50 loss: 0.9288308084011078
  batch 100 loss: 0.9509776008129119
  batch 150 loss: 0.9360207283496856
  batch 200 loss: 0.9590415632724762
  batch 250 loss: 0.9498961985111236
  batch 300 loss: 0.9233718955516815
  batch 350 loss: 0.936958622932434
  batch 400 loss: 0.9601429951190948
  batch 450 loss: 0.953677943944931
  batch 500 loss: 0.9325289261341095
  batch 550 loss: 0.9465616047382355
  batch 600 loss: 0.9271951603889466
  batch 650 loss: 0.947005227804184
  batch 700 loss: 0.9543065977096558
  batch 750 loss: 0.9211225295066834
  batch 800 loss: 0.9214848732948303
  batch 850 loss: 0.9605802047252655
  batch 900 loss: 0.9495481646060944
LOSS train 0.94955 valid 0.97174, valid PER 30.84%
EPOCH 14, Learning Rate: 0.5
  batch 50 loss: 0.8950627684593201
  batch 100 loss: 0.9262738907337189
  batch 150 loss: 0.9054229581356048
  batch 200 loss: 0.9097029733657837
  batch 250 loss: 0.9206362342834473
  batch 300 loss: 0.9525082218647003
  batch 350 loss: 0.9011950731277466
  batch 400 loss: 0.9091992807388306
  batch 450 loss: 0.8987980020046235
  batch 500 loss: 0.9198013484477997
  batch 550 loss: 0.9454310691356659
  batch 600 loss: 0.8981786870956421
  batch 650 loss: 0.9553900575637817
  batch 700 loss: 0.9583400571346283
  batch 750 loss: 0.9215942692756652
  batch 800 loss: 0.8770349383354187
  batch 850 loss: 0.9479108011722565
  batch 900 loss: 0.9167981505393982
LOSS train 0.91680 valid 0.97681, valid PER 31.61%
EPOCH 15, Learning Rate: 0.5
  batch 50 loss: 0.9160628652572632
  batch 100 loss: 0.8918155312538147
  batch 150 loss: 0.8880421936511993
  batch 200 loss: 0.9119538533687591
  batch 250 loss: 0.9129870402812957
  batch 300 loss: 0.8844428980350494
  batch 350 loss: 0.8857949542999267
  batch 400 loss: 0.9054771745204926
  batch 450 loss: 0.9072586739063263
  batch 500 loss: 0.8771127319335937
  batch 550 loss: 0.923736160993576
  batch 600 loss: 0.9282906293869019
  batch 650 loss: 0.9256328463554382
  batch 700 loss: 0.9349258244037628
  batch 750 loss: 0.910925155878067
  batch 800 loss: 0.8897720098495483
  batch 850 loss: 0.9095487105846405
  batch 900 loss: 0.925391948223114
LOSS train 0.92539 valid 0.98848, valid PER 31.05%
EPOCH 16, Learning Rate: 0.5
  batch 50 loss: 0.9234226226806641
  batch 100 loss: 0.8696586132049561
  batch 150 loss: 0.871178343296051
  batch 200 loss: 0.8708511710166931
  batch 250 loss: 0.9047776675224304
  batch 300 loss: 0.8834928691387176
  batch 350 loss: 0.9196060073375701
  batch 400 loss: 0.9095621109008789
  batch 450 loss: 0.9178406977653504
  batch 500 loss: 0.8644030499458313
  batch 550 loss: 0.8920055115222931
  batch 600 loss: 0.8967293238639832
  batch 650 loss: 0.8798518931865692
  batch 700 loss: 0.872187135219574
  batch 750 loss: 0.8773478472232819
  batch 800 loss: 0.9075880873203278
  batch 850 loss: 0.8862787246704101
  batch 900 loss: 0.8859896063804626
LOSS train 0.88599 valid 0.94279, valid PER 29.73%
EPOCH 17, Learning Rate: 0.5
  batch 50 loss: 0.8575284707546235
  batch 100 loss: 0.8736395597457886
  batch 150 loss: 0.87001762509346
  batch 200 loss: 0.8537772238254547
  batch 250 loss: 0.8792908501625061
  batch 300 loss: 0.8690037941932678
  batch 350 loss: 0.8576558351516723
  batch 400 loss: 0.9039729809761048
  batch 450 loss: 0.8979264867305755
  batch 500 loss: 0.8643682730197907
  batch 550 loss: 0.8760147178173066
  batch 600 loss: 0.9199191415309906
  batch 650 loss: 0.8665738940238953
  batch 700 loss: 0.8730989480018616
  batch 750 loss: 0.8697949826717377
  batch 800 loss: 0.8877758002281189
  batch 850 loss: 0.8862661004066468
  batch 900 loss: 0.8603789454698563
LOSS train 0.86038 valid 0.93806, valid PER 29.52%
EPOCH 18, Learning Rate: 0.5
  batch 50 loss: 0.8598861086368561
  batch 100 loss: 0.8697159254550934
  batch 150 loss: 0.8735573101043701
  batch 200 loss: 0.874443883895874
  batch 250 loss: 0.8682543158531189
  batch 300 loss: 0.8459009826183319
  batch 350 loss: 0.8746148312091827
  batch 400 loss: 0.8379062449932099
  batch 450 loss: 0.8749532127380371
  batch 500 loss: 0.8621776854991913
  batch 550 loss: 0.8826611876487732
  batch 600 loss: 0.8513388907909394
  batch 650 loss: 0.8465353477001191
  batch 700 loss: 0.9142680299282074
  batch 750 loss: 0.8630933207273483
  batch 800 loss: 0.8619861674308776
  batch 850 loss: 0.8721378886699677
  batch 900 loss: 0.9174503934383392
LOSS train 0.91745 valid 0.94658, valid PER 29.48%
EPOCH 19, Learning Rate: 0.5
  batch 50 loss: 0.8271042811870575
  batch 100 loss: 0.8147393977642059
  batch 150 loss: 0.8358394050598145
  batch 200 loss: 0.858516161441803
  batch 250 loss: 0.877848709821701
  batch 300 loss: 0.8401224815845489
  batch 350 loss: 0.8418948328495026
  batch 400 loss: 0.8790713489055634
  batch 450 loss: 0.8810205829143524
  batch 500 loss: 0.8578989517688751
  batch 550 loss: 0.8456269490718842
  batch 600 loss: 0.8481889700889588
  batch 650 loss: 0.8987979531288147
  batch 700 loss: 0.8384921312332153
  batch 750 loss: 0.8496778357028961
  batch 800 loss: 0.8727573573589325
  batch 850 loss: 0.8636091279983521
  batch 900 loss: 0.8690687489509582
LOSS train 0.86907 valid 0.97012, valid PER 30.32%
EPOCH 20, Learning Rate: 0.5
  batch 50 loss: 0.8183717048168182
  batch 100 loss: 0.8068703186511993
  batch 150 loss: 0.8163157045841217
  batch 200 loss: 0.8475430035591125
  batch 250 loss: 0.8294387423992157
  batch 300 loss: 0.8537170255184173
  batch 350 loss: 0.807687024474144
  batch 400 loss: 0.8364239430427551
  batch 450 loss: 0.835506043434143
  batch 500 loss: 0.833170803785324
  batch 550 loss: 0.8858238184452056
  batch 600 loss: 0.8300705313682556
  batch 650 loss: 0.8544000625610352
  batch 700 loss: 0.8568841910362244
  batch 750 loss: 0.8366113764047622
  batch 800 loss: 0.8603596425056458
  batch 850 loss: 0.8600517928600311
  batch 900 loss: 0.856171315908432
LOSS train 0.85617 valid 0.93769, valid PER 29.00%
Training finished in 3.0 minutes.
Model saved to checkpoints/20231210_003713/model_20
Loading model from checkpoints/20231210_003713/model_20
SUB: 15.67%, DEL: 13.22%, INS: 1.81%, COR: 71.11%, PER: 30.70%
