Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=0.5)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 6.162900743484497
  batch 100 loss: 3.251708164215088
  batch 150 loss: 3.1730909633636473
  batch 200 loss: 2.9912583112716673
  batch 250 loss: 2.7874022006988524
  batch 300 loss: 2.6165000581741333
  batch 350 loss: 2.4959248065948487
  batch 400 loss: 2.4428410863876344
  batch 450 loss: 2.367885122299194
  batch 500 loss: 2.268735263347626
  batch 550 loss: 2.221263897418976
  batch 600 loss: 2.1894724941253663
  batch 650 loss: 2.1094410371780397
  batch 700 loss: 2.113018457889557
  batch 750 loss: 2.057556617259979
  batch 800 loss: 2.028313400745392
  batch 850 loss: 2.0028148102760315
  batch 900 loss: 1.9889028334617616
LOSS train 1.98890 valid 1.91471, valid PER 74.22%
EPOCH 2:
  batch 50 loss: 1.9426715230941773
  batch 100 loss: 1.8934135055541992
  batch 150 loss: 1.8486797356605529
  batch 200 loss: 1.8511084628105163
  batch 250 loss: 1.8574972820281983
  batch 300 loss: 1.8335696697235107
  batch 350 loss: 1.744668834209442
  batch 400 loss: 1.7668946623802184
  batch 450 loss: 1.734775505065918
  batch 500 loss: 1.7463890361785888
  batch 550 loss: 1.7575867104530334
  batch 600 loss: 1.7028291630744934
  batch 650 loss: 1.730097632408142
  batch 700 loss: 1.694408974647522
  batch 750 loss: 1.6858573269844055
  batch 800 loss: 1.646686327457428
  batch 850 loss: 1.6378890371322632
  batch 900 loss: 1.6675757837295533
LOSS train 1.66758 valid 1.60505, valid PER 62.72%
EPOCH 3:
  batch 50 loss: 1.6350573801994324
  batch 100 loss: 1.5891752672195434
  batch 150 loss: 1.597238621711731
  batch 200 loss: 1.5789070630073547
  batch 250 loss: 1.5642100334167481
  batch 300 loss: 1.5694877529144287
  batch 350 loss: 1.5988123035430908
  batch 400 loss: 1.5808518075942992
  batch 450 loss: 1.5303942346572876
  batch 500 loss: 1.543972637653351
  batch 550 loss: 1.5334345817565918
  batch 600 loss: 1.5020244026184082
  batch 650 loss: 1.471555621623993
  batch 700 loss: 1.5027174687385558
  batch 750 loss: 1.555325973033905
  batch 800 loss: 1.4833903431892395
  batch 850 loss: 1.523517050743103
  batch 900 loss: 1.447300238609314
LOSS train 1.44730 valid 1.41994, valid PER 52.76%
EPOCH 4:
  batch 50 loss: 1.4512104082107544
  batch 100 loss: 1.4690771865844727
  batch 150 loss: 1.4278203701972962
  batch 200 loss: 1.4743981742858887
  batch 250 loss: 1.464905400276184
  batch 300 loss: 1.4630939507484435
  batch 350 loss: 1.3915576863288879
  batch 400 loss: 1.444397795200348
  batch 450 loss: 1.4266281867027282
  batch 500 loss: 1.3875328612327575
  batch 550 loss: 1.41536217212677
  batch 600 loss: 1.4404379844665527
  batch 650 loss: 1.4208418774604796
  batch 700 loss: 1.3791078424453735
  batch 750 loss: 1.3756832790374756
  batch 800 loss: 1.342550117969513
  batch 850 loss: 1.388860409259796
  batch 900 loss: 1.411276023387909
LOSS train 1.41128 valid 1.34848, valid PER 48.54%
EPOCH 5:
  batch 50 loss: 1.358189253807068
  batch 100 loss: 1.342775764465332
  batch 150 loss: 1.3905220413208008
  batch 200 loss: 1.326781519651413
  batch 250 loss: 1.3323702239990234
  batch 300 loss: 1.3546389579772948
  batch 350 loss: 1.3404922795295715
  batch 400 loss: 1.334845290184021
  batch 450 loss: 1.319863693714142
  batch 500 loss: 1.3387584471702576
  batch 550 loss: 1.2855416011810303
  batch 600 loss: 1.3547637343406678
  batch 650 loss: 1.3172491002082825
  batch 700 loss: 1.3504518628120423
  batch 750 loss: 1.2764306962490082
  batch 800 loss: 1.3158253049850464
  batch 850 loss: 1.3153512620925902
  batch 900 loss: 1.3236923980712891
LOSS train 1.32369 valid 1.24671, valid PER 42.57%
EPOCH 6:
  batch 50 loss: 1.3136636090278626
  batch 100 loss: 1.269151258468628
  batch 150 loss: 1.2558106553554536
  batch 200 loss: 1.2727548694610595
  batch 250 loss: 1.289856526851654
  batch 300 loss: 1.2750501799583436
  batch 350 loss: 1.2584771776199342
  batch 400 loss: 1.2664136266708375
  batch 450 loss: 1.260362867116928
  batch 500 loss: 1.2569563698768615
  batch 550 loss: 1.276822783946991
  batch 600 loss: 1.2546072900295258
  batch 650 loss: 1.2555922639369965
  batch 700 loss: 1.2405302286148072
  batch 750 loss: 1.2289367771148683
  batch 800 loss: 1.2260588645935058
  batch 850 loss: 1.2203082203865052
  batch 900 loss: 1.2395400416851043
LOSS train 1.23954 valid 1.19746, valid PER 40.50%
EPOCH 7:
  batch 50 loss: 1.2155800807476043
  batch 100 loss: 1.2362644171714783
  batch 150 loss: 1.225881963968277
  batch 200 loss: 1.1969951260089875
  batch 250 loss: 1.214762408733368
  batch 300 loss: 1.1912209379673004
  batch 350 loss: 1.1939089691638947
  batch 400 loss: 1.2145939993858337
  batch 450 loss: 1.19056281208992
  batch 500 loss: 1.1846876120567322
  batch 550 loss: 1.1976690840721131
  batch 600 loss: 1.207456966638565
  batch 650 loss: 1.1948507368564605
  batch 700 loss: 1.208299342393875
  batch 750 loss: 1.1829006123542785
  batch 800 loss: 1.1783695447444915
  batch 850 loss: 1.2096834325790404
  batch 900 loss: 1.244860006570816
LOSS train 1.24486 valid 1.16424, valid PER 38.85%
EPOCH 8:
  batch 50 loss: 1.183018798828125
  batch 100 loss: 1.1719714796543121
  batch 150 loss: 1.1620425271987915
  batch 200 loss: 1.143443945646286
  batch 250 loss: 1.1648530912399293
  batch 300 loss: 1.0943298494815827
  batch 350 loss: 1.1866783916950225
  batch 400 loss: 1.1583298873901366
  batch 450 loss: 1.1658499503135682
  batch 500 loss: 1.1986385416984557
  batch 550 loss: 1.1165577685832977
  batch 600 loss: 1.1704509341716767
  batch 650 loss: 1.1811580336093903
  batch 700 loss: 1.1400394177436828
  batch 750 loss: 1.1386700832843781
  batch 800 loss: 1.153943121433258
  batch 850 loss: 1.1801912224292754
  batch 900 loss: 1.1576523303985595
LOSS train 1.15765 valid 1.12800, valid PER 36.64%
EPOCH 9:
  batch 50 loss: 1.0932225716114043
  batch 100 loss: 1.13862060546875
  batch 150 loss: 1.1294377994537355
  batch 200 loss: 1.1039858281612396
  batch 250 loss: 1.135928430557251
  batch 300 loss: 1.1355876922607422
  batch 350 loss: 1.1591958725452423
  batch 400 loss: 1.1473657774925232
  batch 450 loss: 1.1320150983333588
  batch 500 loss: 1.1072899973392487
  batch 550 loss: 1.1327447640895842
  batch 600 loss: 1.1419989812374114
  batch 650 loss: 1.109372032880783
  batch 700 loss: 1.0906443178653717
  batch 750 loss: 1.1081996285915374
  batch 800 loss: 1.1209549486637116
  batch 850 loss: 1.1340435469150543
  batch 900 loss: 1.10034388422966
LOSS train 1.10034 valid 1.08400, valid PER 34.24%
EPOCH 10:
  batch 50 loss: 1.0715452587604524
  batch 100 loss: 1.0849294674396515
  batch 150 loss: 1.122173457145691
  batch 200 loss: 1.1155838394165039
  batch 250 loss: 1.102644740343094
  batch 300 loss: 1.074116678237915
  batch 350 loss: 1.089360430240631
  batch 400 loss: 1.0737234902381898
  batch 450 loss: 1.0577515351772309
  batch 500 loss: 1.1003603744506836
  batch 550 loss: 1.1065873181819916
  batch 600 loss: 1.0860297608375549
  batch 650 loss: 1.0727945291996002
  batch 700 loss: 1.0889096295833587
  batch 750 loss: 1.0850508773326875
  batch 800 loss: 1.0913155257701874
  batch 850 loss: 1.086413789987564
  batch 900 loss: 1.1105225360393525
LOSS train 1.11052 valid 1.08470, valid PER 36.02%
EPOCH 11:
  batch 50 loss: 1.0636490952968598
  batch 100 loss: 1.0453963959217072
  batch 150 loss: 1.041410939693451
  batch 200 loss: 1.0913005232810975
  batch 250 loss: 1.0875890398025512
  batch 300 loss: 1.036086767911911
  batch 350 loss: 1.0642277646064757
  batch 400 loss: 1.0769705665111542
  batch 450 loss: 1.0643754720687866
  batch 500 loss: 1.0383861422538758
  batch 550 loss: 1.0594442915916442
  batch 600 loss: 1.039529503583908
  batch 650 loss: 1.0990899801254272
  batch 700 loss: 1.0330834543704988
  batch 750 loss: 1.023422291278839
  batch 800 loss: 1.098800401687622
  batch 850 loss: 1.0829881286621095
  batch 900 loss: 1.069087256193161
LOSS train 1.06909 valid 1.04550, valid PER 33.84%
EPOCH 12:
  batch 50 loss: 1.0363600003719329
  batch 100 loss: 1.0308565855026246
  batch 150 loss: 1.012738298177719
  batch 200 loss: 1.028169471025467
  batch 250 loss: 1.0628881633281708
  batch 300 loss: 1.035508623123169
  batch 350 loss: 1.0319426834583283
  batch 400 loss: 1.0583990848064422
  batch 450 loss: 1.039909038543701
  batch 500 loss: 1.0723174917697906
  batch 550 loss: 0.9895164430141449
  batch 600 loss: 1.0056822943687438
  batch 650 loss: 1.0636620533466339
  batch 700 loss: 1.0370502805709838
  batch 750 loss: 1.0238227987289428
  batch 800 loss: 1.004901418685913
  batch 850 loss: 1.0671050775051116
  batch 900 loss: 1.0714660096168518
LOSS train 1.07147 valid 1.03801, valid PER 33.80%
EPOCH 13:
  batch 50 loss: 0.9952691233158112
  batch 100 loss: 1.0335892605781556
  batch 150 loss: 0.9823972249031067
  batch 200 loss: 1.0071953666210174
  batch 250 loss: 1.0154035258293153
  batch 300 loss: 0.9897620809078217
  batch 350 loss: 1.0108278226852416
  batch 400 loss: 1.0313169062137604
  batch 450 loss: 1.0375598537921906
  batch 500 loss: 1.0022231590747834
  batch 550 loss: 1.0156586265563965
  batch 600 loss: 1.0269694471359252
  batch 650 loss: 1.019327117204666
  batch 700 loss: 1.0215639460086823
  batch 750 loss: 0.9837578713893891
  batch 800 loss: 0.9880310297012329
  batch 850 loss: 1.0430978691577912
  batch 900 loss: 1.0084944427013398
LOSS train 1.00849 valid 1.01811, valid PER 32.70%
EPOCH 14:
  batch 50 loss: 0.9904891788959503
  batch 100 loss: 0.9843747198581696
  batch 150 loss: 0.987546855211258
  batch 200 loss: 1.0099271285533904
  batch 250 loss: 0.9876497280597687
  batch 300 loss: 1.0296173393726349
  batch 350 loss: 0.9709396994113922
  batch 400 loss: 0.9898254358768463
  batch 450 loss: 0.993436849117279
  batch 500 loss: 0.9997438383102417
  batch 550 loss: 1.010817701816559
  batch 600 loss: 0.9799228191375733
  batch 650 loss: 0.9916956996917725
  batch 700 loss: 1.0177498805522918
  batch 750 loss: 0.9897949671745301
  batch 800 loss: 0.9529058194160461
  batch 850 loss: 1.0119203579425813
  batch 900 loss: 0.9927077746391296
LOSS train 0.99271 valid 1.02145, valid PER 33.02%
EPOCH 15:
  batch 50 loss: 0.9966103744506836
  batch 100 loss: 0.9584920573234558
  batch 150 loss: 0.9633784997463226
  batch 200 loss: 1.0047768771648407
  batch 250 loss: 0.9924565052986145
  batch 300 loss: 0.9728348767757415
  batch 350 loss: 0.9493321216106415
  batch 400 loss: 0.9590053260326385
  batch 450 loss: 0.9597728061676025
  batch 500 loss: 0.929413925409317
  batch 550 loss: 0.9678533720970154
  batch 600 loss: 0.9959771025180817
  batch 650 loss: 1.010468670129776
  batch 700 loss: 0.9969558465480804
  batch 750 loss: 0.9836705827713013
  batch 800 loss: 0.9550691092014313
  batch 850 loss: 0.9528676247596741
  batch 900 loss: 0.974344197511673
LOSS train 0.97434 valid 0.99657, valid PER 32.19%
EPOCH 16:
  batch 50 loss: 0.9861334252357483
  batch 100 loss: 0.93068483710289
  batch 150 loss: 0.9264517402648926
  batch 200 loss: 0.9545849215984344
  batch 250 loss: 0.9668209254741669
  batch 300 loss: 0.9597157275676728
  batch 350 loss: 0.9739165198802948
  batch 400 loss: 0.9764187347888946
  batch 450 loss: 0.9913868486881257
  batch 500 loss: 0.9293894302845002
  batch 550 loss: 0.9624820280075074
  batch 600 loss: 0.9484342801570892
  batch 650 loss: 0.9709001398086547
  batch 700 loss: 0.9352467310428619
  batch 750 loss: 0.9538192868232727
  batch 800 loss: 0.9700643932819366
  batch 850 loss: 0.9568256950378418
  batch 900 loss: 0.946579532623291
LOSS train 0.94658 valid 0.97996, valid PER 30.92%
EPOCH 17:
  batch 50 loss: 0.9500168251991272
  batch 100 loss: 0.9445067656040191
  batch 150 loss: 0.9285804033279419
  batch 200 loss: 0.9093572425842286
  batch 250 loss: 0.9345803594589234
  batch 300 loss: 0.9555261516571045
  batch 350 loss: 0.9043575465679169
  batch 400 loss: 0.9602404654026031
  batch 450 loss: 0.950755375623703
  batch 500 loss: 0.9279162287712097
  batch 550 loss: 0.9455102205276489
  batch 600 loss: 0.9896392679214477
  batch 650 loss: 0.9328939282894134
  batch 700 loss: 0.9416017162799836
  batch 750 loss: 0.9102891087532043
  batch 800 loss: 0.9296515262126923
  batch 850 loss: 0.9362448227405548
  batch 900 loss: 0.9244155180454254
LOSS train 0.92442 valid 0.96021, valid PER 30.02%
EPOCH 18:
  batch 50 loss: 0.9330841934680939
  batch 100 loss: 0.9328727054595948
  batch 150 loss: 0.93107985496521
  batch 200 loss: 0.9254752433300019
  batch 250 loss: 0.9194441103935241
  batch 300 loss: 0.9111148369312286
  batch 350 loss: 0.9264672136306763
  batch 400 loss: 0.8937935447692871
  batch 450 loss: 0.948063051700592
  batch 500 loss: 0.9339128017425538
  batch 550 loss: 0.9324639987945557
  batch 600 loss: 0.9190082216262817
  batch 650 loss: 0.9193075096607208
  batch 700 loss: 0.9508046722412109
  batch 750 loss: 0.9223022294044495
  batch 800 loss: 0.9134086799621582
  batch 850 loss: 0.9084611749649047
  batch 900 loss: 0.9483808708190918
LOSS train 0.94838 valid 0.98052, valid PER 31.12%
EPOCH 19:
  batch 50 loss: 0.8874918115139008
  batch 100 loss: 0.8800244975090027
  batch 150 loss: 0.8944653511047364
  batch 200 loss: 0.8976145339012146
  batch 250 loss: 0.9125684928894043
  batch 300 loss: 0.9103713548183441
  batch 350 loss: 0.9115895652770996
  batch 400 loss: 0.921383466720581
  batch 450 loss: 0.926711095571518
  batch 500 loss: 0.9190664994716644
  batch 550 loss: 0.8914834558963776
  batch 600 loss: 0.9099918699264526
  batch 650 loss: 0.9599411487579346
  batch 700 loss: 0.8876876676082611
  batch 750 loss: 0.8798739540576935
  batch 800 loss: 0.9041716468334198
  batch 850 loss: 0.9258888745307923
  batch 900 loss: 0.9146068000793457
LOSS train 0.91461 valid 0.96400, valid PER 30.56%
EPOCH 20:
  batch 50 loss: 0.8900678956508636
  batch 100 loss: 0.875152221918106
  batch 150 loss: 0.8700099623203278
  batch 200 loss: 0.9106002950668335
  batch 250 loss: 0.88668017745018
  batch 300 loss: 0.899712152481079
  batch 350 loss: 0.8704638504981994
  batch 400 loss: 0.8917253363132477
  batch 450 loss: 0.8964667046070098
  batch 500 loss: 0.8657411456108093
  batch 550 loss: 0.9364419507980347
  batch 600 loss: 0.8678489363193512
  batch 650 loss: 0.918363927602768
  batch 700 loss: 0.9018785929679871
  batch 750 loss: 0.8677848470211029
  batch 800 loss: 0.9338724803924561
  batch 850 loss: 0.9083704149723053
  batch 900 loss: 0.9263215470314026
LOSS train 0.92632 valid 0.94321, valid PER 30.14%
Training finished in 4.0 minutes.
Model saved to checkpoints/20231206_213416/model_20
Loading model from checkpoints/20231206_213416/model_20
SUB: 15.63%, DEL: 13.94%, INS: 1.58%, COR: 70.43%, PER: 31.16%
