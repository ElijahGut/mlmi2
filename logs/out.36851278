Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=None)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.168338809013367
  batch 100 loss: 3.1716956233978273
  batch 150 loss: 3.0335053825378417
  batch 200 loss: 2.9206864643096924
  batch 250 loss: 2.8457534551620483
  batch 300 loss: 2.667477078437805
  batch 350 loss: 2.533380374908447
  batch 400 loss: 2.5282718086242677
  batch 450 loss: 2.4907884407043457
  batch 500 loss: 2.3709564924240114
  batch 550 loss: 2.300172245502472
  batch 600 loss: 2.210007390975952
  batch 650 loss: 2.0540856099128724
  batch 700 loss: 2.0505333495140077
  batch 750 loss: 1.9880987405776978
  batch 800 loss: 1.9590669775009155
  batch 850 loss: 1.9033546829223633
  batch 900 loss: 1.8810302996635437
LOSS train 1.88103 valid 1.81582, valid PER 70.32%
EPOCH 2:
  batch 50 loss: 1.832318992614746
  batch 100 loss: 1.780199980735779
  batch 150 loss: 1.754364035129547
  batch 200 loss: 1.7389976453781129
  batch 250 loss: 1.776500415802002
  batch 300 loss: 1.7166990566253661
  batch 350 loss: 1.6283902549743652
  batch 400 loss: 1.6505161643028259
  batch 450 loss: 1.6037013149261474
  batch 500 loss: 1.6205430102348328
  batch 550 loss: 1.6460236024856567
  batch 600 loss: 1.5757259106636048
  batch 650 loss: 1.605999641418457
  batch 700 loss: 1.5905540537834169
  batch 750 loss: 1.5675448393821716
  batch 800 loss: 1.5073317289352417
  batch 850 loss: 1.506245925426483
  batch 900 loss: 1.514621877670288
LOSS train 1.51462 valid 1.41889, valid PER 48.30%
EPOCH 3:
  batch 50 loss: 1.4767055463790895
  batch 100 loss: 1.4625757718086243
  batch 150 loss: 1.4497560119628907
  batch 200 loss: 1.416104896068573
  batch 250 loss: 1.420447371006012
  batch 300 loss: 1.3990080881118774
  batch 350 loss: 1.437150504589081
  batch 400 loss: 1.4218425130844117
  batch 450 loss: 1.3725774002075195
  batch 500 loss: 1.370826256275177
  batch 550 loss: 1.3912749290466309
  batch 600 loss: 1.3592120623588562
  batch 650 loss: 1.3304439544677735
  batch 700 loss: 1.3504111528396607
  batch 750 loss: 1.4211462450027466
  batch 800 loss: 1.3173856878280639
  batch 850 loss: 1.3619647932052612
  batch 900 loss: 1.3042428851127625
LOSS train 1.30424 valid 1.28571, valid PER 40.73%
EPOCH 4:
  batch 50 loss: 1.3389403581619264
  batch 100 loss: 1.3123554229736327
  batch 150 loss: 1.2629827213287355
  batch 200 loss: 1.293712682723999
  batch 250 loss: 1.2959024620056152
  batch 300 loss: 1.3142344617843629
  batch 350 loss: 1.2288543021678924
  batch 400 loss: 1.257075036764145
  batch 450 loss: 1.263323940038681
  batch 500 loss: 1.265921630859375
  batch 550 loss: 1.2677752125263213
  batch 600 loss: 1.2940661811828613
  batch 650 loss: 1.2638412821292877
  batch 700 loss: 1.2433683156967164
  batch 750 loss: 1.2161452555656433
  batch 800 loss: 1.1906468546390534
  batch 850 loss: 1.2252177929878234
  batch 900 loss: 1.2624604189395905
LOSS train 1.26246 valid 1.19263, valid PER 38.46%
EPOCH 5:
  batch 50 loss: 1.1904231095314026
  batch 100 loss: 1.1789206755161286
  batch 150 loss: 1.2461728024482728
  batch 200 loss: 1.165449619293213
  batch 250 loss: 1.180106564760208
  batch 300 loss: 1.2076252019405365
  batch 350 loss: 1.2123380029201507
  batch 400 loss: 1.1907758855819701
  batch 450 loss: 1.1841202974319458
  batch 500 loss: 1.1958339166641236
  batch 550 loss: 1.144033533334732
  batch 600 loss: 1.2465066504478455
  batch 650 loss: 1.1817596983909606
  batch 700 loss: 1.2083005619049072
  batch 750 loss: 1.1442768168449402
  batch 800 loss: 1.1706013786792755
  batch 850 loss: 1.1782168555259704
  batch 900 loss: 1.1835566401481628
LOSS train 1.18356 valid 1.11296, valid PER 35.86%
EPOCH 6:
  batch 50 loss: 1.1749026560783387
  batch 100 loss: 1.1272821760177612
  batch 150 loss: 1.1165488958358765
  batch 200 loss: 1.1620503485202789
  batch 250 loss: 1.140606392621994
  batch 300 loss: 1.1393994510173797
  batch 350 loss: 1.141096032857895
  batch 400 loss: 1.1109002590179444
  batch 450 loss: 1.1508507859706878
  batch 500 loss: 1.1535151553153993
  batch 550 loss: 1.1805881297588348
  batch 600 loss: 1.1338116943836212
  batch 650 loss: 1.1445859062671662
  batch 700 loss: 1.1542847907543183
  batch 750 loss: 1.1297886657714844
  batch 800 loss: 1.1145016968250274
  batch 850 loss: 1.1200367259979247
  batch 900 loss: 1.144366567134857
LOSS train 1.14437 valid 1.09550, valid PER 35.80%
EPOCH 7:
  batch 50 loss: 1.1133595180511475
  batch 100 loss: 1.131641616821289
  batch 150 loss: 1.107694855928421
  batch 200 loss: 1.0755258119106292
  batch 250 loss: 1.0906148028373719
  batch 300 loss: 1.0943714213371276
  batch 350 loss: 1.1035018467903137
  batch 400 loss: 1.1079066908359527
  batch 450 loss: 1.0993438935279847
  batch 500 loss: 1.0963791954517363
  batch 550 loss: 1.068581622838974
  batch 600 loss: 1.0859428322315217
  batch 650 loss: 1.0811443185806275
  batch 700 loss: 1.1120876288414
  batch 750 loss: 1.0907239627838134
  batch 800 loss: 1.1141901326179504
  batch 850 loss: 1.113098464012146
  batch 900 loss: 1.1478978991508484
LOSS train 1.14790 valid 1.08809, valid PER 35.70%
EPOCH 8:
  batch 50 loss: 1.1060932910442351
  batch 100 loss: 1.0716464185714722
  batch 150 loss: 1.076830528974533
  batch 200 loss: 1.053662323951721
  batch 250 loss: 1.0606844353675842
  batch 300 loss: 1.0003487908840178
  batch 350 loss: 1.0882371497154235
  batch 400 loss: 1.037535102367401
  batch 450 loss: 1.0732123267650604
  batch 500 loss: 1.1012579822540283
  batch 550 loss: 1.0321638309955596
  batch 600 loss: 1.0677993142604827
  batch 650 loss: 1.0967277026176452
  batch 700 loss: 1.0497122132778167
  batch 750 loss: 1.0605742347240448
  batch 800 loss: 1.076228060722351
  batch 850 loss: 1.0713485836982728
  batch 900 loss: 1.0553498303890227
LOSS train 1.05535 valid 1.04689, valid PER 34.20%
EPOCH 9:
  batch 50 loss: 1.0132213509082795
  batch 100 loss: 1.055680012702942
  batch 150 loss: 1.0576339054107666
  batch 200 loss: 1.0047244262695312
  batch 250 loss: 1.039048376083374
  batch 300 loss: 1.056005940437317
  batch 350 loss: 1.0672310566902161
  batch 400 loss: 1.0422077214717864
  batch 450 loss: 1.0484135925769806
  batch 500 loss: 0.9858104002475738
  batch 550 loss: 1.0609244751930236
  batch 600 loss: 1.0612516438961028
  batch 650 loss: 1.0281013000011443
  batch 700 loss: 1.0318484258651734
  batch 750 loss: 1.0102173626422881
  batch 800 loss: 1.0480706739425658
  batch 850 loss: 1.0631140542030335
  batch 900 loss: 1.014368097782135
LOSS train 1.01437 valid 1.02551, valid PER 33.34%
EPOCH 10:
  batch 50 loss: 0.9935102558135986
  batch 100 loss: 1.0258981549739838
  batch 150 loss: 1.019421670436859
  batch 200 loss: 1.0741499996185302
  batch 250 loss: 1.0684538662433625
  batch 300 loss: 0.9979409062862397
  batch 350 loss: 1.074842904806137
  batch 400 loss: 0.9946700799465179
  batch 450 loss: 1.007285463809967
  batch 500 loss: 1.026449772119522
  batch 550 loss: 1.0296210849285126
  batch 600 loss: 1.0182139897346496
  batch 650 loss: 0.9909162795543671
  batch 700 loss: 1.003495955467224
  batch 750 loss: 0.9878054642677307
  batch 800 loss: 1.0266522121429444
  batch 850 loss: 1.026737130880356
  batch 900 loss: 1.0316169083118438
LOSS train 1.03162 valid 1.05187, valid PER 34.63%
EPOCH 11:
  batch 50 loss: 0.9682537388801574
  batch 100 loss: 0.9526038014888764
  batch 150 loss: 0.9559572875499726
  batch 200 loss: 1.0165475177764893
  batch 250 loss: 0.995259141921997
  batch 300 loss: 0.9493991386890411
  batch 350 loss: 0.9835191655158997
  batch 400 loss: 0.9860628819465638
  batch 450 loss: 0.9911663675308228
  batch 500 loss: 0.9649993872642517
  batch 550 loss: 0.9804619967937469
  batch 600 loss: 0.9531168293952942
  batch 650 loss: 1.0475279843807221
  batch 700 loss: 0.933901059627533
  batch 750 loss: 0.9705515897274017
  batch 800 loss: 1.0001976430416106
  batch 850 loss: 1.0392173027992249
  batch 900 loss: 1.0223065936565399
LOSS train 1.02231 valid 1.03051, valid PER 32.92%
EPOCH 12:
  batch 50 loss: 0.9750871884822846
  batch 100 loss: 0.9684085786342621
  batch 150 loss: 0.9558193254470825
  batch 200 loss: 0.949715576171875
  batch 250 loss: 0.982278892993927
  batch 300 loss: 0.9610219156742096
  batch 350 loss: 1.0033914983272552
  batch 400 loss: 1.0116046833992005
  batch 450 loss: 1.0045797872543334
  batch 500 loss: 0.9976250672340393
  batch 550 loss: 0.9182841992378235
  batch 600 loss: 0.9414002060890198
  batch 650 loss: 0.9821143543720245
  batch 700 loss: 0.9908875179290771
  batch 750 loss: 0.9618074226379395
  batch 800 loss: 0.9741200387477875
  batch 850 loss: 0.9889685630798339
  batch 900 loss: 0.9851948070526123
LOSS train 0.98519 valid 0.99144, valid PER 32.98%
EPOCH 13:
  batch 50 loss: 0.9322642147541046
  batch 100 loss: 0.9463433372974396
  batch 150 loss: 0.9319642198085785
  batch 200 loss: 0.9593446397781372
  batch 250 loss: 0.9752198302745819
  batch 300 loss: 0.9312588477134705
  batch 350 loss: 0.9675169658660888
  batch 400 loss: 0.9893257880210876
  batch 450 loss: 0.9904017865657806
  batch 500 loss: 0.9395562732219696
  batch 550 loss: 0.9542185926437378
  batch 600 loss: 0.9602093696594238
  batch 650 loss: 0.994103273153305
  batch 700 loss: 0.9965434992313384
  batch 750 loss: 0.9641983246803284
  batch 800 loss: 0.9539156806468964
  batch 850 loss: 0.9979056131839752
  batch 900 loss: 0.9818538463115692
LOSS train 0.98185 valid 0.98599, valid PER 32.25%
EPOCH 14:
  batch 50 loss: 0.9436966419219971
  batch 100 loss: 0.9276088058948517
  batch 150 loss: 0.9344037699699402
  batch 200 loss: 0.9504257369041443
  batch 250 loss: 0.9548286235332489
  batch 300 loss: 0.9688900709152222
  batch 350 loss: 0.9344836199283599
  batch 400 loss: 0.9544920241832733
  batch 450 loss: 0.9362633633613586
  batch 500 loss: 0.942292354106903
  batch 550 loss: 0.9497617781162262
  batch 600 loss: 0.9202039122581482
  batch 650 loss: 0.9572002589702606
  batch 700 loss: 0.9391423559188843
  batch 750 loss: 0.9349338579177856
  batch 800 loss: 0.9146420097351075
  batch 850 loss: 0.9758904993534088
  batch 900 loss: 0.9487493848800659
LOSS train 0.94875 valid 0.97412, valid PER 31.71%
EPOCH 15:
  batch 50 loss: 0.9083421695232391
  batch 100 loss: 0.8924168992042542
  batch 150 loss: 0.9340590631961823
  batch 200 loss: 0.9749265432357788
  batch 250 loss: 0.9438961672782898
  batch 300 loss: 0.9059778738021851
  batch 350 loss: 0.9154308295249939
  batch 400 loss: 0.9134232175350189
  batch 450 loss: 0.9007190716266632
  batch 500 loss: 0.8765751671791077
  batch 550 loss: 0.9020136320590972
  batch 600 loss: 0.9461008203029633
  batch 650 loss: 0.9385356843471527
  batch 700 loss: 0.9419938552379609
  batch 750 loss: 0.9389988017082215
  batch 800 loss: 0.9094961500167846
  batch 850 loss: 0.9241364336013794
  batch 900 loss: 0.9284828388690949
LOSS train 0.92848 valid 1.00702, valid PER 32.69%
EPOCH 16:
  batch 50 loss: 0.9173738193511963
  batch 100 loss: 0.8683241844177246
  batch 150 loss: 0.8790452802181243
  batch 200 loss: 0.8786354422569275
  batch 250 loss: 0.9097131097316742
  batch 300 loss: 0.9034198701381684
  batch 350 loss: 0.9301999199390412
  batch 400 loss: 0.9130627357959747
  batch 450 loss: 0.9214212119579315
  batch 500 loss: 0.8960165059566498
  batch 550 loss: 0.9289078831672668
  batch 600 loss: 0.8942929363250732
  batch 650 loss: 0.9420987260341644
  batch 700 loss: 0.9115820944309234
  batch 750 loss: 0.9405405521392822
  batch 800 loss: 0.9097831463813781
  batch 850 loss: 0.9123767673969269
  batch 900 loss: 0.9090375471115112
LOSS train 0.90904 valid 0.95524, valid PER 30.61%
EPOCH 17:
  batch 50 loss: 0.8964253604412079
  batch 100 loss: 0.8999981915950775
  batch 150 loss: 0.8794108724594116
  batch 200 loss: 0.8767790627479554
  batch 250 loss: 0.9015427231788635
  batch 300 loss: 0.9076194095611573
  batch 350 loss: 0.8913053441047668
  batch 400 loss: 0.986175616979599
  batch 450 loss: 0.95114865899086
  batch 500 loss: 0.8863813412189484
  batch 550 loss: 0.904012371301651
  batch 600 loss: 0.9682248091697693
  batch 650 loss: 0.9269860446453094
  batch 700 loss: 0.9087424743175506
  batch 750 loss: 0.9067650353908538
  batch 800 loss: 0.9268743658065796
  batch 850 loss: 0.9215178596973419
  batch 900 loss: 0.8966722714900971
LOSS train 0.89667 valid 0.98423, valid PER 31.43%
EPOCH 18:
  batch 50 loss: 0.9172904932498932
  batch 100 loss: 0.913033834695816
  batch 150 loss: 0.907007520198822
  batch 200 loss: 0.8829435253143311
  batch 250 loss: 0.9023212575912476
  batch 300 loss: 0.8723108351230622
  batch 350 loss: 0.8870432949066163
  batch 400 loss: 0.8640514886379242
  batch 450 loss: 0.9360390925407409
  batch 500 loss: 0.9617469120025635
  batch 550 loss: 0.9138004112243653
  batch 600 loss: 0.8696343529224396
  batch 650 loss: 0.8623552703857422
  batch 700 loss: 0.9198008322715759
  batch 750 loss: 0.8923751926422119
  batch 800 loss: 0.8728571498394012
  batch 850 loss: 0.8797416579723358
  batch 900 loss: 0.9018284511566163
LOSS train 0.90183 valid 0.93708, valid PER 30.19%
EPOCH 19:
  batch 50 loss: 0.8248959350585937
  batch 100 loss: 0.8594176954030991
  batch 150 loss: 0.8447319686412811
  batch 200 loss: 0.8487719857692718
  batch 250 loss: 0.8790826392173767
  batch 300 loss: 0.879776371717453
  batch 350 loss: 0.8729013776779175
  batch 400 loss: 0.8864990401268006
  batch 450 loss: 0.8893292045593262
  batch 500 loss: 0.8843028259277343
  batch 550 loss: 0.8704092776775361
  batch 600 loss: 0.8763027715682984
  batch 650 loss: 0.9319616186618805
  batch 700 loss: 0.855949091911316
  batch 750 loss: 0.8559053862094879
  batch 800 loss: 0.8979556357860565
  batch 850 loss: 0.8688552510738373
  batch 900 loss: 0.8899147045612336
LOSS train 0.88991 valid 0.93286, valid PER 29.84%
EPOCH 20:
  batch 50 loss: 0.8252623569965363
  batch 100 loss: 0.8251846510171891
  batch 150 loss: 0.8236204844713211
  batch 200 loss: 0.8831570148468018
  batch 250 loss: 0.9055974686145782
  batch 300 loss: 0.9041241788864136
  batch 350 loss: 0.8771105563640594
  batch 400 loss: 0.920771473646164
  batch 450 loss: 0.8923324513435363
  batch 500 loss: 0.8611691009998321
  batch 550 loss: 0.9417453372478485
  batch 600 loss: 0.8837041914463043
  batch 650 loss: 0.8972657358646393
  batch 700 loss: 0.955787924528122
  batch 750 loss: 0.9051176333427429
  batch 800 loss: 0.9120885097980499
  batch 850 loss: 0.954597475528717
  batch 900 loss: 0.9670706105232239
LOSS train 0.96707 valid 0.97426, valid PER 30.66%
Training finished in 5.0 minutes.
Model saved to checkpoints/20231207_172449/model_19
Loading model from checkpoints/20231207_172449/model_19
SUB: 16.36%, DEL: 13.74%, INS: 1.79%, COR: 69.90%, PER: 31.89%
