Namespace(seed=0, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.1, optimiser='sgd')
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 3.9893422079086305
  batch 100 loss: 3.346361699104309
  batch 150 loss: 3.280253758430481
  batch 200 loss: 3.0747498989105226
  batch 250 loss: 2.8437421798706053
  batch 300 loss: 2.652449779510498
  batch 350 loss: 2.5578747987747192
  batch 400 loss: 2.4320521211624144
  batch 450 loss: 2.3430264043807982
  batch 500 loss: 2.259221019744873
  batch 550 loss: 2.1731876349449157
  batch 600 loss: 2.132256190776825
  batch 650 loss: 2.0663288927078245
  batch 700 loss: 2.0041595935821532
  batch 750 loss: 1.932747004032135
  batch 800 loss: 1.91657155752182
  batch 850 loss: 1.848209855556488
  batch 900 loss: 1.8060340785980225
LOSS train 1.80603 valid 1.74557, valid PER 62.78%
EPOCH 2:
  batch 50 loss: 1.743962025642395
  batch 100 loss: 1.6722934222221375
  batch 150 loss: 1.680488109588623
  batch 200 loss: 1.6414690566062928
  batch 250 loss: 1.623826208114624
  batch 300 loss: 1.623506534099579
  batch 350 loss: 1.61756888628006
  batch 400 loss: 1.517176742553711
  batch 450 loss: 1.5562142491340638
  batch 500 loss: 1.5320334100723267
  batch 550 loss: 1.471855821609497
  batch 600 loss: 1.5078256559371948
  batch 650 loss: 1.4570129084587098
  batch 700 loss: 1.471320197582245
  batch 750 loss: 1.3995852518081664
  batch 800 loss: 1.4738574981689454
  batch 850 loss: 1.447063262462616
  batch 900 loss: 1.3996247577667236
LOSS train 1.39962 valid 1.45030, valid PER 47.01%
EPOCH 3:
  batch 50 loss: 1.347744631767273
  batch 100 loss: 1.325396270751953
  batch 150 loss: 1.310566234588623
  batch 200 loss: 1.3105884075164795
  batch 250 loss: 1.3084905743598938
  batch 300 loss: 1.3057756280899049
  batch 350 loss: 1.3697993731498719
  batch 400 loss: 1.285586302280426
  batch 450 loss: 1.2411076962947845
  batch 500 loss: 1.2698144817352295
  batch 550 loss: 1.2528395748138428
  batch 600 loss: 1.2572790551185609
  batch 650 loss: 1.2472846841812133
  batch 700 loss: 1.2376369607448579
  batch 750 loss: 1.2318352818489076
  batch 800 loss: 1.2342824387550353
  batch 850 loss: 1.2506252026557922
  batch 900 loss: 1.2258378863334656
LOSS train 1.22584 valid 1.18410, valid PER 38.41%
EPOCH 4:
  batch 50 loss: 1.1848827946186065
  batch 100 loss: 1.2071399796009064
  batch 150 loss: 1.1845305502414702
  batch 200 loss: 1.1854981625080108
  batch 250 loss: 1.1586105012893677
  batch 300 loss: 1.1083577728271485
  batch 350 loss: 1.1175501906871796
  batch 400 loss: 1.174742579460144
  batch 450 loss: 1.1236690783500671
  batch 500 loss: 1.1710911011695861
  batch 550 loss: 1.142213134765625
  batch 600 loss: 1.1464703714847564
  batch 650 loss: 1.1662928593158721
  batch 700 loss: 1.1125226950645446
  batch 750 loss: 1.093006135225296
  batch 800 loss: 1.1175925374031066
  batch 850 loss: 1.078873485326767
  batch 900 loss: 1.124771100282669
LOSS train 1.12477 valid 1.17285, valid PER 35.29%
EPOCH 5:
  batch 50 loss: 1.0653472471237182
  batch 100 loss: 1.0489960312843323
  batch 150 loss: 1.1039190459251405
  batch 200 loss: 1.0481990373134613
  batch 250 loss: 1.067880026102066
  batch 300 loss: 1.0486357343196868
  batch 350 loss: 1.0578752720355988
  batch 400 loss: 1.0485411012172698
  batch 450 loss: 1.0549247121810914
  batch 500 loss: 1.0763679671287536
  batch 550 loss: 1.0302232885360718
  batch 600 loss: 1.0937805140018464
  batch 650 loss: 1.0909554958343506
  batch 700 loss: 1.0696803772449492
  batch 750 loss: 1.059999134540558
  batch 800 loss: 1.0575977671146393
  batch 850 loss: 1.0858218777179718
  batch 900 loss: 1.0546381998062133
LOSS train 1.05464 valid 1.07238, valid PER 34.68%
EPOCH 6:
  batch 50 loss: 1.0291828441619872
  batch 100 loss: 1.0294353008270263
  batch 150 loss: 1.0092193913459777
  batch 200 loss: 1.0654153692722321
  batch 250 loss: 1.0231167912483214
  batch 300 loss: 0.9974054539203644
  batch 350 loss: 1.0244347596168517
  batch 400 loss: 0.9857521378993988
  batch 450 loss: 0.989745020866394
  batch 500 loss: 1.0001455307006837
  batch 550 loss: 0.9945844268798828
  batch 600 loss: 1.0131511211395263
  batch 650 loss: 1.0527153635025024
  batch 700 loss: 1.0333526289463044
  batch 750 loss: 0.9961784529685974
  batch 800 loss: 1.0214762890338898
  batch 850 loss: 1.0025754630565644
  batch 900 loss: 0.9480951869487763
LOSS train 0.94810 valid 1.05000, valid PER 32.17%
EPOCH 7:
  batch 50 loss: 0.9745905184745789
  batch 100 loss: 0.9937498545646668
  batch 150 loss: 0.9474034476280212
  batch 200 loss: 0.9475622272491455
  batch 250 loss: 0.9397436773777008
  batch 300 loss: 0.9596003293991089
  batch 350 loss: 0.9696014964580536
  batch 400 loss: 0.9547045004367828
  batch 450 loss: 0.9857017838954926
  batch 500 loss: 0.9565951299667358
  batch 550 loss: 0.9659665215015412
  batch 600 loss: 0.9458070611953735
  batch 650 loss: 0.9511064493656158
  batch 700 loss: 0.9724445116519927
  batch 750 loss: 0.9893732130527496
  batch 800 loss: 0.928838357925415
  batch 850 loss: 0.9619298529624939
  batch 900 loss: 0.9368123614788055
LOSS train 0.93681 valid 1.02804, valid PER 32.93%
EPOCH 8:
  batch 50 loss: 0.8825595676898956
  batch 100 loss: 0.9384880554676056
  batch 150 loss: 0.9283083951473237
  batch 200 loss: 0.9030362582206726
  batch 250 loss: 0.8968762493133545
  batch 300 loss: 0.9263067317008972
  batch 350 loss: 0.9242775452136993
  batch 400 loss: 0.8846438002586364
  batch 450 loss: 0.9711243152618408
  batch 500 loss: 0.967558069229126
  batch 550 loss: 0.9188293242454528
  batch 600 loss: 0.9533671557903289
  batch 650 loss: 0.9233231985569
  batch 700 loss: 0.9010858190059662
  batch 750 loss: 0.9321464312076568
  batch 800 loss: 0.9087630593776703
  batch 850 loss: 0.9200542807579041
  batch 900 loss: 0.9180612099170685
LOSS train 0.91806 valid 1.00288, valid PER 31.68%
EPOCH 9:
  batch 50 loss: 0.8663809978961945
  batch 100 loss: 0.8962535607814789
  batch 150 loss: 0.8939503633975983
  batch 200 loss: 0.8777597141265869
  batch 250 loss: 0.9390429878234863
  batch 300 loss: 0.8965947139263153
  batch 350 loss: 0.877371176481247
  batch 400 loss: 0.931608954668045
  batch 450 loss: 0.9215064787864685
  batch 500 loss: 0.9160304510593414
  batch 550 loss: 0.9100839805603027
  batch 600 loss: 0.8829340267181397
  batch 650 loss: 0.9347299313545228
  batch 700 loss: 0.8757223069667817
  batch 750 loss: 0.913957531452179
  batch 800 loss: 0.841819806098938
  batch 850 loss: 0.9022218489646912
  batch 900 loss: 0.9013954424858093
LOSS train 0.90140 valid 0.99588, valid PER 30.50%
EPOCH 10:
  batch 50 loss: 0.8476261878013611
  batch 100 loss: 0.8496583139896393
  batch 150 loss: 0.8361254668235779
  batch 200 loss: 0.8354439473152161
  batch 250 loss: 0.9122408521175385
  batch 300 loss: 0.8904091048240662
  batch 350 loss: 0.8659151601791382
  batch 400 loss: 0.9052252471446991
  batch 450 loss: 0.9265105187892914
  batch 500 loss: 0.8617390215396881
  batch 550 loss: 0.8477912664413452
  batch 600 loss: 0.9011602342128754
  batch 650 loss: 0.8303329956531524
  batch 700 loss: 0.9074851751327515
  batch 750 loss: 0.8968481695652009
  batch 800 loss: 0.9021425235271454
  batch 850 loss: 0.8703419041633605
  batch 900 loss: 0.8849346697330475
LOSS train 0.88493 valid 0.98087, valid PER 31.66%
EPOCH 11:
  batch 50 loss: 0.8218361914157868
  batch 100 loss: 0.8649622285366059
  batch 150 loss: 0.8541804003715515
  batch 200 loss: 0.8417521834373474
  batch 250 loss: 0.8428370153903961
  batch 300 loss: 0.8341859257221222
  batch 350 loss: 0.8554225862026215
  batch 400 loss: 0.8268033862113953
  batch 450 loss: 0.8362071526050567
  batch 500 loss: 0.8435980141162872
  batch 550 loss: 0.8426464968919753
  batch 600 loss: 0.800280065536499
  batch 650 loss: 0.85242258310318
  batch 700 loss: 0.83803213596344
  batch 750 loss: 0.8311597323417663
  batch 800 loss: 0.8419445639848709
  batch 850 loss: 0.8292864787578583
  batch 900 loss: 0.8142263746261597
LOSS train 0.81423 valid 0.95222, valid PER 29.59%
EPOCH 12:
  batch 50 loss: 0.738793643116951
  batch 100 loss: 0.8202718448638916
  batch 150 loss: 0.7929566222429275
  batch 200 loss: 0.8095115005970002
  batch 250 loss: 0.8118025624752044
  batch 300 loss: 0.7959992909431457
  batch 350 loss: 0.8107403671741485
  batch 400 loss: 0.7984909570217132
  batch 450 loss: 0.8262265014648438
  batch 500 loss: 0.8208896565437317
  batch 550 loss: 0.8375294649600983
  batch 600 loss: 0.8125410199165344
  batch 650 loss: 0.8288661420345307
  batch 700 loss: 0.8181694686412812
  batch 750 loss: 0.8286477744579315
  batch 800 loss: 0.832483434677124
  batch 850 loss: 0.859340512752533
  batch 900 loss: 0.8400296628475189
LOSS train 0.84003 valid 0.96148, valid PER 29.89%
EPOCH 13:
  batch 50 loss: 0.7793420708179474
  batch 100 loss: 0.7957892334461212
  batch 150 loss: 0.8028108692169189
  batch 200 loss: 0.7817579460144043
  batch 250 loss: 0.8211138784885407
  batch 300 loss: 0.7675319623947143
  batch 350 loss: 0.8003491652011872
  batch 400 loss: 0.7939537584781646
  batch 450 loss: 0.7868839609622955
  batch 500 loss: 0.7838226354122162
  batch 550 loss: 0.7702471578121185
  batch 600 loss: 0.8112220287322998
  batch 650 loss: 0.8001356494426727
  batch 700 loss: 0.793632470369339
  batch 750 loss: 0.7759716045856476
  batch 800 loss: 0.7864264357089996
  batch 850 loss: 0.820517008304596
  batch 900 loss: 0.8346115756034851
LOSS train 0.83461 valid 0.94688, valid PER 28.89%
EPOCH 14:
  batch 50 loss: 0.763065161705017
  batch 100 loss: 0.7816483008861542
  batch 150 loss: 0.8069643783569336
  batch 200 loss: 0.7401742827892304
  batch 250 loss: 0.7665553653240204
  batch 300 loss: 0.8052393841743469
  batch 350 loss: 0.7773568487167358
  batch 400 loss: 0.7780861276388168
  batch 450 loss: 0.8033552372455597
  batch 500 loss: 0.7651722502708435
  batch 550 loss: 0.7673170959949493
  batch 600 loss: 0.7774023044109345
  batch 650 loss: 0.7830473697185516
  batch 700 loss: 0.7661024016141892
  batch 750 loss: 0.7875224232673645
  batch 800 loss: 0.8015589809417725
  batch 850 loss: 0.8165855145454407
  batch 900 loss: 0.8144502913951874
LOSS train 0.81445 valid 0.96055, valid PER 30.63%
EPOCH 15:
  batch 50 loss: 0.7335903632640839
  batch 100 loss: 0.7168658435344696
  batch 150 loss: 0.751745285987854
  batch 200 loss: 0.7407322371006012
  batch 250 loss: 0.7320689308643341
  batch 300 loss: 0.7631117635965348
  batch 350 loss: 0.7744486808776856
  batch 400 loss: 0.8119748163223267
  batch 450 loss: 0.8286374521255493
  batch 500 loss: 0.7598691552877426
  batch 550 loss: 0.7815915989875794
  batch 600 loss: 0.7690204024314881
  batch 650 loss: 0.7763715302944183
  batch 700 loss: 0.7688084495067596
  batch 750 loss: 0.7862541961669922
  batch 800 loss: 0.7941600215435028
  batch 850 loss: 0.799012885093689
  batch 900 loss: 0.804827532172203
LOSS train 0.80483 valid 0.97807, valid PER 30.56%
EPOCH 16:
  batch 50 loss: 0.7386297416687012
  batch 100 loss: 0.7135859429836273
  batch 150 loss: 0.7702654063701629
  batch 200 loss: 0.7414786446094513
  batch 250 loss: 0.75849635720253
  batch 300 loss: 0.72079580783844
  batch 350 loss: 0.7727708053588868
  batch 400 loss: 0.7180539691448211
  batch 450 loss: 0.7582187819480896
  batch 500 loss: 0.7324758458137512
  batch 550 loss: 0.7566448616981506
  batch 600 loss: 0.7578984606266022
  batch 650 loss: 0.7466431939601899
  batch 700 loss: 0.7548921966552734
  batch 750 loss: 0.757822505235672
  batch 800 loss: 0.7424396395683288
  batch 850 loss: 0.7510147273540497
  batch 900 loss: 0.7714577841758729
LOSS train 0.77146 valid 0.95788, valid PER 28.79%
EPOCH 17:
  batch 50 loss: 0.7202638685703278
  batch 100 loss: 0.7092348372936249
  batch 150 loss: 0.7043598902225494
  batch 200 loss: 0.7137093782424927
  batch 250 loss: 0.7178354370594024
  batch 300 loss: 0.6855094289779663
  batch 350 loss: 0.7107276368141174
  batch 400 loss: 0.6884624880552291
  batch 450 loss: 0.7226520019769669
  batch 500 loss: 0.7271688294410705
  batch 550 loss: 0.7225537395477295
  batch 600 loss: 0.7339085364341735
  batch 650 loss: 0.7337783408164978
  batch 700 loss: 0.7555940461158752
  batch 750 loss: 0.7651257389783859
  batch 800 loss: 0.735872220993042
  batch 850 loss: 0.7455477160215378
  batch 900 loss: 0.739898875951767
LOSS train 0.73990 valid 0.95976, valid PER 29.42%
EPOCH 18:
  batch 50 loss: 0.6880652529001235
  batch 100 loss: 0.7375998294353485
  batch 150 loss: 0.7415490591526032
  batch 200 loss: 0.696481996178627
  batch 250 loss: 0.6884915471076966
  batch 300 loss: 0.7101633626222611
  batch 350 loss: 0.7061455762386322
  batch 400 loss: 0.7233015233278275
  batch 450 loss: 0.7076833885908127
  batch 500 loss: 0.7499729609489441
  batch 550 loss: 0.7341902732849122
  batch 600 loss: 0.7379118287563324
  batch 650 loss: 0.7429978144168854
  batch 700 loss: 0.7424433624744415
  batch 750 loss: 0.6938823294639588
  batch 800 loss: 0.7369564938545227
  batch 850 loss: 0.6808565592765808
  batch 900 loss: 0.716199541091919
LOSS train 0.71620 valid 0.96659, valid PER 29.44%
EPOCH 19:
  batch 50 loss: 0.6665230858325958
  batch 100 loss: 0.6913506054878235
  batch 150 loss: 0.6822471517324448
  batch 200 loss: 0.693763837814331
  batch 250 loss: 0.707531625032425
  batch 300 loss: 0.6972949552536011
  batch 350 loss: 0.6901755970716477
  batch 400 loss: 0.6966020184755325
  batch 450 loss: 0.711420863866806
  batch 500 loss: 0.7121639031171799
  batch 550 loss: 0.7301036751270295
  batch 600 loss: 0.6964160799980164
  batch 650 loss: 0.7271957838535309
  batch 700 loss: 0.7273290753364563
  batch 750 loss: 0.7547385799884796
  batch 800 loss: 0.7151933962106705
  batch 850 loss: 0.7297408556938172
  batch 900 loss: 0.7367455196380616
LOSS train 0.73675 valid 0.98101, valid PER 29.82%
EPOCH 20:
  batch 50 loss: 0.6439939397573471
  batch 100 loss: 0.6324684023857117
  batch 150 loss: 0.6384353733062744
  batch 200 loss: 0.6963041394948959
  batch 250 loss: 0.6815526819229126
  batch 300 loss: 0.6827889716625214
  batch 350 loss: 0.6591927582025527
  batch 400 loss: 0.7246555864810944
  batch 450 loss: 0.6497488784790039
  batch 500 loss: 0.7242363858222961
  batch 550 loss: 0.759937756061554
  batch 600 loss: 0.6909209102392196
  batch 650 loss: 0.7097058320045471
  batch 700 loss: 0.760253494977951
  batch 750 loss: 0.7584781122207641
  batch 800 loss: 0.6952647632360458
  batch 850 loss: 0.7108438926935196
  batch 900 loss: 0.7348953127861023
LOSS train 0.73490 valid 0.96654, valid PER 28.84%
Training finished in 5.0 minutes.
Model saved to checkpoints/20231129_163341/model_13
Loading model from checkpoints/20231129_163341/model_13
SUB: 17.67%, DEL: 10.62%, INS: 2.50%, COR: 71.71%, PER: 30.79%
