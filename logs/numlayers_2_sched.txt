Namespace(seed=123, train_json='train_fbank61.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=2, fbank_dims=23, model_dims=128, concat=1, lr=0.9, vocab='vocab_61.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 568127
EPOCH 1, Learning Rate: 0.9
  batch 50 loss: 5.295189814567566
  batch 100 loss: 3.881965308189392
  batch 150 loss: 3.7879949903488157
  batch 200 loss: 3.667179651260376
  batch 250 loss: 3.5634274864196778
  batch 300 loss: 3.3413171863555906
  batch 350 loss: 3.1011041975021363
  batch 400 loss: 3.0414741611480713
  batch 450 loss: 2.7838008880615233
  batch 500 loss: 2.6169269943237303
  batch 550 loss: 2.508174958229065
  batch 600 loss: 2.397249336242676
  batch 650 loss: 2.2921716022491454
  batch 700 loss: 2.239413104057312
  batch 750 loss: 2.1793246459960938
  batch 800 loss: 2.1257638192176818
  batch 850 loss: 2.0648900508880614
  batch 900 loss: 2.0364993453025817
avg val loss: 3.104659080505371
LOSS train 2.03650 valid 3.10466, valid PER 63.15%
EPOCH 2, Learning Rate: 0.9
  batch 50 loss: 1.9585360550880433
  batch 100 loss: 1.9610749411582946
  batch 150 loss: 1.8174312591552735
  batch 200 loss: 1.8223266410827637
  batch 250 loss: 1.7874114871025086
  batch 300 loss: 1.7667151641845704
  batch 350 loss: 1.7780103135108947
  batch 400 loss: 1.7256333446502685
  batch 450 loss: 1.692014901638031
  batch 500 loss: 1.6681341576576232
  batch 550 loss: 1.6580552506446837
  batch 600 loss: 1.6426528930664062
  batch 650 loss: 1.6032485032081605
  batch 700 loss: 1.588481068611145
  batch 750 loss: 1.5785307049751283
  batch 800 loss: 1.5297623443603516
  batch 850 loss: 1.5579017305374145
  batch 900 loss: 1.5283351421356202
avg val loss: 2.924849271774292
LOSS train 1.52834 valid 2.92485, valid PER 37.53%
EPOCH 3, Learning Rate: 0.9
  batch 50 loss: 1.439704875946045
  batch 100 loss: 1.5448287844657898
  batch 150 loss: 1.5453199553489685
  batch 200 loss: 1.5113547539710999
  batch 250 loss: 1.4724588990211487
  batch 300 loss: 1.4992202234268188
  batch 350 loss: 1.4781211161613463
  batch 400 loss: 1.4703036308288575
  batch 450 loss: 1.4500146627426147
  batch 500 loss: 1.3987885403633118
  batch 550 loss: 1.4269689559936523
  batch 600 loss: 1.3701113557815552
  batch 650 loss: 1.3824247431755066
  batch 700 loss: 1.404013559818268
  batch 750 loss: 1.4268090558052062
  batch 800 loss: 1.4310889005661012
  batch 850 loss: 1.354882208108902
  batch 900 loss: 1.3866553401947022
avg val loss: 2.8746321201324463
LOSS train 1.38666 valid 2.87463, valid PER 33.77%
EPOCH 4, Learning Rate: 0.9
  batch 50 loss: 1.3657597482204438
  batch 100 loss: 1.2989530992507934
  batch 150 loss: 1.339722969532013
  batch 200 loss: 1.3227143597602844
  batch 250 loss: 1.2988625073432922
  batch 300 loss: 1.3239342272281647
  batch 350 loss: 1.3171475839614868
  batch 400 loss: 1.254385266304016
  batch 450 loss: 1.2726164150238037
  batch 500 loss: 1.3598762941360474
  batch 550 loss: 1.297277660369873
  batch 600 loss: 1.283101372718811
  batch 650 loss: 1.3245877933502197
  batch 700 loss: 1.3458550155162812
  batch 750 loss: 1.3412132263183594
  batch 800 loss: 1.2870376670360566
  batch 850 loss: 1.2837321591377258
  batch 900 loss: 1.2720326268672943
avg val loss: 2.8740742206573486
LOSS train 1.27203 valid 2.87407, valid PER 30.88%
EPOCH 5, Learning Rate: 0.9
  batch 50 loss: 1.2633251988887786
  batch 100 loss: 1.2448439252376557
  batch 150 loss: 1.2336950123310089
  batch 200 loss: 1.2687613320350648
  batch 250 loss: 1.2135215401649475
  batch 300 loss: 1.2710565650463104
  batch 350 loss: 1.232834188938141
  batch 400 loss: 1.215980143547058
  batch 450 loss: 1.2200819909572602
  batch 500 loss: 1.1935268998146058
  batch 550 loss: 1.2594080674648285
  batch 600 loss: 1.2209727835655213
  batch 650 loss: 1.222514626979828
  batch 700 loss: 1.2328005802631379
  batch 750 loss: 1.2410717713832855
  batch 800 loss: 1.2616046810150146
  batch 850 loss: 1.287005491256714
  batch 900 loss: 1.2176422214508056
avg val loss: 2.8564980030059814
LOSS train 1.21764 valid 2.85650, valid PER 31.14%
EPOCH 6, Learning Rate: 0.9
  batch 50 loss: 1.1962195944786072
  batch 100 loss: 1.1941287434101104
  batch 150 loss: 1.1736254596710205
  batch 200 loss: 1.1319829201698304
  batch 250 loss: 1.213347784280777
  batch 300 loss: 1.2057596695423127
  batch 350 loss: 1.1738656759262085
  batch 400 loss: 1.1534139835834503
  batch 450 loss: 1.1727798473834992
  batch 500 loss: 1.1373872196674346
  batch 550 loss: 1.1646924912929535
  batch 600 loss: 1.1709730970859527
  batch 650 loss: 1.1579198503494263
  batch 700 loss: 1.1431183576583863
  batch 750 loss: 1.2028080689907075
  batch 800 loss: 1.1834627747535706
  batch 850 loss: 1.228185602426529
  batch 900 loss: 1.2147116684913635
avg val loss: 2.7729995250701904
LOSS train 1.21471 valid 2.77300, valid PER 29.23%
EPOCH 7, Learning Rate: 0.9
  batch 50 loss: 1.1262951683998108
  batch 100 loss: 1.1707363402843476
  batch 150 loss: 1.0917296648025512
  batch 200 loss: 1.112104240655899
  batch 250 loss: 1.1490131771564485
  batch 300 loss: 1.1298756456375123
  batch 350 loss: 1.1784075093269348
  batch 400 loss: 1.136471663713455
  batch 450 loss: 1.0900683069229127
  batch 500 loss: 1.1117210268974305
  batch 550 loss: 1.1102216398715974
  batch 600 loss: 1.1366201531887055
  batch 650 loss: 1.1145273530483246
  batch 700 loss: 1.1588496029376985
  batch 750 loss: 1.131775634288788
  batch 800 loss: 1.1050713992118835
  batch 850 loss: 1.1164909517765045
  batch 900 loss: 1.112864978313446
avg val loss: 2.8227596282958984
LOSS train 1.11286 valid 2.82276, valid PER 28.54%
EPOCH 8, Learning Rate: 0.45
  batch 50 loss: 1.0372922682762147
  batch 100 loss: 1.0014374732971192
  batch 150 loss: 1.0245462954044342
  batch 200 loss: 1.008512487411499
  batch 250 loss: 0.9937024247646332
  batch 300 loss: 0.953327190876007
  batch 350 loss: 0.9896604597568512
  batch 400 loss: 0.9983299005031586
  batch 450 loss: 1.0200205516815186
  batch 500 loss: 0.9908939588069916
  batch 550 loss: 0.9812566781044006
  batch 600 loss: 0.966844230890274
  batch 650 loss: 0.9843506038188934
  batch 700 loss: 0.9709411787986756
  batch 750 loss: 0.9964249527454376
  batch 800 loss: 0.997234228849411
  batch 850 loss: 0.9471795523166656
  batch 900 loss: 0.959458018541336
avg val loss: 2.8018767833709717
LOSS train 0.95946 valid 2.80188, valid PER 25.80%
EPOCH 9, Learning Rate: 0.225
  batch 50 loss: 0.8996019971370697
  batch 100 loss: 0.8970709371566773
  batch 150 loss: 0.8944890356063843
  batch 200 loss: 0.8940864264965057
  batch 250 loss: 0.8732290303707123
  batch 300 loss: 0.8770753085613251
  batch 350 loss: 0.876220018863678
  batch 400 loss: 0.9046846914291382
  batch 450 loss: 0.9118283331394196
  batch 500 loss: 0.9009450912475586
  batch 550 loss: 0.9172830200195312
  batch 600 loss: 0.9232825303077697
  batch 650 loss: 0.9153581738471985
  batch 700 loss: 0.9053034913539887
  batch 750 loss: 0.9348831713199616
  batch 800 loss: 0.9362213122844696
  batch 850 loss: 0.9095794546604157
  batch 900 loss: 0.8797092700004577
avg val loss: 2.8470795154571533
LOSS train 0.87971 valid 2.84708, valid PER 24.71%
EPOCH 10, Learning Rate: 0.1125
  batch 50 loss: 0.8687455844879151
  batch 100 loss: 0.8652883172035217
  batch 150 loss: 0.8811077737808227
  batch 200 loss: 0.8519544959068298
  batch 250 loss: 0.8447521126270294
  batch 300 loss: 0.8604311752319336
  batch 350 loss: 0.8520931434631348
  batch 400 loss: 0.8475823616981506
  batch 450 loss: 0.8303334999084473
  batch 500 loss: 0.8540222120285034
  batch 550 loss: 0.855258082151413
  batch 600 loss: 0.8516820228099823
  batch 650 loss: 0.868475946187973
  batch 700 loss: 0.8541635024547577
  batch 750 loss: 0.8772778797149658
  batch 800 loss: 0.871483314037323
  batch 850 loss: 0.8360978841781617
  batch 900 loss: 0.8309394001960755
avg val loss: 2.88879132270813
LOSS train 0.83094 valid 2.88879, valid PER 24.72%
EPOCH 11, Learning Rate: 0.05625
  batch 50 loss: 0.8570859718322754
  batch 100 loss: 0.8222391194105149
  batch 150 loss: 0.8281351459026337
  batch 200 loss: 0.787900470495224
  batch 250 loss: 0.8295137000083923
  batch 300 loss: 0.7970015132427215
  batch 350 loss: 0.8453185880184173
  batch 400 loss: 0.8257795977592468
  batch 450 loss: 0.8067888867855072
  batch 500 loss: 0.8100986194610595
  batch 550 loss: 0.8289564526081086
  batch 600 loss: 0.8163541114330292
  batch 650 loss: 0.8405578088760376
  batch 700 loss: 0.8844004559516907
  batch 750 loss: 0.8244894289970398
  batch 800 loss: 0.8484609067440033
  batch 850 loss: 0.8396513938903809
  batch 900 loss: 0.8391634237766266
avg val loss: 2.896437644958496
LOSS train 0.83916 valid 2.89644, valid PER 24.00%
EPOCH 12, Learning Rate: 0.028125
  batch 50 loss: 0.8054537498950958
  batch 100 loss: 0.7945898127555847
  batch 150 loss: 0.8038273537158966
  batch 200 loss: 0.8313104796409607
  batch 250 loss: 0.7974874877929687
  batch 300 loss: 0.8252247816324234
  batch 350 loss: 0.8134635841846466
  batch 400 loss: 0.8290010797977447
  batch 450 loss: 0.8079950177669525
  batch 500 loss: 0.8229220151901245
  batch 550 loss: 0.8111778283119202
  batch 600 loss: 0.8169690412282944
  batch 650 loss: 0.8089334672689438
  batch 700 loss: 0.8161664241552353
  batch 750 loss: 0.8347623825073243
  batch 800 loss: 0.7854958653450013
  batch 850 loss: 0.8089627408981324
  batch 900 loss: 0.8413276839256286
avg val loss: 2.9111878871917725
LOSS train 0.84133 valid 2.91119, valid PER 23.81%
EPOCH 13, Learning Rate: 0.0140625
  batch 50 loss: 0.7735832130908966
  batch 100 loss: 0.796796760559082
  batch 150 loss: 0.8225875401496887
  batch 200 loss: 0.7828556954860687
  batch 250 loss: 0.8122219717502595
  batch 300 loss: 0.845110844373703
  batch 350 loss: 0.7728715419769288
  batch 400 loss: 0.7950544035434723
  batch 450 loss: 0.8130699288845062
  batch 500 loss: 0.8083594441413879
  batch 550 loss: 0.8557544445991516
  batch 600 loss: 0.817663061618805
  batch 650 loss: 0.8255511617660523
  batch 700 loss: 0.8353312849998474
  batch 750 loss: 0.7812059187889099
  batch 800 loss: 0.8132713222503662
  batch 850 loss: 0.7868553173542022
  batch 900 loss: 0.8130468690395355
avg val loss: 2.9170968532562256
LOSS train 0.81305 valid 2.91710, valid PER 23.82%
EPOCH 14, Learning Rate: 0.00703125
  batch 50 loss: 0.8148311173915863
  batch 100 loss: 0.7853099095821381
  batch 150 loss: 0.828278397321701
  batch 200 loss: 0.7809980881214141
  batch 250 loss: 0.8089764547348023
  batch 300 loss: 0.7929535984992981
  batch 350 loss: 0.811910206079483
  batch 400 loss: 0.8130264723300934
  batch 450 loss: 0.7950549364089966
  batch 500 loss: 0.7940189015865325
  batch 550 loss: 0.8071567904949188
  batch 600 loss: 0.7945460653305054
  batch 650 loss: 0.8233075785636902
  batch 700 loss: 0.8062361156940461
  batch 750 loss: 0.7995131623744964
  batch 800 loss: 0.8082247203588486
  batch 850 loss: 0.8295791566371917
  batch 900 loss: 0.8330083131790161
avg val loss: 2.910740613937378
LOSS train 0.83301 valid 2.91074, valid PER 23.76%
EPOCH 15, Learning Rate: 0.003515625
  batch 50 loss: 0.7746186029911041
  batch 100 loss: 0.798911971449852
  batch 150 loss: 0.8104019808769226
  batch 200 loss: 0.8369713163375855
  batch 250 loss: 0.8241599190235138
  batch 300 loss: 0.8177461099624633
  batch 350 loss: 0.7994625353813172
  batch 400 loss: 0.8041485750675201
  batch 450 loss: 0.79938645362854
  batch 500 loss: 0.7993564176559448
  batch 550 loss: 0.8452993643283844
  batch 600 loss: 0.837144502401352
  batch 650 loss: 0.8005847406387329
  batch 700 loss: 0.784735095500946
  batch 750 loss: 0.8149036693572999
  batch 800 loss: 0.7729945069551468
  batch 850 loss: 0.7912547349929809
  batch 900 loss: 0.7647272276878357
avg val loss: 2.9097611904144287
LOSS train 0.76473 valid 2.90976, valid PER 23.76%
EPOCH 16, Learning Rate: 0.0017578125
  batch 50 loss: 0.8107791578769684
  batch 100 loss: 0.7970788478851318
  batch 150 loss: 0.8132975280284882
  batch 200 loss: 0.825052455663681
  batch 250 loss: 0.8007313573360443
  batch 300 loss: 0.8056403219699859
  batch 350 loss: 0.8036271607875825
  batch 400 loss: 0.788005159497261
  batch 450 loss: 0.8122400164604187
  batch 500 loss: 0.806591032743454
  batch 550 loss: 0.7879321372509003
  batch 600 loss: 0.8445366871356964
  batch 650 loss: 0.8074458289146423
  batch 700 loss: 0.7747067493200303
  batch 750 loss: 0.7925092113018036
  batch 800 loss: 0.7843645572662353
  batch 850 loss: 0.7925095176696777
  batch 900 loss: 0.782664213180542
avg val loss: 2.91105580329895
LOSS train 0.78266 valid 2.91106, valid PER 23.80%
EPOCH 17, Learning Rate: 0.00087890625
  batch 50 loss: 0.8410764336585999
  batch 100 loss: 0.7581518435478211
  batch 150 loss: 0.8335070633888244
  batch 200 loss: 0.7691946697235107
  batch 250 loss: 0.8160006821155548
  batch 300 loss: 0.7999977076053619
  batch 350 loss: 0.8077738928794861
  batch 400 loss: 0.7948577797412872
  batch 450 loss: 0.800832349061966
  batch 500 loss: 0.8100949919223785
  batch 550 loss: 0.8039131188392639
  batch 600 loss: 0.8520692145824432
  batch 650 loss: 0.7918048334121705
  batch 700 loss: 0.82458287358284
  batch 750 loss: 0.7628123068809509
  batch 800 loss: 0.8148235511779786
  batch 850 loss: 0.8006877636909485
  batch 900 loss: 0.8040031325817109
avg val loss: 2.910980224609375
LOSS train 0.80400 valid 2.91098, valid PER 23.77%
EPOCH 18, Learning Rate: 0.000439453125
  batch 50 loss: 0.7897071063518524
  batch 100 loss: 0.7805743098258973
  batch 150 loss: 0.8492862784862518
  batch 200 loss: 0.8014892196655273
  batch 250 loss: 0.7765027141571045
  batch 300 loss: 0.7842206847667694
  batch 350 loss: 0.7634660375118255
  batch 400 loss: 0.7862919420003891
  batch 450 loss: 0.8028087937831878
  batch 500 loss: 0.8098577010631561
  batch 550 loss: 0.8219205546379089
  batch 600 loss: 0.7918419206142425
  batch 650 loss: 0.7966107952594758
  batch 700 loss: 0.7985036838054657
  batch 750 loss: 0.8085123169422149
  batch 800 loss: 0.8049118137359619
  batch 850 loss: 0.8291027903556824
  batch 900 loss: 0.8054250293970108
avg val loss: 2.911202907562256
LOSS train 0.80543 valid 2.91120, valid PER 23.76%
EPOCH 19, Learning Rate: 0.0002197265625
  batch 50 loss: 0.8128930270671845
  batch 100 loss: 0.8148601388931275
  batch 150 loss: 0.8028931128978729
  batch 200 loss: 0.7868378555774689
  batch 250 loss: 0.8138477408885956
  batch 300 loss: 0.8520123732089996
  batch 350 loss: 0.7924213659763336
  batch 400 loss: 0.7758476150035858
  batch 450 loss: 0.7588487017154694
  batch 500 loss: 0.7819577920436859
  batch 550 loss: 0.8111875820159912
  batch 600 loss: 0.822456088066101
  batch 650 loss: 0.8183122849464417
  batch 700 loss: 0.8368267059326172
  batch 750 loss: 0.7750961160659791
  batch 800 loss: 0.7865122830867768
  batch 850 loss: 0.8142260909080505
  batch 900 loss: 0.7937963604927063
avg val loss: 2.911259412765503
LOSS train 0.79380 valid 2.91126, valid PER 23.78%
EPOCH 20, Learning Rate: 0.00010986328125
  batch 50 loss: 0.7769479751586914
  batch 100 loss: 0.7967469990253448
  batch 150 loss: 0.7981527805328369
  batch 200 loss: 0.7907055878639221
  batch 250 loss: 0.8221256566047669
  batch 300 loss: 0.8110623991489411
  batch 350 loss: 0.7893733656406403
  batch 400 loss: 0.7865682673454285
  batch 450 loss: 0.7775912261009217
  batch 500 loss: 0.810502005815506
  batch 550 loss: 0.8170315802097321
  batch 600 loss: 0.8033966255187989
  batch 650 loss: 0.81115602850914
  batch 700 loss: 0.786860032081604
  batch 750 loss: 0.7890182530879974
  batch 800 loss: 0.8174411582946778
  batch 850 loss: 0.8056414210796357
  batch 900 loss: 0.7938291966915131
avg val loss: 2.9113333225250244
LOSS train 0.79383 valid 2.91133, valid PER 23.76%
Training finished in 7.0 minutes.
Model saved to checkpoints/20231210_143007/model_6
Loading model from checkpoints/20231210_143007/model_6
SUB: 18.07%, DEL: 10.80%, INS: 2.22%, COR: 71.12%, PER: 31.10%
