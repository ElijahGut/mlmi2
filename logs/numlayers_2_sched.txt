Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=2, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 562216
EPOCH 1, Learning Rate: 0.5
  batch 50 loss: 4.727745490074158
  batch 100 loss: 3.314736485481262
  batch 150 loss: 3.248888602256775
  batch 200 loss: 3.140780863761902
  batch 250 loss: 3.0116083431243896
  batch 300 loss: 2.773570032119751
  batch 350 loss: 2.6456730699539186
  batch 400 loss: 2.5339537525177
  batch 450 loss: 2.4505387353897095
  batch 500 loss: 2.323059878349304
  batch 550 loss: 2.264143054485321
  batch 600 loss: 2.1929864287376404
  batch 650 loss: 2.100648372173309
  batch 700 loss: 2.0847812342643737
  batch 750 loss: 2.0199108791351317
  batch 800 loss: 1.9701971793174744
  batch 850 loss: 1.9141748762130737
  batch 900 loss: 1.8818057703971862
running loss: 43.70323824882507
LOSS train 1.88181 valid 1.77075, valid PER 69.24%
EPOCH 2, Learning Rate: 0.5
  batch 50 loss: 1.8403063654899596
  batch 100 loss: 1.7867108798027038
  batch 150 loss: 1.6793586325645447
  batch 200 loss: 1.68883367061615
  batch 250 loss: 1.6745177102088928
  batch 300 loss: 1.6441356706619263
  batch 350 loss: 1.62617947101593
  batch 400 loss: 1.5804648971557618
  batch 450 loss: 1.560293848514557
  batch 500 loss: 1.5557332587242128
  batch 550 loss: 1.5346476888656617
  batch 600 loss: 1.4994598484039308
  batch 650 loss: 1.447315444946289
  batch 700 loss: 1.4803401255607604
  batch 750 loss: 1.4428157019615173
  batch 800 loss: 1.3834454679489137
  batch 850 loss: 1.4160081005096437
  batch 900 loss: 1.3558216381072998
running loss: 32.85883116722107
LOSS train 1.35582 valid 1.26904, valid PER 40.81%
EPOCH 3, Learning Rate: 0.5
  batch 50 loss: 1.2947397541999817
  batch 100 loss: 1.3661126399040222
  batch 150 loss: 1.3806828236579896
  batch 200 loss: 1.2908276522159576
  batch 250 loss: 1.3153043651580811
  batch 300 loss: 1.2925228905677795
  batch 350 loss: 1.3078355669975281
  batch 400 loss: 1.289788591861725
  batch 450 loss: 1.2699754023551941
  batch 500 loss: 1.2299509096145629
  batch 550 loss: 1.2276687479019166
  batch 600 loss: 1.1780874347686767
  batch 650 loss: 1.2155432903766632
  batch 700 loss: 1.2199137127399444
  batch 750 loss: 1.2365373766422272
  batch 800 loss: 1.2441227674484252
  batch 850 loss: 1.2280955278873444
  batch 900 loss: 1.2057301437854766
running loss: 29.718733847141266
LOSS train 1.20573 valid 1.10844, valid PER 34.54%
EPOCH 4, Learning Rate: 0.5
  batch 50 loss: 1.197397528886795
  batch 100 loss: 1.1253341853618621
  batch 150 loss: 1.1502551126480103
  batch 200 loss: 1.1472328388690949
  batch 250 loss: 1.1351445746421813
  batch 300 loss: 1.13597447514534
  batch 350 loss: 1.1233584237098695
  batch 400 loss: 1.0995789563655853
  batch 450 loss: 1.1157558965682983
  batch 500 loss: 1.172387214899063
  batch 550 loss: 1.085653885602951
  batch 600 loss: 1.0887800025939942
  batch 650 loss: 1.154324884414673
  batch 700 loss: 1.1658688008785247
  batch 750 loss: 1.1007640767097473
  batch 800 loss: 1.0959721672534943
  batch 850 loss: 1.0833371007442474
  batch 900 loss: 1.07609757065773
running loss: 26.898506581783295
LOSS train 1.07610 valid 1.03797, valid PER 32.40%
EPOCH 5, Learning Rate: 0.5
  batch 50 loss: 1.0696704816818237
  batch 100 loss: 1.0448572039604187
  batch 150 loss: 1.0583989965915679
  batch 200 loss: 1.0911064159870147
  batch 250 loss: 1.0307384264469146
  batch 300 loss: 1.0596871864795685
  batch 350 loss: 1.0229579353332519
  batch 400 loss: 0.9990148520469666
  batch 450 loss: 1.0137939524650574
  batch 500 loss: 1.0045915400981904
  batch 550 loss: 1.0405839347839356
  batch 600 loss: 1.0491172766685486
  batch 650 loss: 1.053221389055252
  batch 700 loss: 1.0373069405555726
  batch 750 loss: 1.019739100933075
  batch 800 loss: 1.0526799285411834
  batch 850 loss: 1.0445462620258332
  batch 900 loss: 0.9972710943222046
running loss: 24.378163397312164
LOSS train 0.99727 valid 0.94319, valid PER 29.44%
EPOCH 6, Learning Rate: 0.5
  batch 50 loss: 0.9690515303611755
  batch 100 loss: 0.9938802373409271
  batch 150 loss: 0.980977727174759
  batch 200 loss: 0.9560684490203858
  batch 250 loss: 0.9742234170436859
  batch 300 loss: 0.9915159380435944
  batch 350 loss: 0.986782912015915
  batch 400 loss: 0.9682331371307373
  batch 450 loss: 0.9780121064186096
  batch 500 loss: 0.9519330811500549
  batch 550 loss: 0.9683762454986572
  batch 600 loss: 0.9493605482578278
  batch 650 loss: 0.9332423901557922
  batch 700 loss: 0.9464790761470795
  batch 750 loss: 0.9695059883594513
  batch 800 loss: 0.9486943888664245
  batch 850 loss: 0.9846159994602204
  batch 900 loss: 0.9965060877799988
running loss: 21.990052580833435
LOSS train 0.99651 valid 0.91878, valid PER 28.68%
EPOCH 7, Learning Rate: 0.5
  batch 50 loss: 0.9210470974445343
  batch 100 loss: 0.9730758035182953
  batch 150 loss: 0.8924821197986603
  batch 200 loss: 0.91913813829422
  batch 250 loss: 0.956995587348938
  batch 300 loss: 0.9337379682064056
  batch 350 loss: 0.9669049322605133
  batch 400 loss: 0.8902930057048798
  batch 450 loss: 0.9218405699729919
  batch 500 loss: 0.9013802385330201
  batch 550 loss: 0.908311996459961
  batch 600 loss: 0.9281795966625214
  batch 650 loss: 0.8942229855060577
  batch 700 loss: 0.9371431565284729
  batch 750 loss: 0.9132116687297821
  batch 800 loss: 0.9039262902736663
  batch 850 loss: 0.889694322347641
  batch 900 loss: 0.8829481172561645
running loss: 21.27183824777603
LOSS train 0.88295 valid 0.89298, valid PER 28.00%
EPOCH 8, Learning Rate: 0.5
  batch 50 loss: 0.894140305519104
  batch 100 loss: 0.8665752291679383
  batch 150 loss: 0.9025774884223938
  batch 200 loss: 0.87958087682724
  batch 250 loss: 0.8731454396247864
  batch 300 loss: 0.8434764885902405
  batch 350 loss: 0.8720283460617065
  batch 400 loss: 0.8573839223384857
  batch 450 loss: 0.916054458618164
  batch 500 loss: 0.8913408529758453
  batch 550 loss: 0.8879120981693268
  batch 600 loss: 0.8567719316482544
  batch 650 loss: 0.8674153470993042
  batch 700 loss: 0.8953576135635376
  batch 750 loss: 0.8724343931674957
  batch 800 loss: 0.8891674292087555
  batch 850 loss: 0.8513526523113251
  batch 900 loss: 0.8927849996089935
running loss: 21.059327363967896
LOSS train 0.89278 valid 0.88394, valid PER 27.64%
EPOCH 9, Learning Rate: 0.5
  batch 50 loss: 0.8242888379096985
  batch 100 loss: 0.8159221065044403
  batch 150 loss: 0.8401219832897187
  batch 200 loss: 0.8364473462104798
  batch 250 loss: 0.7946057462692261
  batch 300 loss: 0.8549458599090576
  batch 350 loss: 0.8033955872058869
  batch 400 loss: 0.8419976580142975
  batch 450 loss: 0.890983864068985
  batch 500 loss: 0.8334883034229279
  batch 550 loss: 0.8216977691650391
  batch 600 loss: 0.8617778611183167
  batch 650 loss: 0.854612089395523
  batch 700 loss: 0.8320270526409149
  batch 750 loss: 0.8436742568016052
  batch 800 loss: 0.8588308107852936
  batch 850 loss: 0.8526686656475068
  batch 900 loss: 0.8111257791519165
running loss: 20.384662687778473
LOSS train 0.81113 valid 0.83942, valid PER 26.08%
EPOCH 10, Learning Rate: 0.5
  batch 50 loss: 0.7999573934078217
  batch 100 loss: 0.8015765422582626
  batch 150 loss: 0.8259412729740143
  batch 200 loss: 0.7980208134651184
  batch 250 loss: 0.787314156293869
  batch 300 loss: 0.8264480197429657
  batch 350 loss: 0.7915319555997848
  batch 400 loss: 0.790506227016449
  batch 450 loss: 0.8007089924812317
  batch 500 loss: 0.7874124991893768
  batch 550 loss: 0.8063396632671356
  batch 600 loss: 0.8035927665233612
  batch 650 loss: 0.8214785182476043
  batch 700 loss: 0.8228767740726471
  batch 750 loss: 0.828275870680809
  batch 800 loss: 0.817659295797348
  batch 850 loss: 0.8158036863803864
  batch 900 loss: 0.8022843432426453
running loss: 19.954906165599823
LOSS train 0.80228 valid 0.84029, valid PER 26.05%
EPOCH 11, Learning Rate: 0.5
  batch 50 loss: 0.7728156876564026
  batch 100 loss: 0.7663921368122101
  batch 150 loss: 0.7790532314777374
  batch 200 loss: 0.7466604328155517
  batch 250 loss: 0.7535733985900879
  batch 300 loss: 0.7382343626022339
  batch 350 loss: 0.7977961856126785
  batch 400 loss: 0.7621615099906921
  batch 450 loss: 0.7775595986843109
  batch 500 loss: 0.7663152956962586
  batch 550 loss: 0.7962127614021302
  batch 600 loss: 0.7665638947486877
  batch 650 loss: 0.7955677342414856
  batch 700 loss: 0.8403680813312531
  batch 750 loss: 0.7688469958305358
  batch 800 loss: 0.7923300457000733
  batch 850 loss: 0.789041838645935
  batch 900 loss: 0.8044764423370361
running loss: 17.058671295642853
LOSS train 0.80448 valid 0.79486, valid PER 24.96%
EPOCH 12, Learning Rate: 0.5
  batch 50 loss: 0.7053017508983612
  batch 100 loss: 0.7238565057516098
  batch 150 loss: 0.7343087196350098
  batch 200 loss: 0.7466823279857635
  batch 250 loss: 0.7514187288284302
  batch 300 loss: 0.7612910497188569
  batch 350 loss: 0.7316491365432739
  batch 400 loss: 0.7738959872722626
  batch 450 loss: 0.7378708034753799
  batch 500 loss: 0.7697868120670318
  batch 550 loss: 0.7609853452444076
  batch 600 loss: 0.761287831068039
  batch 650 loss: 0.7616370761394501
  batch 700 loss: 0.7447305464744568
  batch 750 loss: 0.7721891593933106
  batch 800 loss: 0.7126986348628997
  batch 850 loss: 0.7792059695720672
  batch 900 loss: 0.7722303485870361
running loss: 18.18707573413849
LOSS train 0.77223 valid 0.83077, valid PER 25.43%
EPOCH 13, Learning Rate: 0.25
  batch 50 loss: 0.6787347388267517
  batch 100 loss: 0.675671147108078
  batch 150 loss: 0.6918226873874664
  batch 200 loss: 0.644637878537178
  batch 250 loss: 0.6548007571697235
  batch 300 loss: 0.714596482515335
  batch 350 loss: 0.6501472049951553
  batch 400 loss: 0.6572708696126938
  batch 450 loss: 0.6698174822330475
  batch 500 loss: 0.6592200261354446
  batch 550 loss: 0.6955307769775391
  batch 600 loss: 0.6728236901760102
  batch 650 loss: 0.6453601890802383
  batch 700 loss: 0.6558499991893768
  batch 750 loss: 0.6371630215644837
  batch 800 loss: 0.6657686775922775
  batch 850 loss: 0.6401177060604095
  batch 900 loss: 0.6642945384979249
running loss: 15.518920004367828
LOSS train 0.66429 valid 0.75179, valid PER 22.93%
EPOCH 14, Learning Rate: 0.25
  batch 50 loss: 0.6362388563156128
  batch 100 loss: 0.6148964804410935
  batch 150 loss: 0.6367237770557403
  batch 200 loss: 0.6291515749692916
  batch 250 loss: 0.6255310213565827
  batch 300 loss: 0.6213348805904388
  batch 350 loss: 0.6278419780731201
  batch 400 loss: 0.6633179301023483
  batch 450 loss: 0.6058320885896683
  batch 500 loss: 0.6341396987438201
  batch 550 loss: 0.6438442808389664
  batch 600 loss: 0.6197180962562561
  batch 650 loss: 0.6440228933095932
  batch 700 loss: 0.6524331033229828
  batch 750 loss: 0.6301045471429825
  batch 800 loss: 0.6227565199136734
  batch 850 loss: 0.6716653007268906
  batch 900 loss: 0.6568469744920731
running loss: 15.279228776693344
LOSS train 0.65685 valid 0.74545, valid PER 23.13%
EPOCH 15, Learning Rate: 0.25
  batch 50 loss: 0.593916449546814
  batch 100 loss: 0.5960763132572174
  batch 150 loss: 0.6126229363679886
  batch 200 loss: 0.6409539783000946
  batch 250 loss: 0.6276609092950821
  batch 300 loss: 0.6216720563173294
  batch 350 loss: 0.6152502852678299
  batch 400 loss: 0.6117567372322082
  batch 450 loss: 0.6130017614364625
  batch 500 loss: 0.6152731984853744
  batch 550 loss: 0.6460604029893875
  batch 600 loss: 0.6490567123889923
  batch 650 loss: 0.6201775121688843
  batch 700 loss: 0.6028864312171937
  batch 750 loss: 0.6369265270233154
  batch 800 loss: 0.6161346626281738
  batch 850 loss: 0.601537726521492
  batch 900 loss: 0.5896524578332901
running loss: 15.071174293756485
LOSS train 0.58965 valid 0.73935, valid PER 22.54%
EPOCH 16, Learning Rate: 0.25
  batch 50 loss: 0.6026866102218628
  batch 100 loss: 0.5677494132518768
  batch 150 loss: 0.5934584152698517
  batch 200 loss: 0.6189315783977508
  batch 250 loss: 0.6055984634160996
  batch 300 loss: 0.6081698411703109
  batch 350 loss: 0.618286463022232
  batch 400 loss: 0.6037105923891067
  batch 450 loss: 0.6173091375827789
  batch 500 loss: 0.595594630241394
  batch 550 loss: 0.5731948041915893
  batch 600 loss: 0.6296491271257401
  batch 650 loss: 0.6334437257051468
  batch 700 loss: 0.5860710054636001
  batch 750 loss: 0.6138496428728104
  batch 800 loss: 0.6055616426467896
  batch 850 loss: 0.6016466003656388
  batch 900 loss: 0.613490959405899
running loss: 14.782987713813782
LOSS train 0.61349 valid 0.75201, valid PER 22.35%
EPOCH 17, Learning Rate: 0.25
  batch 50 loss: 0.603026972413063
  batch 100 loss: 0.5462736809253692
  batch 150 loss: 0.6073618483543396
  batch 200 loss: 0.5624511212110519
  batch 250 loss: 0.5811743998527527
  batch 300 loss: 0.5741373759508133
  batch 350 loss: 0.6000212854146958
  batch 400 loss: 0.59806096971035
  batch 450 loss: 0.5694235831499099
  batch 500 loss: 0.6050663298368454
  batch 550 loss: 0.6089216637611389
  batch 600 loss: 0.6237598037719727
  batch 650 loss: 0.5856400454044342
  batch 700 loss: 0.6053318643569946
  batch 750 loss: 0.5898382896184922
  batch 800 loss: 0.5986154133081436
  batch 850 loss: 0.6082999813556671
  batch 900 loss: 0.5998883008956909
running loss: 13.430249571800232
LOSS train 0.59989 valid 0.74110, valid PER 22.14%
EPOCH 18, Learning Rate: 0.25
  batch 50 loss: 0.5440364950895309
  batch 100 loss: 0.5433648854494095
  batch 150 loss: 0.5988752996921539
  batch 200 loss: 0.5661934667825699
  batch 250 loss: 0.5646917927265167
  batch 300 loss: 0.5594793170690536
  batch 350 loss: 0.5371778535842896
  batch 400 loss: 0.5712943696975707
  batch 450 loss: 0.5765512692928314
  batch 500 loss: 0.563216215968132
  batch 550 loss: 0.6092780345678329
  batch 600 loss: 0.5817190980911255
  batch 650 loss: 0.5695749086141586
  batch 700 loss: 0.5852828574180603
  batch 750 loss: 0.5867762160301209
  batch 800 loss: 0.6169962131977081
  batch 850 loss: 0.5936726230382919
  batch 900 loss: 0.584734650850296
running loss: 13.817483365535736
LOSS train 0.58473 valid 0.73354, valid PER 21.80%
EPOCH 19, Learning Rate: 0.125
  batch 50 loss: 0.5422486609220505
  batch 100 loss: 0.5448820704221725
  batch 150 loss: 0.5241940766572952
  batch 200 loss: 0.5048821687698364
  batch 250 loss: 0.5460906946659088
  batch 300 loss: 0.5561293542385102
  batch 350 loss: 0.544089308977127
  batch 400 loss: 0.548547243475914
  batch 450 loss: 0.5027762222290039
  batch 500 loss: 0.5253623473644257
  batch 550 loss: 0.5402847683429718
  batch 600 loss: 0.5434362587332725
  batch 650 loss: 0.5556043410301208
  batch 700 loss: 0.5474140429496765
  batch 750 loss: 0.513414546251297
  batch 800 loss: 0.4938937848806381
  batch 850 loss: 0.5446516263484955
  batch 900 loss: 0.5293878412246704
running loss: 12.427278280258179
LOSS train 0.52939 valid 0.72980, valid PER 21.87%
EPOCH 20, Learning Rate: 0.125
  batch 50 loss: 0.49183141469955444
  batch 100 loss: 0.5300503385066986
  batch 150 loss: 0.501506866812706
  batch 200 loss: 0.5191939884424209
  batch 250 loss: 0.5315341931581498
  batch 300 loss: 0.515085284113884
  batch 350 loss: 0.49324406385421754
  batch 400 loss: 0.532135306596756
  batch 450 loss: 0.506380797624588
  batch 500 loss: 0.5358156073093414
  batch 550 loss: 0.5344564425945282
  batch 600 loss: 0.5069760888814926
  batch 650 loss: 0.530235406756401
  batch 700 loss: 0.4950696313381195
  batch 750 loss: 0.5067531752586365
  batch 800 loss: 0.524261035323143
  batch 850 loss: 0.5321175855398178
  batch 900 loss: 0.5157570630311966
running loss: 13.277413249015808
LOSS train 0.51576 valid 0.73399, valid PER 21.53%
Training finished in 6.0 minutes.
Model saved to checkpoints/20231210_035800/model_19
Loading model from checkpoints/20231210_035800/model_19
SUB: 14.31%, DEL: 7.98%, INS: 2.09%, COR: 77.71%, PER: 24.37%
