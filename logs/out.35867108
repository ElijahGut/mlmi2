Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=2, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.5, optimiser='sgd')
Total number of model parameters is 562216
EPOCH 1:
  batch 50 loss: 4.17263463973999
  batch 100 loss: 3.2957857275009155
  batch 150 loss: 3.2363880825042726
  batch 200 loss: 3.132430171966553
  batch 250 loss: 2.980954284667969
  batch 300 loss: 2.7632024765014647
  batch 350 loss: 2.6439935207366942
  batch 400 loss: 2.5241182470321655
  batch 450 loss: 2.446793460845947
  batch 500 loss: 2.3134785652160645
  batch 550 loss: 2.254659209251404
  batch 600 loss: 2.184742305278778
  batch 650 loss: 2.0843053579330446
  batch 700 loss: 2.0617249727249147
  batch 750 loss: 1.9789000630378724
  batch 800 loss: 1.9492655730247497
  batch 850 loss: 1.8966253662109376
  batch 900 loss: 1.8492728352546692
LOSS train 1.84927 valid 1.75975, valid PER 67.70%
EPOCH 2:
  batch 50 loss: 1.8071184062957764
  batch 100 loss: 1.7606669020652772
  batch 150 loss: 1.6319522047042847
  batch 200 loss: 1.6613667249679565
  batch 250 loss: 1.6421313405036926
  batch 300 loss: 1.6062514567375183
  batch 350 loss: 1.5794535326957702
  batch 400 loss: 1.5245821046829224
  batch 450 loss: 1.5204437255859375
  batch 500 loss: 1.5115593361854553
  batch 550 loss: 1.4868212962150573
  batch 600 loss: 1.4581596541404724
  batch 650 loss: 1.3985961508750915
  batch 700 loss: 1.4110019826889038
  batch 750 loss: 1.3719013500213624
  batch 800 loss: 1.3275825095176697
  batch 850 loss: 1.3328177833557129
  batch 900 loss: 1.296494312286377
LOSS train 1.29649 valid 1.25848, valid PER 39.21%
EPOCH 3:
  batch 50 loss: 1.2375023996829986
  batch 100 loss: 1.2874940395355225
  batch 150 loss: 1.3278556227684022
  batch 200 loss: 1.2556071043014527
  batch 250 loss: 1.2388821375370025
  batch 300 loss: 1.2348325550556183
  batch 350 loss: 1.2411797261238098
  batch 400 loss: 1.2390401029586793
  batch 450 loss: 1.2011673939228058
  batch 500 loss: 1.1691044223308564
  batch 550 loss: 1.1838054716587068
  batch 600 loss: 1.118295384645462
  batch 650 loss: 1.156123914718628
  batch 700 loss: 1.1456431591510772
  batch 750 loss: 1.1601927387714386
  batch 800 loss: 1.1813645076751709
  batch 850 loss: 1.1447635173797608
  batch 900 loss: 1.1577418279647826
LOSS train 1.15774 valid 1.09154, valid PER 32.80%
EPOCH 4:
  batch 50 loss: 1.147156572341919
  batch 100 loss: 1.0557003712654114
  batch 150 loss: 1.0986074614524841
  batch 200 loss: 1.0877573907375335
  batch 250 loss: 1.0816928672790527
  batch 300 loss: 1.1000671577453613
  batch 350 loss: 1.0852832973003388
  batch 400 loss: 1.0427735471725463
  batch 450 loss: 1.0229310274124146
  batch 500 loss: 1.136919196844101
  batch 550 loss: 1.0377275347709656
  batch 600 loss: 1.0398763406276703
  batch 650 loss: 1.1038874781131744
  batch 700 loss: 1.0887236380577088
  batch 750 loss: 1.042589497566223
  batch 800 loss: 1.0538420379161835
  batch 850 loss: 1.020221540927887
  batch 900 loss: 1.0264416873455047
LOSS train 1.02644 valid 1.01893, valid PER 31.20%
EPOCH 5:
  batch 50 loss: 1.0175497651100158
  batch 100 loss: 0.9971874642372132
  batch 150 loss: 1.0033126616477965
  batch 200 loss: 1.0306403040885925
  batch 250 loss: 0.9809775936603546
  batch 300 loss: 1.038035762310028
  batch 350 loss: 0.9684446573257446
  batch 400 loss: 0.9692393732070923
  batch 450 loss: 0.9802947652339935
  batch 500 loss: 0.9541901028156281
  batch 550 loss: 1.0122978615760803
  batch 600 loss: 1.0215438973903657
  batch 650 loss: 1.0007572317123412
  batch 700 loss: 0.9902220702171326
  batch 750 loss: 0.9742595946788788
  batch 800 loss: 1.0015102338790893
  batch 850 loss: 0.9971630871295929
  batch 900 loss: 0.9707195055484772
LOSS train 0.97072 valid 0.95649, valid PER 29.44%
EPOCH 6:
  batch 50 loss: 0.9155410182476044
  batch 100 loss: 0.9555941522121429
  batch 150 loss: 0.9372026908397675
  batch 200 loss: 0.9086984407901764
  batch 250 loss: 0.9358820903301239
  batch 300 loss: 0.958910322189331
  batch 350 loss: 0.9576577460765838
  batch 400 loss: 0.932597223520279
  batch 450 loss: 0.942463299036026
  batch 500 loss: 0.8811615657806396
  batch 550 loss: 0.898164974451065
  batch 600 loss: 0.9115472149848938
  batch 650 loss: 0.9052636098861694
  batch 700 loss: 0.9202343904972077
  batch 750 loss: 0.9474955070018768
  batch 800 loss: 0.9142869448661805
  batch 850 loss: 0.9560101652145385
  batch 900 loss: 0.9613454532623291
LOSS train 0.96135 valid 0.91055, valid PER 27.82%
EPOCH 7:
  batch 50 loss: 0.8557195520401001
  batch 100 loss: 0.9152106213569641
  batch 150 loss: 0.8474219012260437
  batch 200 loss: 0.8635207498073578
  batch 250 loss: 0.91208829164505
  batch 300 loss: 0.9147011172771454
  batch 350 loss: 0.9380943441390991
  batch 400 loss: 0.8691251397132873
  batch 450 loss: 0.8787501442432404
  batch 500 loss: 0.874576005935669
  batch 550 loss: 0.8803497004508972
  batch 600 loss: 0.8930223321914673
  batch 650 loss: 0.8644746160507202
  batch 700 loss: 0.9083685111999512
  batch 750 loss: 0.8873470544815063
  batch 800 loss: 0.8633653771877289
  batch 850 loss: 0.8778969299793243
  batch 900 loss: 0.8859643959999084
LOSS train 0.88596 valid 0.92092, valid PER 27.96%
EPOCH 8:
  batch 50 loss: 0.8717403733730316
  batch 100 loss: 0.8465971803665161
  batch 150 loss: 0.8569022977352142
  batch 200 loss: 0.8476839423179626
  batch 250 loss: 0.8539415299892426
  batch 300 loss: 0.8407442247867585
  batch 350 loss: 0.8465806698799133
  batch 400 loss: 0.8485416090488433
  batch 450 loss: 0.8788767564296722
  batch 500 loss: 0.8451741671562195
  batch 550 loss: 0.8437534701824189
  batch 600 loss: 0.832177118062973
  batch 650 loss: 0.855021436214447
  batch 700 loss: 0.8609779131412506
  batch 750 loss: 0.8756230461597443
  batch 800 loss: 0.8596995091438293
  batch 850 loss: 0.8229366135597229
  batch 900 loss: 0.8442953646183013
LOSS train 0.84430 valid 0.88449, valid PER 26.87%
EPOCH 9:
  batch 50 loss: 0.8119390475749969
  batch 100 loss: 0.7994646191596985
  batch 150 loss: 0.8070953851938247
  batch 200 loss: 0.7983710592985154
  batch 250 loss: 0.7753330361843109
  batch 300 loss: 0.81085578083992
  batch 350 loss: 0.790615513920784
  batch 400 loss: 0.8237049341201782
  batch 450 loss: 0.8239264893531799
  batch 500 loss: 0.7879223299026489
  batch 550 loss: 0.8015910959243775
  batch 600 loss: 0.813056138753891
  batch 650 loss: 0.8171846914291382
  batch 700 loss: 0.8050445544719697
  batch 750 loss: 0.8256384873390198
  batch 800 loss: 0.8345474338531494
  batch 850 loss: 0.8209063518047333
  batch 900 loss: 0.7926603055000305
LOSS train 0.79266 valid 0.86255, valid PER 26.16%
EPOCH 10:
  batch 50 loss: 0.7797946965694428
  batch 100 loss: 0.7875951594114303
  batch 150 loss: 0.8037511950731278
  batch 200 loss: 0.7827900218963623
  batch 250 loss: 0.7899572372436523
  batch 300 loss: 0.7812628769874572
  batch 350 loss: 0.7768966233730317
  batch 400 loss: 0.7680956125259399
  batch 450 loss: 0.7827016913890839
  batch 500 loss: 0.770368504524231
  batch 550 loss: 0.7731592988967896
  batch 600 loss: 0.7685723876953126
  batch 650 loss: 0.806986073255539
  batch 700 loss: 0.7960383105278015
  batch 750 loss: 0.8103909909725189
  batch 800 loss: 0.8062336659431457
  batch 850 loss: 0.7828711807727814
  batch 900 loss: 0.7725613439083099
LOSS train 0.77256 valid 0.92889, valid PER 28.99%
EPOCH 11:
  batch 50 loss: 0.7805645692348481
  batch 100 loss: 0.7550189352035522
  batch 150 loss: 0.7410766649246215
  batch 200 loss: 0.743213529586792
  batch 250 loss: 0.7450047910213471
  batch 300 loss: 0.7340938830375672
  batch 350 loss: 0.7822710990905761
  batch 400 loss: 0.7322048711776733
  batch 450 loss: 0.7289819186925888
  batch 500 loss: 0.741535474061966
  batch 550 loss: 0.8224554777145385
  batch 600 loss: 0.7908323359489441
  batch 650 loss: 0.7865590620040893
  batch 700 loss: 0.8535167276859283
  batch 750 loss: 0.7590300118923188
  batch 800 loss: 0.7750773131847382
  batch 850 loss: 0.7726650512218476
  batch 900 loss: 0.7842307221889496
LOSS train 0.78423 valid 0.84664, valid PER 25.78%
EPOCH 12:
  batch 50 loss: 0.7025813937187195
  batch 100 loss: 0.7013053894042969
  batch 150 loss: 0.708140766620636
  batch 200 loss: 0.7215348768234253
  batch 250 loss: 0.7080991679430008
  batch 300 loss: 0.8097265064716339
  batch 350 loss: 0.8033735346794129
  batch 400 loss: 0.8020352566242218
  batch 450 loss: 0.7462858593463898
  batch 500 loss: 0.7525163650512695
  batch 550 loss: 0.7635352551937103
  batch 600 loss: 0.760757964849472
  batch 650 loss: 0.7588848531246185
  batch 700 loss: 0.7373086392879487
  batch 750 loss: 0.756108067035675
  batch 800 loss: 0.7432784318923951
  batch 850 loss: 0.775881364941597
  batch 900 loss: 0.7684808146953582
LOSS train 0.76848 valid 0.83988, valid PER 25.34%
EPOCH 13:
  batch 50 loss: 0.6994072520732879
  batch 100 loss: 0.7196793329715728
  batch 150 loss: 0.7308386236429214
  batch 200 loss: 0.688392830491066
  batch 250 loss: 0.6955099284648896
  batch 300 loss: 0.7325638389587402
  batch 350 loss: 0.6938597846031189
  batch 400 loss: 0.7097723454236984
  batch 450 loss: 0.7164591073989868
  batch 500 loss: 0.6979245078563691
  batch 550 loss: 0.7614818423986435
  batch 600 loss: 0.7342133915424347
  batch 650 loss: 0.7149907547235489
  batch 700 loss: 0.7340676319599152
  batch 750 loss: 0.6725826245546341
  batch 800 loss: 0.7204035353660584
  batch 850 loss: 0.7090465521812439
  batch 900 loss: 0.7357237792015076
LOSS train 0.73572 valid 0.82436, valid PER 24.52%
EPOCH 14:
  batch 50 loss: 0.6968139064311981
  batch 100 loss: 0.6610406357049942
  batch 150 loss: 0.6761171519756317
  batch 200 loss: 0.685748074054718
  batch 250 loss: 0.6575406467914582
  batch 300 loss: 0.6719749999046326
  batch 350 loss: 0.6803611040115356
  batch 400 loss: 0.7009242308139801
  batch 450 loss: 0.665158097743988
  batch 500 loss: 0.6965256643295288
  batch 550 loss: 0.7033596593141556
  batch 600 loss: 0.6734059482812882
  batch 650 loss: 0.6968983781337738
  batch 700 loss: 0.6918100893497467
  batch 750 loss: 0.6920275300741195
  batch 800 loss: 0.6875544899702072
  batch 850 loss: 0.722670909166336
  batch 900 loss: 0.7180541902780533
LOSS train 0.71805 valid 0.80432, valid PER 24.18%
EPOCH 15:
  batch 50 loss: 0.6317488944530487
  batch 100 loss: 0.6421888893842698
  batch 150 loss: 0.6653565323352814
  batch 200 loss: 0.6897036266326905
  batch 250 loss: 0.6835973471403122
  batch 300 loss: 0.6761182284355164
  batch 350 loss: 0.6725419688224793
  batch 400 loss: 0.6776509898900985
  batch 450 loss: 0.658297136425972
  batch 500 loss: 0.6634593588113785
  batch 550 loss: 0.706367005109787
  batch 600 loss: 0.7133517670631409
  batch 650 loss: 0.6570452183485032
  batch 700 loss: 0.6657303196191787
  batch 750 loss: 0.6818218320608139
  batch 800 loss: 0.6650245970487595
  batch 850 loss: 0.6811223727464676
  batch 900 loss: 0.6536597108840942
LOSS train 0.65366 valid 0.83479, valid PER 25.34%
EPOCH 16:
  batch 50 loss: 0.6569594269990922
  batch 100 loss: 0.6380084329843521
  batch 150 loss: 0.6733490306138993
  batch 200 loss: 0.6693023169040679
  batch 250 loss: 0.646661177277565
  batch 300 loss: 0.6680310541391372
  batch 350 loss: 0.6660253143310547
  batch 400 loss: 0.662258415222168
  batch 450 loss: 0.67122414290905
  batch 500 loss: 0.6589451837539673
  batch 550 loss: 0.6645457810163498
  batch 600 loss: 0.7074280512332917
  batch 650 loss: 0.6768016797304154
  batch 700 loss: 0.6400458872318268
  batch 750 loss: 0.6941572409868241
  batch 800 loss: 0.648716413974762
  batch 850 loss: 0.6416638016700744
  batch 900 loss: 0.6605381989479064
LOSS train 0.66054 valid 0.81868, valid PER 24.86%
EPOCH 17:
  batch 50 loss: 0.6583929747343064
  batch 100 loss: 0.6035117977857589
  batch 150 loss: 0.6714027190208435
  batch 200 loss: 0.639055027961731
  batch 250 loss: 0.6335170900821686
  batch 300 loss: 0.6252925246953964
  batch 350 loss: 0.6491185706853867
  batch 400 loss: 0.6360948097705841
  batch 450 loss: 0.6574516546726227
  batch 500 loss: 0.6390519714355469
  batch 550 loss: 0.656560565829277
  batch 600 loss: 0.6668612551689148
  batch 650 loss: 0.6459219914674759
  batch 700 loss: 0.6851074069738388
  batch 750 loss: 0.6215293228626251
  batch 800 loss: 0.668682468533516
  batch 850 loss: 0.6631036686897278
  batch 900 loss: 0.6447115075588227
LOSS train 0.64471 valid 0.80946, valid PER 23.81%
EPOCH 18:
  batch 50 loss: 0.6249783051013946
  batch 100 loss: 0.6173211359977722
  batch 150 loss: 0.6895071363449097
  batch 200 loss: 0.6725189775228501
  batch 250 loss: 0.647467416524887
  batch 300 loss: 0.6739656466245652
  batch 350 loss: 0.6198253816366196
  batch 400 loss: 0.6576767015457153
  batch 450 loss: 0.6797580391168594
  batch 500 loss: 0.6472118365764618
  batch 550 loss: 0.6801648038625717
  batch 600 loss: 0.6420175075531006
  batch 650 loss: 0.6307959628105163
  batch 700 loss: 0.6268670284748077
  batch 750 loss: 0.6619296205043793
  batch 800 loss: 0.677498334646225
  batch 850 loss: 0.6524384033679962
  batch 900 loss: 0.6500874537229538
LOSS train 0.65009 valid 0.77423, valid PER 23.20%
EPOCH 19:
  batch 50 loss: 0.5845200282335281
  batch 100 loss: 0.5988538390398026
  batch 150 loss: 0.5908122390508652
  batch 200 loss: 0.5794744545221329
  batch 250 loss: 0.6442196100950242
  batch 300 loss: 0.6272619843482972
  batch 350 loss: 0.6177890408039093
  batch 400 loss: 0.600661917924881
  batch 450 loss: 0.5778533560037613
  batch 500 loss: 0.6152774661779403
  batch 550 loss: 0.6396625220775605
  batch 600 loss: 0.6634069204330444
  batch 650 loss: 0.6447734969854355
  batch 700 loss: 0.6579637485742569
  batch 750 loss: 0.6075761157274246
  batch 800 loss: 0.6056797873973846
  batch 850 loss: 0.6288256967067718
  batch 900 loss: 0.6288087147474289
LOSS train 0.62881 valid 0.79137, valid PER 23.51%
EPOCH 20:
  batch 50 loss: 0.5463828223943711
  batch 100 loss: 0.5947003233432769
  batch 150 loss: 0.5833871126174927
  batch 200 loss: 0.5992892009019851
  batch 250 loss: 0.6274485981464386
  batch 300 loss: 0.6160813617706299
  batch 350 loss: 0.5742840105295182
  batch 400 loss: 0.6096114987134933
  batch 450 loss: 0.5820079451799393
  batch 500 loss: 0.6040408080816269
  batch 550 loss: 0.6226392209529876
  batch 600 loss: 0.6020541912317277
  batch 650 loss: 0.6164432901144028
  batch 700 loss: 0.590049940943718
  batch 750 loss: 0.5819830095767975
  batch 800 loss: 0.6136294758319855
  batch 850 loss: 0.6189016073942184
  batch 900 loss: 0.6114249020814896
LOSS train 0.61142 valid 0.77369, valid PER 23.11%
Training finished in 6.0 minutes.
Model saved to checkpoints/20231204_190357/model_20
Loading model from checkpoints/20231204_190357/model_20
SUB: 15.38%, DEL: 7.58%, INS: 2.11%, COR: 77.05%, PER: 25.07%
