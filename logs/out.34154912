Namespace(seed=0, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.1, optimiser='sgd')
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 3.989341826438904
  batch 100 loss: 3.346375346183777
  batch 150 loss: 3.280289235115051
  batch 200 loss: 3.07484450340271
  batch 250 loss: 2.843814296722412
  batch 300 loss: 2.6525011014938356
  batch 350 loss: 2.5578973293304443
  batch 400 loss: 2.4320875644683837
  batch 450 loss: 2.343057255744934
  batch 500 loss: 2.259300801753998
  batch 550 loss: 2.17319091796875
  batch 600 loss: 2.13227326631546
  batch 650 loss: 2.066383354663849
  batch 700 loss: 2.004030394554138
  batch 750 loss: 1.9329038453102112
  batch 800 loss: 1.9166460514068604
  batch 850 loss: 1.8482248854637147
  batch 900 loss: 1.8058580350875855
LOSS train 1.80586 valid 1.74539, valid PER 62.81%
EPOCH 2:
  batch 50 loss: 1.7440425729751587
  batch 100 loss: 1.6736338353157043
  batch 150 loss: 1.6813922810554505
  batch 200 loss: 1.6416723489761353
  batch 250 loss: 1.6256302762031556
  batch 300 loss: 1.6234672045707703
  batch 350 loss: 1.6164095759391786
  batch 400 loss: 1.5171799015998841
  batch 450 loss: 1.5566227412223816
  batch 500 loss: 1.533156590461731
  batch 550 loss: 1.4729230618476867
  batch 600 loss: 1.5054775500297546
  batch 650 loss: 1.4572782802581787
  batch 700 loss: 1.4742740416526794
  batch 750 loss: 1.4023354458808899
  batch 800 loss: 1.4762294173240662
  batch 850 loss: 1.446707956790924
  batch 900 loss: 1.4019543790817262
LOSS train 1.40195 valid 1.44901, valid PER 47.24%
EPOCH 3:
  batch 50 loss: 1.3546103262901306
  batch 100 loss: 1.3188078022003173
  batch 150 loss: 1.3075754976272582
  batch 200 loss: 1.3218899166584015
  batch 250 loss: 1.3017748010158539
  batch 300 loss: 1.3023012280464172
  batch 350 loss: 1.3660011911392211
  batch 400 loss: 1.2793672955036164
  batch 450 loss: 1.236405122280121
  batch 500 loss: 1.2545979976654054
  batch 550 loss: 1.247383508682251
  batch 600 loss: 1.2639676916599274
  batch 650 loss: 1.2520034074783326
  batch 700 loss: 1.248636453151703
  batch 750 loss: 1.2226300728321076
  batch 800 loss: 1.2416577458381652
  batch 850 loss: 1.239651255607605
  batch 900 loss: 1.2301073849201203
LOSS train 1.23011 valid 1.20370, valid PER 38.63%
EPOCH 4:
  batch 50 loss: 1.188571845293045
  batch 100 loss: 1.207718986272812
  batch 150 loss: 1.1628748512268066
  batch 200 loss: 1.1843452501296996
  batch 250 loss: 1.1542212462425232
  batch 300 loss: 1.0977003264427185
  batch 350 loss: 1.1177924489974975
  batch 400 loss: 1.1654618728160857
  batch 450 loss: 1.1198231840133668
  batch 500 loss: 1.1700708019733428
  batch 550 loss: 1.179834887981415
  batch 600 loss: 1.15309019446373
  batch 650 loss: 1.1627589869499206
  batch 700 loss: 1.1096378815174104
  batch 750 loss: 1.0868866229057312
  batch 800 loss: 1.1189914333820343
  batch 850 loss: 1.090381909608841
  batch 900 loss: 1.1401243913173675
LOSS train 1.14012 valid 1.17722, valid PER 35.64%
EPOCH 5:
  batch 50 loss: 1.0672761416435241
  batch 100 loss: 1.0421947729587555
  batch 150 loss: 1.1057780647277833
  batch 200 loss: 1.0572916662693024
  batch 250 loss: 1.0858441877365113
  batch 300 loss: 1.0542666459083556
  batch 350 loss: 1.092645797729492
  batch 400 loss: 1.0460780096054076
  batch 450 loss: 1.0598426163196564
  batch 500 loss: 1.105297544002533
  batch 550 loss: 1.047302199602127
  batch 600 loss: 1.1073459553718568
  batch 650 loss: 1.1266831612586976
  batch 700 loss: 1.0688749873638153
  batch 750 loss: 1.0643026208877564
  batch 800 loss: 1.049246678352356
  batch 850 loss: 1.0891402602195739
  batch 900 loss: 1.0628838384151458
LOSS train 1.06288 valid 1.09134, valid PER 34.96%
EPOCH 6:
  batch 50 loss: 1.0303270268440246
  batch 100 loss: 1.0360738801956177
  batch 150 loss: 1.007358751296997
  batch 200 loss: 1.04205264210701
  batch 250 loss: 1.0213231062889099
  batch 300 loss: 0.9967586350440979
  batch 350 loss: 1.044244865179062
  batch 400 loss: 1.0080587029457093
  batch 450 loss: 0.9986931109428405
  batch 500 loss: 1.0029455459117889
  batch 550 loss: 0.9886718165874481
  batch 600 loss: 0.9913013100624084
  batch 650 loss: 1.0416799485683441
  batch 700 loss: 1.0245484232902526
  batch 750 loss: 0.9916636705398559
  batch 800 loss: 1.0143125247955322
  batch 850 loss: 0.9969684243202209
  batch 900 loss: 0.9489407563209533
LOSS train 0.94894 valid 1.06677, valid PER 32.93%
EPOCH 7:
  batch 50 loss: 0.9732337403297424
  batch 100 loss: 0.9757150435447692
  batch 150 loss: 0.944949996471405
  batch 200 loss: 0.9284262204170227
  batch 250 loss: 0.9434911108016968
  batch 300 loss: 0.961468162536621
  batch 350 loss: 0.9750656032562256
  batch 400 loss: 0.9348876333236694
  batch 450 loss: 0.9987205231189727
  batch 500 loss: 0.9778485429286957
  batch 550 loss: 0.9726100659370422
  batch 600 loss: 0.9973897457122802
  batch 650 loss: 0.9625245356559753
  batch 700 loss: 0.9755082225799561
  batch 750 loss: 0.9904227364063263
  batch 800 loss: 0.9342468988895416
  batch 850 loss: 0.9524421226978302
  batch 900 loss: 0.9556541776657105
LOSS train 0.95565 valid 1.04842, valid PER 33.13%
EPOCH 8:
  batch 50 loss: 0.9016685450077057
  batch 100 loss: 1.0287306606769562
  batch 150 loss: 1.0103495442867279
  batch 200 loss: 0.9599716699123383
  batch 250 loss: 0.9175386881828308
  batch 300 loss: 0.9503786289691925
  batch 350 loss: 0.9516973662376403
  batch 400 loss: 0.9046649634838104
  batch 450 loss: 0.9717741513252258
  batch 500 loss: 0.973434065580368
  batch 550 loss: 0.9304425573348999
  batch 600 loss: 0.9722242271900177
  batch 650 loss: 0.9242735409736633
  batch 700 loss: 0.8974378776550292
  batch 750 loss: 0.9422035491466523
  batch 800 loss: 0.9140056014060974
  batch 850 loss: 0.9530885887145996
  batch 900 loss: 0.946760630607605
LOSS train 0.94676 valid 1.01131, valid PER 32.31%
EPOCH 9:
  batch 50 loss: 0.8601964843273163
  batch 100 loss: 0.9042293560504914
  batch 150 loss: 0.8995754110813141
  batch 200 loss: 0.9090125107765198
  batch 250 loss: 0.9480913841724395
  batch 300 loss: 0.8815649485588074
  batch 350 loss: 0.881090166568756
  batch 400 loss: 0.9059244394302368
  batch 450 loss: 0.9002441942691803
  batch 500 loss: 0.8837250864505768
  batch 550 loss: 0.8774341905117035
  batch 600 loss: 0.8708519744873047
  batch 650 loss: 0.9215049433708191
  batch 700 loss: 0.8999440205097199
  batch 750 loss: 0.9161283922195435
  batch 800 loss: 0.8820367276668548
  batch 850 loss: 0.9093995308876037
  batch 900 loss: 0.9320771133899689
LOSS train 0.93208 valid 0.99850, valid PER 30.84%
EPOCH 10:
  batch 50 loss: 0.8594854128360748
  batch 100 loss: 0.8335297560691833
  batch 150 loss: 0.8400272166728974
  batch 200 loss: 0.8434866309165955
  batch 250 loss: 0.9069199919700622
  batch 300 loss: 0.8796710884571075
  batch 350 loss: 0.8514164626598358
  batch 400 loss: 0.8818460369110107
  batch 450 loss: 0.923023978471756
  batch 500 loss: 0.8368206298351288
  batch 550 loss: 0.8591003727912903
  batch 600 loss: 0.8925815403461457
  batch 650 loss: 0.8300848174095153
  batch 700 loss: 0.8985629808902741
  batch 750 loss: 0.8821246778964996
  batch 800 loss: 0.8819147336483002
  batch 850 loss: 0.865197845697403
  batch 900 loss: 0.8825622856616974
LOSS train 0.88256 valid 0.97168, valid PER 30.77%
EPOCH 11:
  batch 50 loss: 0.8360657918453217
  batch 100 loss: 0.8572203385829925
  batch 150 loss: 0.8344694125652313
  batch 200 loss: 0.8557604432106019
  batch 250 loss: 0.843811365365982
  batch 300 loss: 0.8450527000427246
  batch 350 loss: 0.8616951656341553
  batch 400 loss: 0.830380666255951
  batch 450 loss: 0.8317325675487518
  batch 500 loss: 0.8473103451728821
  batch 550 loss: 0.8439108896255493
  batch 600 loss: 0.8674140751361847
  batch 650 loss: 0.897742303609848
  batch 700 loss: 0.8781882536411285
  batch 750 loss: 0.8421210682392121
  batch 800 loss: 0.8545898282527924
  batch 850 loss: 0.8528939378261566
  batch 900 loss: 0.869934504032135
LOSS train 0.86993 valid 1.01305, valid PER 31.02%
EPOCH 12:
  batch 50 loss: 0.8021165347099304
  batch 100 loss: 0.9173955857753754
  batch 150 loss: 0.867120326757431
  batch 200 loss: 0.8462743544578553
  batch 250 loss: 0.8604438161849975
  batch 300 loss: 0.8328297460079193
  batch 350 loss: 0.8618007814884185
  batch 400 loss: 0.8235477471351623
  batch 450 loss: 0.8436714005470276
  batch 500 loss: 0.8104017841815948
  batch 550 loss: 0.8626758551597595
  batch 600 loss: 0.8541732943058014
  batch 650 loss: 0.8412812614440918
  batch 700 loss: 0.8206969738006592
  batch 750 loss: 0.8466860246658325
  batch 800 loss: 0.8071907711029053
  batch 850 loss: 0.8513586390018463
  batch 900 loss: 0.8537438297271729
LOSS train 0.85374 valid 1.02393, valid PER 31.72%
EPOCH 13:
  batch 50 loss: 0.8104692566394806
  batch 100 loss: 0.8149905896186829
  batch 150 loss: 0.8264257043600083
  batch 200 loss: 0.8005871379375458
  batch 250 loss: 0.8223407065868378
  batch 300 loss: 0.803186286687851
  batch 350 loss: 0.8249360537528991
  batch 400 loss: 0.7970142006874085
  batch 450 loss: 0.7921022272109985
  batch 500 loss: 0.7922176194190979
  batch 550 loss: 0.7796468436717987
  batch 600 loss: 0.8114491391181946
  batch 650 loss: 0.810332624912262
  batch 700 loss: 0.8199225592613221
  batch 750 loss: 0.8022637641429902
  batch 800 loss: 0.8194653809070587
  batch 850 loss: 0.8260837626457215
  batch 900 loss: 0.845493243932724
LOSS train 0.84549 valid 0.97186, valid PER 29.18%
EPOCH 14:
  batch 50 loss: 0.75152801156044
  batch 100 loss: 0.7847968685626984
  batch 150 loss: 0.8373395049571991
  batch 200 loss: 0.7580817556381225
  batch 250 loss: 0.8098770701885223
  batch 300 loss: 0.8724260377883911
  batch 350 loss: 0.8273192918300629
  batch 400 loss: 0.8006105250120163
  batch 450 loss: 0.7963558793067932
  batch 500 loss: 0.7912414038181305
  batch 550 loss: 0.7861739468574523
  batch 600 loss: 0.7745627522468567
  batch 650 loss: 0.8014393019676208
  batch 700 loss: 0.7846494817733765
  batch 750 loss: 0.8161792039871216
  batch 800 loss: 0.8424379360675812
  batch 850 loss: 0.8619955277442932
  batch 900 loss: 0.8300152838230133
LOSS train 0.83002 valid 0.99071, valid PER 30.81%
EPOCH 15:
  batch 50 loss: 0.759495393037796
  batch 100 loss: 0.7301242089271546
  batch 150 loss: 0.7558215457201004
  batch 200 loss: 0.7348656833171845
  batch 250 loss: 0.7348659163713456
  batch 300 loss: 0.771880692243576
  batch 350 loss: 0.7684088003635406
  batch 400 loss: 0.7644701182842255
  batch 450 loss: 0.7855277156829834
  batch 500 loss: 0.7567361223697663
  batch 550 loss: 0.7812179481983185
  batch 600 loss: 0.7483306735754013
  batch 650 loss: 0.7823558938503266
  batch 700 loss: 0.7860625100135803
  batch 750 loss: 0.7709022629261016
  batch 800 loss: 0.788405841588974
  batch 850 loss: 0.8129441237449646
  batch 900 loss: 0.8036606073379516
LOSS train 0.80366 valid 0.97975, valid PER 30.67%
EPOCH 16:
  batch 50 loss: 0.7570593237876893
  batch 100 loss: 0.7386159992218018
  batch 150 loss: 0.7553316760063171
  batch 200 loss: 0.74289475440979
  batch 250 loss: 0.781894822716713
  batch 300 loss: 0.7571555507183075
  batch 350 loss: 0.7721893644332886
  batch 400 loss: 0.726413232088089
  batch 450 loss: 0.7555617690086365
  batch 500 loss: 0.7387390387058258
  batch 550 loss: 0.7787123060226441
  batch 600 loss: 0.7832780969142914
  batch 650 loss: 0.7466015923023224
  batch 700 loss: 0.738847524523735
  batch 750 loss: 0.7578377950191498
  batch 800 loss: 0.7400253164768219
  batch 850 loss: 0.7399083578586578
  batch 900 loss: 0.7528910267353058
LOSS train 0.75289 valid 1.19963, valid PER 32.01%
EPOCH 17:
  batch 50 loss: 0.808559490442276
  batch 100 loss: 0.7273527765274048
  batch 150 loss: 0.734558892250061
  batch 200 loss: 0.7315735948085785
  batch 250 loss: 0.7406856673955917
  batch 300 loss: 0.7321527647972107
  batch 350 loss: 0.7561399352550506
  batch 400 loss: 0.711267558336258
  batch 450 loss: 0.7472551548480988
  batch 500 loss: 0.7232346665859223
  batch 550 loss: 0.7230756092071533
  batch 600 loss: 0.7474316376447677
  batch 650 loss: 0.7631723248958587
  batch 700 loss: 0.7651960718631744
  batch 750 loss: 0.796881902217865
  batch 800 loss: 0.7493392133712768
  batch 850 loss: 0.7607543325424194
  batch 900 loss: 0.7689107877016067
LOSS train 0.76891 valid 0.96270, valid PER 29.66%
EPOCH 18:
  batch 50 loss: 0.6740637767314911
  batch 100 loss: 0.7292139947414398
  batch 150 loss: 0.7413380312919616
  batch 200 loss: 0.7033500277996063
  batch 250 loss: 0.7183521521091462
  batch 300 loss: 0.7260247600078583
  batch 350 loss: 0.7111759746074676
  batch 400 loss: 0.7396342897415161
  batch 450 loss: 0.7107288241386414
  batch 500 loss: 0.7455330944061279
  batch 550 loss: 0.7798363327980041
  batch 600 loss: 0.7930036056041717
  batch 650 loss: 0.7708625197410583
  batch 700 loss: 0.7730464136600494
  batch 750 loss: 0.6971224343776703
  batch 800 loss: 0.7377346551418305
  batch 850 loss: 0.7074354499578476
  batch 900 loss: 0.7485580039024353
LOSS train 0.74856 valid 0.93347, valid PER 28.89%
EPOCH 19:
  batch 50 loss: 0.6656916034221649
  batch 100 loss: 0.6891898453235626
  batch 150 loss: 0.6736104315519333
  batch 200 loss: 0.6776531058549881
  batch 250 loss: 0.698370960354805
  batch 300 loss: 0.7339289450645446
  batch 350 loss: 0.7034183299541473
  batch 400 loss: 0.7177356535196304
  batch 450 loss: 0.7086945635080337
  batch 500 loss: 0.85139100253582
  batch 550 loss: 1.1858690857887269
  batch 600 loss: 0.9590177154541015
  batch 650 loss: 0.9698505413532257
  batch 700 loss: 0.903194854259491
  batch 750 loss: 0.9203576362133026
  batch 800 loss: 0.8874310004711151
  batch 850 loss: 0.8827897453308106
  batch 900 loss: 0.8798587167263031
LOSS train 0.87986 valid 1.03889, valid PER 31.22%
EPOCH 20:
  batch 50 loss: 0.7813673651218415
  batch 100 loss: 0.7637561690807343
  batch 150 loss: 0.8035636842250824
  batch 200 loss: 0.7955310332775116
  batch 250 loss: 0.7925603234767914
  batch 300 loss: 0.7843201589584351
  batch 350 loss: 0.7649152684211731
  batch 400 loss: 0.8347605967521667
  batch 450 loss: 0.8120696151256561
  batch 500 loss: 0.7999888598918915
  batch 550 loss: 0.8519441270828247
  batch 600 loss: 0.8106219428777695
  batch 650 loss: 0.7888909184932709
  batch 700 loss: 0.8477468025684357
  batch 750 loss: 0.8255116260051727
  batch 800 loss: 0.7783902764320374
  batch 850 loss: 0.7894878071546555
  batch 900 loss: 0.8657183980941773
LOSS train 0.86572 valid 1.03192, valid PER 30.66%
Training finished in 5.0 minutes.
Model saved to checkpoints/20231129_163341/model_18
Loading model from checkpoints/20231129_163341/model_18
SUB: 17.03%, DEL: 11.04%, INS: 2.34%, COR: 71.93%, PER: 30.40%
