Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.01, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='adam', grad_clip=0.5)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.246083645820618
  batch 100 loss: 2.714777584075928
  batch 150 loss: 2.383567714691162
  batch 200 loss: 2.1822765946388243
  batch 250 loss: 2.064671313762665
  batch 300 loss: 1.9327550792694093
  batch 350 loss: 1.8195725417137145
  batch 400 loss: 1.8652956628799437
  batch 450 loss: 1.7794718670845031
  batch 500 loss: 1.751639666557312
  batch 550 loss: 1.699792652130127
  batch 600 loss: 1.6909881258010864
  batch 650 loss: 1.6106361651420593
  batch 700 loss: 1.638902053833008
  batch 750 loss: 1.6263016533851624
  batch 800 loss: 1.6260404300689697
  batch 850 loss: 1.5808158898353577
  batch 900 loss: 1.5728563952445984
LOSS train 1.57286 valid 1.48763, valid PER 49.13%
EPOCH 2:
  batch 50 loss: 1.518035855293274
  batch 100 loss: 1.478699095249176
  batch 150 loss: 1.4564740324020387
  batch 200 loss: 1.5237642478942872
  batch 250 loss: 1.4986351537704468
  batch 300 loss: 1.5122473120689393
  batch 350 loss: 1.4102438116073608
  batch 400 loss: 1.4638407301902772
  batch 450 loss: 1.4138394451141358
  batch 500 loss: 1.4606280493736268
  batch 550 loss: 1.4906266403198243
  batch 600 loss: 1.401670618057251
  batch 650 loss: 1.4467450666427613
  batch 700 loss: 1.4425331616401673
  batch 750 loss: 1.4136950945854188
  batch 800 loss: 1.4498925518989563
  batch 850 loss: 1.4217808890342711
  batch 900 loss: 1.488512098789215
LOSS train 1.48851 valid 1.37188, valid PER 44.52%
EPOCH 3:
  batch 50 loss: 1.4395186305046082
  batch 100 loss: 1.3841690516471863
  batch 150 loss: 1.3771439695358276
  batch 200 loss: 1.3755057072639465
  batch 250 loss: 1.3724883961677552
  batch 300 loss: 1.3521689534187318
  batch 350 loss: 1.4169944357872009
  batch 400 loss: 1.3796703016757965
  batch 450 loss: 1.3702412414550782
  batch 500 loss: 1.3955798959732055
  batch 550 loss: 1.3633333706855775
  batch 600 loss: 1.3431912076473236
  batch 650 loss: 1.3074515056610108
  batch 700 loss: 1.36039231300354
  batch 750 loss: 1.4027294039726257
  batch 800 loss: 1.3281458306312561
  batch 850 loss: 1.3736782240867615
  batch 900 loss: 1.3514250636100769
LOSS train 1.35143 valid 1.31586, valid PER 41.99%
EPOCH 4:
  batch 50 loss: 1.3127617597579957
  batch 100 loss: 1.3263281059265137
  batch 150 loss: 1.2612237441539764
  batch 200 loss: 1.329268513917923
  batch 250 loss: 1.3637263357639313
  batch 300 loss: 1.3937708973884582
  batch 350 loss: 1.3792433333396912
  batch 400 loss: 1.3579881405830383
  batch 450 loss: 1.2978117156028748
  batch 500 loss: 1.3057422375679015
  batch 550 loss: 1.322659001350403
  batch 600 loss: 1.3738507151603698
  batch 650 loss: 1.3333044385910033
  batch 700 loss: 1.337484221458435
  batch 750 loss: 1.313094265460968
  batch 800 loss: 1.2932008969783784
  batch 850 loss: 1.3377872467041017
  batch 900 loss: 1.336631360054016
LOSS train 1.33663 valid 1.27326, valid PER 39.77%
EPOCH 5:
  batch 50 loss: 1.275118272304535
  batch 100 loss: 1.319974443912506
  batch 150 loss: 1.3383807921409607
  batch 200 loss: 1.258791868686676
  batch 250 loss: 1.2542398869991302
  batch 300 loss: 1.2844158565998078
  batch 350 loss: 1.282397346496582
  batch 400 loss: 1.3004796695709229
  batch 450 loss: 1.297693076133728
  batch 500 loss: 1.3196258521080018
  batch 550 loss: 1.2606022763252258
  batch 600 loss: 1.3210041117668152
  batch 650 loss: 1.3056734430789947
  batch 700 loss: 1.3355668950080872
  batch 750 loss: 1.250518287420273
  batch 800 loss: 1.2754478859901428
  batch 850 loss: 1.3090377581119537
  batch 900 loss: 1.3003324818611146
LOSS train 1.30033 valid 1.22047, valid PER 38.64%
EPOCH 6:
  batch 50 loss: 1.2898115158081054
  batch 100 loss: 1.2217542827129364
  batch 150 loss: 1.1991989779472352
  batch 200 loss: 1.2551346850395202
  batch 250 loss: 1.2910768294334412
  batch 300 loss: 1.2608234012126922
  batch 350 loss: 1.264474115371704
  batch 400 loss: 1.2907793378829957
  batch 450 loss: 1.3384951555728912
  batch 500 loss: 1.2735953164100646
  batch 550 loss: 1.2814363598823548
  batch 600 loss: 1.2384395623207092
  batch 650 loss: 1.2803740227222442
  batch 700 loss: 1.273344316482544
  batch 750 loss: 1.2855509769916535
  batch 800 loss: 1.2628879821300507
  batch 850 loss: 1.257062691450119
  batch 900 loss: 1.280287537574768
LOSS train 1.28029 valid 1.22910, valid PER 39.43%
EPOCH 7:
  batch 50 loss: 1.2455786633491517
  batch 100 loss: 1.2873097634315491
  batch 150 loss: 1.2470316076278687
  batch 200 loss: 1.2368150055408478
  batch 250 loss: 1.227641032934189
  batch 300 loss: 1.2368165338039399
  batch 350 loss: 1.2298596143722533
  batch 400 loss: 1.2656296491622925
  batch 450 loss: 1.258080826997757
  batch 500 loss: 1.2349494886398316
  batch 550 loss: 1.256263542175293
  batch 600 loss: 1.2726361203193663
  batch 650 loss: 1.223761224746704
  batch 700 loss: 1.2696604382991792
  batch 750 loss: 1.2051571929454803
  batch 800 loss: 1.2151482594013214
  batch 850 loss: 1.2441508412361144
  batch 900 loss: 1.2918775916099547
LOSS train 1.29188 valid 1.22102, valid PER 38.53%
EPOCH 8:
  batch 50 loss: 1.241505618095398
  batch 100 loss: 1.220225328207016
  batch 150 loss: 1.2106747007369996
  batch 200 loss: 1.1898245632648468
  batch 250 loss: 1.2111931574344634
  batch 300 loss: 1.1513578152656556
  batch 350 loss: 1.2579197883605957
  batch 400 loss: 1.2330502963066101
  batch 450 loss: 1.264216275215149
  batch 500 loss: 1.278455355167389
  batch 550 loss: 1.228936768770218
  batch 600 loss: 1.2682321524620057
  batch 650 loss: 1.2930826902389527
  batch 700 loss: 1.2312088227272033
  batch 750 loss: 1.258379545211792
  batch 800 loss: 1.2626618576049804
  batch 850 loss: 1.2428734958171845
  batch 900 loss: 1.2418476128578186
LOSS train 1.24185 valid 1.20268, valid PER 38.14%
EPOCH 9:
  batch 50 loss: 1.2031058299541473
  batch 100 loss: 1.2292749536037446
  batch 150 loss: 1.2273875761032105
  batch 200 loss: 1.187235370874405
  batch 250 loss: 1.2225522017478943
  batch 300 loss: 1.2198757243156433
  batch 350 loss: 1.2444435095787048
  batch 400 loss: 1.2247621619701385
  batch 450 loss: 1.253454020023346
  batch 500 loss: 1.1761259937286377
  batch 550 loss: 1.2231763792037964
  batch 600 loss: 1.2674160242080688
  batch 650 loss: 1.259926335811615
  batch 700 loss: 1.2034790444374084
  batch 750 loss: 1.2304656934738158
  batch 800 loss: 1.255141830444336
  batch 850 loss: 1.2696761584281921
  batch 900 loss: 1.2458694195747375
LOSS train 1.24587 valid 1.20207, valid PER 38.72%
EPOCH 10:
  batch 50 loss: 1.205415997505188
  batch 100 loss: 1.2430979704856873
  batch 150 loss: 1.2681651711463928
  batch 200 loss: 1.2737289583683014
  batch 250 loss: 1.2264741742610932
  batch 300 loss: 1.212641886472702
  batch 350 loss: 1.2496578478813172
  batch 400 loss: 1.2362714087963105
  batch 450 loss: 1.2109713637828827
  batch 500 loss: 1.275715104341507
  batch 550 loss: 1.2655403423309326
  batch 600 loss: 1.2073426115512849
  batch 650 loss: 1.2632026493549346
  batch 700 loss: 1.2531334745883942
  batch 750 loss: 1.2343537163734437
  batch 800 loss: 1.2522607052326202
  batch 850 loss: 1.266733204126358
  batch 900 loss: 1.278845522403717
LOSS train 1.27885 valid 1.22697, valid PER 39.77%
EPOCH 11:
  batch 50 loss: 1.2060559976100922
  batch 100 loss: 1.2188260304927825
  batch 150 loss: 1.2116577315330506
  batch 200 loss: 1.2402835822105407
  batch 250 loss: 1.2514994430541992
  batch 300 loss: 1.2130257034301757
  batch 350 loss: 1.2253940296173096
  batch 400 loss: 1.2772822678089142
  batch 450 loss: 1.2610881054401397
  batch 500 loss: 1.2506644678115846
  batch 550 loss: 1.223508574962616
  batch 600 loss: 1.2256871926784516
  batch 650 loss: 1.2841942596435547
  batch 700 loss: 1.204393389225006
  batch 750 loss: 1.2335391891002656
  batch 800 loss: 1.2733380007743835
  batch 850 loss: 1.2853912687301636
  batch 900 loss: 1.257113106250763
LOSS train 1.25711 valid 1.18618, valid PER 36.98%
EPOCH 12:
  batch 50 loss: 1.1993783926963806
  batch 100 loss: 1.228800048828125
  batch 150 loss: 1.2077859961986541
  batch 200 loss: 1.2206554555892943
  batch 250 loss: 1.258175733089447
  batch 300 loss: 1.2245926332473756
  batch 350 loss: 1.2012419939041137
  batch 400 loss: 1.2233266389369966
  batch 450 loss: 1.2307455360889434
  batch 500 loss: 1.2757022166252137
  batch 550 loss: 1.1898387718200683
  batch 600 loss: 1.1897286593914032
  batch 650 loss: 1.2545246958732605
  batch 700 loss: 1.2614537346363068
  batch 750 loss: 1.2898215019702912
  batch 800 loss: 1.2393244540691375
  batch 850 loss: 1.255395268201828
  batch 900 loss: 1.2742436182498933
LOSS train 1.27424 valid 1.22851, valid PER 39.44%
EPOCH 13:
  batch 50 loss: 1.2310353004932404
  batch 100 loss: 1.2572448992729186
  batch 150 loss: 1.2342620861530305
  batch 200 loss: 1.2449533545970917
  batch 250 loss: 1.2747323298454285
  batch 300 loss: 1.2094096982479094
  batch 350 loss: 1.2069398975372314
  batch 400 loss: 1.2493740844726562
  batch 450 loss: 1.26250914812088
  batch 500 loss: 1.2170439541339875
  batch 550 loss: 1.1956629180908203
  batch 600 loss: 1.2166302478313447
  batch 650 loss: 1.1947890627384186
  batch 700 loss: 1.2135229682922364
  batch 750 loss: 1.2188645994663239
  batch 800 loss: 1.2199205625057221
  batch 850 loss: 1.2480569529533385
  batch 900 loss: 1.2591055452823638
LOSS train 1.25911 valid 1.23303, valid PER 39.47%
EPOCH 14:
  batch 50 loss: 1.2011168432235717
  batch 100 loss: 1.225308073759079
  batch 150 loss: 1.2038365030288696
  batch 200 loss: 1.19285231590271
  batch 250 loss: 1.2180374193191528
  batch 300 loss: 1.2582240891456604
  batch 350 loss: 1.1990864729881288
  batch 400 loss: 1.2285970902442933
  batch 450 loss: 1.2082345652580262
  batch 500 loss: 1.2476596069335937
  batch 550 loss: 1.2516352105140687
  batch 600 loss: 1.215806610584259
  batch 650 loss: 1.2656596314907074
  batch 700 loss: 1.269484668970108
  batch 750 loss: 1.2090278339385987
  batch 800 loss: 1.2546399688720704
  batch 850 loss: 1.269063651561737
  batch 900 loss: 1.2460829544067382
LOSS train 1.24608 valid 1.22751, valid PER 38.43%
EPOCH 15:
  batch 50 loss: 1.228882521390915
  batch 100 loss: 1.1982632565498352
  batch 150 loss: 1.1982137954235077
  batch 200 loss: 1.256050362586975
  batch 250 loss: 1.2174115431308747
  batch 300 loss: 1.2198847842216491
  batch 350 loss: 1.2034401834011077
  batch 400 loss: 1.2124019587039947
  batch 450 loss: 1.2270763325691223
  batch 500 loss: 1.2291136384010315
  batch 550 loss: 1.2203317832946778
  batch 600 loss: 1.281931482553482
  batch 650 loss: 1.257644236087799
  batch 700 loss: 1.2640706121921539
  batch 750 loss: 1.2520097947120667
  batch 800 loss: 1.2287007880210876
  batch 850 loss: 1.2446515583992004
  batch 900 loss: 1.2679242837429046
LOSS train 1.26792 valid 1.22148, valid PER 37.71%
EPOCH 16:
  batch 50 loss: 1.243572360277176
  batch 100 loss: 1.1710481524467469
  batch 150 loss: 1.1963026547431945
  batch 200 loss: 1.1991529977321624
  batch 250 loss: 1.23881605386734
  batch 300 loss: 1.2064469349384308
  batch 350 loss: 1.2653993773460388
  batch 400 loss: 1.2440787255764008
  batch 450 loss: 1.257701358795166
  batch 500 loss: 1.198583836555481
  batch 550 loss: 1.2346251344680785
  batch 600 loss: 1.227356493473053
  batch 650 loss: 1.2294709920883178
  batch 700 loss: 1.2213516759872436
  batch 750 loss: 1.2159076058864593
  batch 800 loss: 1.2276897263526916
  batch 850 loss: 1.2124891924858092
  batch 900 loss: 1.2209876346588135
LOSS train 1.22099 valid 1.25358, valid PER 40.29%
EPOCH 17:
  batch 50 loss: 1.2193518388271332
  batch 100 loss: 1.2022865104675293
  batch 150 loss: 1.2073505401611329
  batch 200 loss: 1.2155367028713226
  batch 250 loss: 1.2418520390987395
  batch 300 loss: 1.2313191378116608
  batch 350 loss: 1.1983750009536742
  batch 400 loss: 1.234732221364975
  batch 450 loss: 1.2909726190567017
  batch 500 loss: 1.237513701915741
  batch 550 loss: 1.2565257811546326
  batch 600 loss: 1.2975236582756042
  batch 650 loss: 1.2560459423065184
  batch 700 loss: 1.2544893360137939
  batch 750 loss: 1.2204110896587372
  batch 800 loss: 1.25811541557312
  batch 850 loss: 1.2597087121009827
  batch 900 loss: 1.2033273923397063
LOSS train 1.20333 valid 1.25898, valid PER 38.51%
EPOCH 18:
  batch 50 loss: 1.2864528512954712
  batch 100 loss: 1.2619734382629395
  batch 150 loss: 1.241711711883545
  batch 200 loss: 1.2437665057182312
  batch 250 loss: 1.2293089318275452
  batch 300 loss: 1.2292815053462982
  batch 350 loss: 1.2524158263206482
  batch 400 loss: 1.2316616559028626
  batch 450 loss: 1.2895920825004579
  batch 500 loss: 1.218224720954895
  batch 550 loss: 1.2086222290992736
  batch 600 loss: 1.2221502184867858
  batch 650 loss: 1.1802004146575928
  batch 700 loss: 1.2794076550006865
  batch 750 loss: 1.213999719619751
  batch 800 loss: 1.2626631307601928
  batch 850 loss: 1.2236415755748749
  batch 900 loss: 1.2432666444778442
LOSS train 1.24327 valid 1.22661, valid PER 37.90%
EPOCH 19:
  batch 50 loss: 1.1931157398223877
  batch 100 loss: 1.2092267405986785
  batch 150 loss: 1.21288067817688
  batch 200 loss: 1.2155247330665588
  batch 250 loss: 1.2610175800323487
  batch 300 loss: 1.2487650322914123
  batch 350 loss: 1.2289451634883881
  batch 400 loss: 1.2599632728099823
  batch 450 loss: 1.272356504201889
  batch 500 loss: 1.2752327942848205
  batch 550 loss: 1.2642353868484497
  batch 600 loss: 1.2515868628025055
  batch 650 loss: 1.2698206186294556
  batch 700 loss: 1.2311243176460267
  batch 750 loss: 1.2206693923473357
  batch 800 loss: 1.3216824126243591
  batch 850 loss: 1.298279721736908
  batch 900 loss: 1.261037803888321
LOSS train 1.26104 valid 1.24221, valid PER 39.02%
EPOCH 20:
  batch 50 loss: 1.2235113191604614
  batch 100 loss: 1.2623007607460022
  batch 150 loss: 1.21015340924263
  batch 200 loss: 1.2506052875518798
  batch 250 loss: 1.2529662775993347
  batch 300 loss: 1.2937180399894714
  batch 350 loss: 1.2536862885951996
  batch 400 loss: 1.2408709990978242
  batch 450 loss: 1.2457577610015869
  batch 500 loss: 1.2181909787654877
  batch 550 loss: 1.302247531414032
  batch 600 loss: 1.2235607314109802
  batch 650 loss: 1.282702785730362
  batch 700 loss: 1.2788091039657592
  batch 750 loss: 1.2587658977508545
  batch 800 loss: 1.2795106816291808
  batch 850 loss: 1.2952077686786652
  batch 900 loss: 1.34099755525589
LOSS train 1.34100 valid 1.24243, valid PER 39.73%
Training finished in 5.0 minutes.
Model saved to checkpoints/20231207_045303/model_11
Loading model from checkpoints/20231207_045303/model_11
SUB: 19.87%, DEL: 16.57%, INS: 1.90%, COR: 63.56%, PER: 38.34%
