Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.001, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='adam', grad_clip=1.5)
Total number of model parameters is 166952
EPOCH 1, Learning Rate: 0.001
  batch 50 loss: 9.063386659622193
  batch 100 loss: 3.109414095878601
  batch 150 loss: 2.9601780700683595
  batch 200 loss: 2.8216001653671263
  batch 250 loss: 2.7494571733474733
  batch 300 loss: 2.5845130586624148
  batch 350 loss: 2.442870969772339
  batch 400 loss: 2.3788671112060547
  batch 450 loss: 2.3113704347610473
  batch 500 loss: 2.1928696751594545
  batch 550 loss: 2.0857364988327025
  batch 600 loss: 2.0568381524086
  batch 650 loss: 1.9705865025520324
  batch 700 loss: 1.963685266971588
  batch 750 loss: 1.9079436779022216
  batch 800 loss: 1.897531976699829
  batch 850 loss: 1.8657000160217285
  batch 900 loss: 1.829035620689392
LOSS train 1.82904 valid 1.77367, valid PER 67.50%
EPOCH 2, Learning Rate: 0.001
  batch 50 loss: 1.7996916723251344
  batch 100 loss: 1.7509280014038087
  batch 150 loss: 1.7090141105651855
  batch 200 loss: 1.7221431851387023
  batch 250 loss: 1.7189460229873657
  batch 300 loss: 1.7043286108970641
  batch 350 loss: 1.6129403805732727
  batch 400 loss: 1.626314103603363
  batch 450 loss: 1.5970025300979613
  batch 500 loss: 1.613921046257019
  batch 550 loss: 1.633872513771057
  batch 600 loss: 1.5684863209724427
  batch 650 loss: 1.579657289981842
  batch 700 loss: 1.5790329766273499
  batch 750 loss: 1.5599170327186584
  batch 800 loss: 1.4984976482391357
  batch 850 loss: 1.5203698396682739
  batch 900 loss: 1.5498785495758056
LOSS train 1.54988 valid 1.47853, valid PER 52.57%
EPOCH 3, Learning Rate: 0.001
  batch 50 loss: 1.512288966178894
  batch 100 loss: 1.4815814781188965
  batch 150 loss: 1.4397676277160645
  batch 200 loss: 1.4483281373977661
  batch 250 loss: 1.4393029975891114
  batch 300 loss: 1.424122097492218
  batch 350 loss: 1.4487735748291015
  batch 400 loss: 1.4365424251556396
  batch 450 loss: 1.4101579236984252
  batch 500 loss: 1.3974627780914306
  batch 550 loss: 1.3971770739555358
  batch 600 loss: 1.3637477445602417
  batch 650 loss: 1.3574627208709718
  batch 700 loss: 1.3712473511695862
  batch 750 loss: 1.4226749420166016
  batch 800 loss: 1.3550689387321473
  batch 850 loss: 1.3758753442764282
  batch 900 loss: 1.3169529008865357
LOSS train 1.31695 valid 1.30971, valid PER 43.21%
EPOCH 4, Learning Rate: 0.001
  batch 50 loss: 1.310795657634735
  batch 100 loss: 1.3283275628089906
  batch 150 loss: 1.2953206348419188
  batch 200 loss: 1.3363130044937135
  batch 250 loss: 1.3267538785934447
  batch 300 loss: 1.3206082820892333
  batch 350 loss: 1.2610150063037873
  batch 400 loss: 1.3047707629203797
  batch 450 loss: 1.281480312347412
  batch 500 loss: 1.2650749504566192
  batch 550 loss: 1.3076350212097168
  batch 600 loss: 1.3048649990558625
  batch 650 loss: 1.2996192693710327
  batch 700 loss: 1.2824131846427917
  batch 750 loss: 1.2552186620235444
  batch 800 loss: 1.221249405145645
  batch 850 loss: 1.2599370682239532
  batch 900 loss: 1.287962064743042
LOSS train 1.28796 valid 1.20426, valid PER 40.19%
EPOCH 5, Learning Rate: 0.001
  batch 50 loss: 1.2228507018089294
  batch 100 loss: 1.217473613023758
  batch 150 loss: 1.2546250092983247
  batch 200 loss: 1.1794918406009673
  batch 250 loss: 1.19644096493721
  batch 300 loss: 1.2215381479263305
  batch 350 loss: 1.2284911048412324
  batch 400 loss: 1.2296476340293885
  batch 450 loss: 1.2142140400409698
  batch 500 loss: 1.2302717924118043
  batch 550 loss: 1.1802397561073303
  batch 600 loss: 1.2486185133457184
  batch 650 loss: 1.2066457545757294
  batch 700 loss: 1.2223172116279601
  batch 750 loss: 1.1630817914009095
  batch 800 loss: 1.1964673244953155
  batch 850 loss: 1.2032024407386779
  batch 900 loss: 1.2200662672519684
LOSS train 1.22007 valid 1.17370, valid PER 39.48%
EPOCH 6, Learning Rate: 0.001
  batch 50 loss: 1.2046138179302215
  batch 100 loss: 1.1493504559993744
  batch 150 loss: 1.1349638557434083
  batch 200 loss: 1.1610738694667817
  batch 250 loss: 1.194381207227707
  batch 300 loss: 1.143644152879715
  batch 350 loss: 1.172131894826889
  batch 400 loss: 1.1422907638549804
  batch 450 loss: 1.1898998236656189
  batch 500 loss: 1.152617266178131
  batch 550 loss: 1.1920127475261688
  batch 600 loss: 1.1502290844917298
  batch 650 loss: 1.175055650472641
  batch 700 loss: 1.1619922423362732
  batch 750 loss: 1.138021421432495
  batch 800 loss: 1.1307764911651612
  batch 850 loss: 1.125551118850708
  batch 900 loss: 1.1795892560482024
LOSS train 1.17959 valid 1.11274, valid PER 36.60%
EPOCH 7, Learning Rate: 0.001
  batch 50 loss: 1.1317145609855652
  batch 100 loss: 1.1587301397323608
  batch 150 loss: 1.1461191534996034
  batch 200 loss: 1.1118052101135254
  batch 250 loss: 1.1347397923469544
  batch 300 loss: 1.12159343957901
  batch 350 loss: 1.10154523730278
  batch 400 loss: 1.1176142239570617
  batch 450 loss: 1.1058363211154938
  batch 500 loss: 1.113210825920105
  batch 550 loss: 1.1287646627426147
  batch 600 loss: 1.1185491919517516
  batch 650 loss: 1.084129867553711
  batch 700 loss: 1.1245363819599152
  batch 750 loss: 1.0981525850296021
  batch 800 loss: 1.0892183923721312
  batch 850 loss: 1.1301954936981202
  batch 900 loss: 1.1725257647037506
LOSS train 1.17253 valid 1.08865, valid PER 36.30%
EPOCH 8, Learning Rate: 0.001
  batch 50 loss: 1.1000268685817718
  batch 100 loss: 1.0986690986156464
  batch 150 loss: 1.0753539836406707
  batch 200 loss: 1.0591982877254487
  batch 250 loss: 1.0903995144367218
  batch 300 loss: 1.0006621670722962
  batch 350 loss: 1.0834025609493256
  batch 400 loss: 1.0529367446899414
  batch 450 loss: 1.0785944950580597
  batch 500 loss: 1.0906593108177185
  batch 550 loss: 1.0458019268512726
  batch 600 loss: 1.0869876182079314
  batch 650 loss: 1.1222724640369415
  batch 700 loss: 1.05205531001091
  batch 750 loss: 1.0697191536426545
  batch 800 loss: 1.0775898802280426
  batch 850 loss: 1.0659268736839294
  batch 900 loss: 1.042553219795227
LOSS train 1.04255 valid 1.05953, valid PER 34.55%
EPOCH 9, Learning Rate: 0.001
  batch 50 loss: 1.002861121892929
  batch 100 loss: 1.0352894401550292
  batch 150 loss: 1.0524860191345216
  batch 200 loss: 0.9979116201400757
  batch 250 loss: 1.0459209752082825
  batch 300 loss: 1.05967826962471
  batch 350 loss: 1.0797116017341615
  batch 400 loss: 1.0463024127483367
  batch 450 loss: 1.0456716310977936
  batch 500 loss: 1.0063871073722839
  batch 550 loss: 1.0745856058597565
  batch 600 loss: 1.0538759577274321
  batch 650 loss: 1.0262590157985687
  batch 700 loss: 1.02978679895401
  batch 750 loss: 1.0431770694255829
  batch 800 loss: 1.039692587852478
  batch 850 loss: 1.0581039214134216
  batch 900 loss: 1.0147099995613098
LOSS train 1.01471 valid 1.02539, valid PER 33.66%
EPOCH 10, Learning Rate: 0.001
  batch 50 loss: 0.9804714941978454
  batch 100 loss: 1.008146389722824
  batch 150 loss: 1.025873749256134
  batch 200 loss: 1.0359590554237366
  batch 250 loss: 1.0153039026260375
  batch 300 loss: 0.9662393462657929
  batch 350 loss: 1.0305559742450714
  batch 400 loss: 0.990576628446579
  batch 450 loss: 0.9921762192249298
  batch 500 loss: 1.041836817264557
  batch 550 loss: 1.040800280570984
  batch 600 loss: 1.0051972270011902
  batch 650 loss: 0.9950773537158966
  batch 700 loss: 1.003631650209427
  batch 750 loss: 1.0135883581638336
  batch 800 loss: 1.0177949166297913
  batch 850 loss: 1.0272437810897828
  batch 900 loss: 1.0224515295028687
LOSS train 1.02245 valid 1.01018, valid PER 33.18%
EPOCH 11, Learning Rate: 0.001
  batch 50 loss: 0.9531512308120728
  batch 100 loss: 0.9326201415061951
  batch 150 loss: 0.9512743806838989
  batch 200 loss: 1.0126495230197907
  batch 250 loss: 0.9950065457820892
  batch 300 loss: 0.9569982600212097
  batch 350 loss: 0.9832583439350128
  batch 400 loss: 1.004210251569748
  batch 450 loss: 0.9805825555324554
  batch 500 loss: 0.9857282853126526
  batch 550 loss: 0.9675558412075043
  batch 600 loss: 0.9424784457683564
  batch 650 loss: 1.0269291257858277
  batch 700 loss: 0.976632936000824
  batch 750 loss: 0.9911628234386444
  batch 800 loss: 1.017971794605255
  batch 850 loss: 1.0277755200862884
  batch 900 loss: 1.0096537160873413
LOSS train 1.00965 valid 0.99201, valid PER 32.37%
EPOCH 12, Learning Rate: 0.001
  batch 50 loss: 0.9642819404602051
  batch 100 loss: 0.9375819063186646
  batch 150 loss: 0.926790429353714
  batch 200 loss: 0.9449134171009064
  batch 250 loss: 0.9820522904396057
  batch 300 loss: 0.9345003771781921
  batch 350 loss: 0.9505947577953339
  batch 400 loss: 0.9820262587070465
  batch 450 loss: 0.9925218391418457
  batch 500 loss: 0.9906457531452179
  batch 550 loss: 0.9129263591766358
  batch 600 loss: 0.9381384599208832
  batch 650 loss: 0.9939754092693329
  batch 700 loss: 0.9496330642700195
  batch 750 loss: 0.929665379524231
  batch 800 loss: 0.9509254479408265
  batch 850 loss: 0.9980432593822479
  batch 900 loss: 1.0290077424049378
LOSS train 1.02901 valid 1.00264, valid PER 33.38%
EPOCH 13, Learning Rate: 0.001
  batch 50 loss: 0.9043102300167084
  batch 100 loss: 0.9416573452949524
  batch 150 loss: 0.9253378415107727
  batch 200 loss: 0.9557841765880585
  batch 250 loss: 0.9296300411224365
  batch 300 loss: 0.9416404843330384
  batch 350 loss: 0.976289998292923
  batch 400 loss: 0.9377775049209595
  batch 450 loss: 0.9613972640037537
  batch 500 loss: 0.9317757046222687
  batch 550 loss: 0.951409946680069
  batch 600 loss: 0.9376633322238922
  batch 650 loss: 0.9470815896987915
  batch 700 loss: 0.9552585375308991
  batch 750 loss: 0.9397643172740936
  batch 800 loss: 0.9389097499847412
  batch 850 loss: 0.9846205019950867
  batch 900 loss: 0.9609461319446564
LOSS train 0.96095 valid 0.95992, valid PER 31.44%
EPOCH 14, Learning Rate: 0.001
  batch 50 loss: 0.9085967838764191
  batch 100 loss: 0.9058020794391632
  batch 150 loss: 0.9044083070755005
  batch 200 loss: 0.9215891301631928
  batch 250 loss: 0.9444644224643707
  batch 300 loss: 0.9617111456394195
  batch 350 loss: 0.91998304605484
  batch 400 loss: 0.9102958929538727
  batch 450 loss: 0.904223951101303
  batch 500 loss: 0.9266095745563507
  batch 550 loss: 0.9534211230278015
  batch 600 loss: 0.886144814491272
  batch 650 loss: 0.9272085332870483
  batch 700 loss: 0.9406420433521271
  batch 750 loss: 0.9106527483463287
  batch 800 loss: 0.8855421924591065
  batch 850 loss: 0.9387457180023193
  batch 900 loss: 0.9260392880439758
LOSS train 0.92604 valid 0.98562, valid PER 32.32%
EPOCH 15, Learning Rate: 0.001
  batch 50 loss: 0.9106898498535156
  batch 100 loss: 0.8805600249767304
  batch 150 loss: 0.8876734745502471
  batch 200 loss: 0.9429573023319244
  batch 250 loss: 0.919020721912384
  batch 300 loss: 0.8979975831508636
  batch 350 loss: 0.878268814086914
  batch 400 loss: 0.903206547498703
  batch 450 loss: 0.9064836049079895
  batch 500 loss: 0.8738202345371247
  batch 550 loss: 0.8876051044464112
  batch 600 loss: 0.9124228084087371
  batch 650 loss: 0.9246871185302734
  batch 700 loss: 0.9259494304656982
  batch 750 loss: 0.9150164079666138
  batch 800 loss: 0.9069425094127656
  batch 850 loss: 0.9005604898929596
  batch 900 loss: 0.9112006187438965
LOSS train 0.91120 valid 0.94465, valid PER 30.66%
EPOCH 16, Learning Rate: 0.001
  batch 50 loss: 0.8931708371639252
  batch 100 loss: 0.8638234221935273
  batch 150 loss: 0.8694941401481628
  batch 200 loss: 0.8801006138324737
  batch 250 loss: 0.8890518581867218
  batch 300 loss: 0.8991124260425568
  batch 350 loss: 0.9063141989707947
  batch 400 loss: 0.8877486014366149
  batch 450 loss: 0.9249031615257263
  batch 500 loss: 0.8621624088287354
  batch 550 loss: 0.9126491379737854
  batch 600 loss: 0.8896163952350616
  batch 650 loss: 0.9045203840732574
  batch 700 loss: 0.8905688726902008
  batch 750 loss: 0.8869956254959106
  batch 800 loss: 0.8929640853404999
  batch 850 loss: 0.8770648002624511
  batch 900 loss: 0.8763031113147736
LOSS train 0.87630 valid 0.95565, valid PER 30.96%
EPOCH 17, Learning Rate: 0.001
  batch 50 loss: 0.873662782907486
  batch 100 loss: 0.8967148303985596
  batch 150 loss: 0.8614369201660156
  batch 200 loss: 0.853186331987381
  batch 250 loss: 0.8741912543773651
  batch 300 loss: 0.8764429199695587
  batch 350 loss: 0.840529454946518
  batch 400 loss: 0.9326166892051697
  batch 450 loss: 0.9083007109165192
  batch 500 loss: 0.8738907563686371
  batch 550 loss: 0.862081971168518
  batch 600 loss: 0.9192255973815918
  batch 650 loss: 0.8618265664577485
  batch 700 loss: 0.853370314836502
  batch 750 loss: 0.8447102665901184
  batch 800 loss: 0.8533778655529022
  batch 850 loss: 0.8685204708576202
  batch 900 loss: 0.8390104031562805
LOSS train 0.83901 valid 0.94101, valid PER 29.85%
EPOCH 18, Learning Rate: 0.001
  batch 50 loss: 0.859476068019867
  batch 100 loss: 0.849957275390625
  batch 150 loss: 0.8674123299121856
  batch 200 loss: 0.837862434387207
  batch 250 loss: 0.8463240313529968
  batch 300 loss: 0.8306163311004638
  batch 350 loss: 0.866643522977829
  batch 400 loss: 0.8290432584285736
  batch 450 loss: 0.8952217292785645
  batch 500 loss: 0.8496002006530762
  batch 550 loss: 0.8482591831684112
  batch 600 loss: 0.8418056035041809
  batch 650 loss: 0.8296635556221008
  batch 700 loss: 0.8709952294826507
  batch 750 loss: 0.8553691005706787
  batch 800 loss: 0.8460043573379517
  batch 850 loss: 0.8482831561565399
  batch 900 loss: 0.8675124514102935
LOSS train 0.86751 valid 0.94224, valid PER 29.93%
EPOCH 19, Learning Rate: 0.001
  batch 50 loss: 0.7913898169994354
  batch 100 loss: 0.7859902703762054
  batch 150 loss: 0.8175960338115692
  batch 200 loss: 0.8398789191246032
  batch 250 loss: 0.8516133499145507
  batch 300 loss: 0.8450156772136688
  batch 350 loss: 0.8262785696983337
  batch 400 loss: 0.8299506509304047
  batch 450 loss: 0.8400547885894776
  batch 500 loss: 0.8411038196086884
  batch 550 loss: 0.8193482601642609
  batch 600 loss: 0.8470512390136719
  batch 650 loss: 0.8899717020988465
  batch 700 loss: 0.8409505927562714
  batch 750 loss: 0.7976625835895539
  batch 800 loss: 0.8544519209861755
  batch 850 loss: 0.8610282909870147
  batch 900 loss: 0.8469257020950317
LOSS train 0.84693 valid 0.93885, valid PER 29.44%
EPOCH 20, Learning Rate: 0.001
  batch 50 loss: 0.7979419243335724
  batch 100 loss: 0.82589200258255
  batch 150 loss: 0.7948379480838775
  batch 200 loss: 0.8177107572555542
  batch 250 loss: 0.8260126900672913
  batch 300 loss: 0.8392814522981644
  batch 350 loss: 0.8042446804046631
  batch 400 loss: 0.8332318067550659
  batch 450 loss: 0.842304527759552
  batch 500 loss: 0.8166773092746734
  batch 550 loss: 0.8698884928226471
  batch 600 loss: 0.8004089307785034
  batch 650 loss: 0.8471090042591095
  batch 700 loss: 0.823973468542099
  batch 750 loss: 0.8462287902832031
  batch 800 loss: 0.8524242281913758
  batch 850 loss: 0.8430121541023254
  batch 900 loss: 0.8632077419757843
LOSS train 0.86321 valid 0.91226, valid PER 28.97%
Training finished in 3.0 minutes.
Model saved to checkpoints/20231210_003713/model_20
Loading model from checkpoints/20231210_003713/model_20
SUB: 15.60%, DEL: 13.57%, INS: 1.67%, COR: 70.83%, PER: 30.84%
