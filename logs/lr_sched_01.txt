Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.1, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 166952
EPOCH 1, Learning Rate: 0.1
  batch 50 loss: 7.745971570014953
  batch 100 loss: 3.295362296104431
  batch 150 loss: 3.2486917400360107
  batch 200 loss: 3.220787878036499
  batch 250 loss: 3.184152913093567
  batch 300 loss: 3.130120401382446
  batch 350 loss: 3.084015545845032
  batch 400 loss: 3.0329520511627197
  batch 450 loss: 2.9606712675094604
  batch 500 loss: 2.863921284675598
  batch 550 loss: 2.7966961097717284
  batch 600 loss: 2.742771420478821
  batch 650 loss: 2.674844493865967
  batch 700 loss: 2.6510701608657836
  batch 750 loss: 2.6015340518951415
  batch 800 loss: 2.5705275774002074
  batch 850 loss: 2.546910037994385
  batch 900 loss: 2.497003421783447
running loss: 59.14668011665344
LOSS train 2.49700 valid 2.46948, valid PER 81.88%
EPOCH 2, Learning Rate: 0.1
  batch 50 loss: 2.469251608848572
  batch 100 loss: 2.4198916816711424
  batch 150 loss: 2.364733543395996
  batch 200 loss: 2.352583646774292
  batch 250 loss: 2.3469012451171873
  batch 300 loss: 2.321975131034851
  batch 350 loss: 2.254878993034363
  batch 400 loss: 2.26049334526062
  batch 450 loss: 2.2239566683769225
  batch 500 loss: 2.202768232822418
  batch 550 loss: 2.2133583927154543
  batch 600 loss: 2.1618653964996337
  batch 650 loss: 2.1717595911026
  batch 700 loss: 2.141247019767761
  batch 750 loss: 2.136174421310425
  batch 800 loss: 2.0814191555976866
  batch 850 loss: 2.0752091884613035
  batch 900 loss: 2.083257715702057
running loss: 49.600550413131714
LOSS train 2.08326 valid 2.04713, valid PER 76.53%
EPOCH 3, Learning Rate: 0.1
  batch 50 loss: 2.05993882894516
  batch 100 loss: 2.007007622718811
  batch 150 loss: 2.02390873670578
  batch 200 loss: 2.00095130443573
  batch 250 loss: 1.9707907509803773
  batch 300 loss: 1.9686853766441346
  batch 350 loss: 1.991812071800232
  batch 400 loss: 1.9525082898139954
  batch 450 loss: 1.9066394352912903
  batch 500 loss: 1.9146944785118103
  batch 550 loss: 1.8974922966957093
  batch 600 loss: 1.863821792602539
  batch 650 loss: 1.845869207382202
  batch 700 loss: 1.8593598818778991
  batch 750 loss: 1.8960140419006348
  batch 800 loss: 1.8290488481521607
  batch 850 loss: 1.8540533566474915
  batch 900 loss: 1.7943976783752442
running loss: 43.11483943462372
LOSS train 1.79440 valid 1.78251, valid PER 68.12%
EPOCH 4, Learning Rate: 0.1
  batch 50 loss: 1.7962835121154785
  batch 100 loss: 1.8087115645408631
  batch 150 loss: 1.7650011467933655
  batch 200 loss: 1.79321346282959
  batch 250 loss: 1.784716238975525
  batch 300 loss: 1.7745733809471131
  batch 350 loss: 1.7150027799606322
  batch 400 loss: 1.7463012957572936
  batch 450 loss: 1.7332945919036866
  batch 500 loss: 1.6991885280609131
  batch 550 loss: 1.7185852932929992
  batch 600 loss: 1.7306744813919068
  batch 650 loss: 1.726310510635376
  batch 700 loss: 1.6928105711936952
  batch 750 loss: 1.6625598549842835
  batch 800 loss: 1.639321312904358
  batch 850 loss: 1.6721497535705567
  batch 900 loss: 1.6941871953010559
running loss: 39.65595066547394
LOSS train 1.69419 valid 1.64777, valid PER 63.50%
EPOCH 5, Learning Rate: 0.1
  batch 50 loss: 1.6510109639167785
  batch 100 loss: 1.640411605834961
  batch 150 loss: 1.6615377569198608
  batch 200 loss: 1.6024235248565675
  batch 250 loss: 1.6139020204544068
  batch 300 loss: 1.6289218139648438
  batch 350 loss: 1.6146515107154846
  batch 400 loss: 1.6161485028266906
  batch 450 loss: 1.6035076808929443
  batch 500 loss: 1.6127578234672546
  batch 550 loss: 1.5402619314193726
  batch 600 loss: 1.6204337453842164
  batch 650 loss: 1.5734595561027527
  batch 700 loss: 1.6018055629730226
  batch 750 loss: 1.5492271280288696
  batch 800 loss: 1.5710204243659973
  batch 850 loss: 1.5673394441604613
  batch 900 loss: 1.577201280593872
running loss: 36.74139666557312
LOSS train 1.57720 valid 1.51596, valid PER 57.80%
EPOCH 6, Learning Rate: 0.1
  batch 50 loss: 1.568561646938324
  batch 100 loss: 1.5275447726249696
  batch 150 loss: 1.5212654733657838
  batch 200 loss: 1.5201736879348755
  batch 250 loss: 1.5442046904563904
  batch 300 loss: 1.5137324571609496
  batch 350 loss: 1.5222275853157043
  batch 400 loss: 1.5071058320999144
  batch 450 loss: 1.5179817414283752
  batch 500 loss: 1.4956706881523132
  batch 550 loss: 1.516626124382019
  batch 600 loss: 1.4855944991111756
  batch 650 loss: 1.4980990481376648
  batch 700 loss: 1.4916531014442445
  batch 750 loss: 1.4759985494613648
  batch 800 loss: 1.4567106103897094
  batch 850 loss: 1.4686690759658814
  batch 900 loss: 1.483546736240387
running loss: 35.312076449394226
LOSS train 1.48355 valid 1.44803, valid PER 55.82%
EPOCH 7, Learning Rate: 0.1
  batch 50 loss: 1.4532586431503296
  batch 100 loss: 1.4726488161087037
  batch 150 loss: 1.4535411143302917
  batch 200 loss: 1.444173321723938
  batch 250 loss: 1.451240186691284
  batch 300 loss: 1.4150073146820068
  batch 350 loss: 1.4242697548866272
  batch 400 loss: 1.4372069501876832
  batch 450 loss: 1.4312620639801026
  batch 500 loss: 1.420338637828827
  batch 550 loss: 1.4188080215454102
  batch 600 loss: 1.4316785502433778
  batch 650 loss: 1.4152499771118163
  batch 700 loss: 1.4226181674003602
  batch 750 loss: 1.40803715467453
  batch 800 loss: 1.406740357875824
  batch 850 loss: 1.419372935295105
  batch 900 loss: 1.4554117941856384
running loss: 33.035558462142944
LOSS train 1.45541 valid 1.40906, valid PER 52.03%
EPOCH 8, Learning Rate: 0.1
  batch 50 loss: 1.4040245723724365
  batch 100 loss: 1.3939687609672546
  batch 150 loss: 1.3836652278900146
  batch 200 loss: 1.355674502849579
  batch 250 loss: 1.3820900106430054
  batch 300 loss: 1.3100478982925414
  batch 350 loss: 1.3879524970054626
  batch 400 loss: 1.360438380241394
  batch 450 loss: 1.3745128774642945
  batch 500 loss: 1.400854983329773
  batch 550 loss: 1.3276405668258666
  batch 600 loss: 1.3737648439407348
  batch 650 loss: 1.39854900598526
  batch 700 loss: 1.3428777289390563
  batch 750 loss: 1.346541178226471
  batch 800 loss: 1.356220713853836
  batch 850 loss: 1.3608477187156678
  batch 900 loss: 1.3375873470306396
running loss: 32.00285840034485
LOSS train 1.33759 valid 1.30575, valid PER 45.46%
EPOCH 9, Learning Rate: 0.1
  batch 50 loss: 1.286938054561615
  batch 100 loss: 1.3393462800979614
  batch 150 loss: 1.3296709609031678
  batch 200 loss: 1.2976684033870698
  batch 250 loss: 1.337298445701599
  batch 300 loss: 1.3292665696144104
  batch 350 loss: 1.3442370998859405
  batch 400 loss: 1.3255003786087036
  batch 450 loss: 1.3323291969299316
  batch 500 loss: 1.2864399504661561
  batch 550 loss: 1.3181201171875
  batch 600 loss: 1.3350079870223999
  batch 650 loss: 1.289972871541977
  batch 700 loss: 1.2867055606842042
  batch 750 loss: 1.2979707646369933
  batch 800 loss: 1.3180183672904968
  batch 850 loss: 1.3204650425910949
  batch 900 loss: 1.263971474170685
running loss: 31.778913974761963
LOSS train 1.26397 valid 1.24205, valid PER 41.28%
EPOCH 10, Learning Rate: 0.1
  batch 50 loss: 1.2495925641059875
  batch 100 loss: 1.2694877064228058
  batch 150 loss: 1.2964635157585145
  batch 200 loss: 1.288202475309372
  batch 250 loss: 1.2813106787204742
  batch 300 loss: 1.257749493122101
  batch 350 loss: 1.2567470014095306
  batch 400 loss: 1.2393648207187653
  batch 450 loss: 1.235744047164917
  batch 500 loss: 1.2684360373020171
  batch 550 loss: 1.2852357816696167
  batch 600 loss: 1.2546814382076263
  batch 650 loss: 1.238224103450775
  batch 700 loss: 1.2545393776893616
  batch 750 loss: 1.2514528381824492
  batch 800 loss: 1.2672034060955049
  batch 850 loss: 1.2488679373264313
  batch 900 loss: 1.2666138815879822
running loss: 29.794195771217346
LOSS train 1.26661 valid 1.23728, valid PER 42.05%
EPOCH 11, Learning Rate: 0.1
  batch 50 loss: 1.2270080590248107
  batch 100 loss: 1.2272954618930816
  batch 150 loss: 1.2113040351867677
  batch 200 loss: 1.2613590157032013
  batch 250 loss: 1.2317626929283143
  batch 300 loss: 1.191003292798996
  batch 350 loss: 1.2414881825447082
  batch 400 loss: 1.235375736951828
  batch 450 loss: 1.2178329610824585
  batch 500 loss: 1.1964586114883422
  batch 550 loss: 1.2239219903945924
  batch 600 loss: 1.203884515762329
  batch 650 loss: 1.2478890681266785
  batch 700 loss: 1.2002029967308045
  batch 750 loss: 1.1938295686244964
  batch 800 loss: 1.2591262757778168
  batch 850 loss: 1.2244003677368165
  batch 900 loss: 1.2415923476219177
running loss: 28.561043620109558
LOSS train 1.24159 valid 1.17465, valid PER 38.89%
EPOCH 12, Learning Rate: 0.1
  batch 50 loss: 1.2048810184001923
  batch 100 loss: 1.1849559664726257
  batch 150 loss: 1.1761638486385346
  batch 200 loss: 1.1869556057453154
  batch 250 loss: 1.2150945329666138
  batch 300 loss: 1.1931343269348145
  batch 350 loss: 1.194201568365097
  batch 400 loss: 1.2048502099514007
  batch 450 loss: 1.1859569561481476
  batch 500 loss: 1.2208693492412568
  batch 550 loss: 1.1370060324668885
  batch 600 loss: 1.1440204298496246
  batch 650 loss: 1.2096935963630677
  batch 700 loss: 1.1728783357143402
  batch 750 loss: 1.1827868151664733
  batch 800 loss: 1.1496165668964387
  batch 850 loss: 1.2088839566707612
  batch 900 loss: 1.2037654912471771
running loss: 27.223976969718933
LOSS train 1.20377 valid 1.13921, valid PER 37.36%
EPOCH 13, Learning Rate: 0.1
  batch 50 loss: 1.1312201201915741
  batch 100 loss: 1.1836700344085693
  batch 150 loss: 1.1368418955802917
  batch 200 loss: 1.1613763904571532
  batch 250 loss: 1.155226868391037
  batch 300 loss: 1.1277181506156921
  batch 350 loss: 1.1600260019302369
  batch 400 loss: 1.1753837192058563
  batch 450 loss: 1.1801620876789094
  batch 500 loss: 1.138381907939911
  batch 550 loss: 1.154582040309906
  batch 600 loss: 1.1522896051406861
  batch 650 loss: 1.1576809227466582
  batch 700 loss: 1.1726913344860077
  batch 750 loss: 1.1183792066574096
  batch 800 loss: 1.136564166545868
  batch 850 loss: 1.184179471731186
  batch 900 loss: 1.164151132106781
running loss: 28.602797985076904
LOSS train 1.16415 valid 1.14145, valid PER 37.22%
EPOCH 14, Learning Rate: 0.05
  batch 50 loss: 1.1163549935817718
  batch 100 loss: 1.1333857405185699
  batch 150 loss: 1.0785280644893647
  batch 200 loss: 1.12621945977211
  batch 250 loss: 1.102250381708145
  batch 300 loss: 1.1276753222942353
  batch 350 loss: 1.081905163526535
  batch 400 loss: 1.1041176211833954
  batch 450 loss: 1.0807585275173188
  batch 500 loss: 1.0983398842811585
  batch 550 loss: 1.1206226170063018
  batch 600 loss: 1.0897841799259185
  batch 650 loss: 1.1123952496051788
  batch 700 loss: 1.12415545463562
  batch 750 loss: 1.0929156029224396
  batch 800 loss: 1.040552432537079
  batch 850 loss: 1.1158745563030243
  batch 900 loss: 1.0899269163608551
running loss: 26.939145863056183
LOSS train 1.08993 valid 1.10045, valid PER 36.06%
EPOCH 15, Learning Rate: 0.05
  batch 50 loss: 1.1128575217723846
  batch 100 loss: 1.0730524122714997
  batch 150 loss: 1.0835818350315094
  batch 200 loss: 1.1139687311649322
  batch 250 loss: 1.1011111044883728
  batch 300 loss: 1.0678645932674409
  batch 350 loss: 1.08887180685997
  batch 400 loss: 1.0822541069984437
  batch 450 loss: 1.0851580059528352
  batch 500 loss: 1.05492631316185
  batch 550 loss: 1.0927820801734924
  batch 600 loss: 1.0989336955547333
  batch 650 loss: 1.1021054351329804
  batch 700 loss: 1.1038151741027833
  batch 750 loss: 1.0826577293872832
  batch 800 loss: 1.073949338197708
  batch 850 loss: 1.065679157972336
  batch 900 loss: 1.0943547976016998
running loss: 25.71836817264557
LOSS train 1.09435 valid 1.09185, valid PER 35.75%
EPOCH 16, Learning Rate: 0.05
  batch 50 loss: 1.0870778441429139
  batch 100 loss: 1.0499114894866943
  batch 150 loss: 1.0573926663398743
  batch 200 loss: 1.0716753447055816
  batch 250 loss: 1.0811144375801087
  batch 300 loss: 1.0815452778339385
  batch 350 loss: 1.0905836355686187
  batch 400 loss: 1.0889866518974305
  batch 450 loss: 1.1024575817584992
  batch 500 loss: 1.0499819576740266
  batch 550 loss: 1.096234254837036
  batch 600 loss: 1.0857322108745575
  batch 650 loss: 1.0752751398086549
  batch 700 loss: 1.0440354061126709
  batch 750 loss: 1.0731255841255187
  batch 800 loss: 1.0905928564071656
  batch 850 loss: 1.0469337904453277
  batch 900 loss: 1.0654599034786225
running loss: 24.924409210681915
LOSS train 1.06546 valid 1.06752, valid PER 34.51%
EPOCH 17, Learning Rate: 0.05
  batch 50 loss: 1.071867663860321
  batch 100 loss: 1.0733150017261506
  batch 150 loss: 1.0451415848731995
  batch 200 loss: 1.0436424148082732
  batch 250 loss: 1.0587402009963989
  batch 300 loss: 1.067925843000412
  batch 350 loss: 1.0298275220394135
  batch 400 loss: 1.0958270847797393
  batch 450 loss: 1.0930081844329833
  batch 500 loss: 1.0536401522159577
  batch 550 loss: 1.088446124792099
  batch 600 loss: 1.1200134706497193
  batch 650 loss: 1.056545296907425
  batch 700 loss: 1.044866806268692
  batch 750 loss: 1.029745967388153
  batch 800 loss: 1.0487396121025085
  batch 850 loss: 1.0403863155841828
  batch 900 loss: 1.0331079876422882
running loss: 26.290412604808807
LOSS train 1.03311 valid 1.05640, valid PER 34.36%
EPOCH 18, Learning Rate: 0.025
  batch 50 loss: 1.0474545073509216
  batch 100 loss: 1.0560904550552368
  batch 150 loss: 1.0527893269062043
  batch 200 loss: 1.0243410658836365
  batch 250 loss: 1.024827618598938
  batch 300 loss: 1.0313691437244414
  batch 350 loss: 1.039213913679123
  batch 400 loss: 1.019407719373703
  batch 450 loss: 1.0700339305400848
  batch 500 loss: 1.0459516894817353
  batch 550 loss: 1.036959730386734
  batch 600 loss: 1.013035763502121
  batch 650 loss: 1.006383376121521
  batch 700 loss: 1.0566402685642242
  batch 750 loss: 1.0242660534381867
  batch 800 loss: 1.0247153294086457
  batch 850 loss: 1.008928278684616
  batch 900 loss: 1.0558983874320984
running loss: 24.40677547454834
LOSS train 1.05590 valid 1.05557, valid PER 34.33%
EPOCH 19, Learning Rate: 0.025
  batch 50 loss: 0.9889021170139313
  batch 100 loss: 1.006006462574005
  batch 150 loss: 1.0175031352043151
  batch 200 loss: 1.0286595118045807
  batch 250 loss: 1.0400979697704316
  batch 300 loss: 1.021339831352234
  batch 350 loss: 1.0268802416324616
  batch 400 loss: 1.0430661916732789
  batch 450 loss: 1.0329477286338806
  batch 500 loss: 1.0309001564979554
  batch 550 loss: 1.003125511407852
  batch 600 loss: 1.0372927761077881
  batch 650 loss: 1.07137504696846
  batch 700 loss: 1.016260906457901
  batch 750 loss: 0.9907210099697114
  batch 800 loss: 1.0383373188972473
  batch 850 loss: 1.0407859659194947
  batch 900 loss: 1.042584080696106
running loss: 24.408706188201904
LOSS train 1.04258 valid 1.05021, valid PER 34.18%
EPOCH 20, Learning Rate: 0.0125
  batch 50 loss: 1.0070879769325256
  batch 100 loss: 1.006510101556778
  batch 150 loss: 0.993325457572937
  batch 200 loss: 1.02974799990654
  batch 250 loss: 0.9968189132213593
  batch 300 loss: 1.0342455887794495
  batch 350 loss: 0.9814151895046234
  batch 400 loss: 1.0085652363300324
  batch 450 loss: 0.9999012875556946
  batch 500 loss: 0.9693640482425689
  batch 550 loss: 1.047600736618042
  batch 600 loss: 0.9795803093910217
  batch 650 loss: 1.0076958310604096
  batch 700 loss: 1.0161998426914216
  batch 750 loss: 0.9977456438541412
  batch 800 loss: 1.0401005089282989
  batch 850 loss: 1.0237922668457031
  batch 900 loss: 1.0336087095737456
running loss: 24.599228739738464
LOSS train 1.03361 valid 1.03907, valid PER 33.94%
Training finished in 4.0 minutes.
Model saved to checkpoints/20231210_033155/model_20
Loading model from checkpoints/20231210_033155/model_20
SUB: 15.28%, DEL: 18.55%, INS: 1.27%, COR: 66.18%, PER: 35.09%
