Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.1, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 166952
EPOCH 1, Learning Rate: 0.1
  batch 50 loss: 7.745971646308899
  batch 100 loss: 3.295362238883972
  batch 150 loss: 3.248691782951355
  batch 200 loss: 3.2207878494262694
  batch 250 loss: 3.184152841567993
  batch 300 loss: 3.1301202058792112
  batch 350 loss: 3.084015312194824
  batch 400 loss: 3.0329516649246218
  batch 450 loss: 2.960670690536499
  batch 500 loss: 2.8639207649230958
  batch 550 loss: 2.7966956424713136
  batch 600 loss: 2.7427708959579467
  batch 650 loss: 2.674843945503235
  batch 700 loss: 2.6510697984695435
  batch 750 loss: 2.601533694267273
  batch 800 loss: 2.570527310371399
  batch 850 loss: 2.546909866333008
  batch 900 loss: 2.49700345993042
avg val loss: 2.469475507736206
LOSS train 2.49700 valid 2.46948, valid PER 81.88%
EPOCH 2, Learning Rate: 0.1
  batch 50 loss: 2.469251503944397
  batch 100 loss: 2.419891748428345
  batch 150 loss: 2.364733533859253
  batch 200 loss: 2.352583546638489
  batch 250 loss: 2.3469012212753295
  batch 300 loss: 2.3219752883911133
  batch 350 loss: 2.254878783226013
  batch 400 loss: 2.260492973327637
  batch 450 loss: 2.223956606388092
  batch 500 loss: 2.2027680182456972
  batch 550 loss: 2.213358380794525
  batch 600 loss: 2.16186537027359
  batch 650 loss: 2.1717595767974855
  batch 700 loss: 2.141246955394745
  batch 750 loss: 2.136174376010895
  batch 800 loss: 2.0814191627502443
  batch 850 loss: 2.075209333896637
  batch 900 loss: 2.08325781583786
avg val loss: 2.0471339225769043
LOSS train 2.08326 valid 2.04713, valid PER 76.53%
EPOCH 3, Learning Rate: 0.1
  batch 50 loss: 2.059938957691193
  batch 100 loss: 2.007007825374603
  batch 150 loss: 2.0239091396331785
  batch 200 loss: 2.00095134973526
  batch 250 loss: 1.9707908511161805
  batch 300 loss: 1.9686851167678834
  batch 350 loss: 1.9918120694160462
  batch 400 loss: 1.9525081753730773
  batch 450 loss: 1.9066393661499024
  batch 500 loss: 1.9146943521499633
  batch 550 loss: 1.8974924755096436
  batch 600 loss: 1.8638215231895447
  batch 650 loss: 1.8458693623542786
  batch 700 loss: 1.8593599200248718
  batch 750 loss: 1.8960138702392577
  batch 800 loss: 1.829048354625702
  batch 850 loss: 1.8540535020828246
  batch 900 loss: 1.7943973231315613
avg val loss: 1.7825111150741577
LOSS train 1.79440 valid 1.78251, valid PER 68.12%
EPOCH 4, Learning Rate: 0.1
  batch 50 loss: 1.7962832975387573
  batch 100 loss: 1.8087109661102294
  batch 150 loss: 1.7650013637542725
  batch 200 loss: 1.7932137155532837
  batch 250 loss: 1.7847160720825195
  batch 300 loss: 1.7745733308792113
  batch 350 loss: 1.7150027227401734
  batch 400 loss: 1.746301257610321
  batch 450 loss: 1.7332943606376647
  batch 500 loss: 1.6991888117790221
  batch 550 loss: 1.7185850715637208
  batch 600 loss: 1.7306745386123656
  batch 650 loss: 1.726310532093048
  batch 700 loss: 1.6928106808662415
  batch 750 loss: 1.6625593280792237
  batch 800 loss: 1.6393212938308717
  batch 850 loss: 1.672149977684021
  batch 900 loss: 1.6941878485679627
avg val loss: 1.6477634906768799
LOSS train 1.69419 valid 1.64776, valid PER 63.50%
EPOCH 5, Learning Rate: 0.1
  batch 50 loss: 1.6510112857818604
  batch 100 loss: 1.6404119420051575
  batch 150 loss: 1.6615382075309753
  batch 200 loss: 1.6024234294891357
  batch 250 loss: 1.6139020729064941
  batch 300 loss: 1.6289218473434448
  batch 350 loss: 1.6146512341499328
  batch 400 loss: 1.616148362159729
  batch 450 loss: 1.6035081148147583
  batch 500 loss: 1.6127578926086426
  batch 550 loss: 1.5402621126174927
  batch 600 loss: 1.620433440208435
  batch 650 loss: 1.5734599900245667
  batch 700 loss: 1.601805624961853
  batch 750 loss: 1.549229655265808
  batch 800 loss: 1.5710196018218994
  batch 850 loss: 1.567338960170746
  batch 900 loss: 1.5772021436691284
avg val loss: 1.5159565210342407
LOSS train 1.57720 valid 1.51596, valid PER 57.80%
EPOCH 6, Learning Rate: 0.1
  batch 50 loss: 1.5685510563850402
  batch 100 loss: 1.5275452136993408
  batch 150 loss: 1.5212652349472047
  batch 200 loss: 1.5201731681823731
  batch 250 loss: 1.5442049717903137
  batch 300 loss: 1.5137336945533753
  batch 350 loss: 1.5222275471687317
  batch 400 loss: 1.5071043682098388
  batch 450 loss: 1.5178875207901001
  batch 500 loss: 1.4956528663635253
  batch 550 loss: 1.5166663861274718
  batch 600 loss: 1.4856066155433654
  batch 650 loss: 1.4985335850715638
  batch 700 loss: 1.4916564631462097
  batch 750 loss: 1.4760147047042846
  batch 800 loss: 1.4566343879699708
  batch 850 loss: 1.4686325550079347
  batch 900 loss: 1.4835922980308534
avg val loss: 1.448345422744751
LOSS train 1.48359 valid 1.44835, valid PER 55.83%
EPOCH 7, Learning Rate: 0.1
  batch 50 loss: 1.4532801413536072
  batch 100 loss: 1.4724982261657715
  batch 150 loss: 1.4539931964874269
  batch 200 loss: 1.444041726589203
  batch 250 loss: 1.451222984790802
  batch 300 loss: 1.4152930235862733
  batch 350 loss: 1.4243267846107484
  batch 400 loss: 1.4370651078224181
  batch 450 loss: 1.4311059522628784
  batch 500 loss: 1.4202761125564576
  batch 550 loss: 1.418771550655365
  batch 600 loss: 1.431618549823761
  batch 650 loss: 1.4151641750335693
  batch 700 loss: 1.4232052040100098
  batch 750 loss: 1.4077184343338012
  batch 800 loss: 1.4067044472694397
  batch 850 loss: 1.4193983912467956
  batch 900 loss: 1.4553855395317077
avg val loss: 1.4083843231201172
LOSS train 1.45539 valid 1.40838, valid PER 51.99%
EPOCH 8, Learning Rate: 0.1
  batch 50 loss: 1.403879897594452
  batch 100 loss: 1.3939264988899231
  batch 150 loss: 1.3836772203445435
  batch 200 loss: 1.355692822933197
  batch 250 loss: 1.3821929812431335
  batch 300 loss: 1.3099208068847656
  batch 350 loss: 1.3891568970680237
  batch 400 loss: 1.360716426372528
  batch 450 loss: 1.3744421362876893
  batch 500 loss: 1.4001626062393189
  batch 550 loss: 1.3278138446807861
  batch 600 loss: 1.373731164932251
  batch 650 loss: 1.3983075213432312
  batch 700 loss: 1.3428633069992066
  batch 750 loss: 1.3472647070884705
  batch 800 loss: 1.3564945316314698
  batch 850 loss: 1.36059312582016
  batch 900 loss: 1.3369605588912963
avg val loss: 1.3069844245910645
LOSS train 1.33696 valid 1.30698, valid PER 45.42%
EPOCH 9, Learning Rate: 0.1
  batch 50 loss: 1.2867240524291992
  batch 100 loss: 1.3394175148010254
  batch 150 loss: 1.3297339749336243
  batch 200 loss: 1.2982186841964722
  batch 250 loss: 1.3370150780677796
  batch 300 loss: 1.3290574312210084
  batch 350 loss: 1.3440985918045043
  batch 400 loss: 1.3255963516235352
  batch 450 loss: 1.3337146496772767
  batch 500 loss: 1.287144935131073
  batch 550 loss: 1.3185264873504638
  batch 600 loss: 1.3356037616729737
  batch 650 loss: 1.289861377477646
  batch 700 loss: 1.2862933325767516
  batch 750 loss: 1.2972515785694123
  batch 800 loss: 1.3174789214134217
  batch 850 loss: 1.3214953017234803
  batch 900 loss: 1.2625245881080627
avg val loss: 1.2476937770843506
LOSS train 1.26252 valid 1.24769, valid PER 41.42%
EPOCH 10, Learning Rate: 0.1
  batch 50 loss: 1.2500760555267334
  batch 100 loss: 1.2679296576976775
  batch 150 loss: 1.296696321964264
  batch 200 loss: 1.286859291791916
  batch 250 loss: 1.2836456847190858
  batch 300 loss: 1.2559360432624818
  batch 350 loss: 1.2565945386886597
  batch 400 loss: 1.2411204552650452
  batch 450 loss: 1.2337113177776338
  batch 500 loss: 1.2679382693767547
  batch 550 loss: 1.2843834567070007
  batch 600 loss: 1.2529580867290497
  batch 650 loss: 1.2382619750499726
  batch 700 loss: 1.2548661875724791
  batch 750 loss: 1.2514509642124176
  batch 800 loss: 1.2688945269584655
  batch 850 loss: 1.248901789188385
  batch 900 loss: 1.2652697777748108
avg val loss: 1.2328959703445435
LOSS train 1.26527 valid 1.23290, valid PER 41.93%
EPOCH 11, Learning Rate: 0.1
  batch 50 loss: 1.2267124462127685
  batch 100 loss: 1.226924228668213
  batch 150 loss: 1.20956778049469
  batch 200 loss: 1.2629312360286713
  batch 250 loss: 1.2298587930202485
  batch 300 loss: 1.1962593722343444
  batch 350 loss: 1.2363495922088623
  batch 400 loss: 1.2345542287826539
  batch 450 loss: 1.2155751955509186
  batch 500 loss: 1.1966359007358551
  batch 550 loss: 1.2220836353302003
  batch 600 loss: 1.1988419723510741
  batch 650 loss: 1.2529905927181244
  batch 700 loss: 1.2018854141235351
  batch 750 loss: 1.194667363166809
  batch 800 loss: 1.2599499440193176
  batch 850 loss: 1.228472512960434
  batch 900 loss: 1.2318402755260467
avg val loss: 1.1796165704727173
LOSS train 1.23184 valid 1.17962, valid PER 39.21%
EPOCH 12, Learning Rate: 0.1
  batch 50 loss: 1.20144956946373
  batch 100 loss: 1.190576412677765
  batch 150 loss: 1.1791843891143798
  batch 200 loss: 1.1873263216018677
  batch 250 loss: 1.2164806032180786
  batch 300 loss: 1.1926131093502044
  batch 350 loss: 1.1860246813297273
  batch 400 loss: 1.2076834905147553
  batch 450 loss: 1.1831586682796478
  batch 500 loss: 1.2189612042903901
  batch 550 loss: 1.1409326469898224
  batch 600 loss: 1.1406875658035278
  batch 650 loss: 1.2093668675422669
  batch 700 loss: 1.1752502119541168
  batch 750 loss: 1.179770073890686
  batch 800 loss: 1.1495481383800508
  batch 850 loss: 1.20675687789917
  batch 900 loss: 1.2048055136203766
avg val loss: 1.14218270778656
LOSS train 1.20481 valid 1.14218, valid PER 37.40%
EPOCH 13, Learning Rate: 0.1
  batch 50 loss: 1.1338280963897704
  batch 100 loss: 1.18284197807312
  batch 150 loss: 1.134785076379776
  batch 200 loss: 1.156246064901352
  batch 250 loss: 1.1518837904930115
  batch 300 loss: 1.1281636321544648
  batch 350 loss: 1.157948180437088
  batch 400 loss: 1.1760970640182495
  batch 450 loss: 1.1768907225131988
  batch 500 loss: 1.1398503589630127
  batch 550 loss: 1.153829698562622
  batch 600 loss: 1.1546014714241029
  batch 650 loss: 1.15885679602623
  batch 700 loss: 1.1707189452648163
  batch 750 loss: 1.1245400249958037
  batch 800 loss: 1.1406885266304017
  batch 850 loss: 1.184995526075363
  batch 900 loss: 1.1626957261562347
avg val loss: 1.136997103691101
LOSS train 1.16270 valid 1.13700, valid PER 37.33%
EPOCH 14, Learning Rate: 0.1
  batch 50 loss: 1.1346551811695098
  batch 100 loss: 1.1495838451385498
  batch 150 loss: 1.1067287492752076
  batch 200 loss: 1.145798726081848
  batch 250 loss: 1.1394469785690307
  batch 300 loss: 1.1515280771255494
  batch 350 loss: 1.1139698123931885
  batch 400 loss: 1.125321683883667
  batch 450 loss: 1.1121368169784547
  batch 500 loss: 1.1261682403087616
  batch 550 loss: 1.1503633773326873
  batch 600 loss: 1.1201476490497588
  batch 650 loss: 1.1376352381706238
  batch 700 loss: 1.1534055411815642
  batch 750 loss: 1.1141339004039765
  batch 800 loss: 1.0631248509883882
  batch 850 loss: 1.1441399836540223
  batch 900 loss: 1.1126535618305207
avg val loss: 1.1096348762512207
LOSS train 1.11265 valid 1.10963, valid PER 36.38%
EPOCH 15, Learning Rate: 0.1
  batch 50 loss: 1.1328141355514527
  batch 100 loss: 1.091353759765625
  batch 150 loss: 1.1057339549064635
  batch 200 loss: 1.1356448817253113
  batch 250 loss: 1.1230653381347657
  batch 300 loss: 1.0881245696544648
  batch 350 loss: 1.1089708697795868
  batch 400 loss: 1.0940535032749177
  batch 450 loss: 1.101270637512207
  batch 500 loss: 1.0754023611545562
  batch 550 loss: 1.110443618297577
  batch 600 loss: 1.1253737437725067
  batch 650 loss: 1.1170971047878266
  batch 700 loss: 1.1284220004081726
  batch 750 loss: 1.096361256837845
  batch 800 loss: 1.080620038509369
  batch 850 loss: 1.0817032170295715
  batch 900 loss: 1.107704598903656
avg val loss: 1.1026086807250977
LOSS train 1.10770 valid 1.10261, valid PER 36.14%
EPOCH 16, Learning Rate: 0.1
  batch 50 loss: 1.1004732429981232
  batch 100 loss: 1.0610891103744506
  batch 150 loss: 1.0681848895549775
  batch 200 loss: 1.0829920864105225
  batch 250 loss: 1.1010494124889374
  batch 300 loss: 1.0896474385261536
  batch 350 loss: 1.1027061128616333
  batch 400 loss: 1.1075099742412566
  batch 450 loss: 1.1105640923976898
  batch 500 loss: 1.057812798023224
  batch 550 loss: 1.1080081510543822
  batch 600 loss: 1.0921703052520753
  batch 650 loss: 1.0922693884372712
  batch 700 loss: 1.0651527106761933
  batch 750 loss: 1.085872322320938
  batch 800 loss: 1.1021828496456145
  batch 850 loss: 1.0545429384708405
  batch 900 loss: 1.0718287301063538
avg val loss: 1.0604907274246216
LOSS train 1.07183 valid 1.06049, valid PER 34.06%
EPOCH 17, Learning Rate: 0.1
  batch 50 loss: 1.0746823048591614
  batch 100 loss: 1.0836506962776185
  batch 150 loss: 1.053607439994812
  batch 200 loss: 1.0580921292304992
  batch 250 loss: 1.0657258486747743
  batch 300 loss: 1.0738049840927124
  batch 350 loss: 1.0302911520004272
  batch 400 loss: 1.1031291580200195
  batch 450 loss: 1.0976679491996766
  batch 500 loss: 1.0627738070487975
  batch 550 loss: 1.085211704969406
  batch 600 loss: 1.1264644491672515
  batch 650 loss: 1.0539631342887879
  batch 700 loss: 1.0577800524234773
  batch 750 loss: 1.0327858054637908
  batch 800 loss: 1.0524403262138367
  batch 850 loss: 1.041591968536377
  batch 900 loss: 1.044561949968338
avg val loss: 1.0549664497375488
LOSS train 1.04456 valid 1.05497, valid PER 34.21%
EPOCH 18, Learning Rate: 0.1
  batch 50 loss: 1.0495750224590301
  batch 100 loss: 1.0657724630832672
  batch 150 loss: 1.0672326505184173
  batch 200 loss: 1.037513988018036
  batch 250 loss: 1.046702026128769
  batch 300 loss: 1.0487116003036498
  batch 350 loss: 1.0489949345588685
  batch 400 loss: 1.0197492885589599
  batch 450 loss: 1.083803083896637
  batch 500 loss: 1.068819055557251
  batch 550 loss: 1.0567474246025086
  batch 600 loss: 1.0310308504104615
  batch 650 loss: 1.014886612892151
  batch 700 loss: 1.0678191101551056
  batch 750 loss: 1.037765291929245
  batch 800 loss: 1.0372697949409484
  batch 850 loss: 1.0152156221866608
  batch 900 loss: 1.0683150374889374
avg val loss: 1.0770703554153442
LOSS train 1.06832 valid 1.07707, valid PER 34.86%
EPOCH 19, Learning Rate: 0.05
  batch 50 loss: 0.9734649956226349
  batch 100 loss: 0.982061802148819
  batch 150 loss: 0.9952706003189087
  batch 200 loss: 0.9962874412536621
  batch 250 loss: 1.015734075307846
  batch 300 loss: 0.9922927343845367
  batch 350 loss: 1.00145525932312
  batch 400 loss: 1.0107560098171233
  batch 450 loss: 1.0076857900619507
  batch 500 loss: 1.0044069230556487
  batch 550 loss: 0.9721658313274384
  batch 600 loss: 1.0092249238491058
  batch 650 loss: 1.047402789592743
  batch 700 loss: 0.9847358047962189
  batch 750 loss: 0.9653492105007172
  batch 800 loss: 1.0099831211566925
  batch 850 loss: 1.0138741397857667
  batch 900 loss: 1.014964997768402
avg val loss: 1.0265024900436401
LOSS train 1.01496 valid 1.02650, valid PER 32.91%
EPOCH 20, Learning Rate: 0.05
  batch 50 loss: 0.9773744118213653
  batch 100 loss: 0.9838859498500824
  batch 150 loss: 0.9717675352096558
  batch 200 loss: 0.9998996841907501
  batch 250 loss: 0.9704837131500245
  batch 300 loss: 1.0104082381725312
  batch 350 loss: 0.9607994723320007
  batch 400 loss: 0.9828366816043854
  batch 450 loss: 0.98216956615448
  batch 500 loss: 0.9496871745586395
  batch 550 loss: 1.025432014465332
  batch 600 loss: 0.9572312927246094
  batch 650 loss: 0.9899262309074401
  batch 700 loss: 1.0020974051952363
  batch 750 loss: 0.9737414574623108
  batch 800 loss: 1.0183943402767182
  batch 850 loss: 1.0070962512493133
  batch 900 loss: 1.01158651471138
avg val loss: 1.0163146257400513
LOSS train 1.01159 valid 1.01631, valid PER 32.90%
Training finished in 3.0 minutes.
Model saved to checkpoints/20231210_133925/model_20
Loading model from checkpoints/20231210_133925/model_20
SUB: 15.88%, DEL: 17.10%, INS: 1.33%, COR: 67.02%, PER: 34.31%
