Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=0.5, is_bidir=False)
Total number of model parameters is 166952
EPOCH 1, Learning Rate: 0.5
  batch 50 loss: 6.162901058197021
  batch 100 loss: 3.2517095851898192
  batch 150 loss: 3.173094277381897
  batch 200 loss: 2.9912634181976316
  batch 250 loss: 2.7874070453643798
  batch 300 loss: 2.6165033054351805
  batch 350 loss: 2.4959280490875244
  batch 400 loss: 2.442843270301819
  batch 450 loss: 2.367894797325134
  batch 500 loss: 2.268739154338837
  batch 550 loss: 2.2212659764289855
  batch 600 loss: 2.189474432468414
  batch 650 loss: 2.109442803859711
  batch 700 loss: 2.113019578456879
  batch 750 loss: 2.0575580620765686
  batch 800 loss: 2.0283147144317626
  batch 850 loss: 2.002815508842468
  batch 900 loss: 1.98890394449234
LOSS train 1.98890 valid 1.91471, valid PER 74.22%
EPOCH 2, Learning Rate: 0.5
  batch 50 loss: 1.9426720333099365
  batch 100 loss: 1.8934108185768128
  batch 150 loss: 1.8486825680732728
  batch 200 loss: 1.8511082911491394
  batch 250 loss: 1.8574967336654664
  batch 300 loss: 1.8335775208473206
  batch 350 loss: 1.7446658873558045
  batch 400 loss: 1.766894600391388
  batch 450 loss: 1.734777021408081
  batch 500 loss: 1.7463847851753236
  batch 550 loss: 1.7575830483436585
  batch 600 loss: 1.7028381180763246
  batch 650 loss: 1.7300873184204102
  batch 700 loss: 1.6944351863861085
  batch 750 loss: 1.6858243107795716
  batch 800 loss: 1.6466910743713379
  batch 850 loss: 1.6379058933258057
  batch 900 loss: 1.6675748109817505
LOSS train 1.66757 valid 1.60498, valid PER 62.79%
EPOCH 3, Learning Rate: 0.5
  batch 50 loss: 1.6347188568115234
  batch 100 loss: 1.5892238593101502
  batch 150 loss: 1.5972010469436646
  batch 200 loss: 1.578946485519409
  batch 250 loss: 1.564221031665802
  batch 300 loss: 1.5695643258094787
  batch 350 loss: 1.5988965845108032
  batch 400 loss: 1.5811597967147828
  batch 450 loss: 1.5305687594413757
  batch 500 loss: 1.5440120029449462
  batch 550 loss: 1.533359375
  batch 600 loss: 1.5013033056259155
  batch 650 loss: 1.471467945575714
  batch 700 loss: 1.5023004603385925
  batch 750 loss: 1.5553792786598206
  batch 800 loss: 1.4833463144302368
  batch 850 loss: 1.5234342265129088
  batch 900 loss: 1.4470847201347352
LOSS train 1.44708 valid 1.42000, valid PER 52.73%
EPOCH 4, Learning Rate: 0.5
  batch 50 loss: 1.4515040802955628
  batch 100 loss: 1.4695345973968506
  batch 150 loss: 1.4278870558738708
  batch 200 loss: 1.4746436285972595
  batch 250 loss: 1.465535056591034
  batch 300 loss: 1.4626400923728944
  batch 350 loss: 1.3916359090805053
  batch 400 loss: 1.444729313850403
  batch 450 loss: 1.4264098405838013
  batch 500 loss: 1.3875534844398498
  batch 550 loss: 1.4155811953544617
  batch 600 loss: 1.4400844645500184
  batch 650 loss: 1.4207108426094055
  batch 700 loss: 1.3796121191978454
  batch 750 loss: 1.3753939843177796
  batch 800 loss: 1.342483651638031
  batch 850 loss: 1.3890474891662599
  batch 900 loss: 1.41137389421463
LOSS train 1.41137 valid 1.34629, valid PER 48.47%
EPOCH 5, Learning Rate: 0.5
  batch 50 loss: 1.3579001927375793
  batch 100 loss: 1.3426387763023377
  batch 150 loss: 1.390641791820526
  batch 200 loss: 1.3271185994148254
  batch 250 loss: 1.332266592979431
  batch 300 loss: 1.3542997527122498
  batch 350 loss: 1.3404662466049195
  batch 400 loss: 1.335115852355957
  batch 450 loss: 1.3200060796737672
  batch 500 loss: 1.3389040589332581
  batch 550 loss: 1.2851769471168517
  batch 600 loss: 1.3544385862350463
  batch 650 loss: 1.3168743014335633
  batch 700 loss: 1.3505809235572814
  batch 750 loss: 1.276751480102539
  batch 800 loss: 1.315203276872635
  batch 850 loss: 1.3154770565032958
  batch 900 loss: 1.3239077603816987
LOSS train 1.32391 valid 1.24678, valid PER 42.60%
EPOCH 6, Learning Rate: 0.5
  batch 50 loss: 1.3139418458938599
  batch 100 loss: 1.2695956063270568
  batch 150 loss: 1.2561473381519317
  batch 200 loss: 1.271978225708008
  batch 250 loss: 1.2911881446838378
  batch 300 loss: 1.2754686737060548
  batch 350 loss: 1.2588664698600769
  batch 400 loss: 1.2670536637306213
  batch 450 loss: 1.2605620002746583
  batch 500 loss: 1.260756742954254
  batch 550 loss: 1.2752668738365174
  batch 600 loss: 1.2559152281284331
  batch 650 loss: 1.2570156705379487
  batch 700 loss: 1.241206705570221
  batch 750 loss: 1.2279153382778167
  batch 800 loss: 1.226120845079422
  batch 850 loss: 1.22167627453804
  batch 900 loss: 1.2375634133815765
LOSS train 1.23756 valid 1.20155, valid PER 40.66%
EPOCH 7, Learning Rate: 0.5
  batch 50 loss: 1.217329248189926
  batch 100 loss: 1.2400938177108765
  batch 150 loss: 1.2207011914253234
  batch 200 loss: 1.20102600812912
  batch 250 loss: 1.2147131419181825
  batch 300 loss: 1.1912839710712433
  batch 350 loss: 1.1918941533565521
  batch 400 loss: 1.211904799938202
  batch 450 loss: 1.1908637583255768
  batch 500 loss: 1.184755141735077
  batch 550 loss: 1.1960776805877686
  batch 600 loss: 1.2047411620616912
  batch 650 loss: 1.1987220644950867
  batch 700 loss: 1.2085797667503357
  batch 750 loss: 1.1794342720508575
  batch 800 loss: 1.179286915063858
  batch 850 loss: 1.2108104860782622
  batch 900 loss: 1.238489238023758
LOSS train 1.23849 valid 1.15969, valid PER 38.31%
EPOCH 8, Learning Rate: 0.5
  batch 50 loss: 1.1768615746498108
  batch 100 loss: 1.1755163502693176
  batch 150 loss: 1.155291826725006
  batch 200 loss: 1.1473296117782592
  batch 250 loss: 1.1604320538043975
  batch 300 loss: 1.0939418065547943
  batch 350 loss: 1.1874176025390626
  batch 400 loss: 1.1634559190273286
  batch 450 loss: 1.1645228552818299
  batch 500 loss: 1.1977432680130005
  batch 550 loss: 1.1144018435478211
  batch 600 loss: 1.1612143516540527
  batch 650 loss: 1.1867391657829285
  batch 700 loss: 1.1419194257259369
  batch 750 loss: 1.14270245552063
  batch 800 loss: 1.1449898493289947
  batch 850 loss: 1.1744289541244506
  batch 900 loss: 1.1573671686649323
LOSS train 1.15737 valid 1.13218, valid PER 37.02%
EPOCH 9, Learning Rate: 0.5
  batch 50 loss: 1.0883679187297821
  batch 100 loss: 1.137847683429718
  batch 150 loss: 1.1289763641357422
  batch 200 loss: 1.1097148728370667
  batch 250 loss: 1.131323971748352
  batch 300 loss: 1.1359222149848938
  batch 350 loss: 1.1468376064300536
  batch 400 loss: 1.1505561780929565
  batch 450 loss: 1.131930968761444
  batch 500 loss: 1.103155562877655
  batch 550 loss: 1.1267628455162049
  batch 600 loss: 1.1424702727794647
  batch 650 loss: 1.106675386428833
  batch 700 loss: 1.0819710528850555
  batch 750 loss: 1.1057494735717774
  batch 800 loss: 1.1156627762317657
  batch 850 loss: 1.1347943329811097
  batch 900 loss: 1.104353688955307
LOSS train 1.10435 valid 1.08217, valid PER 34.96%
EPOCH 10, Learning Rate: 0.5
  batch 50 loss: 1.0720553243160247
  batch 100 loss: 1.081715236902237
  batch 150 loss: 1.1214672207832337
  batch 200 loss: 1.1134125244617463
  batch 250 loss: 1.0970721316337586
  batch 300 loss: 1.0729879426956177
  batch 350 loss: 1.0965434181690217
  batch 400 loss: 1.060266354084015
  batch 450 loss: 1.0551899576187134
  batch 500 loss: 1.096064190864563
  batch 550 loss: 1.1070008766651154
  batch 600 loss: 1.0821241760253906
  batch 650 loss: 1.0595722103118896
  batch 700 loss: 1.0823156869411468
  batch 750 loss: 1.0871355628967285
  batch 800 loss: 1.0787565326690673
  batch 850 loss: 1.0916711628437041
  batch 900 loss: 1.1151927268505097
LOSS train 1.11519 valid 1.07622, valid PER 35.93%
EPOCH 11, Learning Rate: 0.5
  batch 50 loss: 1.0569601666927337
  batch 100 loss: 1.0460014832019806
  batch 150 loss: 1.0392029356956483
  batch 200 loss: 1.0894909024238586
  batch 250 loss: 1.085603632926941
  batch 300 loss: 1.0348520612716674
  batch 350 loss: 1.0598267555236816
  batch 400 loss: 1.067157130241394
  batch 450 loss: 1.0690277302265168
  batch 500 loss: 1.040603004693985
  batch 550 loss: 1.052192392349243
  batch 600 loss: 1.0399001812934876
  batch 650 loss: 1.1043882822990418
  batch 700 loss: 1.0341107058525085
  batch 750 loss: 1.0277765238285064
  batch 800 loss: 1.0920008301734925
  batch 850 loss: 1.0838636231422425
  batch 900 loss: 1.0619349122047423
LOSS train 1.06193 valid 1.03715, valid PER 33.78%
EPOCH 12, Learning Rate: 0.5
  batch 50 loss: 1.0357495176792144
  batch 100 loss: 1.0373906636238097
  batch 150 loss: 1.0127863848209382
  batch 200 loss: 1.0300959718227387
  batch 250 loss: 1.0574522757530211
  batch 300 loss: 1.0432184278964995
  batch 350 loss: 1.0328591668605804
  batch 400 loss: 1.0488150584697724
  batch 450 loss: 1.0408909785747529
  batch 500 loss: 1.0775344681739807
  batch 550 loss: 0.9962921297550201
  batch 600 loss: 1.0059234523773193
  batch 650 loss: 1.0593477010726928
  batch 700 loss: 1.0461169409751891
  batch 750 loss: 1.0256303071975708
  batch 800 loss: 1.0031788766384124
  batch 850 loss: 1.068125706911087
  batch 900 loss: 1.057605584859848
LOSS train 1.05761 valid 1.02159, valid PER 33.40%
EPOCH 13, Learning Rate: 0.5
  batch 50 loss: 0.9893704116344452
  batch 100 loss: 1.0241619408130647
  batch 150 loss: 0.9794305562973022
  batch 200 loss: 1.0107946634292602
  batch 250 loss: 1.016017189025879
  batch 300 loss: 0.9913467812538147
  batch 350 loss: 1.014642686843872
  batch 400 loss: 1.0297790467739105
  batch 450 loss: 1.0326724720001221
  batch 500 loss: 1.000591642856598
  batch 550 loss: 1.013365557193756
  batch 600 loss: 1.0170084190368653
  batch 650 loss: 1.019471869468689
  batch 700 loss: 1.0253163540363313
  batch 750 loss: 0.9812300872802734
  batch 800 loss: 0.9866434788703918
  batch 850 loss: 1.0391772639751435
  batch 900 loss: 1.0016849792003633
LOSS train 1.00168 valid 1.01723, valid PER 32.72%
EPOCH 14, Learning Rate: 0.5
  batch 50 loss: 0.9875539755821228
  batch 100 loss: 0.9877061355113983
  batch 150 loss: 0.9861257076263428
  batch 200 loss: 1.000935890674591
  batch 250 loss: 0.9887365412712097
  batch 300 loss: 1.024466996192932
  batch 350 loss: 0.9657617008686066
  batch 400 loss: 0.9859177362918854
  batch 450 loss: 0.9866636157035827
  batch 500 loss: 1.0031797111034393
  batch 550 loss: 1.0132402610778808
  batch 600 loss: 0.9678136324882507
  batch 650 loss: 1.0010366570949554
  batch 700 loss: 1.0138932752609253
  batch 750 loss: 0.9915660440921783
  batch 800 loss: 0.9504244041442871
  batch 850 loss: 1.0123007893562317
  batch 900 loss: 0.9744744098186493
LOSS train 0.97447 valid 1.02228, valid PER 33.29%
EPOCH 15, Learning Rate: 0.5
  batch 50 loss: 0.997714673280716
  batch 100 loss: 0.9643218219280243
  batch 150 loss: 0.9652934622764587
  batch 200 loss: 1.002399742603302
  batch 250 loss: 0.9829957246780395
  batch 300 loss: 0.9681774151325225
  batch 350 loss: 0.9594648265838623
  batch 400 loss: 0.9770761096477508
  batch 450 loss: 0.9680814862251281
  batch 500 loss: 0.9292426836490632
  batch 550 loss: 0.9769294261932373
  batch 600 loss: 0.9993124067783355
  batch 650 loss: 0.998784590959549
  batch 700 loss: 0.9865988636016846
  batch 750 loss: 0.9816035199165344
  batch 800 loss: 0.9569148278236389
  batch 850 loss: 0.9445390999317169
  batch 900 loss: 0.968341566324234
LOSS train 0.96834 valid 1.00697, valid PER 32.87%
EPOCH 16, Learning Rate: 0.5
  batch 50 loss: 0.9836160922050476
  batch 100 loss: 0.9179544258117676
  batch 150 loss: 0.9304092717170716
  batch 200 loss: 0.9518251168727875
  batch 250 loss: 0.9651937901973724
  batch 300 loss: 0.9518929076194763
  batch 350 loss: 0.9697619700431823
  batch 400 loss: 0.9688870859146118
  batch 450 loss: 0.975854514837265
  batch 500 loss: 0.9175037384033203
  batch 550 loss: 0.9543038988113404
  batch 600 loss: 0.9388814043998718
  batch 650 loss: 0.9621266579627991
  batch 700 loss: 0.9383876991271972
  batch 750 loss: 0.9553633666038513
  batch 800 loss: 0.9629351484775543
  batch 850 loss: 0.9562973475456238
  batch 900 loss: 0.9452157390117645
LOSS train 0.94522 valid 0.98034, valid PER 30.72%
EPOCH 17, Learning Rate: 0.5
  batch 50 loss: 0.9382467871904373
  batch 100 loss: 0.9449577331542969
  batch 150 loss: 0.9248089957237243
  batch 200 loss: 0.9101552283763885
  batch 250 loss: 0.9336773252487183
  batch 300 loss: 0.9566867899894714
  batch 350 loss: 0.9084547972679138
  batch 400 loss: 0.9621932637691498
  batch 450 loss: 0.9561393451690674
  batch 500 loss: 0.9340589213371276
  batch 550 loss: 0.9502912390232087
  batch 600 loss: 0.98606738448143
  batch 650 loss: 0.9296936202049255
  batch 700 loss: 0.9330876982212066
  batch 750 loss: 0.9112713921070099
  batch 800 loss: 0.914096075296402
  batch 850 loss: 0.9273402738571167
  batch 900 loss: 0.9164465701580048
LOSS train 0.91645 valid 0.95874, valid PER 29.93%
EPOCH 18, Learning Rate: 0.5
  batch 50 loss: 0.9246151494979858
  batch 100 loss: 0.9269268631935119
  batch 150 loss: 0.932519565820694
  batch 200 loss: 0.9186446392536163
  batch 250 loss: 0.9202990543842315
  batch 300 loss: 0.917898690700531
  batch 350 loss: 0.9311051344871522
  batch 400 loss: 0.884963356256485
  batch 450 loss: 0.9563460004329681
  batch 500 loss: 0.9355281460285186
  batch 550 loss: 0.932753267288208
  batch 600 loss: 0.9122355830669403
  batch 650 loss: 0.9101778399944306
  batch 700 loss: 0.9495374810695648
  batch 750 loss: 0.9158687770366669
  batch 800 loss: 0.9129385495185852
  batch 850 loss: 0.900792293548584
  batch 900 loss: 0.9411553108692169
LOSS train 0.94116 valid 0.97615, valid PER 31.20%
EPOCH 19, Learning Rate: 0.5
  batch 50 loss: 0.8785693192481995
  batch 100 loss: 0.8771194469928741
  batch 150 loss: 0.9036982941627503
  batch 200 loss: 0.9032448196411133
  batch 250 loss: 0.9180483984947204
  batch 300 loss: 0.9114233016967773
  batch 350 loss: 0.9157099413871765
  batch 400 loss: 0.9173646903038025
  batch 450 loss: 0.9203152287006379
  batch 500 loss: 0.9143930387496948
  batch 550 loss: 0.8848991775512696
  batch 600 loss: 0.9056394052505493
  batch 650 loss: 0.963761739730835
  batch 700 loss: 0.8920103788375855
  batch 750 loss: 0.8686539316177369
  batch 800 loss: 0.915166071653366
  batch 850 loss: 0.9238066351413727
  batch 900 loss: 0.9182818734645843
LOSS train 0.91828 valid 0.97169, valid PER 30.58%
EPOCH 20, Learning Rate: 0.5
  batch 50 loss: 0.8863933050632476
  batch 100 loss: 0.8762517070770264
  batch 150 loss: 0.8590941274166107
  batch 200 loss: 0.9002164340019226
  batch 250 loss: 0.8904634845256806
  batch 300 loss: 0.904460437297821
  batch 350 loss: 0.8709016072750092
  batch 400 loss: 0.8860392510890961
  batch 450 loss: 0.9001080393791199
  batch 500 loss: 0.8645520150661469
  batch 550 loss: 0.9351633298397064
  batch 600 loss: 0.8682088053226471
  batch 650 loss: 0.9082975459098815
  batch 700 loss: 0.8982115149497986
  batch 750 loss: 0.8677313870191574
  batch 800 loss: 0.9343347775936127
  batch 850 loss: 0.9067480444908143
  batch 900 loss: 0.925058377981186
LOSS train 0.92506 valid 0.94342, valid PER 29.91%
Training finished in 4.0 minutes.
Model saved to checkpoints/20231207_201245/model_20
Loading model from checkpoints/20231207_201245/model_20
SUB: 15.57%, DEL: 13.89%, INS: 1.79%, COR: 70.54%, PER: 31.25%
