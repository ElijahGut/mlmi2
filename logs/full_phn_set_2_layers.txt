Namespace(seed=123, train_json='train_fbank61.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=2, fbank_dims=23, model_dims=128, concat=1, lr=0.9, vocab='vocab_61.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 568127
EPOCH 1, Learning Rate: 0.9
  batch 50 loss: 5.295190877914429
  batch 100 loss: 3.8819590759277345
  batch 150 loss: 3.7879956483840944
  batch 200 loss: 3.6667218732833864
  batch 250 loss: 3.5672971391677857
  batch 300 loss: 3.32653546333313
  batch 350 loss: 3.1218039751052857
  batch 400 loss: 3.0284343576431274
  batch 450 loss: 2.7989861869812014
  batch 500 loss: 2.6272055816650393
  batch 550 loss: 2.504890561103821
  batch 600 loss: 2.3946739721298216
  batch 650 loss: 2.295271887779236
  batch 700 loss: 2.238787977695465
  batch 750 loss: 2.175373876094818
  batch 800 loss: 2.125460994243622
  batch 850 loss: 2.0780778312683106
  batch 900 loss: 2.0249059796333313
running loss: 47.136189699172974
LOSS train 2.02491 valid 3.15298, valid PER 62.98%
EPOCH 2, Learning Rate: 0.9
  batch 50 loss: 1.962797803878784
  batch 100 loss: 1.9522581267356873
  batch 150 loss: 1.814703233242035
  batch 200 loss: 1.8167456245422364
  batch 250 loss: 1.7875914907455444
  batch 300 loss: 1.7540003561973572
  batch 350 loss: 1.7798000288009643
  batch 400 loss: 1.7324521803855897
  batch 450 loss: 1.6990957355499268
  batch 500 loss: 1.6708146333694458
  batch 550 loss: 1.6671869921684266
  batch 600 loss: 1.636591534614563
  batch 650 loss: 1.5928498768806458
  batch 700 loss: 1.5785445809364318
  batch 750 loss: 1.5754450392723083
  batch 800 loss: 1.539693365097046
  batch 850 loss: 1.5558778309822083
  batch 900 loss: 1.4984155344963073
running loss: 37.20116341114044
LOSS train 1.49842 valid 2.90210, valid PER 38.93%
EPOCH 3, Learning Rate: 0.9
  batch 50 loss: 1.4294805026054382
  batch 100 loss: 1.4954109764099122
  batch 150 loss: 1.5261800622940063
  batch 200 loss: 1.4754490923881531
  batch 250 loss: 1.4512979578971863
  batch 300 loss: 1.4782220005989075
  batch 350 loss: 1.4760239338874817
  batch 400 loss: 1.4599647903442383
  batch 450 loss: 1.4311612606048585
  batch 500 loss: 1.3914609611034394
  batch 550 loss: 1.4075470542907715
  batch 600 loss: 1.3481183528900147
  batch 650 loss: 1.3855539989471435
  batch 700 loss: 1.3623607635498047
  batch 750 loss: 1.3848623371124267
  batch 800 loss: 1.4200505757331847
  batch 850 loss: 1.3543349492549897
  batch 900 loss: 1.4111364102363586
running loss: 34.333027958869934
LOSS train 1.41114 valid 2.80728, valid PER 33.53%
EPOCH 4, Learning Rate: 0.9
  batch 50 loss: 1.378924194574356
  batch 100 loss: 1.2954100465774536
  batch 150 loss: 1.3189595389366149
  batch 200 loss: 1.305880113840103
  batch 250 loss: 1.3061307621002198
  batch 300 loss: 1.31680722117424
  batch 350 loss: 1.2915749597549437
  batch 400 loss: 1.2617156541347503
  batch 450 loss: 1.272070322036743
  batch 500 loss: 1.3451379239559174
  batch 550 loss: 1.2746982562541962
  batch 600 loss: 1.2812368631362916
  batch 650 loss: 1.3068223476409913
  batch 700 loss: 1.3383547830581666
  batch 750 loss: 1.2948929715156554
  batch 800 loss: 1.2455067706108094
  batch 850 loss: 1.2690457916259765
  batch 900 loss: 1.2634980630874635
running loss: 30.305129647254944
LOSS train 1.26350 valid 2.86526, valid PER 30.25%
EPOCH 5, Learning Rate: 0.9
  batch 50 loss: 1.2222606766223907
  batch 100 loss: 1.2132789623737334
  batch 150 loss: 1.2177693927288056
  batch 200 loss: 1.2484630846977234
  batch 250 loss: 1.1887836730480195
  batch 300 loss: 1.2537963616847991
  batch 350 loss: 1.1989804434776306
  batch 400 loss: 1.1969023883342742
  batch 450 loss: 1.186231701374054
  batch 500 loss: 1.1525565683841705
  batch 550 loss: 1.222804206609726
  batch 600 loss: 1.2098401284217835
  batch 650 loss: 1.2190830290317536
  batch 700 loss: 1.2144933831691742
  batch 750 loss: 1.193815323114395
  batch 800 loss: 1.2446678960323334
  batch 850 loss: 1.2401941287517548
  batch 900 loss: 1.1990352368354797
running loss: 28.924600481987
LOSS train 1.19904 valid 2.79569, valid PER 30.39%
EPOCH 6, Learning Rate: 0.9
  batch 50 loss: 1.182226275205612
  batch 100 loss: 1.1768556892871858
  batch 150 loss: 1.1561230540275573
  batch 200 loss: 1.1089101123809815
  batch 250 loss: 1.1672509431838989
  batch 300 loss: 1.1710773992538452
  batch 350 loss: 1.1901761722564697
  batch 400 loss: 1.1520896637439728
  batch 450 loss: 1.1745540189743042
  batch 500 loss: 1.0991897130012511
  batch 550 loss: 1.14627019405365
  batch 600 loss: 1.1417532062530518
  batch 650 loss: 1.1395993292331696
  batch 700 loss: 1.11681410074234
  batch 750 loss: 1.1870104932785035
  batch 800 loss: 1.1371061146259307
  batch 850 loss: 1.1720334112644195
  batch 900 loss: 1.1928614711761474
running loss: 26.667315542697906
LOSS train 1.19286 valid 2.77133, valid PER 29.28%
EPOCH 7, Learning Rate: 0.9
  batch 50 loss: 1.093801691532135
  batch 100 loss: 1.147404112815857
  batch 150 loss: 1.0594173002243041
  batch 200 loss: 1.1133036291599274
  batch 250 loss: 1.1760040807724
  batch 300 loss: 1.1145978081226349
  batch 350 loss: 1.1562878060340882
  batch 400 loss: 1.0831881606578826
  batch 450 loss: 1.1000779032707215
  batch 500 loss: 1.0931333827972411
  batch 550 loss: 1.109527359008789
  batch 600 loss: 1.0926807725429535
  batch 650 loss: 1.065723456144333
  batch 700 loss: 1.1027789151668548
  batch 750 loss: 1.0663309121131896
  batch 800 loss: 1.0826827597618103
  batch 850 loss: 1.1170298385620117
  batch 900 loss: 1.0850644671916962
running loss: 25.71845656633377
LOSS train 1.08506 valid 2.79914, valid PER 29.27%
EPOCH 8, Learning Rate: 0.9
  batch 50 loss: 1.0754223108291625
  batch 100 loss: 1.036101816892624
  batch 150 loss: 1.0821694099903107
  batch 200 loss: 1.0529371392726898
  batch 250 loss: 1.0507286441326142
  batch 300 loss: 1.012280068397522
  batch 350 loss: 1.0506669116020202
  batch 400 loss: 1.0586880564689636
  batch 450 loss: 1.1070572638511658
  batch 500 loss: 1.0847967779636383
  batch 550 loss: 1.0661408197879791
  batch 600 loss: 1.0365724396705627
  batch 650 loss: 1.059155397415161
  batch 700 loss: 1.0702066266536712
  batch 750 loss: 1.0795436537265777
  batch 800 loss: 1.0672943270206452
  batch 850 loss: 1.0380994987487793
  batch 900 loss: 1.0525311064720153
running loss: 24.780496299266815
LOSS train 1.05253 valid 2.81179, valid PER 26.85%
EPOCH 9, Learning Rate: 0.9
  batch 50 loss: 1.0196950876712798
  batch 100 loss: 0.9920457422733306
  batch 150 loss: 1.0118144941329956
  batch 200 loss: 1.0026586854457855
  batch 250 loss: 0.9893434238433838
  batch 300 loss: 1.0377071964740754
  batch 350 loss: 1.025350320339203
  batch 400 loss: 1.0772177767753601
  batch 450 loss: 1.0680703353881835
  batch 500 loss: 1.0348738729953766
  batch 550 loss: 1.0515454280376435
  batch 600 loss: 1.0685141777992249
  batch 650 loss: 1.0413511323928832
  batch 700 loss: 1.0190059661865234
  batch 750 loss: 1.05517351269722
  batch 800 loss: 1.089264681339264
  batch 850 loss: 1.064944509267807
  batch 900 loss: 1.0164800465106965
running loss: 25.397092282772064
LOSS train 1.01648 valid 2.87791, valid PER 26.69%
EPOCH 10, Learning Rate: 0.45
  batch 50 loss: 0.9473063540458679
  batch 100 loss: 0.933800722360611
  batch 150 loss: 0.9429952323436737
  batch 200 loss: 0.9062709188461304
  batch 250 loss: 0.9204996192455291
  batch 300 loss: 0.9167966270446777
  batch 350 loss: 0.8914016079902649
  batch 400 loss: 0.8681050181388855
  batch 450 loss: 0.8776884937286377
  batch 500 loss: 0.8893038392066955
  batch 550 loss: 0.9127189648151398
  batch 600 loss: 0.9084096550941467
  batch 650 loss: 0.9180004370212554
  batch 700 loss: 0.9012970113754273
  batch 750 loss: 0.913971129655838
  batch 800 loss: 0.9307526516914367
  batch 850 loss: 0.8778715932369232
  batch 900 loss: 0.8813077425956726
running loss: 21.89548361301422
LOSS train 0.88131 valid 2.89686, valid PER 25.16%
EPOCH 11, Learning Rate: 0.45
  batch 50 loss: 0.8759538471698761
  batch 100 loss: 0.8520673191547394
  batch 150 loss: 0.8580203759670257
  batch 200 loss: 0.8322055506706237
  batch 250 loss: 0.8607914268970489
  batch 300 loss: 0.8301766884326934
  batch 350 loss: 0.8759379315376282
  batch 400 loss: 0.8732874155044555
  batch 450 loss: 0.864313553571701
  batch 500 loss: 0.8719476163387299
  batch 550 loss: 0.8641769027709961
  batch 600 loss: 0.8620593070983886
  batch 650 loss: 0.8636120223999023
  batch 700 loss: 0.9485314214229583
  batch 750 loss: 0.8702490019798279
  batch 800 loss: 0.8866133737564087
  batch 850 loss: 0.887134518623352
  batch 900 loss: 0.8831330955028533
running loss: 19.826686680316925
LOSS train 0.88313 valid 2.89468, valid PER 24.42%
EPOCH 12, Learning Rate: 0.45
  batch 50 loss: 0.8236031806468964
  batch 100 loss: 0.804063423871994
  batch 150 loss: 0.8303544628620148
  batch 200 loss: 0.8610100138187409
  batch 250 loss: 0.8233448362350464
  batch 300 loss: 0.8582650899887085
  batch 350 loss: 0.831834477186203
  batch 400 loss: 0.8690780878067017
  batch 450 loss: 0.8437108445167542
  batch 500 loss: 0.8477672922611237
  batch 550 loss: 0.8550515484809875
  batch 600 loss: 0.8618309736251831
  batch 650 loss: 0.8631378138065338
  batch 700 loss: 0.8411382794380188
  batch 750 loss: 0.8910244655609131
  batch 800 loss: 0.8184648680686951
  batch 850 loss: 0.8431712067127228
  batch 900 loss: 0.8756263434886933
running loss: 20.80325961112976
LOSS train 0.87563 valid 2.94908, valid PER 24.35%
EPOCH 13, Learning Rate: 0.225
  batch 50 loss: 0.7751073324680329
  batch 100 loss: 0.7883565819263458
  batch 150 loss: 0.7947415137290954
  batch 200 loss: 0.7410672700405121
  batch 250 loss: 0.7814720010757447
  batch 300 loss: 0.8072835278511047
  batch 350 loss: 0.7460756015777588
  batch 400 loss: 0.7727774119377137
  batch 450 loss: 0.7864138460159302
  batch 500 loss: 0.7844627547264099
  batch 550 loss: 0.8240608584880829
  batch 600 loss: 0.7876761871576309
  batch 650 loss: 0.7839196240901947
  batch 700 loss: 0.7951422667503357
  batch 750 loss: 0.7387807273864746
  batch 800 loss: 0.7741112804412842
  batch 850 loss: 0.7722740417718887
  batch 900 loss: 0.8178980755805969
running loss: 18.041391372680664
LOSS train 0.81790 valid 2.94993, valid PER 23.07%
EPOCH 14, Learning Rate: 0.225
  batch 50 loss: 0.7601944100856781
  batch 100 loss: 0.7306695342063904
  batch 150 loss: 0.7742850255966186
  batch 200 loss: 0.7468423926830292
  batch 250 loss: 0.7554471927881241
  batch 300 loss: 0.747952139377594
  batch 350 loss: 0.7487793982028961
  batch 400 loss: 0.7795563805103302
  batch 450 loss: 0.7354753994941712
  batch 500 loss: 0.753007572889328
  batch 550 loss: 0.7756281864643096
  batch 600 loss: 0.7479589629173279
  batch 650 loss: 0.7830107688903809
  batch 700 loss: 0.7623031961917878
  batch 750 loss: 0.758384485244751
  batch 800 loss: 0.7612816339731217
  batch 850 loss: 0.7823358833789825
  batch 900 loss: 0.770289477109909
running loss: 17.811656057834625
LOSS train 0.77029 valid 2.93545, valid PER 23.12%
EPOCH 15, Learning Rate: 0.225
  batch 50 loss: 0.7024240797758102
  batch 100 loss: 0.7351195853948593
  batch 150 loss: 0.7554089260101319
  batch 200 loss: 0.7712715148925782
  batch 250 loss: 0.7680120539665222
  batch 300 loss: 0.7578453433513641
  batch 350 loss: 0.7373137140274048
  batch 400 loss: 0.7414195036888123
  batch 450 loss: 0.737136282324791
  batch 500 loss: 0.724514741897583
  batch 550 loss: 0.7722902369499206
  batch 600 loss: 0.795218266248703
  batch 650 loss: 0.7430994915962219
  batch 700 loss: 0.7166831493377686
  batch 750 loss: 0.777222353219986
  batch 800 loss: 0.721981635093689
  batch 850 loss: 0.744207135438919
  batch 900 loss: 0.7165064424276352
running loss: 18.389274299144745
LOSS train 0.71651 valid 2.96772, valid PER 23.32%
EPOCH 16, Learning Rate: 0.1125
  batch 50 loss: 0.7306483769416809
  batch 100 loss: 0.7160627472400666
  batch 150 loss: 0.7276463437080384
  batch 200 loss: 0.7360833442211151
  batch 250 loss: 0.7034227859973907
  batch 300 loss: 0.7126595282554626
  batch 350 loss: 0.7149967551231384
  batch 400 loss: 0.6942209208011627
  batch 450 loss: 0.727123429775238
  batch 500 loss: 0.7067523318529129
  batch 550 loss: 0.6838137400150299
  batch 600 loss: 0.7639638817310334
  batch 650 loss: 0.7141862368583679
  batch 700 loss: 0.6685384446382523
  batch 750 loss: 0.7102616345882415
  batch 800 loss: 0.6817485147714615
  batch 850 loss: 0.6894866526126862
  batch 900 loss: 0.6997092545032502
running loss: 16.657104074954987
LOSS train 0.69971 valid 2.98816, valid PER 22.70%
EPOCH 17, Learning Rate: 0.1125
  batch 50 loss: 0.732465386390686
  batch 100 loss: 0.6472583872079849
  batch 150 loss: 0.7131003457307815
  batch 200 loss: 0.6676803696155548
  batch 250 loss: 0.6954889553785324
  batch 300 loss: 0.6876055777072907
  batch 350 loss: 0.703939049243927
  batch 400 loss: 0.7027541548013687
  batch 450 loss: 0.6691971892118453
  batch 500 loss: 0.7179041028022766
  batch 550 loss: 0.6913565790653229
  batch 600 loss: 0.7358441442251206
  batch 650 loss: 0.6785747480392456
  batch 700 loss: 0.7121501171588898
  batch 750 loss: 0.6701293909549713
  batch 800 loss: 0.6996215462684632
  batch 850 loss: 0.7008864045143127
  batch 900 loss: 0.6982091534137725
running loss: 16.31369060277939
LOSS train 0.69821 valid 3.00823, valid PER 22.58%
EPOCH 18, Learning Rate: 0.1125
  batch 50 loss: 0.661300967335701
  batch 100 loss: 0.6651559865474701
  batch 150 loss: 0.7202088153362274
  batch 200 loss: 0.683284347653389
  batch 250 loss: 0.6559620362520218
  batch 300 loss: 0.6687314826250076
  batch 350 loss: 0.6406769990921021
  batch 400 loss: 0.6779498106241226
  batch 450 loss: 0.6880609869956971
  batch 500 loss: 0.6843512904644012
  batch 550 loss: 0.7094893503189087
  batch 600 loss: 0.6885771840810776
  batch 650 loss: 0.6957522577047348
  batch 700 loss: 0.6950216144323349
  batch 750 loss: 0.7039312952756882
  batch 800 loss: 0.7069378858804702
  batch 850 loss: 0.7070728594064712
  batch 900 loss: 0.6957514566183091
running loss: 16.910376489162445
LOSS train 0.69575 valid 3.02126, valid PER 22.58%
EPOCH 19, Learning Rate: 0.05625
  batch 50 loss: 0.6688003623485566
  batch 100 loss: 0.6797268956899643
  batch 150 loss: 0.6660230362415314
  batch 200 loss: 0.6486316472291946
  batch 250 loss: 0.6667036640644074
  batch 300 loss: 0.7048877483606338
  batch 350 loss: 0.663454931974411
  batch 400 loss: 0.6429882436990738
  batch 450 loss: 0.635730704665184
  batch 500 loss: 0.6443631494045258
  batch 550 loss: 0.6788259249925613
  batch 600 loss: 0.6834273678064346
  batch 650 loss: 0.678322240114212
  batch 700 loss: 0.6939450591802597
  batch 750 loss: 0.6436891293525696
  batch 800 loss: 0.6582386541366577
  batch 850 loss: 0.6805418539047241
  batch 900 loss: 0.6571629333496094
running loss: 15.113626629114151
LOSS train 0.65716 valid 3.02356, valid PER 22.38%
EPOCH 20, Learning Rate: 0.05625
  batch 50 loss: 0.6453719979524613
  batch 100 loss: 0.6531910592317581
  batch 150 loss: 0.6546133577823638
  batch 200 loss: 0.6532189184427262
  batch 250 loss: 0.6734093606472016
  batch 300 loss: 0.675330120921135
  batch 350 loss: 0.6356801748275757
  batch 400 loss: 0.6491441857814789
  batch 450 loss: 0.6405699646472931
  batch 500 loss: 0.6654484552145005
  batch 550 loss: 0.6887372195720672
  batch 600 loss: 0.6538411855697632
  batch 650 loss: 0.6799118864536285
  batch 700 loss: 0.6549643790721893
  batch 750 loss: 0.6605606538057327
  batch 800 loss: 0.6702564638853074
  batch 850 loss: 0.6679369765520096
  batch 900 loss: 0.6436838048696518
running loss: 16.451806664466858
LOSS train 0.64368 valid 3.06295, valid PER 22.47%
Training finished in 6.0 minutes.
Model saved to checkpoints/20231210_050510/model_6
Loading model from checkpoints/20231210_050510/model_6
SUB: 17.04%, DEL: 12.04%, INS: 1.90%, COR: 70.92%, PER: 30.99%
