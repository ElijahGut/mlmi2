Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.8, optimiser='sgd')
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.1646624946594235
  batch 100 loss: 3.1752349090576173
  batch 150 loss: 3.0281988430023192
  batch 200 loss: 2.9083149814605713
  batch 250 loss: 2.8160940074920653
  batch 300 loss: 2.713265986442566
  batch 350 loss: 2.4702235507965087
  batch 400 loss: 2.382742819786072
  batch 450 loss: 2.303018901348114
  batch 500 loss: 2.174648127555847
  batch 550 loss: 2.101141543388367
  batch 600 loss: 2.0699671411514284
  batch 650 loss: 1.9747537350654603
  batch 700 loss: 1.9683394646644592
  batch 750 loss: 1.9048073267936707
  batch 800 loss: 1.8770430922508239
  batch 850 loss: 1.8204268932342529
  batch 900 loss: 1.8137319612503051
LOSS train 1.81373 valid 1.78335, valid PER 68.26%
EPOCH 2:
  batch 50 loss: 1.768432219028473
  batch 100 loss: 1.7319644737243651
  batch 150 loss: 1.6158475923538207
  batch 200 loss: 1.6470713925361633
  batch 250 loss: 1.6276644635200501
  batch 300 loss: 1.5993104100227356
  batch 350 loss: 1.5937932467460632
  batch 400 loss: 1.5350630402565002
  batch 450 loss: 1.5398473381996154
  batch 500 loss: 1.5085315108299255
  batch 550 loss: 1.4743417644500731
  batch 600 loss: 1.476114523410797
  batch 650 loss: 1.414580113887787
  batch 700 loss: 1.459478142261505
  batch 750 loss: 1.4007968211174011
  batch 800 loss: 1.3687689042091369
  batch 850 loss: 1.3952054023742675
  batch 900 loss: 1.3574249505996705
LOSS train 1.35742 valid 1.36267, valid PER 43.81%
EPOCH 3:
  batch 50 loss: 1.296457724571228
  batch 100 loss: 1.3570537781715393
  batch 150 loss: 1.3504968094825744
  batch 200 loss: 1.2859475588798523
  batch 250 loss: 1.3041716635227203
  batch 300 loss: 1.2922556805610657
  batch 350 loss: 1.316510249376297
  batch 400 loss: 1.2740202116966248
  batch 450 loss: 1.251364471912384
  batch 500 loss: 1.2347332525253296
  batch 550 loss: 1.2452661335468291
  batch 600 loss: 1.1758287537097931
  batch 650 loss: 1.2158808815479278
  batch 700 loss: 1.2188232946395874
  batch 750 loss: 1.2476716148853302
  batch 800 loss: 1.2518377268314362
  batch 850 loss: 1.2218902862071992
  batch 900 loss: 1.2148229050636292
LOSS train 1.21482 valid 1.19625, valid PER 38.23%
EPOCH 4:
  batch 50 loss: 1.2201324200630188
  batch 100 loss: 1.1475475811958313
  batch 150 loss: 1.194887993335724
  batch 200 loss: 1.1529108369350434
  batch 250 loss: 1.1444228172302247
  batch 300 loss: 1.1484106802940368
  batch 350 loss: 1.1589426493644714
  batch 400 loss: 1.1254174017906189
  batch 450 loss: 1.1333727955818176
  batch 500 loss: 1.1925574409961701
  batch 550 loss: 1.1372220611572266
  batch 600 loss: 1.096648586988449
  batch 650 loss: 1.170285668373108
  batch 700 loss: 1.1960334527492522
  batch 750 loss: 1.1197010207176208
  batch 800 loss: 1.1247679162025452
  batch 850 loss: 1.1191667580604554
  batch 900 loss: 1.1205633008480071
LOSS train 1.12056 valid 1.16272, valid PER 35.62%
EPOCH 5:
  batch 50 loss: 1.095309056043625
  batch 100 loss: 1.0833567833900453
  batch 150 loss: 1.1102194750308991
  batch 200 loss: 1.1304833114147186
  batch 250 loss: 1.0789198338985444
  batch 300 loss: 1.1006113576889038
  batch 350 loss: 1.0507174682617189
  batch 400 loss: 1.0452845621109008
  batch 450 loss: 1.036212650537491
  batch 500 loss: 1.0312608778476715
  batch 550 loss: 1.0671713066101074
  batch 600 loss: 1.0922462058067322
  batch 650 loss: 1.0851535129547119
  batch 700 loss: 1.0789909827709199
  batch 750 loss: 1.0556677091121673
  batch 800 loss: 1.0918326699733734
  batch 850 loss: 1.075980315208435
  batch 900 loss: 1.0643569231033325
LOSS train 1.06436 valid 1.09338, valid PER 34.30%
EPOCH 6:
  batch 50 loss: 1.047055870294571
  batch 100 loss: 1.0369926440715789
  batch 150 loss: 1.035050015449524
  batch 200 loss: 1.0009196650981904
  batch 250 loss: 1.0592741537094117
  batch 300 loss: 1.0493105161190033
  batch 350 loss: 1.0460243487358094
  batch 400 loss: 1.010232310295105
  batch 450 loss: 1.0407531583309173
  batch 500 loss: 0.9833446729183197
  batch 550 loss: 1.0124247443675995
  batch 600 loss: 1.035946365594864
  batch 650 loss: 1.0061006772518157
  batch 700 loss: 0.981433709859848
  batch 750 loss: 1.0306472742557526
  batch 800 loss: 1.0224779653549194
  batch 850 loss: 1.0394823634624482
  batch 900 loss: 1.0409200668334961
LOSS train 1.04092 valid 1.05461, valid PER 33.49%
EPOCH 7:
  batch 50 loss: 0.961911963224411
  batch 100 loss: 1.044685307741165
  batch 150 loss: 0.9517225956916809
  batch 200 loss: 0.9864652037620545
  batch 250 loss: 1.0150221967697144
  batch 300 loss: 0.9829593908786773
  batch 350 loss: 1.014068511724472
  batch 400 loss: 0.9672859048843384
  batch 450 loss: 0.9637920999526978
  batch 500 loss: 0.9737219893932343
  batch 550 loss: 0.9903888988494873
  batch 600 loss: 0.9621249747276306
  batch 650 loss: 0.9542604708671569
  batch 700 loss: 1.0612467634677887
  batch 750 loss: 0.9848854875564576
  batch 800 loss: 0.9865166103839874
  batch 850 loss: 1.003058171272278
  batch 900 loss: 0.9746612024307251
LOSS train 0.97466 valid 1.08904, valid PER 34.14%
EPOCH 8:
  batch 50 loss: 0.9911619901657105
  batch 100 loss: 0.9362518393993378
  batch 150 loss: 0.9610418999195098
  batch 200 loss: 0.9203373146057129
  batch 250 loss: 0.9323522007465362
  batch 300 loss: 0.9620535266399384
  batch 350 loss: 0.9332444155216217
  batch 400 loss: 0.951505491733551
  batch 450 loss: 1.0027989053726196
  batch 500 loss: 0.9477184104919434
  batch 550 loss: 0.9506007909774781
  batch 600 loss: 0.905526169538498
  batch 650 loss: 0.9232940280437469
  batch 700 loss: 0.9836540687084198
  batch 750 loss: 0.98993319272995
  batch 800 loss: 1.0233724653720855
  batch 850 loss: 0.9471995413303376
  batch 900 loss: 0.97863196849823
LOSS train 0.97863 valid 1.04478, valid PER 32.27%
EPOCH 9:
  batch 50 loss: 0.929517183303833
  batch 100 loss: 0.8877369081974029
  batch 150 loss: 0.9686291408538819
  batch 200 loss: 0.9504922866821289
  batch 250 loss: 0.9124851727485657
  batch 300 loss: 0.9464344203472137
  batch 350 loss: 0.9223747706413269
  batch 400 loss: 1.018150771856308
  batch 450 loss: 1.001165385246277
  batch 500 loss: 1.0050280892848968
  batch 550 loss: 1.0334462547302246
  batch 600 loss: 1.0299213552474975
  batch 650 loss: 0.9785590195655822
  batch 700 loss: 0.955083976984024
  batch 750 loss: 0.9384336233139038
  batch 800 loss: 0.9523519587516784
  batch 850 loss: 0.9579787027835845
  batch 900 loss: 0.9299682366847992
LOSS train 0.92997 valid 1.02702, valid PER 31.00%
EPOCH 10:
  batch 50 loss: 0.9093788349628449
  batch 100 loss: 0.9183543229103088
  batch 150 loss: 0.9494172012805939
  batch 200 loss: 0.9038132905960083
  batch 250 loss: 0.9002820110321045
  batch 300 loss: 0.9133594393730163
  batch 350 loss: 0.8833046972751617
  batch 400 loss: 0.8818497323989868
  batch 450 loss: 0.874927476644516
  batch 500 loss: 0.9006397080421448
  batch 550 loss: 0.899395443201065
  batch 600 loss: 0.8906580042839051
  batch 650 loss: 0.9134930098056793
  batch 700 loss: 0.9233687508106232
  batch 750 loss: 0.9268623125553132
  batch 800 loss: 0.9100288450717926
  batch 850 loss: 0.888738008737564
  batch 900 loss: 0.8841629505157471
LOSS train 0.88416 valid 0.99476, valid PER 31.31%
EPOCH 11:
  batch 50 loss: 0.8685365676879883
  batch 100 loss: 0.8620989882946014
  batch 150 loss: 0.8475307810306549
  batch 200 loss: 0.8315284204483032
  batch 250 loss: 0.8635998117923737
  batch 300 loss: 0.8943623948097229
  batch 350 loss: 0.9070995718240737
  batch 400 loss: 0.8646825313568115
  batch 450 loss: 0.8464542376995087
  batch 500 loss: 0.8658332669734955
  batch 550 loss: 0.8899929666519165
  batch 600 loss: 0.8599969339370728
  batch 650 loss: 0.8778421485424042
  batch 700 loss: 0.9462507998943329
  batch 750 loss: 0.8800843572616577
  batch 800 loss: 0.9099531030654907
  batch 850 loss: 0.8981111145019531
  batch 900 loss: 0.9355407750606537
LOSS train 0.93554 valid 1.02407, valid PER 31.76%
EPOCH 12:
  batch 50 loss: 0.830658153295517
  batch 100 loss: 0.8470565521717072
  batch 150 loss: 0.8668410408496857
  batch 200 loss: 0.8804127156734467
  batch 250 loss: 0.8170180249214173
  batch 300 loss: 0.887178646326065
  batch 350 loss: 0.8634399819374085
  batch 400 loss: 0.8902569365501404
  batch 450 loss: 0.8434202790260314
  batch 500 loss: 0.8637752604484558
  batch 550 loss: 0.858193621635437
  batch 600 loss: 0.8584809386730194
  batch 650 loss: 0.8570556354522705
  batch 700 loss: 0.8329218602180481
  batch 750 loss: 0.8861738216876983
  batch 800 loss: 0.8257758748531342
  batch 850 loss: 0.8474376130104065
  batch 900 loss: 0.870542596578598
LOSS train 0.87054 valid 1.01094, valid PER 30.90%
EPOCH 13:
  batch 50 loss: 0.8162891566753387
  batch 100 loss: 0.8041079592704773
  batch 150 loss: 0.8255350887775421
  batch 200 loss: 0.8358909904956817
  batch 250 loss: 0.8232932913303376
  batch 300 loss: 0.8495336222648621
  batch 350 loss: 0.8131985592842103
  batch 400 loss: 0.8114937555789947
  batch 450 loss: 0.8587250697612763
  batch 500 loss: 0.8202704060077667
  batch 550 loss: 0.8787535178661346
  batch 600 loss: 0.8360872149467469
  batch 650 loss: 0.8317878258228302
  batch 700 loss: 0.8630891954898834
  batch 750 loss: 0.8320578849315643
  batch 800 loss: 0.8438056766986847
  batch 850 loss: 0.8229057854413986
  batch 900 loss: 0.8452988129854202
LOSS train 0.84530 valid 0.96793, valid PER 29.22%
EPOCH 14:
  batch 50 loss: 0.7990367960929871
  batch 100 loss: 0.7752241504192352
  batch 150 loss: 0.8110618209838867
  batch 200 loss: 0.8083879566192627
  batch 250 loss: 0.856538838148117
  batch 300 loss: 0.8977897417545319
  batch 350 loss: 0.8519117319583893
  batch 400 loss: 0.8731985878944397
  batch 450 loss: 0.8174434053897858
  batch 500 loss: 0.825785278081894
  batch 550 loss: 0.8421010053157807
  batch 600 loss: 0.833778088092804
  batch 650 loss: 0.8528964078426361
  batch 700 loss: 0.8648026823997498
  batch 750 loss: 0.8456247961521148
  batch 800 loss: 0.800233850479126
  batch 850 loss: 0.8624875795841217
  batch 900 loss: 0.8503398466110229
LOSS train 0.85034 valid 0.97957, valid PER 30.50%
EPOCH 15:
  batch 50 loss: 0.7567055952548981
  batch 100 loss: 0.7776999390125274
  batch 150 loss: 0.7730722737312317
  batch 200 loss: 0.8224667727947235
  batch 250 loss: 0.8267687833309174
  batch 300 loss: 0.8249079895019531
  batch 350 loss: 0.79058953166008
  batch 400 loss: 0.8993063676357269
  batch 450 loss: 0.8588576316833496
  batch 500 loss: 0.8420198404788971
  batch 550 loss: 0.8731165659427643
  batch 600 loss: 0.8838230562210083
  batch 650 loss: 0.8347803938388825
  batch 700 loss: 0.8171829390525818
  batch 750 loss: 0.8512823593616485
  batch 800 loss: 0.829905196428299
  batch 850 loss: 0.8328765904903412
  batch 900 loss: 0.7999892163276673
LOSS train 0.79999 valid 0.97713, valid PER 30.74%
EPOCH 16:
  batch 50 loss: 0.7799906754493713
  batch 100 loss: 0.7600270259380341
  batch 150 loss: 0.798142557144165
  batch 200 loss: 0.8295174741744995
  batch 250 loss: 0.7990227043628693
  batch 300 loss: 0.8220892655849457
  batch 350 loss: 0.7977965223789215
  batch 400 loss: 0.7708211076259613
  batch 450 loss: 0.8192982959747315
  batch 500 loss: 0.8252222943305969
  batch 550 loss: 0.8108645141124725
  batch 600 loss: 0.8351875555515289
  batch 650 loss: 0.8190413439273834
  batch 700 loss: 0.8221395647525788
  batch 750 loss: 0.8401812982559204
  batch 800 loss: 0.8319968545436859
  batch 850 loss: 0.8076057612895966
  batch 900 loss: 0.8144205904006958
LOSS train 0.81442 valid 0.98580, valid PER 30.24%
EPOCH 17:
  batch 50 loss: 0.7989232039451599
  batch 100 loss: 0.7266118609905243
  batch 150 loss: 0.8147151327133179
  batch 200 loss: 0.7772179353237152
  batch 250 loss: 0.8614731538295746
  batch 300 loss: 0.8245018339157104
  batch 350 loss: 0.8452766978740692
  batch 400 loss: 0.8449716794490815
  batch 450 loss: 0.8134177613258362
  batch 500 loss: 0.787117509841919
  batch 550 loss: 0.8063823139667511
  batch 600 loss: 0.8234736704826355
  batch 650 loss: 0.8043749558925629
  batch 700 loss: 0.8462600243091584
  batch 750 loss: 0.8105827343463897
  batch 800 loss: 0.9458420324325562
  batch 850 loss: 0.8819029152393341
  batch 900 loss: 0.8548594689369202
LOSS train 0.85486 valid 1.00797, valid PER 30.34%
EPOCH 18:
  batch 50 loss: 0.8065524923801423
  batch 100 loss: 0.8081173777580262
  batch 150 loss: 0.8787783634662628
  batch 200 loss: 0.8317536640167237
  batch 250 loss: 0.7914294803142548
  batch 300 loss: 0.7943433141708374
  batch 350 loss: 0.8015658402442932
  batch 400 loss: 0.8212044978141785
  batch 450 loss: 0.8312424111366272
  batch 500 loss: 0.8366932106018067
  batch 550 loss: 0.8653996062278747
  batch 600 loss: 0.8255143117904663
  batch 650 loss: 0.7963586008548736
  batch 700 loss: 0.792554497718811
  batch 750 loss: 0.798580116033554
  batch 800 loss: 0.8109665787220002
  batch 850 loss: 0.8014345574378967
  batch 900 loss: 0.822350156903267
LOSS train 0.82235 valid 0.99599, valid PER 29.76%
EPOCH 19:
  batch 50 loss: 0.7734585893154144
  batch 100 loss: 0.7661838471889496
  batch 150 loss: 0.734057970046997
  batch 200 loss: 0.7485865950584412
  batch 250 loss: 0.7837082946300507
  batch 300 loss: 0.8089043575525284
  batch 350 loss: 0.8053718173503875
  batch 400 loss: 0.7787199103832245
  batch 450 loss: 0.7387128055095673
  batch 500 loss: 0.7655190062522889
  batch 550 loss: 0.8471640646457672
  batch 600 loss: 0.801766619682312
  batch 650 loss: 0.7952417492866516
  batch 700 loss: 0.8047950518131256
  batch 750 loss: 0.7520579636096955
  batch 800 loss: 0.7761164128780365
  batch 850 loss: 0.8374292874336242
  batch 900 loss: 0.7976876854896545
LOSS train 0.79769 valid 0.99605, valid PER 30.54%
EPOCH 20:
  batch 50 loss: 0.7641798067092895
  batch 100 loss: 0.7912064778804779
  batch 150 loss: 0.7861047947406768
  batch 200 loss: 0.8125393891334534
  batch 250 loss: 0.823238707780838
  batch 300 loss: 0.792464554309845
  batch 350 loss: 0.7831748855113984
  batch 400 loss: 0.8808823227882385
  batch 450 loss: 0.8054143726825714
  batch 500 loss: 0.8113350629806518
  batch 550 loss: 0.8229986441135406
  batch 600 loss: 0.782210283279419
  batch 650 loss: 0.823656724691391
  batch 700 loss: 0.8011492300033569
  batch 750 loss: 0.7756456542015076
  batch 800 loss: 0.8378516602516174
  batch 850 loss: 0.8468201231956481
  batch 900 loss: 0.8396158725023269
LOSS train 0.83962 valid 1.00910, valid PER 31.18%
Training finished in 3.0 minutes.
Model saved to checkpoints/20231129_165405/model_13
Loading model from checkpoints/20231129_165405/model_13
SUB: 17.24%, DEL: 11.08%, INS: 2.66%, COR: 71.68%, PER: 30.97%
