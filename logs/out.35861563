Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.5, optimiser='sgd')
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 11.404234962463379
  batch 100 loss: 10.738182220458985
  batch 150 loss: 10.698575191497802
  batch 200 loss: 10.427136688232421
  batch 250 loss: 10.43640573501587
  batch 300 loss: 10.549966716766358
  batch 350 loss: 10.406076240539551
  batch 400 loss: 10.62867847442627
  batch 450 loss: 10.479939022064208
  batch 500 loss: 10.855805397033691
  batch 550 loss: 10.578770389556885
  batch 600 loss: 10.322712497711182
  batch 650 loss: 10.493767929077148
  batch 700 loss: 10.427129383087157
  batch 750 loss: 10.480947380065919
  batch 800 loss: 10.14320924758911
  batch 850 loss: 10.33704511642456
  batch 900 loss: 10.390701026916505
LOSS train 10.39070 valid 10.60195, valid PER 76.32%
EPOCH 2:
  batch 50 loss: 10.253790664672852
  batch 100 loss: 10.37140630722046
  batch 150 loss: 10.565912971496582
  batch 200 loss: 10.142152976989745
  batch 250 loss: 10.286445732116698
  batch 300 loss: 10.459690961837769
  batch 350 loss: 10.500013275146484
  batch 400 loss: 10.3706205368042
  batch 450 loss: 10.332712268829345
  batch 500 loss: 10.368848571777344
  batch 550 loss: 10.469479675292968
  batch 600 loss: 10.388885841369628
  batch 650 loss: 10.385475578308105
  batch 700 loss: 10.567304553985595
  batch 750 loss: 10.636455154418945
  batch 800 loss: 10.269827060699463
  batch 850 loss: 10.696350116729736
  batch 900 loss: 10.398779468536377
LOSS train 10.39878 valid 10.67053, valid PER 74.44%
EPOCH 3:
  batch 50 loss: 10.461604995727539
  batch 100 loss: 10.648090515136719
  batch 150 loss: 10.42166690826416
  batch 200 loss: 10.372070655822753
  batch 250 loss: 10.384153480529784
  batch 300 loss: 10.371900367736817
  batch 350 loss: 10.391814994812012
  batch 400 loss: 10.44689064025879
  batch 450 loss: 10.716048851013184
  batch 500 loss: 10.254009456634522
  batch 550 loss: 10.410206327438354
  batch 600 loss: 10.360891180038452
  batch 650 loss: 10.312962532043457
  batch 700 loss: 10.56645523071289
  batch 750 loss: 10.455282917022705
  batch 800 loss: 10.370568180084229
  batch 850 loss: 10.288393802642823
  batch 900 loss: 10.161968154907226
LOSS train 10.16197 valid 10.65143, valid PER 75.26%
EPOCH 4:
  batch 50 loss: 10.236750049591064
  batch 100 loss: 10.817768573760986
  batch 150 loss: 10.111482181549071
  batch 200 loss: 10.435528411865235
  batch 250 loss: 10.177846355438232
  batch 300 loss: 10.14820384979248
  batch 350 loss: 10.482809791564941
  batch 400 loss: 10.350867385864257
  batch 450 loss: 10.682527618408203
  batch 500 loss: 10.266485843658447
  batch 550 loss: 10.591984100341797
  batch 600 loss: 10.545496406555175
  batch 650 loss: 10.460071296691895
  batch 700 loss: 10.403312606811523
  batch 750 loss: 10.158369007110595
  batch 800 loss: 10.102282695770263
  batch 850 loss: 10.006459693908692
  batch 900 loss: 10.376925964355468
LOSS train 10.37693 valid 10.55849, valid PER 90.06%
EPOCH 5:
  batch 50 loss: 10.241922664642335
  batch 100 loss: 10.348532066345214
  batch 150 loss: 10.060182857513428
  batch 200 loss: 10.465608825683594
  batch 250 loss: 10.032206573486327
  batch 300 loss: 10.221549758911133
  batch 350 loss: 10.59849515914917
  batch 400 loss: 10.213458271026612
  batch 450 loss: 10.178389053344727
  batch 500 loss: 10.049300003051759
  batch 550 loss: 10.144357872009277
  batch 600 loss: 10.163395004272461
  batch 650 loss: 10.021184349060059
  batch 700 loss: 10.061139125823974
  batch 750 loss: 10.105261726379394
  batch 800 loss: 10.43588451385498
  batch 850 loss: 10.402320461273193
  batch 900 loss: 10.302606287002563
LOSS train 10.30261 valid 10.44147, valid PER 94.01%
EPOCH 6:
  batch 50 loss: 10.301540451049805
  batch 100 loss: 10.158580131530762
  batch 150 loss: 10.046090545654296
  batch 200 loss: 10.148357944488525
  batch 250 loss: 10.293053750991822
  batch 300 loss: 9.746347455978393
  batch 350 loss: 9.829273719787597
  batch 400 loss: 10.354545345306397
  batch 450 loss: 10.124565362930298
  batch 500 loss: 10.063154277801514
  batch 550 loss: 10.358081893920899
  batch 600 loss: 9.985141773223877
  batch 650 loss: 10.160938053131103
  batch 700 loss: 10.332033557891846
  batch 750 loss: 10.084782934188842
  batch 800 loss: 10.246059703826905
  batch 850 loss: 10.0756099319458
  batch 900 loss: 10.172236881256104
LOSS train 10.17224 valid 10.24490, valid PER 95.58%
EPOCH 7:
  batch 50 loss: 10.051598625183106
  batch 100 loss: 10.251161308288575
  batch 150 loss: 10.027390232086182
  batch 200 loss: 9.927740001678467
  batch 250 loss: 10.23683032989502
  batch 300 loss: 9.848362979888917
  batch 350 loss: 10.129251098632812
  batch 400 loss: 10.183931922912597
  batch 450 loss: 10.04465892791748
  batch 500 loss: 9.82015133857727
  batch 550 loss: 10.031402359008789
  batch 600 loss: 10.213769664764405
  batch 650 loss: 10.233058910369873
  batch 700 loss: 9.74806487083435
  batch 750 loss: 10.016961193084716
  batch 800 loss: 9.938772220611572
  batch 850 loss: 10.021381101608277
  batch 900 loss: 9.938869037628173
LOSS train 9.93887 valid 10.15672, valid PER 112.84%
EPOCH 8:
  batch 50 loss: 10.246893634796143
  batch 100 loss: 9.974317245483398
  batch 150 loss: 10.176082859039306
  batch 200 loss: 9.736858873367309
  batch 250 loss: 9.77409496307373
  batch 300 loss: 10.0339080619812
  batch 350 loss: 9.889149684906005
  batch 400 loss: 9.846518592834473
  batch 450 loss: 9.847176923751832
  batch 500 loss: 9.906107006072999
  batch 550 loss: 10.093296737670899
  batch 600 loss: 10.068247594833373
  batch 650 loss: 9.932236890792847
  batch 700 loss: 9.563821926116944
  batch 750 loss: 10.020899295806885
  batch 800 loss: 10.044820957183838
  batch 850 loss: 10.335499954223632
  batch 900 loss: 9.729089403152466
LOSS train 9.72909 valid 10.10978, valid PER 98.20%
EPOCH 9:
  batch 50 loss: 9.849638652801513
  batch 100 loss: 10.156904220581055
  batch 150 loss: 9.956102619171142
  batch 200 loss: 9.880563135147094
  batch 250 loss: 9.681066827774048
  batch 300 loss: 9.966469001770019
  batch 350 loss: 9.961990194320679
  batch 400 loss: 9.698422384262084
  batch 450 loss: 10.061343116760254
  batch 500 loss: 9.687685842514037
  batch 550 loss: 9.78688087463379
  batch 600 loss: 10.201581764221192
  batch 650 loss: 9.826424770355224
  batch 700 loss: 9.988758106231689
  batch 750 loss: 9.81424310684204
  batch 800 loss: 9.734423685073853
  batch 850 loss: 9.942817993164063
  batch 900 loss: 9.799678783416748
LOSS train 9.79968 valid 10.07316, valid PER 108.91%
EPOCH 10:
  batch 50 loss: 9.762007141113282
  batch 100 loss: 9.759986152648926
  batch 150 loss: 9.677757225036622
  batch 200 loss: 9.841331195831298
  batch 250 loss: 9.909173202514648
  batch 300 loss: 10.074819164276123
  batch 350 loss: 9.879133701324463
  batch 400 loss: 9.994083881378174
  batch 450 loss: 9.759765701293945
  batch 500 loss: 9.892528676986695
  batch 550 loss: 9.96656909942627
  batch 600 loss: 9.78025155067444
  batch 650 loss: 9.826809902191162
  batch 700 loss: 9.70389741897583
  batch 750 loss: 9.776090469360351
  batch 800 loss: 9.74650954246521
  batch 850 loss: 9.786396360397339
  batch 900 loss: 9.872583770751953
LOSS train 9.87258 valid 10.05369, valid PER 117.41%
EPOCH 11:
  batch 50 loss: 9.913645420074463
  batch 100 loss: 9.943571605682372
  batch 150 loss: 9.59922360420227
  batch 200 loss: 9.836753692626953
  batch 250 loss: 9.730511045455932
  batch 300 loss: 9.804011344909668
  batch 350 loss: 9.53785665512085
  batch 400 loss: 9.702533950805664
  batch 450 loss: 9.79871265411377
  batch 500 loss: 9.805638761520386
  batch 550 loss: 9.77047682762146
  batch 600 loss: 9.75079273223877
  batch 650 loss: 9.929480190277099
  batch 700 loss: 9.561421298980713
  batch 750 loss: 9.907389945983887
  batch 800 loss: 9.684635486602783
  batch 850 loss: 9.698930835723877
  batch 900 loss: 9.71702699661255
LOSS train 9.71703 valid 10.03743, valid PER 112.92%
EPOCH 12:
  batch 50 loss: 9.718453636169434
  batch 100 loss: 9.571578330993653
  batch 150 loss: 9.759741096496581
  batch 200 loss: 10.07279948234558
  batch 250 loss: 9.777401847839355
  batch 300 loss: 9.40308970451355
  batch 350 loss: 9.947336044311523
  batch 400 loss: 10.169402599334717
  batch 450 loss: 9.665966606140136
  batch 500 loss: 9.747866859436035
  batch 550 loss: 9.568616619110108
  batch 600 loss: 9.841083068847656
  batch 650 loss: 9.819593267440796
  batch 700 loss: 9.650307292938232
  batch 750 loss: 9.560484609603883
  batch 800 loss: 9.417367191314698
  batch 850 loss: 9.57962646484375
  batch 900 loss: 9.741997566223144
LOSS train 9.74200 valid 9.96017, valid PER 107.89%
EPOCH 13:
  batch 50 loss: 9.457519025802613
  batch 100 loss: 9.856732873916625
  batch 150 loss: 9.541059055328368
  batch 200 loss: 9.795658330917359
  batch 250 loss: 9.63242322921753
  batch 300 loss: 9.796667337417603
  batch 350 loss: 9.667167301177978
  batch 400 loss: 9.667112388610839
  batch 450 loss: 9.71396409034729
  batch 500 loss: 9.789697494506836
  batch 550 loss: 9.745046491622924
  batch 600 loss: 9.927145833969115
  batch 650 loss: 9.47882303237915
  batch 700 loss: 9.59719560623169
  batch 750 loss: 9.489125280380248
  batch 800 loss: 9.694658575057984
  batch 850 loss: 9.708638706207275
  batch 900 loss: 9.613138723373414
LOSS train 9.61314 valid 9.95651, valid PER 109.15%
EPOCH 14:
  batch 50 loss: 9.514507608413696
  batch 100 loss: 9.995068798065185
  batch 150 loss: 9.552029857635498
  batch 200 loss: 9.52704068183899
  batch 250 loss: 9.532766342163086
  batch 300 loss: 9.618741722106934
  batch 350 loss: 9.907428703308106
  batch 400 loss: 9.326950435638429
  batch 450 loss: 9.792388229370117
  batch 500 loss: 9.77500665664673
  batch 550 loss: 9.500090646743775
  batch 600 loss: 9.812267341613769
  batch 650 loss: 9.60485493659973
  batch 700 loss: 9.365549478530884
  batch 750 loss: 9.531202144622803
  batch 800 loss: 9.745041828155518
  batch 850 loss: 9.580472717285156
  batch 900 loss: 9.62293864250183
LOSS train 9.62294 valid 9.89640, valid PER 117.72%
EPOCH 15:
  batch 50 loss: 9.536593341827393
  batch 100 loss: 9.398450841903687
  batch 150 loss: 9.576192016601562
  batch 200 loss: 9.724820051193237
  batch 250 loss: 9.79456046104431
  batch 300 loss: 9.499851064682007
  batch 350 loss: 9.421208238601684
  batch 400 loss: 9.648134269714355
  batch 450 loss: 9.5713267993927
  batch 500 loss: 9.621016845703124
  batch 550 loss: 9.774686994552612
  batch 600 loss: 9.457589235305786
  batch 650 loss: 9.732250328063964
  batch 700 loss: 9.398860836029053
  batch 750 loss: 9.696932163238525
  batch 800 loss: 9.621524801254273
  batch 850 loss: 9.758256931304931
  batch 900 loss: 9.66202343940735
LOSS train 9.66202 valid 9.83966, valid PER 112.37%
EPOCH 16:
  batch 50 loss: 9.597251071929932
  batch 100 loss: 9.701162567138672
  batch 150 loss: 9.65876434326172
  batch 200 loss: 9.741133832931519
  batch 250 loss: 9.54020462989807
  batch 300 loss: 9.396812238693236
  batch 350 loss: 9.632625312805176
  batch 400 loss: 9.681899795532226
  batch 450 loss: 9.156689176559448
  batch 500 loss: 9.510746965408325
  batch 550 loss: 9.845330867767334
  batch 600 loss: 9.438957319259643
  batch 650 loss: 9.724401121139527
  batch 700 loss: 9.4524507522583
  batch 750 loss: 9.189982690811156
  batch 800 loss: 9.535405836105348
  batch 850 loss: 9.713050832748413
  batch 900 loss: 9.596514883041381
LOSS train 9.59651 valid 9.86521, valid PER 123.31%
EPOCH 17:
  batch 50 loss: 9.375579853057861
  batch 100 loss: 9.449881839752198
  batch 150 loss: 9.489032649993897
  batch 200 loss: 9.490378856658936
  batch 250 loss: 9.271617822647094
  batch 300 loss: 9.600781145095825
  batch 350 loss: 9.459028358459472
  batch 400 loss: 9.700977392196656
  batch 450 loss: 9.800880336761475
  batch 500 loss: 9.613885068893433
  batch 550 loss: 9.384496183395386
  batch 600 loss: 9.93078546524048
  batch 650 loss: 9.4395201587677
  batch 700 loss: 9.627043590545654
  batch 750 loss: 9.614965801239014
  batch 800 loss: 9.636799812316895
  batch 850 loss: 9.229703016281128
  batch 900 loss: 9.628936777114868
LOSS train 9.62894 valid 9.84558, valid PER 111.40%
EPOCH 18:
  batch 50 loss: 9.228186244964599
  batch 100 loss: 9.584577322006226
  batch 150 loss: 9.549942903518676
  batch 200 loss: 9.730006189346314
  batch 250 loss: 9.547375926971435
  batch 300 loss: 9.721693458557128
  batch 350 loss: 9.679805755615234
  batch 400 loss: 9.508662967681884
  batch 450 loss: 9.451920680999756
  batch 500 loss: 9.522271785736084
  batch 550 loss: 9.336020402908325
  batch 600 loss: 9.623389043807983
  batch 650 loss: 9.320456342697144
  batch 700 loss: 9.351411247253418
  batch 750 loss: 9.578062896728516
  batch 800 loss: 9.71420485496521
  batch 850 loss: 9.376850757598877
  batch 900 loss: 9.232657289505005
LOSS train 9.23266 valid 9.77410, valid PER 118.20%
EPOCH 19:
  batch 50 loss: 9.543627462387086
  batch 100 loss: 9.469809627532959
  batch 150 loss: 9.596939334869385
  batch 200 loss: 9.59157099723816
  batch 250 loss: 9.343179836273194
  batch 300 loss: 9.405416984558105
  batch 350 loss: 9.4849139213562
  batch 400 loss: 9.780895214080811
  batch 450 loss: 9.5869495677948
  batch 500 loss: 9.67097601890564
  batch 550 loss: 9.563324785232544
  batch 600 loss: 9.389571170806885
  batch 650 loss: 9.623383588790894
  batch 700 loss: 9.540112180709839
  batch 750 loss: 9.60768009185791
  batch 800 loss: 9.454173526763917
  batch 850 loss: 9.368745832443237
  batch 900 loss: 9.253226108551026
LOSS train 9.25323 valid 9.80817, valid PER 115.31%
EPOCH 20:
  batch 50 loss: 9.399775056838989
  batch 100 loss: 9.489310884475708
  batch 150 loss: 9.520647983551026
  batch 200 loss: 9.401652660369873
  batch 250 loss: 9.611145057678222
  batch 300 loss: 9.425833530426026
  batch 350 loss: 9.581556539535523
  batch 400 loss: 9.54236620903015
  batch 450 loss: 9.422227420806884
  batch 500 loss: 9.319936113357544
  batch 550 loss: 9.341334524154663
  batch 600 loss: 9.503805990219115
  batch 650 loss: 9.653452644348144
  batch 700 loss: 9.597721443176269
  batch 750 loss: 9.521684589385986
  batch 800 loss: 9.291600465774536
  batch 850 loss: 9.436099939346313
  batch 900 loss: 9.425892391204833
LOSS train 9.42589 valid 9.81706, valid PER 118.58%
Training finished in 4.0 minutes.
Model saved to checkpoints/20231204_181829/model_18
Loading model from checkpoints/20231204_181829/model_18
SUB: 28.53%, DEL: 0.79%, INS: 88.07%, COR: 70.68%, PER: 117.39%
