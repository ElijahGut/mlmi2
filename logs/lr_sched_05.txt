Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 166952
EPOCH 1, Learning Rate: 0.5
  batch 50 loss: 4.77323175907135
  batch 100 loss: 3.209726881980896
  batch 150 loss: 3.0344210481643676
  batch 200 loss: 2.7748667860031127
  batch 250 loss: 2.6124279499053955
  batch 300 loss: 2.454259824752808
  batch 350 loss: 2.3738812685012816
  batch 400 loss: 2.315317392349243
  batch 450 loss: 2.2512377214431765
  batch 500 loss: 2.150974979400635
  batch 550 loss: 2.1261744475364686
  batch 600 loss: 2.0657847690582276
  batch 650 loss: 1.9737617135047913
  batch 700 loss: 1.9777233600616455
  batch 750 loss: 1.9160278129577637
  batch 800 loss: 1.922748508453369
  batch 850 loss: 1.8716695713996887
  batch 900 loss: 1.8682053232192992
avg val loss: 1.839070439338684
LOSS train 1.86821 valid 1.83907, valid PER 71.35%
EPOCH 2, Learning Rate: 0.5
  batch 50 loss: 1.8122872972488404
  batch 100 loss: 1.7427425503730773
  batch 150 loss: 1.7420821738243104
  batch 200 loss: 1.7663992023468018
  batch 250 loss: 1.74264657497406
  batch 300 loss: 1.7154041194915772
  batch 350 loss: 1.6247255301475525
  batch 400 loss: 1.662580940723419
  batch 450 loss: 1.6078159809112549
  batch 500 loss: 1.635519607067108
  batch 550 loss: 1.6501617002487183
  batch 600 loss: 1.5832202696800233
  batch 650 loss: 1.6344748663902282
  batch 700 loss: 1.592690625190735
  batch 750 loss: 1.5783940935134888
  batch 800 loss: 1.5131794357299804
  batch 850 loss: 1.5364371562004089
  batch 900 loss: 1.549061508178711
avg val loss: 1.4537255764007568
LOSS train 1.54906 valid 1.45373, valid PER 56.87%
EPOCH 3, Learning Rate: 0.5
  batch 50 loss: 1.5212499189376831
  batch 100 loss: 1.4785944390296937
  batch 150 loss: 1.473994324207306
  batch 200 loss: 1.4582884073257447
  batch 250 loss: 1.4614763450622559
  batch 300 loss: 1.459193341732025
  batch 350 loss: 1.497130239009857
  batch 400 loss: 1.4779644370079041
  batch 450 loss: 1.4362329387664794
  batch 500 loss: 1.4481181979179383
  batch 550 loss: 1.4362685585021973
  batch 600 loss: 1.3963914823532104
  batch 650 loss: 1.4097080636024475
  batch 700 loss: 1.4036848521232606
  batch 750 loss: 1.4918142104148864
  batch 800 loss: 1.373710217475891
  batch 850 loss: 1.4281651878356934
  batch 900 loss: 1.3643039059638977
avg val loss: 1.3276070356369019
LOSS train 1.36430 valid 1.32761, valid PER 49.33%
EPOCH 4, Learning Rate: 0.5
  batch 50 loss: 1.3600986909866333
  batch 100 loss: 1.3808481359481812
  batch 150 loss: 1.336755805015564
  batch 200 loss: 1.3617959880828858
  batch 250 loss: 1.3631472897529602
  batch 300 loss: 1.3658107924461365
  batch 350 loss: 1.2958519208431243
  batch 400 loss: 1.3635961127281189
  batch 450 loss: 1.3273688638210297
  batch 500 loss: 1.3004915010929108
  batch 550 loss: 1.3332902574539185
  batch 600 loss: 1.3705261397361754
  batch 650 loss: 1.3248568749427796
  batch 700 loss: 1.3001339387893678
  batch 750 loss: 1.2897310853004456
  batch 800 loss: 1.2642348062992097
  batch 850 loss: 1.3028958630561829
  batch 900 loss: 1.3288452124595642
avg val loss: 1.233900785446167
LOSS train 1.32885 valid 1.23390, valid PER 42.97%
EPOCH 5, Learning Rate: 0.5
  batch 50 loss: 1.260111050605774
  batch 100 loss: 1.2486727273464202
  batch 150 loss: 1.2947631406784057
  batch 200 loss: 1.2290987622737886
  batch 250 loss: 1.2464980578422546
  batch 300 loss: 1.262123613357544
  batch 350 loss: 1.270329852104187
  batch 400 loss: 1.2491027212142944
  batch 450 loss: 1.2334963726997374
  batch 500 loss: 1.2386735999584197
  batch 550 loss: 1.1909828627109527
  batch 600 loss: 1.2743434846401214
  batch 650 loss: 1.2218674743175506
  batch 700 loss: 1.2684308540821077
  batch 750 loss: 1.1934923422336579
  batch 800 loss: 1.24518275141716
  batch 850 loss: 1.2318677186965943
  batch 900 loss: 1.2361355328559875
avg val loss: 1.1462260484695435
LOSS train 1.23614 valid 1.14623, valid PER 38.41%
EPOCH 6, Learning Rate: 0.5
  batch 50 loss: 1.225086897611618
  batch 100 loss: 1.1821214044094086
  batch 150 loss: 1.1656892216205597
  batch 200 loss: 1.183696129322052
  batch 250 loss: 1.217177333831787
  batch 300 loss: 1.1851807463169097
  batch 350 loss: 1.1794428706169129
  batch 400 loss: 1.1621322405338288
  batch 450 loss: 1.2020171403884887
  batch 500 loss: 1.1700696301460267
  batch 550 loss: 1.187932769060135
  batch 600 loss: 1.1655616140365601
  batch 650 loss: 1.1929289388656616
  batch 700 loss: 1.1614664340019225
  batch 750 loss: 1.1585209119319915
  batch 800 loss: 1.128972064256668
  batch 850 loss: 1.1265943038463593
  batch 900 loss: 1.1629032611846923
avg val loss: 1.1156123876571655
LOSS train 1.16290 valid 1.11561, valid PER 36.57%
EPOCH 7, Learning Rate: 0.5
  batch 50 loss: 1.1372875094413757
  batch 100 loss: 1.1533881032466888
  batch 150 loss: 1.129846475124359
  batch 200 loss: 1.116119726896286
  batch 250 loss: 1.1361752128601075
  batch 300 loss: 1.1188951492309571
  batch 350 loss: 1.1236248123645782
  batch 400 loss: 1.1236942398548126
  batch 450 loss: 1.1148529815673829
  batch 500 loss: 1.1004965829849243
  batch 550 loss: 1.1088431143760682
  batch 600 loss: 1.1228955972194672
  batch 650 loss: 1.1179553866386414
  batch 700 loss: 1.1214460122585297
  batch 750 loss: 1.1071548473834991
  batch 800 loss: 1.1030454683303832
  batch 850 loss: 1.124370802640915
  batch 900 loss: 1.1600026595592499
avg val loss: 1.1005141735076904
LOSS train 1.16000 valid 1.10051, valid PER 36.26%
EPOCH 8, Learning Rate: 0.5
  batch 50 loss: 1.0893193173408509
  batch 100 loss: 1.0718479228019715
  batch 150 loss: 1.0716881382465362
  batch 200 loss: 1.0405450034141541
  batch 250 loss: 1.0924914300441741
  batch 300 loss: 1.017944266796112
  batch 350 loss: 1.1093144929409027
  batch 400 loss: 1.0993581438064575
  batch 450 loss: 1.0918938148021697
  batch 500 loss: 1.1274904692173005
  batch 550 loss: 1.04391260266304
  batch 600 loss: 1.0920136392116546
  batch 650 loss: 1.1188447284698486
  batch 700 loss: 1.0460780835151673
  batch 750 loss: 1.0560335969924928
  batch 800 loss: 1.0867220878601074
  batch 850 loss: 1.0886140429973603
  batch 900 loss: 1.0756683075428009
avg val loss: 1.0479457378387451
LOSS train 1.07567 valid 1.04795, valid PER 33.36%
EPOCH 9, Learning Rate: 0.5
  batch 50 loss: 1.0107486999034883
  batch 100 loss: 1.0484198212623597
  batch 150 loss: 1.0270082235336304
  batch 200 loss: 1.0125914680957795
  batch 250 loss: 1.0444193518161773
  batch 300 loss: 1.0547600042819978
  batch 350 loss: 1.0745007586479187
  batch 400 loss: 1.0671278631687164
  batch 450 loss: 1.02944495677948
  batch 500 loss: 1.0102929389476776
  batch 550 loss: 1.0315352714061736
  batch 600 loss: 1.0439535355567933
  batch 650 loss: 1.027737616300583
  batch 700 loss: 1.0119169068336487
  batch 750 loss: 1.0296889281272887
  batch 800 loss: 1.0488443207740783
  batch 850 loss: 1.0523605132102967
  batch 900 loss: 1.0177517449855804
avg val loss: 1.015654444694519
LOSS train 1.01775 valid 1.01565, valid PER 31.70%
EPOCH 10, Learning Rate: 0.5
  batch 50 loss: 0.9675940668582916
  batch 100 loss: 0.987149875164032
  batch 150 loss: 1.026098643541336
  batch 200 loss: 1.0103218412399293
  batch 250 loss: 1.0157500863075257
  batch 300 loss: 0.9842582786083222
  batch 350 loss: 1.0211825013160705
  batch 400 loss: 0.9774398148059845
  batch 450 loss: 0.9700953423976898
  batch 500 loss: 1.0145694768428803
  batch 550 loss: 1.0227052187919616
  batch 600 loss: 1.0165705931186677
  batch 650 loss: 0.9851335120201111
  batch 700 loss: 1.0046061992645263
  batch 750 loss: 0.9926086068153381
  batch 800 loss: 1.0071780037879945
  batch 850 loss: 1.012221451997757
  batch 900 loss: 1.033952339887619
avg val loss: 1.0183465480804443
LOSS train 1.03395 valid 1.01835, valid PER 33.38%
EPOCH 11, Learning Rate: 0.25
  batch 50 loss: 0.9437167227268219
  batch 100 loss: 0.9007112348079681
  batch 150 loss: 0.9175587499141693
  batch 200 loss: 0.9433129000663757
  batch 250 loss: 0.9411509799957275
  batch 300 loss: 0.8871149539947509
  batch 350 loss: 0.9247777342796326
  batch 400 loss: 0.9222914731502533
  batch 450 loss: 0.9275897979736328
  batch 500 loss: 0.9122818064689636
  batch 550 loss: 0.9025111615657806
  batch 600 loss: 0.9032131946086883
  batch 650 loss: 0.9539572656154632
  batch 700 loss: 0.894419127702713
  batch 750 loss: 0.9060923886299134
  batch 800 loss: 0.9465888965129853
  batch 850 loss: 0.9452606701850891
  batch 900 loss: 0.9362941670417786
avg val loss: 0.956084132194519
LOSS train 0.93629 valid 0.95608, valid PER 30.35%
EPOCH 12, Learning Rate: 0.25
  batch 50 loss: 0.9103695797920227
  batch 100 loss: 0.8837710690498352
  batch 150 loss: 0.8692919504642487
  batch 200 loss: 0.8909743678569794
  batch 250 loss: 0.901278727054596
  batch 300 loss: 0.900395382642746
  batch 350 loss: 0.9003606426715851
  batch 400 loss: 0.913878014087677
  batch 450 loss: 0.9198619067668915
  batch 500 loss: 0.9185289442539215
  batch 550 loss: 0.8607105398178101
  batch 600 loss: 0.8816478395462036
  batch 650 loss: 0.925000410079956
  batch 700 loss: 0.9167547142505645
  batch 750 loss: 0.8984471094608307
  batch 800 loss: 0.8826302587985992
  batch 850 loss: 0.9323107290267945
  batch 900 loss: 0.9167769968509674
avg val loss: 0.9518627524375916
LOSS train 0.91678 valid 0.95186, valid PER 30.42%
EPOCH 13, Learning Rate: 0.25
  batch 50 loss: 0.8560960459709167
  batch 100 loss: 0.8887704491615296
  batch 150 loss: 0.8737993466854096
  batch 200 loss: 0.8949448585510253
  batch 250 loss: 0.8747677171230316
  batch 300 loss: 0.8618111979961395
  batch 350 loss: 0.8606986391544342
  batch 400 loss: 0.8945422554016114
  batch 450 loss: 0.8859961426258087
  batch 500 loss: 0.8547777652740478
  batch 550 loss: 0.8871518504619599
  batch 600 loss: 0.8667951798439026
  batch 650 loss: 0.8934702360630036
  batch 700 loss: 0.9096314907073975
  batch 750 loss: 0.8589832437038422
  batch 800 loss: 0.8602976202964783
  batch 850 loss: 0.9074482417106629
  batch 900 loss: 0.8923197376728058
avg val loss: 0.9409103393554688
LOSS train 0.89232 valid 0.94091, valid PER 29.65%
EPOCH 14, Learning Rate: 0.25
  batch 50 loss: 0.8468157720565795
  batch 100 loss: 0.8670412528514863
  batch 150 loss: 0.8559368658065796
  batch 200 loss: 0.8661000740528106
  batch 250 loss: 0.8773052358627319
  batch 300 loss: 0.8967229807376862
  batch 350 loss: 0.8344791984558105
  batch 400 loss: 0.8598969233036041
  batch 450 loss: 0.8517362010478974
  batch 500 loss: 0.8536418879032135
  batch 550 loss: 0.8881484568119049
  batch 600 loss: 0.8404674327373505
  batch 650 loss: 0.8865731739997864
  batch 700 loss: 0.8965421617031097
  batch 750 loss: 0.8637722480297089
  batch 800 loss: 0.8299985814094544
  batch 850 loss: 0.8837862062454224
  batch 900 loss: 0.8672403991222382
avg val loss: 0.9435577392578125
LOSS train 0.86724 valid 0.94356, valid PER 29.78%
EPOCH 15, Learning Rate: 0.125
  batch 50 loss: 0.8666225457191468
  batch 100 loss: 0.8169577503204346
  batch 150 loss: 0.8077016985416412
  batch 200 loss: 0.8397134518623353
  batch 250 loss: 0.8415519881248474
  batch 300 loss: 0.8027667438983918
  batch 350 loss: 0.8153016209602356
  batch 400 loss: 0.8153908145427704
  batch 450 loss: 0.8076760303974152
  batch 500 loss: 0.7837665367126465
  batch 550 loss: 0.8190724420547485
  batch 600 loss: 0.8264721858501435
  batch 650 loss: 0.8263883304595947
  batch 700 loss: 0.8411793375015258
  batch 750 loss: 0.8293622565269471
  batch 800 loss: 0.8011533796787262
  batch 850 loss: 0.7966898119449616
  batch 900 loss: 0.818829801082611
avg val loss: 0.9142399430274963
LOSS train 0.81883 valid 0.91424, valid PER 28.90%
EPOCH 16, Learning Rate: 0.125
  batch 50 loss: 0.816573121547699
  batch 100 loss: 0.7771083116531372
  batch 150 loss: 0.7905404746532441
  batch 200 loss: 0.7858241951465607
  batch 250 loss: 0.8122222542762756
  batch 300 loss: 0.8017005610466004
  batch 350 loss: 0.8156428158283233
  batch 400 loss: 0.8197624886035919
  batch 450 loss: 0.8360598361492158
  batch 500 loss: 0.7826170504093171
  batch 550 loss: 0.8024388962984085
  batch 600 loss: 0.801767692565918
  batch 650 loss: 0.8001087462902069
  batch 700 loss: 0.7819411385059357
  batch 750 loss: 0.8031494796276093
  batch 800 loss: 0.8120223069190979
  batch 850 loss: 0.8155106890201569
  batch 900 loss: 0.8088622796535492
avg val loss: 0.9052700400352478
LOSS train 0.80886 valid 0.90527, valid PER 27.86%
EPOCH 17, Learning Rate: 0.125
  batch 50 loss: 0.783113911151886
  batch 100 loss: 0.7989890599250793
  batch 150 loss: 0.7903904020786285
  batch 200 loss: 0.774792162179947
  batch 250 loss: 0.7969350230693817
  batch 300 loss: 0.7920811378955841
  batch 350 loss: 0.7661886829137802
  batch 400 loss: 0.8260821497440338
  batch 450 loss: 0.8001487171649933
  batch 500 loss: 0.7726952087879181
  batch 550 loss: 0.800061068534851
  batch 600 loss: 0.8236510300636292
  batch 650 loss: 0.7786937522888183
  batch 700 loss: 0.7848409414291382
  batch 750 loss: 0.7830334544181824
  batch 800 loss: 0.7810100173950195
  batch 850 loss: 0.7963223487138749
  batch 900 loss: 0.775335436463356
avg val loss: 0.9003859758377075
LOSS train 0.77534 valid 0.90039, valid PER 27.99%
EPOCH 18, Learning Rate: 0.125
  batch 50 loss: 0.7874567532539367
  batch 100 loss: 0.7865858912467957
  batch 150 loss: 0.7987943542003632
  batch 200 loss: 0.7916799587011337
  batch 250 loss: 0.7851093173027038
  batch 300 loss: 0.7641429084539414
  batch 350 loss: 0.7843080437183381
  batch 400 loss: 0.7535045278072358
  batch 450 loss: 0.8123716068267822
  batch 500 loss: 0.7763986301422119
  batch 550 loss: 0.801750146150589
  batch 600 loss: 0.7738719999790191
  batch 650 loss: 0.7608809661865235
  batch 700 loss: 0.8090844202041626
  batch 750 loss: 0.7719029510021209
  batch 800 loss: 0.7761693823337555
  batch 850 loss: 0.7627871310710908
  batch 900 loss: 0.8209163069725036
avg val loss: 0.8985346555709839
LOSS train 0.82092 valid 0.89853, valid PER 28.04%
EPOCH 19, Learning Rate: 0.125
  batch 50 loss: 0.7421167612075805
  batch 100 loss: 0.7483260381221771
  batch 150 loss: 0.7662666058540344
  batch 200 loss: 0.771970477104187
  batch 250 loss: 0.7901744604110718
  batch 300 loss: 0.7713172233104706
  batch 350 loss: 0.7722367858886718
  batch 400 loss: 0.7815537834167481
  batch 450 loss: 0.8011257219314575
  batch 500 loss: 0.7796340751647949
  batch 550 loss: 0.7751231122016907
  batch 600 loss: 0.7747053480148316
  batch 650 loss: 0.8218357872962951
  batch 700 loss: 0.7543665289878845
  batch 750 loss: 0.7759585642814636
  batch 800 loss: 0.7890725576877594
  batch 850 loss: 0.7969190788269043
  batch 900 loss: 0.7870120728015899
avg val loss: 0.9133642911911011
LOSS train 0.78701 valid 0.91336, valid PER 28.30%
EPOCH 20, Learning Rate: 0.0625
  batch 50 loss: 0.7596556389331818
  batch 100 loss: 0.7444300055503845
  batch 150 loss: 0.7458974933624267
  batch 200 loss: 0.7563597118854523
  batch 250 loss: 0.7434977024793625
  batch 300 loss: 0.7579832965135574
  batch 350 loss: 0.7227257090806961
  batch 400 loss: 0.7488926774263382
  batch 450 loss: 0.7416990101337433
  batch 500 loss: 0.7236925983428955
  batch 550 loss: 0.7860686886310577
  batch 600 loss: 0.7472401309013367
  batch 650 loss: 0.7624677264690399
  batch 700 loss: 0.7535193228721618
  batch 750 loss: 0.7277780658006668
  batch 800 loss: 0.7715070390701294
  batch 850 loss: 0.7668186414241791
  batch 900 loss: 0.7732023543119431
avg val loss: 0.8979685306549072
LOSS train 0.77320 valid 0.89797, valid PER 27.30%
Training finished in 3.0 minutes.
Model saved to checkpoints/20231210_133925/model_20
Loading model from checkpoints/20231210_133925/model_20
SUB: 14.89%, DEL: 12.50%, INS: 1.81%, COR: 72.61%, PER: 29.20%
