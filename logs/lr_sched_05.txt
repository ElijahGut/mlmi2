Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 166952
EPOCH 1, Learning Rate: 0.5
  batch 50 loss: 4.773231449127198
  batch 100 loss: 3.2097250032424927
  batch 150 loss: 3.034412121772766
  batch 200 loss: 2.7748588514328003
  batch 250 loss: 2.612419214248657
  batch 300 loss: 2.4542557191848755
  batch 350 loss: 2.3738760805130004
  batch 400 loss: 2.3153163146972657
  batch 450 loss: 2.2512318015098574
  batch 500 loss: 2.150960638523102
  batch 550 loss: 2.1261138892173768
  batch 600 loss: 2.065899667739868
  batch 650 loss: 1.9736427354812622
  batch 700 loss: 1.9786944198608398
  batch 750 loss: 1.914424340724945
  batch 800 loss: 1.9226983046531678
  batch 850 loss: 1.8704944777488708
  batch 900 loss: 1.8674338030815125
running loss: 43.036489725112915
LOSS train 1.86743 valid 1.84190, valid PER 71.09%
EPOCH 2, Learning Rate: 0.5
  batch 50 loss: 1.8133553218841554
  batch 100 loss: 1.7410357880592346
  batch 150 loss: 1.741589560508728
  batch 200 loss: 1.7681737637519837
  batch 250 loss: 1.7430688214302064
  batch 300 loss: 1.715132224559784
  batch 350 loss: 1.6226292490959167
  batch 400 loss: 1.65893794298172
  batch 450 loss: 1.606297917366028
  batch 500 loss: 1.6365212631225585
  batch 550 loss: 1.6486232566833496
  batch 600 loss: 1.5843045449256896
  batch 650 loss: 1.635792806148529
  batch 700 loss: 1.5931988215446473
  batch 750 loss: 1.5764919853210448
  batch 800 loss: 1.5120459818840026
  batch 850 loss: 1.5388119673728944
  batch 900 loss: 1.5496708369255066
running loss: 36.60546326637268
LOSS train 1.54967 valid 1.45546, valid PER 56.81%
EPOCH 3, Learning Rate: 0.5
  batch 50 loss: 1.52070152759552
  batch 100 loss: 1.4756856966018677
  batch 150 loss: 1.4786451840400696
  batch 200 loss: 1.459822039604187
  batch 250 loss: 1.4618630957603456
  batch 300 loss: 1.4591983914375306
  batch 350 loss: 1.4940096306800843
  batch 400 loss: 1.4794228720664977
  batch 450 loss: 1.4337324023246765
  batch 500 loss: 1.4496710467338563
  batch 550 loss: 1.4365396094322205
  batch 600 loss: 1.3995039892196655
  batch 650 loss: 1.4102693557739259
  batch 700 loss: 1.4044014072418214
  batch 750 loss: 1.4883148384094238
  batch 800 loss: 1.3762067031860352
  batch 850 loss: 1.4300014638900758
  batch 900 loss: 1.3618482875823974
running loss: 33.22017800807953
LOSS train 1.36185 valid 1.33400, valid PER 49.51%
EPOCH 4, Learning Rate: 0.5
  batch 50 loss: 1.36640127658844
  batch 100 loss: 1.3830948352813721
  batch 150 loss: 1.334731833934784
  batch 200 loss: 1.3619444727897645
  batch 250 loss: 1.364979429244995
  batch 300 loss: 1.3680337405204772
  batch 350 loss: 1.2980923998355864
  batch 400 loss: 1.362091224193573
  batch 450 loss: 1.3280270636081695
  batch 500 loss: 1.3023220372200013
  batch 550 loss: 1.3284185862541198
  batch 600 loss: 1.3696749687194825
  batch 650 loss: 1.3237581968307495
  batch 700 loss: 1.2965774726867676
  batch 750 loss: 1.2858191561698913
  batch 800 loss: 1.2602406358718872
  batch 850 loss: 1.3022028231620788
  batch 900 loss: 1.3285442233085631
running loss: 31.185546278953552
LOSS train 1.32854 valid 1.23186, valid PER 42.43%
EPOCH 5, Learning Rate: 0.5
  batch 50 loss: 1.2593098735809327
  batch 100 loss: 1.2487241864204406
  batch 150 loss: 1.2941807413101196
  batch 200 loss: 1.2244127345085145
  batch 250 loss: 1.2432057297229766
  batch 300 loss: 1.2594099605083466
  batch 350 loss: 1.2711341488361358
  batch 400 loss: 1.2477512121200562
  batch 450 loss: 1.2255888056755067
  batch 500 loss: 1.2384448051452637
  batch 550 loss: 1.1857971835136414
  batch 600 loss: 1.2785994219779968
  batch 650 loss: 1.2269795215129853
  batch 700 loss: 1.2685342669486999
  batch 750 loss: 1.1817044854164123
  batch 800 loss: 1.2403193759918212
  batch 850 loss: 1.2244922971725465
  batch 900 loss: 1.238640582561493
running loss: 28.026407837867737
LOSS train 1.23864 valid 1.13686, valid PER 38.00%
EPOCH 6, Learning Rate: 0.5
  batch 50 loss: 1.2280040526390075
  batch 100 loss: 1.1635388147830963
  batch 150 loss: 1.1646638214588165
  batch 200 loss: 1.178099855184555
  batch 250 loss: 1.2147134816646576
  batch 300 loss: 1.1914845180511475
  batch 350 loss: 1.166311068534851
  batch 400 loss: 1.1623099172115325
  batch 450 loss: 1.20232613325119
  batch 500 loss: 1.1728561639785766
  batch 550 loss: 1.1915813159942628
  batch 600 loss: 1.161543035507202
  batch 650 loss: 1.1970645773410797
  batch 700 loss: 1.1594129419326782
  batch 750 loss: 1.1598297584056854
  batch 800 loss: 1.1454172110557557
  batch 850 loss: 1.1429290914535521
  batch 900 loss: 1.1591766703128814
running loss: 27.554185271263123
LOSS train 1.15918 valid 1.11459, valid PER 36.02%
EPOCH 7, Learning Rate: 0.5
  batch 50 loss: 1.1325325918197633
  batch 100 loss: 1.1601269280910491
  batch 150 loss: 1.1269093823432923
  batch 200 loss: 1.1126439940929413
  batch 250 loss: 1.1310115480422973
  batch 300 loss: 1.1024439072608947
  batch 350 loss: 1.1222736847400665
  batch 400 loss: 1.1111593890190123
  batch 450 loss: 1.1173943269252777
  batch 500 loss: 1.0990000581741333
  batch 550 loss: 1.1187056124210357
  batch 600 loss: 1.1333204066753388
  batch 650 loss: 1.104785338640213
  batch 700 loss: 1.1334085988998412
  batch 750 loss: 1.0961372089385986
  batch 800 loss: 1.1038967895507812
  batch 850 loss: 1.129742932319641
  batch 900 loss: 1.163380788564682
running loss: 26.4543958902359
LOSS train 1.16338 valid 1.07189, valid PER 35.49%
EPOCH 8, Learning Rate: 0.5
  batch 50 loss: 1.091313406229019
  batch 100 loss: 1.083890858888626
  batch 150 loss: 1.080975741147995
  batch 200 loss: 1.0445696496963501
  batch 250 loss: 1.087144981622696
  batch 300 loss: 1.0135762476921082
  batch 350 loss: 1.104522545337677
  batch 400 loss: 1.071850106716156
  batch 450 loss: 1.0747297728061675
  batch 500 loss: 1.109971216917038
  batch 550 loss: 1.040191468000412
  batch 600 loss: 1.0916050457954407
  batch 650 loss: 1.1140727138519286
  batch 700 loss: 1.0576609289646148
  batch 750 loss: 1.0616761994361879
  batch 800 loss: 1.0889792358875274
  batch 850 loss: 1.0867504358291626
  batch 900 loss: 1.0764717257022858
running loss: 25.770499765872955
LOSS train 1.07647 valid 1.05053, valid PER 33.64%
EPOCH 9, Learning Rate: 0.5
  batch 50 loss: 1.0214961874485016
  batch 100 loss: 1.043661628961563
  batch 150 loss: 1.0460351026058197
  batch 200 loss: 1.0184253704547883
  batch 250 loss: 1.035022464990616
  batch 300 loss: 1.0494796431064606
  batch 350 loss: 1.0815444910526275
  batch 400 loss: 1.0511921274662017
  batch 450 loss: 1.047209986448288
  batch 500 loss: 1.0278271460533142
  batch 550 loss: 1.0327339684963226
  batch 600 loss: 1.0481884384155273
  batch 650 loss: 1.0306534254550934
  batch 700 loss: 1.0106770408153534
  batch 750 loss: 1.0276766955852508
  batch 800 loss: 1.059031250476837
  batch 850 loss: 1.0680692040920257
  batch 900 loss: 1.0078061413764954
running loss: 25.874527513980865
LOSS train 1.00781 valid 1.01624, valid PER 32.22%
EPOCH 10, Learning Rate: 0.25
  batch 50 loss: 0.9379427242279053
  batch 100 loss: 0.9434918200969696
  batch 150 loss: 0.9740777623653412
  batch 200 loss: 0.9626815783977508
  batch 250 loss: 0.9666701602935791
  batch 300 loss: 0.9219776272773743
  batch 350 loss: 0.9543736910820008
  batch 400 loss: 0.9091475117206573
  batch 450 loss: 0.924292985200882
  batch 500 loss: 0.9441679704189301
  batch 550 loss: 0.978260703086853
  batch 600 loss: 0.946626033782959
  batch 650 loss: 0.9177959060668945
  batch 700 loss: 0.9485736632347107
  batch 750 loss: 0.9359380841255188
  batch 800 loss: 0.9445695888996124
  batch 850 loss: 0.9481818544864654
  batch 900 loss: 0.9646185684204102
running loss: 22.302665948867798
LOSS train 0.96462 valid 0.96507, valid PER 31.62%
EPOCH 11, Learning Rate: 0.25
  batch 50 loss: 0.9027941024303436
  batch 100 loss: 0.9122257375717163
  batch 150 loss: 0.9084121918678284
  batch 200 loss: 0.9491477954387665
  batch 250 loss: 0.9382817077636719
  batch 300 loss: 0.89976118683815
  batch 350 loss: 0.9138238263130188
  batch 400 loss: 0.9264061427116395
  batch 450 loss: 0.9285984992980957
  batch 500 loss: 0.9248874187469482
  batch 550 loss: 0.900401200056076
  batch 600 loss: 0.9062587356567383
  batch 650 loss: 0.9584267115592957
  batch 700 loss: 0.8939285695552825
  batch 750 loss: 0.9054590785503387
  batch 800 loss: 0.9344110226631165
  batch 850 loss: 0.9394230127334595
  batch 900 loss: 0.9421836185455322
running loss: 22.031020164489746
LOSS train 0.94218 valid 0.96348, valid PER 30.45%
EPOCH 12, Learning Rate: 0.25
  batch 50 loss: 0.9122239863872528
  batch 100 loss: 0.8858598995208741
  batch 150 loss: 0.8724800515174865
  batch 200 loss: 0.8836590778827668
  batch 250 loss: 0.923263521194458
  batch 300 loss: 0.9042406249046325
  batch 350 loss: 0.9161636328697205
  batch 400 loss: 0.9164589667320251
  batch 450 loss: 0.9090522789955139
  batch 500 loss: 0.9184338510036468
  batch 550 loss: 0.866128557920456
  batch 600 loss: 0.8823208999633789
  batch 650 loss: 0.9242979717254639
  batch 700 loss: 0.9101974868774414
  batch 750 loss: 0.9160823607444764
  batch 800 loss: 0.8805696594715119
  batch 850 loss: 0.9520696079730988
  batch 900 loss: 0.9418670046329498
running loss: 21.8704274892807
LOSS train 0.94187 valid 0.95297, valid PER 30.58%
EPOCH 13, Learning Rate: 0.25
  batch 50 loss: 0.8726893603801728
  batch 100 loss: 0.8916461670398712
  batch 150 loss: 0.8698795187473297
  batch 200 loss: 0.8934814536571503
  batch 250 loss: 0.885558842420578
  batch 300 loss: 0.8666438591480256
  batch 350 loss: 0.8666845118999481
  batch 400 loss: 0.9005637419223785
  batch 450 loss: 0.8772134935855865
  batch 500 loss: 0.8578242790699006
  batch 550 loss: 0.8856267035007477
  batch 600 loss: 0.87000373005867
  batch 650 loss: 0.8999193644523621
  batch 700 loss: 0.9121440601348877
  batch 750 loss: 0.8512172281742096
  batch 800 loss: 0.8654099106788635
  batch 850 loss: 0.9248183524608612
  batch 900 loss: 0.8847275507450104
running loss: 21.83484524488449
LOSS train 0.88473 valid 0.95085, valid PER 29.94%
EPOCH 14, Learning Rate: 0.25
  batch 50 loss: 0.8559008586406708
  batch 100 loss: 0.8794655191898346
  batch 150 loss: 0.8599576699733734
  batch 200 loss: 0.8621086513996125
  batch 250 loss: 0.8727253007888794
  batch 300 loss: 0.9200614285469055
  batch 350 loss: 0.8607168090343476
  batch 400 loss: 0.857717274427414
  batch 450 loss: 0.8537507462501526
  batch 500 loss: 0.8822486960887909
  batch 550 loss: 0.9222536635398865
  batch 600 loss: 0.8486512410640716
  batch 650 loss: 0.9057398617267609
  batch 700 loss: 0.9018772602081299
  batch 750 loss: 0.871901478767395
  batch 800 loss: 0.8294621646404267
  batch 850 loss: 0.8908802092075347
  batch 900 loss: 0.8721887803077698
running loss: 21.98920077085495
LOSS train 0.87219 valid 0.94549, valid PER 30.14%
EPOCH 15, Learning Rate: 0.125
  batch 50 loss: 0.8632490026950836
  batch 100 loss: 0.8292995619773865
  batch 150 loss: 0.814710738658905
  batch 200 loss: 0.8479214501380921
  batch 250 loss: 0.849847559928894
  batch 300 loss: 0.8072396981716156
  batch 350 loss: 0.8262998056411743
  batch 400 loss: 0.8213815784454346
  batch 450 loss: 0.8171253407001495
  batch 500 loss: 0.7862311863899231
  batch 550 loss: 0.821819759607315
  batch 600 loss: 0.8347228825092315
  batch 650 loss: 0.8267844069004059
  batch 700 loss: 0.8444715464115142
  batch 750 loss: 0.831333978176117
  batch 800 loss: 0.812201257944107
  batch 850 loss: 0.7981332433223725
  batch 900 loss: 0.824574682712555
running loss: 19.394786775112152
LOSS train 0.82457 valid 0.92284, valid PER 29.28%
EPOCH 16, Learning Rate: 0.125
  batch 50 loss: 0.8382875013351441
  batch 100 loss: 0.7750564122200012
  batch 150 loss: 0.790743944644928
  batch 200 loss: 0.7880464911460876
  batch 250 loss: 0.8261514520645141
  batch 300 loss: 0.8103053307533264
  batch 350 loss: 0.8238562762737274
  batch 400 loss: 0.8246360230445862
  batch 450 loss: 0.8415092587471008
  batch 500 loss: 0.7850185394287109
  batch 550 loss: 0.7986936950683594
  batch 600 loss: 0.80067467212677
  batch 650 loss: 0.8145553457736969
  batch 700 loss: 0.790141921043396
  batch 750 loss: 0.8081224942207337
  batch 800 loss: 0.8225381553173066
  batch 850 loss: 0.814727452993393
  batch 900 loss: 0.8132414436340332
running loss: 18.9428653717041
LOSS train 0.81324 valid 0.91824, valid PER 28.54%
EPOCH 17, Learning Rate: 0.125
  batch 50 loss: 0.7950483238697053
  batch 100 loss: 0.8029318988323212
  batch 150 loss: 0.7949084985256195
  batch 200 loss: 0.7846657812595368
  batch 250 loss: 0.7984288799762725
  batch 300 loss: 0.8036734926700592
  batch 350 loss: 0.7657871234416962
  batch 400 loss: 0.8295851695537567
  batch 450 loss: 0.80581125497818
  batch 500 loss: 0.7830306541919708
  batch 550 loss: 0.7979484808444977
  batch 600 loss: 0.8395535230636597
  batch 650 loss: 0.7877884352207184
  batch 700 loss: 0.7975092422962189
  batch 750 loss: 0.7856118965148926
  batch 800 loss: 0.7891812294721603
  batch 850 loss: 0.7951073586940766
  batch 900 loss: 0.7866951060295105
running loss: 20.782374501228333
LOSS train 0.78670 valid 0.91373, valid PER 28.28%
EPOCH 18, Learning Rate: 0.0625
  batch 50 loss: 0.7840151870250702
  batch 100 loss: 0.783464002609253
  batch 150 loss: 0.7955538403987884
  batch 200 loss: 0.7830901730060578
  batch 250 loss: 0.773880705833435
  batch 300 loss: 0.7496170210838318
  batch 350 loss: 0.7671351087093353
  batch 400 loss: 0.7529910707473755
  batch 450 loss: 0.7936006343364715
  batch 500 loss: 0.7635929048061371
  batch 550 loss: 0.7914693915843963
  batch 600 loss: 0.7600351285934448
  batch 650 loss: 0.7571360814571381
  batch 700 loss: 0.794025467634201
  batch 750 loss: 0.7567400598526001
  batch 800 loss: 0.7655014193058014
  batch 850 loss: 0.7524619448184967
  batch 900 loss: 0.8010719799995423
running loss: 18.606284081935883
LOSS train 0.80107 valid 0.90118, valid PER 28.02%
EPOCH 19, Learning Rate: 0.0625
  batch 50 loss: 0.7422308027744293
  batch 100 loss: 0.7300225573778153
  batch 150 loss: 0.7617737817764282
  batch 200 loss: 0.7595477414131164
  batch 250 loss: 0.7800112199783326
  batch 300 loss: 0.7646055519580841
  batch 350 loss: 0.756527795791626
  batch 400 loss: 0.7707272326946258
  batch 450 loss: 0.7771377325057983
  batch 500 loss: 0.7639538860321045
  batch 550 loss: 0.7569039690494538
  batch 600 loss: 0.7670151615142822
  batch 650 loss: 0.8056093156337738
  batch 700 loss: 0.7345811426639557
  batch 750 loss: 0.7512641936540604
  batch 800 loss: 0.7788747894763947
  batch 850 loss: 0.783681960105896
  batch 900 loss: 0.7753450798988343
running loss: 18.374626100063324
LOSS train 0.77535 valid 0.90772, valid PER 28.06%
EPOCH 20, Learning Rate: 0.0625
  batch 50 loss: 0.7458774542808533
  batch 100 loss: 0.746835527420044
  batch 150 loss: 0.7435502523183822
  batch 200 loss: 0.7642600607872009
  batch 250 loss: 0.7465331995487213
  batch 300 loss: 0.753025336265564
  batch 350 loss: 0.7384482371807098
  batch 400 loss: 0.7543972891569137
  batch 450 loss: 0.750488270521164
  batch 500 loss: 0.729382357597351
  batch 550 loss: 0.8037938034534454
  batch 600 loss: 0.7470826482772828
  batch 650 loss: 0.7659257090091706
  batch 700 loss: 0.7678845405578614
  batch 750 loss: 0.7404733103513718
  batch 800 loss: 0.7753526377677917
  batch 850 loss: 0.7851826810836792
  batch 900 loss: 0.7789898073673248
running loss: 18.70882773399353
LOSS train 0.77899 valid 0.90547, valid PER 28.14%
Training finished in 4.0 minutes.
Model saved to checkpoints/20231210_031841/model_18
Loading model from checkpoints/20231210_031841/model_18
SUB: 15.53%, DEL: 13.36%, INS: 1.72%, COR: 71.11%, PER: 30.61%
