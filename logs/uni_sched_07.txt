Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.7, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 83496
EPOCH 1, Learning Rate: 0.7
  batch 50 loss: 4.525361604690552
  batch 100 loss: 3.20732861995697
  batch 150 loss: 3.0381683349609374
  batch 200 loss: 2.756871223449707
  batch 250 loss: 2.5735825300216675
  batch 300 loss: 2.429007806777954
  batch 350 loss: 2.339262828826904
  batch 400 loss: 2.303750114440918
  batch 450 loss: 2.227493920326233
  batch 500 loss: 2.1296750998497007
  batch 550 loss: 2.095868294239044
  batch 600 loss: 2.0448054885864257
  batch 650 loss: 1.9912282347679138
  batch 700 loss: 1.9956322240829467
  batch 750 loss: 1.9603222131729126
  batch 800 loss: 1.9468762683868408
  batch 850 loss: 1.8852301669120788
  batch 900 loss: 1.8659870553016662
avg val loss: 1.7969127893447876
LOSS train 1.86599 valid 1.79691, valid PER 68.66%
EPOCH 2, Learning Rate: 0.7
  batch 50 loss: 1.831321234703064
  batch 100 loss: 1.8022091889381409
  batch 150 loss: 1.765630555152893
  batch 200 loss: 1.7933355212211608
  batch 250 loss: 1.7807498741149903
  batch 300 loss: 1.7638057112693786
  batch 350 loss: 1.6758346390724181
  batch 400 loss: 1.6909862399101256
  batch 450 loss: 1.6253612422943116
  batch 500 loss: 1.6840123558044433
  batch 550 loss: 1.6825129532814025
  batch 600 loss: 1.6170352482795716
  batch 650 loss: 1.6695403385162353
  batch 700 loss: 1.6447639608383178
  batch 750 loss: 1.6183608841896058
  batch 800 loss: 1.5680884718894958
  batch 850 loss: 1.5786263418197632
  batch 900 loss: 1.5866623878479005
avg val loss: 1.5047085285186768
LOSS train 1.58666 valid 1.50471, valid PER 52.18%
EPOCH 3, Learning Rate: 0.7
  batch 50 loss: 1.551160626411438
  batch 100 loss: 1.544209794998169
  batch 150 loss: 1.5138229894638062
  batch 200 loss: 1.5038987517356872
  batch 250 loss: 1.4861870741844176
  batch 300 loss: 1.5080992150306702
  batch 350 loss: 1.5659458470344543
  batch 400 loss: 1.5198354959487914
  batch 450 loss: 1.516294436454773
  batch 500 loss: 1.5154485249519347
  batch 550 loss: 1.4871926665306092
  batch 600 loss: 1.4982415008544923
  batch 650 loss: 1.481460952758789
  batch 700 loss: 1.483987395763397
  batch 750 loss: 1.5248291850090028
  batch 800 loss: 1.4755552124977112
  batch 850 loss: 1.4891044855117799
  batch 900 loss: 1.4644014763832092
avg val loss: 1.4085497856140137
LOSS train 1.46440 valid 1.40855, valid PER 44.13%
EPOCH 4, Learning Rate: 0.7
  batch 50 loss: 1.4390209412574768
  batch 100 loss: 1.475709569454193
  batch 150 loss: 1.4176904797554015
  batch 200 loss: 1.4700361108779907
  batch 250 loss: 1.5097152400016784
  batch 300 loss: 1.5092053461074828
  batch 350 loss: 1.4140866136550903
  batch 400 loss: 1.4315574479103088
  batch 450 loss: 1.434562978744507
  batch 500 loss: 1.4656897854804993
  batch 550 loss: 1.4501887464523315
  batch 600 loss: 1.4590861225128173
  batch 650 loss: 1.4575263452529907
  batch 700 loss: 1.4082428646087646
  batch 750 loss: 1.3913648211956025
  batch 800 loss: 1.4157191812992096
  batch 850 loss: 1.4239989686012269
  batch 900 loss: 1.4381769609451294
avg val loss: 1.3662461042404175
LOSS train 1.43818 valid 1.36625, valid PER 44.61%
EPOCH 5, Learning Rate: 0.7
  batch 50 loss: 1.4099419021606445
  batch 100 loss: 1.40597599029541
  batch 150 loss: 1.4879116368293763
  batch 200 loss: 1.343423137664795
  batch 250 loss: 1.331141710281372
  batch 300 loss: 1.418951256275177
  batch 350 loss: 1.3874872040748596
  batch 400 loss: 1.3950335216522216
  batch 450 loss: 1.3880636167526246
  batch 500 loss: 1.418968951702118
  batch 550 loss: 1.348680284023285
  batch 600 loss: 1.4087206959724425
  batch 650 loss: 1.3892991662025451
  batch 700 loss: 1.3936607503890992
  batch 750 loss: 1.3560828924179078
  batch 800 loss: 1.4033118534088134
  batch 850 loss: 1.4454123282432556
  batch 900 loss: 1.4282546377182006
avg val loss: 1.3009305000305176
LOSS train 1.42825 valid 1.30093, valid PER 41.13%
EPOCH 6, Learning Rate: 0.7
  batch 50 loss: 1.4106753087043762
  batch 100 loss: 1.3731298708915711
  batch 150 loss: 1.3937447929382325
  batch 200 loss: 1.3586376929283142
  batch 250 loss: 1.425389018058777
  batch 300 loss: 1.3931315112113953
  batch 350 loss: 1.410195116996765
  batch 400 loss: 1.3606317400932313
  batch 450 loss: 1.3936273241043091
  batch 500 loss: 1.397103705406189
  batch 550 loss: 1.3877761602401733
  batch 600 loss: 1.3726355648040771
  batch 650 loss: 1.3744765377044679
  batch 700 loss: 1.3547771286964416
  batch 750 loss: 1.3350873744487763
  batch 800 loss: 1.3337995386123658
  batch 850 loss: 1.3487863337993622
  batch 900 loss: 1.383408977985382
avg val loss: 1.3153293132781982
LOSS train 1.38341 valid 1.31533, valid PER 41.39%
EPOCH 7, Learning Rate: 0.35
  batch 50 loss: 1.3119335198402404
  batch 100 loss: 1.2973654294013977
  batch 150 loss: 1.2546878254413605
  batch 200 loss: 1.2386451876163482
  batch 250 loss: 1.2626194298267364
  batch 300 loss: 1.2352904152870179
  batch 350 loss: 1.2311267578601837
  batch 400 loss: 1.265879442691803
  batch 450 loss: 1.2638558530807495
  batch 500 loss: 1.2281976222991944
  batch 550 loss: 1.209992254972458
  batch 600 loss: 1.252216238975525
  batch 650 loss: 1.2340641915798187
  batch 700 loss: 1.2521655666828155
  batch 750 loss: 1.2238194036483765
  batch 800 loss: 1.2108000922203064
  batch 850 loss: 1.2489376950263977
  batch 900 loss: 1.276039035320282
avg val loss: 1.1878582239151
LOSS train 1.27604 valid 1.18786, valid PER 38.52%
EPOCH 8, Learning Rate: 0.35
  batch 50 loss: 1.224725694656372
  batch 100 loss: 1.2020694124698639
  batch 150 loss: 1.2316761875152589
  batch 200 loss: 1.1929242634773254
  batch 250 loss: 1.2151535892486571
  batch 300 loss: 1.1696668934822083
  batch 350 loss: 1.254346125125885
  batch 400 loss: 1.2157052421569825
  batch 450 loss: 1.2417090344429016
  batch 500 loss: 1.2381174397468566
  batch 550 loss: 1.187906221151352
  batch 600 loss: 1.2321986055374146
  batch 650 loss: 1.2411650121212006
  batch 700 loss: 1.1963346743583678
  batch 750 loss: 1.2312953805923461
  batch 800 loss: 1.2121523332595825
  batch 850 loss: 1.21738551735878
  batch 900 loss: 1.1866612231731415
avg val loss: 1.1585711240768433
LOSS train 1.18666 valid 1.15857, valid PER 37.14%
EPOCH 9, Learning Rate: 0.35
  batch 50 loss: 1.17196311712265
  batch 100 loss: 1.2056060826778412
  batch 150 loss: 1.2213707149028779
  batch 200 loss: 1.1564810943603516
  batch 250 loss: 1.1910297882556915
  batch 300 loss: 1.204582622051239
  batch 350 loss: 1.2061535143852233
  batch 400 loss: 1.2051231718063355
  batch 450 loss: 1.2055809020996093
  batch 500 loss: 1.17560231924057
  batch 550 loss: 1.2196652948856355
  batch 600 loss: 1.2001423501968385
  batch 650 loss: 1.2039594912528992
  batch 700 loss: 1.1841000998020172
  batch 750 loss: 1.184357008934021
  batch 800 loss: 1.201149319410324
  batch 850 loss: 1.2223901510238648
  batch 900 loss: 1.17401526927948
avg val loss: 1.1579391956329346
LOSS train 1.17402 valid 1.15794, valid PER 37.38%
EPOCH 10, Learning Rate: 0.35
  batch 50 loss: 1.1461912155151368
  batch 100 loss: 1.1893301212787628
  batch 150 loss: 1.2054747748374939
  batch 200 loss: 1.1988942694664002
  batch 250 loss: 1.183178222179413
  batch 300 loss: 1.1213173604011535
  batch 350 loss: 1.1794344425201415
  batch 400 loss: 1.1778908956050873
  batch 450 loss: 1.1574869132041932
  batch 500 loss: 1.264827778339386
  batch 550 loss: 1.2016036224365234
  batch 600 loss: 1.1720414006710052
  batch 650 loss: 1.1694263517856598
  batch 700 loss: 1.2009167647361756
  batch 750 loss: 1.1679926705360413
  batch 800 loss: 1.1784659373760222
  batch 850 loss: 1.2537138116359712
  batch 900 loss: 1.2178337371349335
avg val loss: 1.1608270406723022
LOSS train 1.21783 valid 1.16083, valid PER 37.81%
EPOCH 11, Learning Rate: 0.175
  batch 50 loss: 1.1311990213394165
  batch 100 loss: 1.0983691656589507
  batch 150 loss: 1.096106423139572
  batch 200 loss: 1.160949798822403
  batch 250 loss: 1.1367922973632814
  batch 300 loss: 1.0883918869495393
  batch 350 loss: 1.1461057138442994
  batch 400 loss: 1.11930468916893
  batch 450 loss: 1.1304903781414033
  batch 500 loss: 1.0824466145038605
  batch 550 loss: 1.1267425072193147
  batch 600 loss: 1.1014979875087738
  batch 650 loss: 1.1672956717014313
  batch 700 loss: 1.081360831260681
  batch 750 loss: 1.0885742449760436
  batch 800 loss: 1.121507661342621
  batch 850 loss: 1.1499753451347352
  batch 900 loss: 1.1219755291938782
avg val loss: 1.0961730480194092
LOSS train 1.12198 valid 1.09617, valid PER 35.36%
EPOCH 12, Learning Rate: 0.175
  batch 50 loss: 1.104002717733383
  batch 100 loss: 1.085341707468033
  batch 150 loss: 1.067517170906067
  batch 200 loss: 1.0923848700523378
  batch 250 loss: 1.1137009024620057
  batch 300 loss: 1.0821417152881623
  batch 350 loss: 1.102508965730667
  batch 400 loss: 1.1316145074367523
  batch 450 loss: 1.0947034072875976
  batch 500 loss: 1.1247116124629974
  batch 550 loss: 1.0523351275920867
  batch 600 loss: 1.061119669675827
  batch 650 loss: 1.1211608493328094
  batch 700 loss: 1.0808729791641236
  batch 750 loss: 1.0834062922000884
  batch 800 loss: 1.0682486283779145
  batch 850 loss: 1.0986832642555238
  batch 900 loss: 1.1204685068130493
avg val loss: 1.084128737449646
LOSS train 1.12047 valid 1.08413, valid PER 34.78%
EPOCH 13, Learning Rate: 0.175
  batch 50 loss: 1.0600354743003846
  batch 100 loss: 1.0836214113235474
  batch 150 loss: 1.0570754075050355
  batch 200 loss: 1.0899930238723754
  batch 250 loss: 1.0733178317546845
  batch 300 loss: 1.0552921164035798
  batch 350 loss: 1.0715706050395966
  batch 400 loss: 1.0761204957962036
  batch 450 loss: 1.1178706610202789
  batch 500 loss: 1.0549025511741639
  batch 550 loss: 1.1028325581550598
  batch 600 loss: 1.0894584357738495
  batch 650 loss: 1.0799215984344483
  batch 700 loss: 1.087363930940628
  batch 750 loss: 1.0694913232326508
  batch 800 loss: 1.0497382962703705
  batch 850 loss: 1.1126581048965454
  batch 900 loss: 1.1099681556224823
avg val loss: 1.081342339515686
LOSS train 1.10997 valid 1.08134, valid PER 34.32%
EPOCH 14, Learning Rate: 0.175
  batch 50 loss: 1.0541170489788056
  batch 100 loss: 1.070623676776886
  batch 150 loss: 1.0713217389583587
  batch 200 loss: 1.0722587645053863
  batch 250 loss: 1.0771538519859314
  batch 300 loss: 1.0990095794200898
  batch 350 loss: 1.0484936094284059
  batch 400 loss: 1.0662057542800902
  batch 450 loss: 1.0471913969516755
  batch 500 loss: 1.0746382832527162
  batch 550 loss: 1.073419246673584
  batch 600 loss: 1.0653637254238129
  batch 650 loss: 1.0781406223773957
  batch 700 loss: 1.1119212317466736
  batch 750 loss: 1.0553432178497315
  batch 800 loss: 1.0184969007968903
  batch 850 loss: 1.0847523307800293
  batch 900 loss: 1.0708448088169098
avg val loss: 1.0819696187973022
LOSS train 1.07084 valid 1.08197, valid PER 34.46%
EPOCH 15, Learning Rate: 0.0875
  batch 50 loss: 1.0803853583335876
  batch 100 loss: 1.0244441616535187
  batch 150 loss: 1.0191673946380615
  batch 200 loss: 1.056807098388672
  batch 250 loss: 1.0341505885124207
  batch 300 loss: 1.0297215449810029
  batch 350 loss: 1.0303634262084962
  batch 400 loss: 1.020533561706543
  batch 450 loss: 1.0105016887187959
  batch 500 loss: 0.9978349328041076
  batch 550 loss: 1.0355859923362731
  batch 600 loss: 1.0483450639247893
  batch 650 loss: 1.0582185196876526
  batch 700 loss: 1.0633902764320373
  batch 750 loss: 1.024541347026825
  batch 800 loss: 1.0196851408481598
  batch 850 loss: 1.0247602045536042
  batch 900 loss: 1.0481748473644257
avg val loss: 1.0610588788986206
LOSS train 1.04817 valid 1.06106, valid PER 34.09%
EPOCH 16, Learning Rate: 0.0875
  batch 50 loss: 1.0492872262001038
  batch 100 loss: 0.998250504732132
  batch 150 loss: 1.0249641513824463
  batch 200 loss: 1.0185267412662506
  batch 250 loss: 1.0629814708232879
  batch 300 loss: 1.0401565492153169
  batch 350 loss: 1.0305299866199493
  batch 400 loss: 1.011488118171692
  batch 450 loss: 1.0471838319301605
  batch 500 loss: 0.9841809904575348
  batch 550 loss: 1.0217757391929627
  batch 600 loss: 1.040820517539978
  batch 650 loss: 1.040036653280258
  batch 700 loss: 0.98922896027565
  batch 750 loss: 1.0324910688400268
  batch 800 loss: 1.0237996828556062
  batch 850 loss: 0.9980667793750763
  batch 900 loss: 1.0120171093940735
avg val loss: 1.05572509765625
LOSS train 1.01202 valid 1.05573, valid PER 33.20%
EPOCH 17, Learning Rate: 0.0875
  batch 50 loss: 1.0299584567546844
  batch 100 loss: 1.0189652681350707
  batch 150 loss: 1.0061858797073364
  batch 200 loss: 1.0095891439914704
  batch 250 loss: 1.0454789447784423
  batch 300 loss: 1.0219584155082702
  batch 350 loss: 0.9860761511325836
  batch 400 loss: 1.0533300864696502
  batch 450 loss: 1.025526077747345
  batch 500 loss: 1.0053756058216095
  batch 550 loss: 1.0168997406959535
  batch 600 loss: 1.0427819430828094
  batch 650 loss: 0.9896157658100129
  batch 700 loss: 1.0039412474632263
  batch 750 loss: 1.0116334855556488
  batch 800 loss: 0.9807581734657288
  batch 850 loss: 1.0043998205661773
  batch 900 loss: 0.9843052625656128
avg val loss: 1.0444397926330566
LOSS train 0.98431 valid 1.04444, valid PER 33.44%
EPOCH 18, Learning Rate: 0.0875
  batch 50 loss: 1.0062053406238556
  batch 100 loss: 1.0021326398849488
  batch 150 loss: 1.0393619203567506
  batch 200 loss: 1.0097080636024476
  batch 250 loss: 1.0084602117538453
  batch 300 loss: 1.007537271976471
  batch 350 loss: 1.0301247823238373
  batch 400 loss: 1.0108233201503753
  batch 450 loss: 1.0455942893028258
  batch 500 loss: 1.024449381828308
  batch 550 loss: 1.0087969589233399
  batch 600 loss: 0.9854453682899476
  batch 650 loss: 0.9865660035610199
  batch 700 loss: 1.029423656463623
  batch 750 loss: 0.9966990637779236
  batch 800 loss: 1.0139957928657533
  batch 850 loss: 0.9872755980491639
  batch 900 loss: 1.0221934282779694
avg val loss: 1.0474201440811157
LOSS train 1.02219 valid 1.04742, valid PER 33.36%
EPOCH 19, Learning Rate: 0.04375
  batch 50 loss: 0.9662302923202515
  batch 100 loss: 0.9592668068408966
  batch 150 loss: 0.9743672895431519
  batch 200 loss: 1.0008671605587005
  batch 250 loss: 1.00626455783844
  batch 300 loss: 0.9783153975009918
  batch 350 loss: 0.9853917503356934
  batch 400 loss: 1.0027289974689484
  batch 450 loss: 0.9939071583747864
  batch 500 loss: 0.9962539672851562
  batch 550 loss: 0.9698022449016571
  batch 600 loss: 0.9811604535579681
  batch 650 loss: 1.003294380903244
  batch 700 loss: 0.9636168992519378
  batch 750 loss: 0.9611552405357361
  batch 800 loss: 0.9967567718029022
  batch 850 loss: 0.995049684047699
  batch 900 loss: 0.9847687149047851
avg val loss: 1.0317319631576538
LOSS train 0.98477 valid 1.03173, valid PER 32.77%
EPOCH 20, Learning Rate: 0.04375
  batch 50 loss: 0.9565352070331573
  batch 100 loss: 0.9770920729637146
  batch 150 loss: 0.9864250469207764
  batch 200 loss: 0.9747394299507142
  batch 250 loss: 0.9754693865776062
  batch 300 loss: 1.010001882314682
  batch 350 loss: 0.936419620513916
  batch 400 loss: 0.9871214175224304
  batch 450 loss: 0.9758020472526551
  batch 500 loss: 0.965061604976654
  batch 550 loss: 1.0174863076210021
  batch 600 loss: 0.9399288821220398
  batch 650 loss: 0.9765722930431366
  batch 700 loss: 0.9906713604927063
  batch 750 loss: 0.9816990458965301
  batch 800 loss: 0.9930545341968536
  batch 850 loss: 0.9746961927413941
  batch 900 loss: 1.0044051778316498
avg val loss: 1.0310720205307007
LOSS train 1.00441 valid 1.03107, valid PER 32.40%
Training finished in 2.0 minutes.
Model saved to checkpoints/20231210_140745/model_20
Loading model from checkpoints/20231210_140745/model_20
SUB: 18.71%, DEL: 13.75%, INS: 1.36%, COR: 67.54%, PER: 33.82%
