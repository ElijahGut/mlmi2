Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.7, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 83496
EPOCH 1, Learning Rate: 0.7
  batch 50 loss: 4.5253613090515135
  batch 100 loss: 3.207329998016357
  batch 150 loss: 3.0381860160827636
  batch 200 loss: 2.7569640827178956
  batch 250 loss: 2.5738850831985474
  batch 300 loss: 2.4289432621002196
  batch 350 loss: 2.3392262744903562
  batch 400 loss: 2.3036501383781434
  batch 450 loss: 2.2270539474487303
  batch 500 loss: 2.132046720981598
  batch 550 loss: 2.094516649246216
  batch 600 loss: 2.0429864048957826
  batch 650 loss: 1.9905084896087646
  batch 700 loss: 1.9899112248420716
  batch 750 loss: 1.9563066458702087
  batch 800 loss: 1.9495054841041566
  batch 850 loss: 1.8843739795684815
  batch 900 loss: 1.8651785469055175
running loss: 44.12539553642273
LOSS train 1.86518 valid 1.81297, valid PER 68.55%
EPOCH 2, Learning Rate: 0.7
  batch 50 loss: 1.830431227684021
  batch 100 loss: 1.7961911034584046
  batch 150 loss: 1.7682853293418885
  batch 200 loss: 1.7941891193389892
  batch 250 loss: 1.782388322353363
  batch 300 loss: 1.7711487698554993
  batch 350 loss: 1.6719283318519593
  batch 400 loss: 1.6835056591033934
  batch 450 loss: 1.6230346274375915
  batch 500 loss: 1.6805554246902465
  batch 550 loss: 1.676158881187439
  batch 600 loss: 1.6091773104667664
  batch 650 loss: 1.6635213565826417
  batch 700 loss: 1.635994484424591
  batch 750 loss: 1.6219294214248656
  batch 800 loss: 1.5580808877944947
  batch 850 loss: 1.5711889719963075
  batch 900 loss: 1.5979989099502563
running loss: 36.74859273433685
LOSS train 1.59800 valid 1.51606, valid PER 53.09%
EPOCH 3, Learning Rate: 0.7
  batch 50 loss: 1.551097800731659
  batch 100 loss: 1.538136475086212
  batch 150 loss: 1.5232305335998535
  batch 200 loss: 1.5086587047576905
  batch 250 loss: 1.50903080701828
  batch 300 loss: 1.4826551961898804
  batch 350 loss: 1.5591179203987122
  batch 400 loss: 1.5317474341392516
  batch 450 loss: 1.549069004058838
  batch 500 loss: 1.517720968723297
  batch 550 loss: 1.4905601811408997
  batch 600 loss: 1.4660233855247498
  batch 650 loss: 1.4609792280197142
  batch 700 loss: 1.4986553621292114
  batch 750 loss: 1.5729977869987488
  batch 800 loss: 1.478420889377594
  batch 850 loss: 1.4924655628204346
  batch 900 loss: 1.4540534329414367
running loss: 35.14545726776123
LOSS train 1.45405 valid 1.38146, valid PER 44.43%
EPOCH 4, Learning Rate: 0.7
  batch 50 loss: 1.4715861868858338
  batch 100 loss: 1.4718879175186157
  batch 150 loss: 1.4250851702690124
  batch 200 loss: 1.48054181098938
  batch 250 loss: 1.4597928953170776
  batch 300 loss: 1.4807135486602783
  batch 350 loss: 1.395075123310089
  batch 400 loss: 1.447164921760559
  batch 450 loss: 1.4218178987503052
  batch 500 loss: 1.4014111495018005
  batch 550 loss: 1.4411534142494202
  batch 600 loss: 1.4448321437835694
  batch 650 loss: 1.4507464337348939
  batch 700 loss: 1.4174756288528443
  batch 750 loss: 1.368973286151886
  batch 800 loss: 1.385881106853485
  batch 850 loss: 1.4368660974502563
  batch 900 loss: 1.4804742693901063
running loss: 34.680365562438965
LOSS train 1.48047 valid 1.32418, valid PER 42.33%
EPOCH 5, Learning Rate: 0.7
  batch 50 loss: 1.3886684727668763
  batch 100 loss: 1.3748469376564025
  batch 150 loss: 1.4376782870292664
  batch 200 loss: 1.3566979038715363
  batch 250 loss: 1.3774529933929442
  batch 300 loss: 1.4050352787971496
  batch 350 loss: 1.4041121554374696
  batch 400 loss: 1.4375908970832825
  batch 450 loss: 1.4325043988227844
  batch 500 loss: 1.4690455055236817
  batch 550 loss: 1.4093544936180116
  batch 600 loss: 1.468221526145935
  batch 650 loss: 1.4251992678642273
  batch 700 loss: 1.4708193302154542
  batch 750 loss: 1.3752832221984863
  batch 800 loss: 1.428959846496582
  batch 850 loss: 1.4349632358551025
  batch 900 loss: 1.4447404909133912
running loss: 32.98085618019104
LOSS train 1.44474 valid 1.38214, valid PER 44.36%
EPOCH 6, Learning Rate: 0.7
  batch 50 loss: 1.4326021814346312
  batch 100 loss: 1.372584502696991
  batch 150 loss: 1.3502359986305237
  batch 200 loss: 1.3899437236785888
  batch 250 loss: 1.4215421319007873
  batch 300 loss: 1.38745849609375
  batch 350 loss: 1.3917088842391967
  batch 400 loss: 1.3871679949760436
  batch 450 loss: 1.4600143313407898
  batch 500 loss: 1.4155402731895448
  batch 550 loss: 1.4186984276771546
  batch 600 loss: 1.4205593824386598
  batch 650 loss: 1.4455460119247436
  batch 700 loss: 1.4474607396125794
  batch 750 loss: 1.3583032083511353
  batch 800 loss: 1.384641923904419
  batch 850 loss: 1.4026244282722473
  batch 900 loss: 1.410309808254242
running loss: 33.90034854412079
LOSS train 1.41031 valid 1.37072, valid PER 43.47%
EPOCH 7, Learning Rate: 0.35
  batch 50 loss: 1.3713831782341004
  batch 100 loss: 1.3630870723724364
  batch 150 loss: 1.303389961719513
  batch 200 loss: 1.2897516679763794
  batch 250 loss: 1.29118173122406
  batch 300 loss: 1.2727541708946228
  batch 350 loss: 1.26170671582222
  batch 400 loss: 1.2923814105987548
  batch 450 loss: 1.2839426624774932
  batch 500 loss: 1.2721833491325378
  batch 550 loss: 1.2484598517417909
  batch 600 loss: 1.2711487317085266
  batch 650 loss: 1.2386339712142944
  batch 700 loss: 1.2517212045192718
  batch 750 loss: 1.2459999132156372
  batch 800 loss: 1.2317921495437623
  batch 850 loss: 1.2640094614028932
  batch 900 loss: 1.313953468799591
running loss: 29.104200661182404
LOSS train 1.31395 valid 1.21067, valid PER 39.25%
EPOCH 8, Learning Rate: 0.35
  batch 50 loss: 1.241735382080078
  batch 100 loss: 1.2051498627662658
  batch 150 loss: 1.2469329380989074
  batch 200 loss: 1.2117685604095458
  batch 250 loss: 1.2419077038764954
  batch 300 loss: 1.1867293334007263
  batch 350 loss: 1.2839481937885284
  batch 400 loss: 1.203179533481598
  batch 450 loss: 1.225502965450287
  batch 500 loss: 1.2461750793457032
  batch 550 loss: 1.2103943467140197
  batch 600 loss: 1.2310130846500398
  batch 650 loss: 1.274124835729599
  batch 700 loss: 1.2180601310729982
  batch 750 loss: 1.2389809966087342
  batch 800 loss: 1.2247524750232697
  batch 850 loss: 1.228765802383423
  batch 900 loss: 1.186188131570816
running loss: 29.032679080963135
LOSS train 1.18619 valid 1.18440, valid PER 38.47%
EPOCH 9, Learning Rate: 0.35
  batch 50 loss: 1.2057345271110536
  batch 100 loss: 1.2493001091480256
  batch 150 loss: 1.2308213317394257
  batch 200 loss: 1.1567311191558838
  batch 250 loss: 1.2033749949932098
  batch 300 loss: 1.2156980335712433
  batch 350 loss: 1.2223165702819825
  batch 400 loss: 1.195452116727829
  batch 450 loss: 1.1984495794773102
  batch 500 loss: 1.1794704496860504
  batch 550 loss: 1.2461303901672363
  batch 600 loss: 1.2017871940135956
  batch 650 loss: 1.2209629285335541
  batch 700 loss: 1.2270142996311189
  batch 750 loss: 1.2231189954280852
  batch 800 loss: 1.2339555096626282
  batch 850 loss: 1.246559920310974
  batch 900 loss: 1.1982023859024047
running loss: 29.674492955207825
LOSS train 1.19820 valid 1.17222, valid PER 37.58%
EPOCH 10, Learning Rate: 0.175
  batch 50 loss: 1.1480432534217835
  batch 100 loss: 1.1651972353458404
  batch 150 loss: 1.1781403291225434
  batch 200 loss: 1.190730732679367
  batch 250 loss: 1.163227676153183
  batch 300 loss: 1.1084156847000122
  batch 350 loss: 1.1697063004970552
  batch 400 loss: 1.1150286042690276
  batch 450 loss: 1.114685559272766
  batch 500 loss: 1.1655936968326568
  batch 550 loss: 1.1531912422180175
  batch 600 loss: 1.1452863717079163
  batch 650 loss: 1.147415690422058
  batch 700 loss: 1.1691396272182464
  batch 750 loss: 1.1348354423046112
  batch 800 loss: 1.1515145933628081
  batch 850 loss: 1.1814230167865754
  batch 900 loss: 1.1482650852203369
running loss: 27.706851720809937
LOSS train 1.14827 valid 1.13136, valid PER 36.63%
EPOCH 11, Learning Rate: 0.175
  batch 50 loss: 1.1154040169715882
  batch 100 loss: 1.0816972506046296
  batch 150 loss: 1.1156660974025727
  batch 200 loss: 1.1434867525100707
  batch 250 loss: 1.129443210363388
  batch 300 loss: 1.0759483194351196
  batch 350 loss: 1.143390073776245
  batch 400 loss: 1.1291258668899535
  batch 450 loss: 1.132688764333725
  batch 500 loss: 1.0950047659873963
  batch 550 loss: 1.1190302073955536
  batch 600 loss: 1.1074426364898682
  batch 650 loss: 1.1641117346286773
  batch 700 loss: 1.0747414278984069
  batch 750 loss: 1.0949705564975738
  batch 800 loss: 1.1389771234989166
  batch 850 loss: 1.157467039823532
  batch 900 loss: 1.1475135111808776
running loss: 26.842652440071106
LOSS train 1.14751 valid 1.09370, valid PER 35.43%
EPOCH 12, Learning Rate: 0.175
  batch 50 loss: 1.125872519016266
  batch 100 loss: 1.1222264647483826
  batch 150 loss: 1.0893588423728944
  batch 200 loss: 1.1177607786655426
  batch 250 loss: 1.1356820726394654
  batch 300 loss: 1.1112568545341492
  batch 350 loss: 1.1125036466121674
  batch 400 loss: 1.137643756866455
  batch 450 loss: 1.1080017912387847
  batch 500 loss: 1.1335969948768616
  batch 550 loss: 1.0691718637943268
  batch 600 loss: 1.0758334445953368
  batch 650 loss: 1.1395766389369966
  batch 700 loss: 1.0980921578407288
  batch 750 loss: 1.1016857028007507
  batch 800 loss: 1.0903950726985931
  batch 850 loss: 1.1004025483131408
  batch 900 loss: 1.1293983566761017
running loss: 25.973938822746277
LOSS train 1.12940 valid 1.08666, valid PER 35.44%
EPOCH 13, Learning Rate: 0.175
  batch 50 loss: 1.0751302981376647
  batch 100 loss: 1.1014902377128601
  batch 150 loss: 1.0698721897602081
  batch 200 loss: 1.1010230255126954
  batch 250 loss: 1.0880507004261017
  batch 300 loss: 1.082046926021576
  batch 350 loss: 1.0809422636032104
  batch 400 loss: 1.1264674651622772
  batch 450 loss: 1.1185866820812225
  batch 500 loss: 1.0665871608257294
  batch 550 loss: 1.0922849571704865
  batch 600 loss: 1.0896393048763275
  batch 650 loss: 1.1023045408725738
  batch 700 loss: 1.0982048058509826
  batch 750 loss: 1.084680449962616
  batch 800 loss: 1.0837853121757508
  batch 850 loss: 1.1586362099647523
  batch 900 loss: 1.1457781159877778
running loss: 26.830526053905487
LOSS train 1.14578 valid 1.10032, valid PER 35.28%
EPOCH 14, Learning Rate: 0.0875
  batch 50 loss: 1.09068620800972
  batch 100 loss: 1.1183564400672912
  batch 150 loss: 1.1021175038814546
  batch 200 loss: 1.0983492588996888
  batch 250 loss: 1.0805127561092376
  batch 300 loss: 1.0920739555358887
  batch 350 loss: 1.0532765889167786
  batch 400 loss: 1.0608596897125244
  batch 450 loss: 1.0770988261699677
  batch 500 loss: 1.0829300379753113
  batch 550 loss: 1.09569726228714
  batch 600 loss: 1.0570184004306793
  batch 650 loss: 1.067905386686325
  batch 700 loss: 1.0841686618328095
  batch 750 loss: 1.0498902106285095
  batch 800 loss: 1.004114307165146
  batch 850 loss: 1.0816262125968934
  batch 900 loss: 1.048848351240158
running loss: 26.31696444749832
LOSS train 1.04885 valid 1.08164, valid PER 34.42%
EPOCH 15, Learning Rate: 0.04375
  batch 50 loss: 1.0845065414905548
  batch 100 loss: 1.0553921592235564
  batch 150 loss: 1.0360784029960632
  batch 200 loss: 1.0629853951930999
  batch 250 loss: 1.0514024364948273
  batch 300 loss: 1.0420817387104035
  batch 350 loss: 1.0283055579662324
  batch 400 loss: 1.031450707912445
  batch 450 loss: 1.0380019235610962
  batch 500 loss: 1.010885614156723
  batch 550 loss: 1.0459256231784821
  batch 600 loss: 1.0578315246105194
  batch 650 loss: 1.049490602016449
  batch 700 loss: 1.0658268272876739
  batch 750 loss: 1.051500735282898
  batch 800 loss: 1.0288009309768678
  batch 850 loss: 1.0053501284122468
  batch 900 loss: 1.0483797013759613
running loss: 24.21455281972885
LOSS train 1.04838 valid 1.05817, valid PER 34.10%
EPOCH 16, Learning Rate: 0.04375
  batch 50 loss: 1.0751135766506195
  batch 100 loss: 1.0212035751342774
  batch 150 loss: 1.0424339652061463
  batch 200 loss: 1.0311369407176971
  batch 250 loss: 1.051733214855194
  batch 300 loss: 1.044929301738739
  batch 350 loss: 1.0446820056438446
  batch 400 loss: 1.0281961643695832
  batch 450 loss: 1.0568273901939391
  batch 500 loss: 0.9940760886669159
  batch 550 loss: 1.0262815749645233
  batch 600 loss: 1.0360791015625
  batch 650 loss: 1.0413989782333375
  batch 700 loss: 0.9976533496379852
  batch 750 loss: 1.0258785748481751
  batch 800 loss: 1.008573397397995
  batch 850 loss: 1.0211022543907164
  batch 900 loss: 1.0188239645957946
running loss: 23.89748305082321
LOSS train 1.01882 valid 1.05647, valid PER 33.62%
EPOCH 17, Learning Rate: 0.04375
  batch 50 loss: 1.031738773584366
  batch 100 loss: 1.0293489706516266
  batch 150 loss: 0.9953583335876465
  batch 200 loss: 1.0128214812278749
  batch 250 loss: 1.0270703268051147
  batch 300 loss: 1.0105840957164764
  batch 350 loss: 0.996124849319458
  batch 400 loss: 1.0639687430858613
  batch 450 loss: 1.0349977731704711
  batch 500 loss: 1.0115272629261016
  batch 550 loss: 1.0281966125965118
  batch 600 loss: 1.0581799232959748
  batch 650 loss: 1.0149858093261719
  batch 700 loss: 1.0102546310424805
  batch 750 loss: 1.019273636341095
  batch 800 loss: 1.002404615879059
  batch 850 loss: 1.0025153291225433
  batch 900 loss: 1.0092458403110505
running loss: 25.025328636169434
LOSS train 1.00925 valid 1.05252, valid PER 33.77%
EPOCH 18, Learning Rate: 0.021875
  batch 50 loss: 1.0159332573413848
  batch 100 loss: 1.0209574842453002
  batch 150 loss: 1.043444494009018
  batch 200 loss: 1.0135856795310973
  batch 250 loss: 1.012122174501419
  batch 300 loss: 1.0187802875041962
  batch 350 loss: 1.0363372480869293
  batch 400 loss: 0.9952907240390778
  batch 450 loss: 1.0332764077186585
  batch 500 loss: 1.0211574280261992
  batch 550 loss: 0.9904887020587921
  batch 600 loss: 0.9927916371822357
  batch 650 loss: 0.9829958236217499
  batch 700 loss: 1.0353256356716156
  batch 750 loss: 1.010587272644043
  batch 800 loss: 1.0024593710899352
  batch 850 loss: 0.9797934663295745
  batch 900 loss: 1.0207394301891326
running loss: 23.37945204973221
LOSS train 1.02074 valid 1.04707, valid PER 33.40%
EPOCH 19, Learning Rate: 0.021875
  batch 50 loss: 0.97779656291008
  batch 100 loss: 0.9724316203594208
  batch 150 loss: 0.9868957674503327
  batch 200 loss: 1.0191199147701264
  batch 250 loss: 1.0404711151123047
  batch 300 loss: 1.0169450390338897
  batch 350 loss: 0.9943452751636506
  batch 400 loss: 1.0280511391162872
  batch 450 loss: 1.005914454460144
  batch 500 loss: 1.0194991683959962
  batch 550 loss: 0.9871146762371064
  batch 600 loss: 1.0164058530330657
  batch 650 loss: 1.0379994237422943
  batch 700 loss: 0.9896020078659058
  batch 750 loss: 0.9857269585132599
  batch 800 loss: 1.0280587434768678
  batch 850 loss: 1.0331458604335786
  batch 900 loss: 1.0086412382125856
running loss: 24.471233010292053
LOSS train 1.00864 valid 1.04527, valid PER 33.34%
EPOCH 20, Learning Rate: 0.0109375
  batch 50 loss: 0.9914763438701629
  batch 100 loss: 1.0135345458984375
  batch 150 loss: 1.0106507563591003
  batch 200 loss: 0.9983052814006805
  batch 250 loss: 1.0043259477615356
  batch 300 loss: 1.0277065014839173
  batch 350 loss: 0.9666319489479065
  batch 400 loss: 0.9946968841552735
  batch 450 loss: 0.9914810800552368
  batch 500 loss: 0.9913677144050598
  batch 550 loss: 1.0382463419437409
  batch 600 loss: 0.9581763744354248
  batch 650 loss: 1.0062627911567688
  batch 700 loss: 1.006533453464508
  batch 750 loss: 0.9972621631622315
  batch 800 loss: 1.030399055480957
  batch 850 loss: 1.0131828725337981
  batch 900 loss: 1.0149976229667663
running loss: 23.31864297389984
LOSS train 1.01500 valid 1.04272, valid PER 33.28%
Training finished in 2.0 minutes.
Model saved to checkpoints/20231210_042040/model_20
Loading model from checkpoints/20231210_042040/model_20
SUB: 19.03%, DEL: 14.64%, INS: 1.28%, COR: 66.33%, PER: 34.95%
