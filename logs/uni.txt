Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=0.5, is_bidir=True)
Total number of model parameters is 83496
EPOCH 1, Learning Rate: 0.5
  batch 50 loss: 6.2834999465942385
  batch 100 loss: 3.3069616031646727
  batch 150 loss: 3.2475034761428834
  batch 200 loss: 3.139484887123108
  batch 250 loss: 2.9508515310287478
  batch 300 loss: 2.7263569688796996
  batch 350 loss: 2.599942054748535
  batch 400 loss: 2.5343515825271608
  batch 450 loss: 2.462720251083374
  batch 500 loss: 2.366677470207214
  batch 550 loss: 2.3075058603286744
  batch 600 loss: 2.2645982980728148
  batch 650 loss: 2.20557359457016
  batch 700 loss: 2.201892304420471
  batch 750 loss: 2.1705588793754576
  batch 800 loss: 2.139357147216797
  batch 850 loss: 2.1009380054473876
  batch 900 loss: 2.079743971824646
LOSS train 2.07974 valid 2.04343, valid PER 77.52%
EPOCH 2, Learning Rate: 0.5
  batch 50 loss: 2.051866648197174
  batch 100 loss: 2.008800535202026
  batch 150 loss: 1.9618740820884704
  batch 200 loss: 1.9902968215942383
  batch 250 loss: 1.982166693210602
  batch 300 loss: 1.9648942971229553
  batch 350 loss: 1.8822744870185852
  batch 400 loss: 1.9006005883216859
  batch 450 loss: 1.8530898666381836
  batch 500 loss: 1.8771670579910278
  batch 550 loss: 1.8722884154319763
  batch 600 loss: 1.8251129221916198
  batch 650 loss: 1.8543833684921265
  batch 700 loss: 1.8142966938018799
  batch 750 loss: 1.8156089758872986
  batch 800 loss: 1.7607328963279725
  batch 850 loss: 1.763530113697052
  batch 900 loss: 1.7707588410377502
LOSS train 1.77076 valid 1.71176, valid PER 68.61%
EPOCH 3, Learning Rate: 0.5
  batch 50 loss: 1.7424811148643493
  batch 100 loss: 1.7137752175331116
  batch 150 loss: 1.7133335638046265
  batch 200 loss: 1.7043711996078492
  batch 250 loss: 1.6847371411323548
  batch 300 loss: 1.6911770296096802
  batch 350 loss: 1.71962087392807
  batch 400 loss: 1.7027269458770753
  batch 450 loss: 1.6649780011177062
  batch 500 loss: 1.6521316933631898
  batch 550 loss: 1.634972836971283
  batch 600 loss: 1.6269541001319885
  batch 650 loss: 1.5885797071456909
  batch 700 loss: 1.6076263546943665
  batch 750 loss: 1.6492121195793152
  batch 800 loss: 1.593318214416504
  batch 850 loss: 1.631835868358612
  batch 900 loss: 1.569388554096222
LOSS train 1.56939 valid 1.52452, valid PER 54.63%
EPOCH 4, Learning Rate: 0.5
  batch 50 loss: 1.5755149841308593
  batch 100 loss: 1.5909804844856261
  batch 150 loss: 1.5167018938064576
  batch 200 loss: 1.5722044682502747
  batch 250 loss: 1.5727968907356262
  batch 300 loss: 1.5918064737319946
  batch 350 loss: 1.5054003167152405
  batch 400 loss: 1.5396872234344483
  batch 450 loss: 1.5372739791870118
  batch 500 loss: 1.5068370294570923
  batch 550 loss: 1.5399284482002258
  batch 600 loss: 1.5410470080375671
  batch 650 loss: 1.5457244324684143
  batch 700 loss: 1.49514221906662
  batch 750 loss: 1.4811330771446227
  batch 800 loss: 1.491820592880249
  batch 850 loss: 1.487830023765564
  batch 900 loss: 1.5235065388679505
LOSS train 1.52351 valid 1.42436, valid PER 48.06%
EPOCH 5, Learning Rate: 0.5
  batch 50 loss: 1.472926995754242
  batch 100 loss: 1.4541339707374572
  batch 150 loss: 1.4994886469841004
  batch 200 loss: 1.4313747715950011
  batch 250 loss: 1.4224578094482423
  batch 300 loss: 1.457770755290985
  batch 350 loss: 1.4565160036087037
  batch 400 loss: 1.484194803237915
  batch 450 loss: 1.4491879773139953
  batch 500 loss: 1.4702138304710388
  batch 550 loss: 1.408131959438324
  batch 600 loss: 1.4739476418495179
  batch 650 loss: 1.406356966495514
  batch 700 loss: 1.4692355942726136
  batch 750 loss: 1.4114909267425537
  batch 800 loss: 1.4430827164649964
  batch 850 loss: 1.4348774981498718
  batch 900 loss: 1.4411567640304566
LOSS train 1.44116 valid 1.35473, valid PER 44.46%
EPOCH 6, Learning Rate: 0.5
  batch 50 loss: 1.430292751789093
  batch 100 loss: 1.3915467882156372
  batch 150 loss: 1.386351056098938
  batch 200 loss: 1.4001543021202087
  batch 250 loss: 1.4179651188850402
  batch 300 loss: 1.37839581489563
  batch 350 loss: 1.3690947079658509
  batch 400 loss: 1.3863029861450196
  batch 450 loss: 1.4188031792640685
  batch 500 loss: 1.381604573726654
  batch 550 loss: 1.39769935131073
  batch 600 loss: 1.388483374118805
  batch 650 loss: 1.402338788509369
  batch 700 loss: 1.3747295761108398
  batch 750 loss: 1.33052743434906
  batch 800 loss: 1.3617083168029784
  batch 850 loss: 1.3357886958122254
  batch 900 loss: 1.3777261233329774
LOSS train 1.37773 valid 1.29857, valid PER 42.65%
EPOCH 7, Learning Rate: 0.5
  batch 50 loss: 1.3580295300483705
  batch 100 loss: 1.3703388524055482
  batch 150 loss: 1.3617255687713623
  batch 200 loss: 1.34428612947464
  batch 250 loss: 1.3505718111991882
  batch 300 loss: 1.3353790187835692
  batch 350 loss: 1.343785843849182
  batch 400 loss: 1.3427637815475464
  batch 450 loss: 1.3375766038894654
  batch 500 loss: 1.3361238241195679
  batch 550 loss: 1.3280817413330077
  batch 600 loss: 1.3483442878723144
  batch 650 loss: 1.3166524386405944
  batch 700 loss: 1.3570729684829712
  batch 750 loss: 1.3093401885032654
  batch 800 loss: 1.3037966465950013
  batch 850 loss: 1.3352648162841796
  batch 900 loss: 1.3849559926986694
LOSS train 1.38496 valid 1.28506, valid PER 42.84%
EPOCH 8, Learning Rate: 0.5
  batch 50 loss: 1.3333232116699218
  batch 100 loss: 1.3099102973937988
  batch 150 loss: 1.3116485381126404
  batch 200 loss: 1.2821405959129333
  batch 250 loss: 1.3273598170280456
  batch 300 loss: 1.2637809681892396
  batch 350 loss: 1.3459547233581544
  batch 400 loss: 1.2728158521652222
  batch 450 loss: 1.3256695699691772
  batch 500 loss: 1.3243707180023194
  batch 550 loss: 1.285524950027466
  batch 600 loss: 1.312771499156952
  batch 650 loss: 1.3558231234550475
  batch 700 loss: 1.289213526248932
  batch 750 loss: 1.311659026145935
  batch 800 loss: 1.2994373667240142
  batch 850 loss: 1.3069474148750304
  batch 900 loss: 1.2641942012310028
LOSS train 1.26419 valid 1.24494, valid PER 40.17%
EPOCH 9, Learning Rate: 0.5
  batch 50 loss: 1.240679190158844
  batch 100 loss: 1.3079965662956239
  batch 150 loss: 1.2837485551834107
  batch 200 loss: 1.2392742598056794
  batch 250 loss: 1.280901997089386
  batch 300 loss: 1.2900195980072022
  batch 350 loss: 1.286626420021057
  batch 400 loss: 1.272560214996338
  batch 450 loss: 1.2859922003746034
  batch 500 loss: 1.2436840975284575
  batch 550 loss: 1.3091507422924042
  batch 600 loss: 1.2942474579811096
  batch 650 loss: 1.2612656593322753
  batch 700 loss: 1.2570838236808777
  batch 750 loss: 1.2823955070972444
  batch 800 loss: 1.28970623254776
  batch 850 loss: 1.298738307952881
  batch 900 loss: 1.255565595626831
LOSS train 1.25557 valid 1.20531, valid PER 39.73%
EPOCH 10, Learning Rate: 0.5
  batch 50 loss: 1.2310443365573882
  batch 100 loss: 1.2404080128669739
  batch 150 loss: 1.273138962984085
  batch 200 loss: 1.2690890896320344
  batch 250 loss: 1.2871843671798706
  batch 300 loss: 1.1988041877746582
  batch 350 loss: 1.2692127203941346
  batch 400 loss: 1.2263591551780701
  batch 450 loss: 1.202904121875763
  batch 500 loss: 1.2710028100013733
  batch 550 loss: 1.2862125539779663
  batch 600 loss: 1.2507544028759003
  batch 650 loss: 1.2552522385120393
  batch 700 loss: 1.2508794975280761
  batch 750 loss: 1.2283636355400085
  batch 800 loss: 1.283202509880066
  batch 850 loss: 1.2683837151527404
  batch 900 loss: 1.2628288435935975
LOSS train 1.26283 valid 1.21713, valid PER 40.61%
EPOCH 11, Learning Rate: 0.5
  batch 50 loss: 1.208359181880951
  batch 100 loss: 1.20792249917984
  batch 150 loss: 1.1915092432498933
  batch 200 loss: 1.2428744757175445
  batch 250 loss: 1.2405635952949523
  batch 300 loss: 1.1846191430091857
  batch 350 loss: 1.227572467327118
  batch 400 loss: 1.2389846563339233
  batch 450 loss: 1.2523657917976379
  batch 500 loss: 1.2202319478988648
  batch 550 loss: 1.2091030299663543
  batch 600 loss: 1.1945408153533936
  batch 650 loss: 1.2639316952228545
  batch 700 loss: 1.1868784463405608
  batch 750 loss: 1.2213327538967134
  batch 800 loss: 1.2405696415901184
  batch 850 loss: 1.2378388440608978
  batch 900 loss: 1.238772715330124
LOSS train 1.23877 valid 1.18114, valid PER 38.91%
EPOCH 12, Learning Rate: 0.5
  batch 50 loss: 1.2393528163433074
  batch 100 loss: 1.205832654237747
  batch 150 loss: 1.1923398721218108
  batch 200 loss: 1.2000591409206391
  batch 250 loss: 1.2288873648643495
  batch 300 loss: 1.1877891063690185
  batch 350 loss: 1.2051781868934632
  batch 400 loss: 1.2216862428188324
  batch 450 loss: 1.1990526306629181
  batch 500 loss: 1.2215707874298096
  batch 550 loss: 1.1732202935218812
  batch 600 loss: 1.1666817569732666
  batch 650 loss: 1.2399360764026641
  batch 700 loss: 1.2040029871463775
  batch 750 loss: 1.1882573342323304
  batch 800 loss: 1.160322654247284
  batch 850 loss: 1.2312182426452636
  batch 900 loss: 1.23970578789711
LOSS train 1.23971 valid 1.17406, valid PER 38.88%
EPOCH 13, Learning Rate: 0.5
  batch 50 loss: 1.1599632275104523
  batch 100 loss: 1.1966618227958679
  batch 150 loss: 1.154920015335083
  batch 200 loss: 1.2086219465732575
  batch 250 loss: 1.1822583043575288
  batch 300 loss: 1.1616037464141846
  batch 350 loss: 1.1894974541664123
  batch 400 loss: 1.193437819480896
  batch 450 loss: 1.248246239423752
  batch 500 loss: 1.169717344045639
  batch 550 loss: 1.1977407622337342
  batch 600 loss: 1.1857877326011659
  batch 650 loss: 1.1716862773895265
  batch 700 loss: 1.169215749502182
  batch 750 loss: 1.164617451429367
  batch 800 loss: 1.1526678311824798
  batch 850 loss: 1.2213476753234864
  batch 900 loss: 1.229994854927063
LOSS train 1.22999 valid 1.17656, valid PER 37.69%
EPOCH 14, Learning Rate: 0.5
  batch 50 loss: 1.1847785520553589
  batch 100 loss: 1.188777893781662
  batch 150 loss: 1.1703792190551758
  batch 200 loss: 1.1843253552913666
  batch 250 loss: 1.1809378373622894
  batch 300 loss: 1.1919765174388885
  batch 350 loss: 1.1427318739891053
  batch 400 loss: 1.1871314942836761
  batch 450 loss: 1.134372181892395
  batch 500 loss: 1.1983894443511962
  batch 550 loss: 1.216744383573532
  batch 600 loss: 1.1671522665023804
  batch 650 loss: 1.2092186665534974
  batch 700 loss: 1.1937979316711427
  batch 750 loss: 1.152490713596344
  batch 800 loss: 1.128712943792343
  batch 850 loss: 1.1679370534420013
  batch 900 loss: 1.1769690442085265
LOSS train 1.17697 valid 1.19569, valid PER 39.45%
EPOCH 15, Learning Rate: 0.5
  batch 50 loss: 1.2078815054893495
  batch 100 loss: 1.1363265872001649
  batch 150 loss: 1.1631062698364258
  batch 200 loss: 1.1947741055488585
  batch 250 loss: 1.155104945898056
  batch 300 loss: 1.1211603379249573
  batch 350 loss: 1.1435108840465547
  batch 400 loss: 1.1461469686031343
  batch 450 loss: 1.1488531863689422
  batch 500 loss: 1.1085429799556732
  batch 550 loss: 1.1619445037841798
  batch 600 loss: 1.1518510377407074
  batch 650 loss: 1.1635169363021851
  batch 700 loss: 1.1776720404624939
  batch 750 loss: 1.1808735096454621
  batch 800 loss: 1.1429671084880828
  batch 850 loss: 1.1346561324596405
  batch 900 loss: 1.1703324401378632
LOSS train 1.17033 valid 1.13027, valid PER 37.11%
EPOCH 16, Learning Rate: 0.5
  batch 50 loss: 1.1988512372970581
  batch 100 loss: 1.1276256883144378
  batch 150 loss: 1.1454995930194856
  batch 200 loss: 1.1248730945587158
  batch 250 loss: 1.162735035419464
  batch 300 loss: 1.1538339805603028
  batch 350 loss: 1.1735281467437744
  batch 400 loss: 1.1353914654254913
  batch 450 loss: 1.1697373831272124
  batch 500 loss: 1.164580957889557
  batch 550 loss: 1.14619122505188
  batch 600 loss: 1.1540558171272277
  batch 650 loss: 1.19761239528656
  batch 700 loss: 1.128850291967392
  batch 750 loss: 1.1354823696613312
  batch 800 loss: 1.1645770633220673
  batch 850 loss: 1.127068064212799
  batch 900 loss: 1.141452406644821
LOSS train 1.14145 valid 1.13682, valid PER 36.48%
EPOCH 17, Learning Rate: 0.5
  batch 50 loss: 1.1641857862472533
  batch 100 loss: 1.1184196043014527
  batch 150 loss: 1.1112667798995972
  batch 200 loss: 1.1185617387294768
  batch 250 loss: 1.1579277276992799
  batch 300 loss: 1.117451411485672
  batch 350 loss: 1.1036630368232727
  batch 400 loss: 1.1999706292152406
  batch 450 loss: 1.1541797351837157
  batch 500 loss: 1.1469234597682954
  batch 550 loss: 1.154791635274887
  batch 600 loss: 1.199883863925934
  batch 650 loss: 1.1232402884960175
  batch 700 loss: 1.1390227937698365
  batch 750 loss: 1.126240200996399
  batch 800 loss: 1.1083925426006318
  batch 850 loss: 1.1132554852962493
  batch 900 loss: 1.1149364256858825
LOSS train 1.11494 valid 1.13348, valid PER 36.83%
EPOCH 18, Learning Rate: 0.5
  batch 50 loss: 1.109718201160431
  batch 100 loss: 1.128974916934967
  batch 150 loss: 1.153032692670822
  batch 200 loss: 1.1253894233703614
  batch 250 loss: 1.141996990442276
  batch 300 loss: 1.1337175214290618
  batch 350 loss: 1.1591951036453247
  batch 400 loss: 1.1124214625358582
  batch 450 loss: 1.150808584690094
  batch 500 loss: 1.135360621213913
  batch 550 loss: 1.0891871058940887
  batch 600 loss: 1.0852355325222016
  batch 650 loss: 1.105725862979889
  batch 700 loss: 1.1395164728164673
  batch 750 loss: 1.1277718567848205
  batch 800 loss: 1.108992874622345
  batch 850 loss: 1.0963881039619445
  batch 900 loss: 1.1300737261772156
LOSS train 1.13007 valid 1.11951, valid PER 36.14%
EPOCH 19, Learning Rate: 0.5
  batch 50 loss: 1.082197197675705
  batch 100 loss: 1.0832811534404754
  batch 150 loss: 1.0876380515098572
  batch 200 loss: 1.0959799361228943
  batch 250 loss: 1.1346812796592713
  batch 300 loss: 1.125375257730484
  batch 350 loss: 1.1077009570598602
  batch 400 loss: 1.131670743227005
  batch 450 loss: 1.1052547311782837
  batch 500 loss: 1.1310505318641662
  batch 550 loss: 1.1307603430747986
  batch 600 loss: 1.1349937188625336
  batch 650 loss: 1.1352020812034607
  batch 700 loss: 1.0816304469108582
  batch 750 loss: 1.0836335122585297
  batch 800 loss: 1.1243608689308167
  batch 850 loss: 1.145829952955246
  batch 900 loss: 1.1000204801559448
LOSS train 1.10002 valid 1.10154, valid PER 35.83%
EPOCH 20, Learning Rate: 0.5
  batch 50 loss: 1.0715301465988158
  batch 100 loss: 1.0754794311523437
  batch 150 loss: 1.0937286126613617
  batch 200 loss: 1.104449371099472
  batch 250 loss: 1.1120643639564514
  batch 300 loss: 1.1342927026748657
  batch 350 loss: 1.0843598198890687
  batch 400 loss: 1.0697905600070954
  batch 450 loss: 1.0897808480262756
  batch 500 loss: 1.084456832408905
  batch 550 loss: 1.1322823131084443
  batch 600 loss: 1.0599382174015046
  batch 650 loss: 1.11182577252388
  batch 700 loss: 1.1136274456977844
  batch 750 loss: 1.093872262239456
  batch 800 loss: 1.096726771593094
  batch 850 loss: 1.1112012028694154
  batch 900 loss: 1.084871870279312
LOSS train 1.08487 valid 1.09575, valid PER 36.23%
Training finished in 2.0 minutes.
Model saved to checkpoints/20231209_103306/model_20
Loading model from checkpoints/20231209_103306/model_20
SUB: 19.27%, DEL: 17.45%, INS: 1.06%, COR: 63.28%, PER: 37.77%
