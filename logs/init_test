Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.164661588668824
  batch 100 loss: 3.1752331352233885
  batch 150 loss: 3.0281986236572265
  batch 200 loss: 2.9083058500289916
  batch 250 loss: 2.816084384918213
  batch 300 loss: 2.6697351121902466
  batch 350 loss: 2.5023756933212282
  batch 400 loss: 2.40158233165741
  batch 450 loss: 2.308008794784546
  batch 500 loss: 2.18451874256134
  batch 550 loss: 2.108509986400604
  batch 600 loss: 2.079750416278839
  batch 650 loss: 1.976990156173706
  batch 700 loss: 1.9706689023971558
  batch 750 loss: 1.900302438735962
  batch 800 loss: 1.886273853778839
  batch 850 loss: 1.8234472584724426
  batch 900 loss: 1.8247359204292297
LOSS train 1.82474 valid 1.77826, valid PER 68.06%
EPOCH 2:
  batch 50 loss: 1.7400638699531554
  batch 100 loss: 1.6861701202392578
  batch 150 loss: 1.6798817443847656
  batch 200 loss: 1.6590818333625794
  batch 250 loss: 1.674174394607544
  batch 300 loss: 1.6192601275444032
  batch 350 loss: 1.531275737285614
  batch 400 loss: 1.540216588973999
  batch 450 loss: 1.483858654499054
  batch 500 loss: 1.511573166847229
  batch 550 loss: 1.5130879831314088
  batch 600 loss: 1.4603882455825805
  batch 650 loss: 1.4631455087661742
  batch 700 loss: 1.4543942403793335
  batch 750 loss: 1.4181587052345277
  batch 800 loss: 1.3646880602836609
  batch 850 loss: 1.3597735357284546
  batch 900 loss: 1.3859193873405458
LOSS train 1.38592 valid 1.33755, valid PER 42.31%
EPOCH 3:
  batch 50 loss: 1.326499400138855
  batch 100 loss: 1.3379982924461364
  batch 150 loss: 1.3460222744941712
  batch 200 loss: 1.2849606966972351
  batch 250 loss: 1.2737198531627656
  batch 300 loss: 1.2798337745666504
  batch 350 loss: 1.310011739730835
  batch 400 loss: 1.275743043422699
  batch 450 loss: 1.2706795740127563
  batch 500 loss: 1.2685961651802062
  batch 550 loss: 1.2994684505462646
  batch 600 loss: 1.2372087812423707
  batch 650 loss: 1.1899700617790223
  batch 700 loss: 1.2336374509334564
  batch 750 loss: 1.2779795706272126
  batch 800 loss: 1.202702214717865
  batch 850 loss: 1.226858642101288
  batch 900 loss: 1.173877671957016
LOSS train 1.17388 valid 1.25135, valid PER 37.60%
EPOCH 4:
  batch 50 loss: 1.1612856888771057
  batch 100 loss: 1.179073097705841
  batch 150 loss: 1.1240652656555177
  batch 200 loss: 1.1606138443946838
  batch 250 loss: 1.1763252377510072
  batch 300 loss: 1.1764053070545197
  batch 350 loss: 1.1243919014930726
  batch 400 loss: 1.1681581544876098
  batch 450 loss: 1.1313842236995697
  batch 500 loss: 1.1192955994606018
  batch 550 loss: 1.1685506522655487
  batch 600 loss: 1.1679464530944825
  batch 650 loss: 1.1732106077671052
  batch 700 loss: 1.1216366219520568
  batch 750 loss: 1.1066002702713014
  batch 800 loss: 1.0745308029651641
  batch 850 loss: 1.134702820777893
  batch 900 loss: 1.1641760277748108
LOSS train 1.16418 valid 1.14313, valid PER 35.54%
EPOCH 5:
  batch 50 loss: 1.052184864282608
  batch 100 loss: 1.0761453330516815
  batch 150 loss: 1.1145241022109986
  batch 200 loss: 1.0371102976799011
  batch 250 loss: 1.0912938904762268
  batch 300 loss: 1.1174621522426604
  batch 350 loss: 1.0819916784763337
  batch 400 loss: 1.0729867315292358
  batch 450 loss: 1.069370905160904
  batch 500 loss: 1.0801291477680206
  batch 550 loss: 1.0356333947181702
  batch 600 loss: 1.1098147642612457
  batch 650 loss: 1.062609485387802
  batch 700 loss: 1.1123534190654754
  batch 750 loss: 1.020584820508957
  batch 800 loss: 1.0726638865470886
  batch 850 loss: 1.083853075504303
  batch 900 loss: 1.0797205352783203
LOSS train 1.07972 valid 1.10031, valid PER 34.74%
EPOCH 6:
  batch 50 loss: 1.0328997194766998
  batch 100 loss: 1.0018895053863526
  batch 150 loss: 1.0317169845104217
  batch 200 loss: 1.027592579126358
  batch 250 loss: 1.0397233009338378
  batch 300 loss: 1.018579944372177
  batch 350 loss: 1.030832291841507
  batch 400 loss: 1.0297962832450867
  batch 450 loss: 1.0548144161701203
  batch 500 loss: 1.037820965051651
  batch 550 loss: 1.0467845046520232
  batch 600 loss: 0.9970730793476105
  batch 650 loss: 1.0259054601192474
  batch 700 loss: 1.0137950551509858
  batch 750 loss: 1.009873331785202
  batch 800 loss: 0.9997466230392456
  batch 850 loss: 0.985008932352066
  batch 900 loss: 1.0170354700088502
LOSS train 1.01704 valid 1.08053, valid PER 33.92%
EPOCH 7:
  batch 50 loss: 0.9781712698936462
  batch 100 loss: 1.001744383573532
  batch 150 loss: 0.9765800786018372
  batch 200 loss: 0.9659655475616455
  batch 250 loss: 0.9459354376792908
  batch 300 loss: 0.9657010507583618
  batch 350 loss: 0.9464777565002441
  batch 400 loss: 0.9664146947860718
  batch 450 loss: 0.9853428554534912
  batch 500 loss: 0.9681018447875976
  batch 550 loss: 0.9544914424419403
  batch 600 loss: 0.9783663272857666
  batch 650 loss: 1.016427128314972
  batch 700 loss: 1.0145530009269714
  batch 750 loss: 0.9891810142993926
  batch 800 loss: 0.9548539757728577
  batch 850 loss: 0.9874174654483795
  batch 900 loss: 1.027823830842972
LOSS train 1.02782 valid 1.04334, valid PER 32.75%
EPOCH 8:
  batch 50 loss: 0.9721860408782959
  batch 100 loss: 0.9379516959190368
  batch 150 loss: 0.9488534367084503
  batch 200 loss: 0.9162924194335937
  batch 250 loss: 0.935378770828247
  batch 300 loss: 0.8843529057502747
  batch 350 loss: 0.9691250252723694
  batch 400 loss: 0.9194396650791168
  batch 450 loss: 0.9524250316619873
  batch 500 loss: 0.9916539824008942
  batch 550 loss: 0.9144055998325348
  batch 600 loss: 0.9471978533267975
  batch 650 loss: 0.9843143987655639
  batch 700 loss: 0.9103585386276245
  batch 750 loss: 0.9300131177902222
  batch 800 loss: 0.9418628573417663
  batch 850 loss: 0.9195325088500976
  batch 900 loss: 0.9220820128917694
LOSS train 0.92208 valid 1.00266, valid PER 30.56%
EPOCH 9:
  batch 50 loss: 0.859249438047409
  batch 100 loss: 0.9094359683990478
  batch 150 loss: 0.9179716157913208
  batch 200 loss: 0.8812979876995086
  batch 250 loss: 0.9071045815944672
  batch 300 loss: 0.9116295278072357
  batch 350 loss: 0.9360042440891266
  batch 400 loss: 0.9176711332798004
  batch 450 loss: 0.9072378849983216
  batch 500 loss: 0.8717169547080994
  batch 550 loss: 0.9153790664672852
  batch 600 loss: 0.9388423883914947
  batch 650 loss: 0.91597491979599
  batch 700 loss: 0.8939904177188873
  batch 750 loss: 0.8962071096897125
  batch 800 loss: 0.912754385471344
  batch 850 loss: 0.9451822137832642
  batch 900 loss: 0.8864641404151916
LOSS train 0.88646 valid 1.01832, valid PER 31.98%
EPOCH 10:
  batch 50 loss: 0.8566154313087463
  batch 100 loss: 0.8747932577133178
  batch 150 loss: 0.8856168234348297
  batch 200 loss: 0.8819047021865845
  batch 250 loss: 0.8857110929489136
  batch 300 loss: 0.8568694627285004
  batch 350 loss: 0.890644268989563
  batch 400 loss: 0.8439620292186737
  batch 450 loss: 0.8421305644512177
  batch 500 loss: 0.888299605846405
  batch 550 loss: 0.8870743119716644
  batch 600 loss: 0.8825938045978546
  batch 650 loss: 0.8481909346580505
  batch 700 loss: 0.8799300038814545
  batch 750 loss: 0.8773386979103088
  batch 800 loss: 0.8824422478675842
  batch 850 loss: 0.8935256338119507
  batch 900 loss: 0.8938546276092529
LOSS train 0.89385 valid 1.06050, valid PER 33.74%
EPOCH 11:
  batch 50 loss: 0.8216525089740753
  batch 100 loss: 0.8004074430465699
  batch 150 loss: 0.8323742914199829
  batch 200 loss: 0.8888163721561432
  batch 250 loss: 0.8717483711242676
  batch 300 loss: 0.826893516778946
  batch 350 loss: 0.8724022090435029
  batch 400 loss: 0.8710922038555146
  batch 450 loss: 0.870084924697876
  batch 500 loss: 0.8737535035610199
  batch 550 loss: 0.87541055560112
  batch 600 loss: 0.8397396326065063
  batch 650 loss: 0.90693399310112
  batch 700 loss: 0.8229899382591248
  batch 750 loss: 0.8441817569732666
  batch 800 loss: 0.8645803701877594
  batch 850 loss: 0.9240831208229064
  batch 900 loss: 0.8955557310581207
LOSS train 0.89556 valid 0.98522, valid PER 30.23%
EPOCH 12:
  batch 50 loss: 0.8441155809164047
  batch 100 loss: 0.8163205432891846
  batch 150 loss: 0.8060034668445587
  batch 200 loss: 0.8373851203918456
  batch 250 loss: 0.856665141582489
  batch 300 loss: 0.8322215723991394
  batch 350 loss: 0.845663434267044
  batch 400 loss: 0.8564186239242554
  batch 450 loss: 0.854131977558136
  batch 500 loss: 0.8752705383300782
  batch 550 loss: 0.8259728538990021
  batch 600 loss: 0.8122817516326905
  batch 650 loss: 0.8642102324962616
  batch 700 loss: 0.8723143529891968
  batch 750 loss: 0.8292566585540772
  batch 800 loss: 0.8409832578897476
  batch 850 loss: 0.8755955731868744
  batch 900 loss: 0.9044744312763214
LOSS train 0.90447 valid 1.01243, valid PER 31.08%
EPOCH 13:
  batch 50 loss: 0.9307835900783539
  batch 100 loss: 0.9192068099975585
  batch 150 loss: 0.8753476703166961
  batch 200 loss: 0.8571053409576416
  batch 250 loss: 0.8419865441322326
  batch 300 loss: 0.8297435760498046
  batch 350 loss: 0.8662946844100952
  batch 400 loss: 0.8745496499538422
  batch 450 loss: 0.858218948841095
  batch 500 loss: 0.8325628876686096
  batch 550 loss: 0.8511551070213318
  batch 600 loss: 0.8889850664138794
  batch 650 loss: 0.892367125749588
  batch 700 loss: 0.8661395466327667
  batch 750 loss: 0.8215577983856202
  batch 800 loss: 0.8499572992324829
  batch 850 loss: 0.8623737788200379
  batch 900 loss: 0.8604033613204956
LOSS train 0.86040 valid 0.97481, valid PER 29.96%
EPOCH 14:
  batch 50 loss: 0.8035169279575348
  batch 100 loss: 0.8265434300899506
  batch 150 loss: 0.7886958646774292
  batch 200 loss: 0.8118091189861297
  batch 250 loss: 0.8037034952640534
  batch 300 loss: 0.828432629108429
  batch 350 loss: 0.799569673538208
  batch 400 loss: 0.8355352485179901
  batch 450 loss: 0.7964353144168854
  batch 500 loss: 0.8108637630939484
  batch 550 loss: 0.8179404199123382
  batch 600 loss: 0.7852363532781601
  batch 650 loss: 0.8323618388175964
  batch 700 loss: 0.8382652044296265
  batch 750 loss: 0.8011091059446335
  batch 800 loss: 0.78221275806427
  batch 850 loss: 0.8716258573532104
  batch 900 loss: 0.8368678343296051
LOSS train 0.83687 valid 0.96733, valid PER 30.12%
EPOCH 15:
  batch 50 loss: 0.7833968037366867
  batch 100 loss: 0.7521950888633728
  batch 150 loss: 0.7660699498653412
  batch 200 loss: 0.8058235210180282
  batch 250 loss: 0.8048920595645904
  batch 300 loss: 0.7852199625968933
  batch 350 loss: 0.7901866245269775
  batch 400 loss: 0.7882024401426315
  batch 450 loss: 0.8010365557670593
  batch 500 loss: 0.7622692370414734
  batch 550 loss: 0.7858550298213959
  batch 600 loss: 0.8144823813438415
  batch 650 loss: 0.8176042830944061
  batch 700 loss: 0.8134280061721801
  batch 750 loss: 0.8048130571842194
  batch 800 loss: 0.7880329221487046
  batch 850 loss: 0.7911568999290466
  batch 900 loss: 0.8109870612621307
LOSS train 0.81099 valid 1.00541, valid PER 30.84%
EPOCH 16:
  batch 50 loss: 0.8027878844738007
  batch 100 loss: 0.7537530332803726
  batch 150 loss: 0.7548980903625488
  batch 200 loss: 0.766621515750885
  batch 250 loss: 0.7751204025745392
  batch 300 loss: 0.7841790270805359
  batch 350 loss: 0.9108767068386078
  batch 400 loss: 0.8697242081165314
  batch 450 loss: 0.8698967254161835
  batch 500 loss: 0.7990290439128875
  batch 550 loss: 0.8297917020320892
  batch 600 loss: 0.8211942768096924
  batch 650 loss: 0.8505575895309448
  batch 700 loss: 0.8363698256015778
  batch 750 loss: 0.8276395905017853
  batch 800 loss: 0.8270325982570648
  batch 850 loss: 0.7838969093561172
  batch 900 loss: 0.7926334536075592
LOSS train 0.79263 valid 0.96122, valid PER 30.01%
EPOCH 17:
  batch 50 loss: 0.7881908571720123
  batch 100 loss: 0.7919539594650269
  batch 150 loss: 0.7789875304698944
  batch 200 loss: 0.7642874771356583
  batch 250 loss: 0.8036534333229065
  batch 300 loss: 0.7871439063549042
  batch 350 loss: 0.763625658750534
  batch 400 loss: 0.8102675628662109
  batch 450 loss: 0.7920365536212921
  batch 500 loss: 0.7802856945991516
  batch 550 loss: 0.7962017500400543
  batch 600 loss: 0.8210847640037536
  batch 650 loss: 0.780102686882019
  batch 700 loss: 0.7782389330863952
  batch 750 loss: 0.7726903486251832
  batch 800 loss: 0.7853006327152252
  batch 850 loss: 0.8255464768409729
  batch 900 loss: 0.769965397119522
LOSS train 0.76997 valid 0.97055, valid PER 29.18%
EPOCH 18:
  batch 50 loss: 0.7378738558292389
  batch 100 loss: 0.7747736155986786
  batch 150 loss: 0.7779888010025025
  batch 200 loss: 0.7523330265283584
  batch 250 loss: 0.7549891185760498
  batch 300 loss: 0.7743474733829498
  batch 350 loss: 0.7917766773700714
  batch 400 loss: 0.7696950078010559
  batch 450 loss: 0.8064177632331848
  batch 500 loss: 0.7616217070817948
  batch 550 loss: 0.7610502851009369
  batch 600 loss: 0.7476217585802079
  batch 650 loss: 0.7844935297966004
  batch 700 loss: 0.7858124017715454
  batch 750 loss: 0.7510653978586197
  batch 800 loss: 0.7706007635593415
  batch 850 loss: 0.7824971854686738
  batch 900 loss: 0.7937735450267792
LOSS train 0.79377 valid 0.96949, valid PER 30.05%
EPOCH 19:
  batch 50 loss: 0.694475611448288
  batch 100 loss: 0.7067966735363007
  batch 150 loss: 0.7447877609729767
  batch 200 loss: 0.7460571885108948
  batch 250 loss: 0.7606474256515503
  batch 300 loss: 0.7489322578907013
  batch 350 loss: 0.7439305645227432
  batch 400 loss: 0.7730215811729431
  batch 450 loss: 0.7609066605567932
  batch 500 loss: 0.7690815341472625
  batch 550 loss: 0.7906327617168426
  batch 600 loss: 0.7742426145076752
  batch 650 loss: 0.862282065153122
  batch 700 loss: 0.7826116478443146
  batch 750 loss: 0.7725516039133072
  batch 800 loss: 0.8256559455394745
  batch 850 loss: 0.8648030626773834
  batch 900 loss: 0.8021973812580109
LOSS train 0.80220 valid 0.98636, valid PER 30.10%
EPOCH 20:
  batch 50 loss: 0.7448681628704071
  batch 100 loss: 0.7226418709754944
  batch 150 loss: 0.7382401877641678
  batch 200 loss: 0.7371797287464141
  batch 250 loss: 0.7834867942333221
  batch 300 loss: 0.8277363431453705
  batch 350 loss: 0.797162070274353
  batch 400 loss: 0.7796476101875305
  batch 450 loss: 0.7854636800289154
  batch 500 loss: 0.7421487122774124
  batch 550 loss: 0.8047584438323975
  batch 600 loss: 0.8112055468559265
  batch 650 loss: 0.8218319481611251
  batch 700 loss: 0.8219562578201294
  batch 750 loss: 0.7927973413467407
  batch 800 loss: 0.8357404375076294
  batch 850 loss: 0.8129770362377167
  batch 900 loss: 0.8387521207332611
LOSS train 0.83875 valid 1.05625, valid PER 31.39%
Training finished in 5.0 minutes.
Model saved to checkpoints/20231122_155027/model_16
Loading model from checkpoints/20231122_155027/model_16
SUB: 17.57%, DEL: 11.80%, INS: 2.86%, COR: 70.62%, PER: 32.24%
