Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.6, optimiser='sgd')
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.158271284103393
  batch 100 loss: 3.1642279195785523
  batch 150 loss: 3.0587754917144774
  batch 200 loss: 2.9455657386779786
  batch 250 loss: 2.8801191949844362
  batch 300 loss: 2.7189941215515137
  batch 350 loss: 2.601484055519104
  batch 400 loss: 2.539203600883484
  batch 450 loss: 2.4899761295318603
  batch 500 loss: 2.5115697288513186
  batch 550 loss: 2.3120858764648435
  batch 600 loss: 2.2433739137649535
  batch 650 loss: 2.1315900015830995
  batch 700 loss: 2.1306908249855043
  batch 750 loss: 2.0842064309120176
  batch 800 loss: 2.0626120710372926
  batch 850 loss: 2.01875203371048
  batch 900 loss: 2.0067628979682923
LOSS train 2.00676 valid 1.95640, valid PER 75.33%
EPOCH 2:
  batch 50 loss: 1.951100423336029
  batch 100 loss: 1.9250746083259582
  batch 150 loss: 1.8950013756752013
  batch 200 loss: 1.8848165702819824
  batch 250 loss: 1.9059542393684388
  batch 300 loss: 1.8579100823402406
  batch 350 loss: 1.7859165000915527
  batch 400 loss: 1.792359561920166
  batch 450 loss: 1.7616421270370484
  batch 500 loss: 1.7734952783584594
  batch 550 loss: 1.781911585330963
  batch 600 loss: 1.7190711212158203
  batch 650 loss: 1.7536379647254945
  batch 700 loss: 1.7534069180488587
  batch 750 loss: 1.720327410697937
  batch 800 loss: 1.6643256592750548
  batch 850 loss: 1.6770851230621338
  batch 900 loss: 1.6920977115631104
LOSS train 1.69210 valid 1.58414, valid PER 61.93%
EPOCH 3:
  batch 50 loss: 1.6616251683235168
  batch 100 loss: 1.6424969744682312
  batch 150 loss: 1.6337819576263428
  batch 200 loss: 1.6245844912528993
  batch 250 loss: 1.5952723550796508
  batch 300 loss: 1.602381205558777
  batch 350 loss: 1.6388339495658875
  batch 400 loss: 1.6103967666625976
  batch 450 loss: 1.6031598210334779
  batch 500 loss: 1.5599618291854858
  batch 550 loss: 1.5917067694664002
  batch 600 loss: 1.5644655013084412
  batch 650 loss: 1.5236048936843871
  batch 700 loss: 1.564639708995819
  batch 750 loss: 1.6021679902076722
  batch 800 loss: 1.512420437335968
  batch 850 loss: 1.5469493174552917
  batch 900 loss: 1.497132782936096
LOSS train 1.49713 valid 1.41646, valid PER 51.90%
EPOCH 4:
  batch 50 loss: 1.5043541312217712
  batch 100 loss: 1.540900809764862
  batch 150 loss: 1.462980191707611
  batch 200 loss: 1.5116442275047302
  batch 250 loss: 1.4985435009002686
  batch 300 loss: 1.519543881416321
  batch 350 loss: 1.4328913927078246
  batch 400 loss: 1.4927323269844055
  batch 450 loss: 1.4719116282463074
  batch 500 loss: 1.4532171201705932
  batch 550 loss: 1.4650646567344665
  batch 600 loss: 1.4843584299087524
  batch 650 loss: 1.4872707414627075
  batch 700 loss: 1.448375425338745
  batch 750 loss: 1.408954598903656
  batch 800 loss: 1.3946120572090148
  batch 850 loss: 1.4170674633979798
  batch 900 loss: 1.4675992703437806
LOSS train 1.46760 valid 1.29617, valid PER 43.01%
EPOCH 5:
  batch 50 loss: 1.396968126296997
  batch 100 loss: 1.379125108718872
  batch 150 loss: 1.428310558795929
  batch 200 loss: 1.3473558974266053
  batch 250 loss: 1.3734710788726807
  batch 300 loss: 1.4216219806671142
  batch 350 loss: 1.3799588394165039
  batch 400 loss: 1.390537211894989
  batch 450 loss: 1.373071084022522
  batch 500 loss: 1.397281346321106
  batch 550 loss: 1.346669476032257
  batch 600 loss: 1.4040790247917174
  batch 650 loss: 1.3649383902549743
  batch 700 loss: 1.3892209076881408
  batch 750 loss: 1.3499981927871705
  batch 800 loss: 1.3429708528518676
  batch 850 loss: 1.4015760970115663
  batch 900 loss: 1.398317071199417
LOSS train 1.39832 valid 1.21174, valid PER 40.39%
EPOCH 6:
  batch 50 loss: 1.3827455830574036
  batch 100 loss: 1.3248406982421874
  batch 150 loss: 1.2959200465679168
  batch 200 loss: 1.357253520488739
  batch 250 loss: 1.370777714252472
  batch 300 loss: 1.3315874314308167
  batch 350 loss: 1.3330544233322144
  batch 400 loss: 1.3261095356941224
  batch 450 loss: 1.3993598890304566
  batch 500 loss: 1.341945354938507
  batch 550 loss: 1.3805597615242005
  batch 600 loss: 1.3039638948440553
  batch 650 loss: 1.3390927577018739
  batch 700 loss: 1.3156526517868041
  batch 750 loss: 1.2998738372325898
  batch 800 loss: 1.3069408774375915
  batch 850 loss: 1.3272950160503387
  batch 900 loss: 1.3295278549194336
LOSS train 1.32953 valid 1.22584, valid PER 39.91%
EPOCH 7:
  batch 50 loss: 1.349436845779419
  batch 100 loss: 1.3440608978271484
  batch 150 loss: 1.3255802941322328
  batch 200 loss: 1.2846741449832917
  batch 250 loss: 1.2992149376869202
  batch 300 loss: 1.2729510903358459
  batch 350 loss: 1.2887541031837464
  batch 400 loss: 1.2814546966552733
  batch 450 loss: 1.2666618144512176
  batch 500 loss: 1.2759299850463868
  batch 550 loss: 1.269743173122406
  batch 600 loss: 1.284818980693817
  batch 650 loss: 1.2701888573169708
  batch 700 loss: 1.2965998017787934
  batch 750 loss: 1.2491509664058684
  batch 800 loss: 1.262990403175354
  batch 850 loss: 1.3079588866233827
  batch 900 loss: 1.339169328212738
LOSS train 1.33917 valid 1.22168, valid PER 39.96%
EPOCH 8:
  batch 50 loss: 1.271629034280777
  batch 100 loss: 1.3101299715042114
  batch 150 loss: 1.3041633117198943
  batch 200 loss: 1.248976776599884
  batch 250 loss: 1.2687705588340759
  batch 300 loss: 1.2297455763816834
  batch 350 loss: 1.2945899736881257
  batch 400 loss: 1.2596139025688171
  batch 450 loss: 1.2596426677703858
  batch 500 loss: 1.323913505077362
  batch 550 loss: 1.2198487627506256
  batch 600 loss: 1.284982807636261
  batch 650 loss: 1.3071767020225524
  batch 700 loss: 1.2595664286613464
  batch 750 loss: 1.2569457936286925
  batch 800 loss: 1.289606122970581
  batch 850 loss: 1.264040846824646
  batch 900 loss: 1.2504818224906922
LOSS train 1.25048 valid 1.14475, valid PER 38.13%
EPOCH 9:
  batch 50 loss: 1.1987250804901124
  batch 100 loss: 1.2524603259563447
  batch 150 loss: 1.2666561925411224
  batch 200 loss: 1.1867030990123748
  batch 250 loss: 1.228576090335846
  batch 300 loss: 1.2190749442577362
  batch 350 loss: 1.252285989522934
  batch 400 loss: 1.2309959065914153
  batch 450 loss: 1.2239379954338074
  batch 500 loss: 1.2014179635047912
  batch 550 loss: 1.2425832307338716
  batch 600 loss: 1.2747252345085145
  batch 650 loss: 1.2021692371368409
  batch 700 loss: 1.2096899914741517
  batch 750 loss: 1.2041364014148712
  batch 800 loss: 1.243839728832245
  batch 850 loss: 1.3096996498107911
  batch 900 loss: 1.2097428691387178
LOSS train 1.20974 valid 1.10440, valid PER 36.67%
EPOCH 10:
  batch 50 loss: 1.1567280077934265
  batch 100 loss: 1.208460544347763
  batch 150 loss: 1.2133048248291016
  batch 200 loss: 1.2362113213539123
  batch 250 loss: 1.2082636165618896
  batch 300 loss: 1.1521407330036164
  batch 350 loss: 1.225296800136566
  batch 400 loss: 1.1604886543750763
  batch 450 loss: 1.1835110259056092
  batch 500 loss: 1.2023824405670167
  batch 550 loss: 1.2210133492946624
  batch 600 loss: 1.176103036403656
  batch 650 loss: 1.19090283036232
  batch 700 loss: 1.2224062025547027
  batch 750 loss: 1.1975747168064117
  batch 800 loss: 1.229277846813202
  batch 850 loss: 1.2151230990886688
  batch 900 loss: 1.189783067703247
LOSS train 1.18978 valid 1.15393, valid PER 38.60%
EPOCH 11:
  batch 50 loss: 1.1712978625297545
  batch 100 loss: 1.1348499822616578
  batch 150 loss: 1.1580303370952607
  batch 200 loss: 1.221786242723465
  batch 250 loss: 1.1677229332923889
  batch 300 loss: 1.166826652288437
  batch 350 loss: 1.2070784699916839
  batch 400 loss: 1.2083324635028838
  batch 450 loss: 1.188214386701584
  batch 500 loss: 1.140905956029892
  batch 550 loss: 1.1502337777614593
  batch 600 loss: 1.1349957740306855
  batch 650 loss: 1.2243725419044496
  batch 700 loss: 1.1407982802391052
  batch 750 loss: 1.1492809283733367
  batch 800 loss: 1.1881674242019653
  batch 850 loss: 1.2044506454467774
  batch 900 loss: 1.1885143625736236
LOSS train 1.18851 valid 1.08242, valid PER 36.39%
EPOCH 12:
  batch 50 loss: 1.1676387321949004
  batch 100 loss: 1.1466082787513734
  batch 150 loss: 1.129517239332199
  batch 200 loss: 1.1429888963699342
  batch 250 loss: 1.1777381932735442
  batch 300 loss: 1.1321595239639282
  batch 350 loss: 1.1307647049427032
  batch 400 loss: 1.1779634463787079
  batch 450 loss: 1.1534970927238464
  batch 500 loss: 1.1962608945369722
  batch 550 loss: 1.1683422720432282
  batch 600 loss: 1.2344576013088226
  batch 650 loss: 1.215655175447464
  batch 700 loss: 1.2273859941959382
  batch 750 loss: 1.1761737620830537
  batch 800 loss: 1.169976873397827
  batch 850 loss: 1.216156176328659
  batch 900 loss: 1.20172412276268
LOSS train 1.20172 valid 1.08568, valid PER 36.40%
EPOCH 13:
  batch 50 loss: 1.1623594856262207
  batch 100 loss: 1.1786097872257233
  batch 150 loss: 1.1804654061794282
  batch 200 loss: 1.1811520314216615
  batch 250 loss: 1.1692538177967071
  batch 300 loss: 1.1496888327598571
  batch 350 loss: 1.1683082902431488
  batch 400 loss: 1.1917132365703582
  batch 450 loss: 1.184856435060501
  batch 500 loss: 1.151670877933502
  batch 550 loss: 1.1772396612167357
  batch 600 loss: 1.1480494666099548
  batch 650 loss: 1.1607253754138946
  batch 700 loss: 1.1982908487319945
  batch 750 loss: 1.1404127144813538
  batch 800 loss: 1.1491346895694732
  batch 850 loss: 1.154151486158371
  batch 900 loss: 1.1729776644706726
LOSS train 1.17298 valid 1.05075, valid PER 35.21%
EPOCH 14:
  batch 50 loss: 1.1464539992809295
  batch 100 loss: 1.133161941766739
  batch 150 loss: 1.1264772450923919
  batch 200 loss: 1.1413596296310424
  batch 250 loss: 1.1421610558032989
  batch 300 loss: 1.163251725435257
  batch 350 loss: 1.13242067694664
  batch 400 loss: 1.1312935733795166
  batch 450 loss: 1.133657910823822
  batch 500 loss: 1.133833132982254
  batch 550 loss: 1.15553835272789
  batch 600 loss: 1.1228387451171875
  batch 650 loss: 1.1578183245658875
  batch 700 loss: 1.1525170361995698
  batch 750 loss: 1.1503657138347625
  batch 800 loss: 1.0754259538650512
  batch 850 loss: 1.1498262810707092
  batch 900 loss: 1.1246622836589812
LOSS train 1.12466 valid 1.07328, valid PER 35.08%
EPOCH 15:
  batch 50 loss: 1.10731250166893
  batch 100 loss: 1.0848546087741853
  batch 150 loss: 1.0987189483642579
  batch 200 loss: 1.1708849108219146
  batch 250 loss: 1.139002764225006
  batch 300 loss: 1.1231880164146424
  batch 350 loss: 1.111524007320404
  batch 400 loss: 1.1035477328300476
  batch 450 loss: 1.110306634902954
  batch 500 loss: 1.0679419696331025
  batch 550 loss: 1.109025515317917
  batch 600 loss: 1.1644032418727874
  batch 650 loss: 1.152794132232666
  batch 700 loss: 1.1296886074543
  batch 750 loss: 1.1311787056922913
  batch 800 loss: 1.1261147499084472
  batch 850 loss: 1.1134404361248016
  batch 900 loss: 1.1611760234832764
LOSS train 1.16118 valid 1.10133, valid PER 36.28%
EPOCH 16:
  batch 50 loss: 1.1621163606643676
  batch 100 loss: 1.1063743400573731
  batch 150 loss: 1.1160936737060547
  batch 200 loss: 1.1204117393493653
  batch 250 loss: 1.1698299252986908
  batch 300 loss: 1.133396953344345
  batch 350 loss: 1.1615673780441285
  batch 400 loss: 1.1493219792842866
  batch 450 loss: 1.179954196214676
  batch 500 loss: 1.2422125077247619
  batch 550 loss: 1.225841497182846
  batch 600 loss: 1.1667423272132873
  batch 650 loss: 1.1486730909347533
  batch 700 loss: 1.1372042715549469
  batch 750 loss: 1.1512331449985505
  batch 800 loss: 1.155229924917221
  batch 850 loss: 1.1278787386417388
  batch 900 loss: 1.1426365685462951
LOSS train 1.14264 valid 1.08323, valid PER 35.43%
EPOCH 17:
  batch 50 loss: 1.1290564131736756
  batch 100 loss: 1.1074003636837007
  batch 150 loss: 1.1553719222545624
  batch 200 loss: 1.162094544172287
  batch 250 loss: 1.1377249252796173
  batch 300 loss: 1.1469423615932464
  batch 350 loss: 1.0993113732337951
  batch 400 loss: 1.1431455051898955
  batch 450 loss: 1.1963448357582092
  batch 500 loss: 1.1242357635498046
  batch 550 loss: 1.1428893148899077
  batch 600 loss: 1.1987978947162627
  batch 650 loss: 1.1449632263183593
  batch 700 loss: 1.1293786776065826
  batch 750 loss: 1.0922141075134277
  batch 800 loss: 1.1421105754375458
  batch 850 loss: 1.1285384583473206
  batch 900 loss: 1.1442113077640534
LOSS train 1.14421 valid 1.06134, valid PER 34.78%
EPOCH 18:
  batch 50 loss: 1.12236137509346
  batch 100 loss: 1.1517906510829925
  batch 150 loss: 1.1442959415912628
  batch 200 loss: 1.1065104067325593
  batch 250 loss: 1.1138408768177033
  batch 300 loss: 1.0949661099910737
  batch 350 loss: 1.170771597623825
  batch 400 loss: 1.099475840330124
  batch 450 loss: 1.1469308590888978
  batch 500 loss: 1.1313909995555878
  batch 550 loss: 1.133170521259308
  batch 600 loss: 1.0973596477508545
  batch 650 loss: 1.0812642443180085
  batch 700 loss: 1.1809977877140045
  batch 750 loss: 1.113588901758194
  batch 800 loss: 1.0972396183013915
  batch 850 loss: 1.083157298564911
  batch 900 loss: 1.1128247559070588
LOSS train 1.11282 valid 1.04799, valid PER 34.69%
EPOCH 19:
  batch 50 loss: 1.0425510728359222
  batch 100 loss: 1.086257472038269
  batch 150 loss: 1.1035884296894074
  batch 200 loss: 1.0821683371067048
  batch 250 loss: 1.1016815280914307
  batch 300 loss: 1.0868152916431426
  batch 350 loss: 1.0898599457740783
  batch 400 loss: 1.1078870630264281
  batch 450 loss: 1.0887392795085906
  batch 500 loss: 1.1059494817256927
  batch 550 loss: 1.0750533771514892
  batch 600 loss: 1.0819571220874786
  batch 650 loss: 1.143488850593567
  batch 700 loss: 1.0820860159397125
  batch 750 loss: 1.109496418237686
  batch 800 loss: 1.1216339731216431
  batch 850 loss: 1.117686868906021
  batch 900 loss: 1.1071946918964386
LOSS train 1.10719 valid 1.04373, valid PER 34.94%
EPOCH 20:
  batch 50 loss: 1.0599354875087739
  batch 100 loss: 1.0649888181686402
  batch 150 loss: 1.0517787718772889
  batch 200 loss: 1.0870285093784333
  batch 250 loss: 1.0813199985027313
  batch 300 loss: 1.1215167582035064
  batch 350 loss: 1.061225893497467
  batch 400 loss: 1.0655667996406555
  batch 450 loss: 1.0612612640857697
  batch 500 loss: 1.0510472810268403
  batch 550 loss: 1.1472839224338531
  batch 600 loss: 1.0962838912010193
  batch 650 loss: 1.091628793478012
  batch 700 loss: 1.0968838047981262
  batch 750 loss: 1.083092495203018
  batch 800 loss: 1.1152388393878936
  batch 850 loss: 1.0938287162780762
  batch 900 loss: 1.0827600407600402
LOSS train 1.08276 valid 1.01468, valid PER 33.17%
Training finished in 5.0 minutes.
Model saved to checkpoints/20231206_172906/model_20
Loading model from checkpoints/20231206_172906/model_20
SUB: 16.15%, DEL: 17.02%, INS: 1.39%, COR: 66.83%, PER: 34.56%
