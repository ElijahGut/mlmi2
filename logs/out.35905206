Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=2, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.2, optimiser='sgd')
Total number of model parameters is 562216
EPOCH 1:
  batch 50 loss: 4.174411153793335
  batch 100 loss: 3.294867744445801
  batch 150 loss: 3.2335696411132813
  batch 200 loss: 3.1266775846481325
  batch 250 loss: 2.9635278463363646
  batch 300 loss: 2.746692600250244
  batch 350 loss: 2.627012376785278
  batch 400 loss: 2.504848017692566
  batch 450 loss: 2.430539164543152
  batch 500 loss: 2.2927690029144285
  batch 550 loss: 2.2309881973266603
  batch 600 loss: 2.155159227848053
  batch 650 loss: 2.055963702201843
  batch 700 loss: 2.034208722114563
  batch 750 loss: 1.9461445903778076
  batch 800 loss: 1.9217609071731567
  batch 850 loss: 1.8685639810562134
  batch 900 loss: 1.8158309292793273
LOSS train 1.81583 valid 1.77031, valid PER 68.19%
EPOCH 2:
  batch 50 loss: 1.7686618328094483
  batch 100 loss: 1.714306001663208
  batch 150 loss: 1.59761892080307
  batch 200 loss: 1.6271734189987184
  batch 250 loss: 1.5980030274391175
  batch 300 loss: 1.5569817686080933
  batch 350 loss: 1.5338178229331971
  batch 400 loss: 1.4853522348403931
  batch 450 loss: 1.469823615550995
  batch 500 loss: 1.468132872581482
  batch 550 loss: 1.4239252734184265
  batch 600 loss: 1.4078541040420531
  batch 650 loss: 1.3339842867851257
  batch 700 loss: 1.351954402923584
  batch 750 loss: 1.3021925711631774
  batch 800 loss: 1.2676757669448853
  batch 850 loss: 1.2845153164863587
  batch 900 loss: 1.2427590680122376
LOSS train 1.24276 valid 1.23543, valid PER 38.23%
EPOCH 3:
  batch 50 loss: 1.1768015933036804
  batch 100 loss: 1.250651160478592
  batch 150 loss: 1.2686281383037568
  batch 200 loss: 1.2160846424102782
  batch 250 loss: 1.1984907817840575
  batch 300 loss: 1.1889981985092164
  batch 350 loss: 1.2009862744808197
  batch 400 loss: 1.1770624470710755
  batch 450 loss: 1.1936146855354308
  batch 500 loss: 1.1484902822971343
  batch 550 loss: 1.1594651579856872
  batch 600 loss: 1.0924901258945465
  batch 650 loss: 1.1195860695838928
  batch 700 loss: 1.1098352324962617
  batch 750 loss: 1.1231885707378388
  batch 800 loss: 1.1347264850139618
  batch 850 loss: 1.1004205417633057
  batch 900 loss: 1.117448116540909
LOSS train 1.11745 valid 1.07834, valid PER 33.49%
EPOCH 4:
  batch 50 loss: 1.1055121099948884
  batch 100 loss: 1.0265122580528259
  batch 150 loss: 1.0544827842712403
  batch 200 loss: 1.0399266111850738
  batch 250 loss: 1.0252950489521027
  batch 300 loss: 1.0478843319416047
  batch 350 loss: 1.0132717835903167
  batch 400 loss: 0.9954898178577423
  batch 450 loss: 0.9893286740779876
  batch 500 loss: 1.073212662935257
  batch 550 loss: 0.989660849571228
  batch 600 loss: 0.9908004188537598
  batch 650 loss: 1.0590171527862549
  batch 700 loss: 1.0560120224952698
  batch 750 loss: 1.0151984405517578
  batch 800 loss: 0.987547961473465
  batch 850 loss: 0.988053857088089
  batch 900 loss: 0.9944768464565277
LOSS train 0.99448 valid 1.00124, valid PER 30.86%
EPOCH 5:
  batch 50 loss: 0.9424653673171997
  batch 100 loss: 0.9704906988143921
  batch 150 loss: 1.0012081956863403
  batch 200 loss: 0.975524697303772
  batch 250 loss: 0.9337591314315796
  batch 300 loss: 0.9772307586669922
  batch 350 loss: 0.9207688403129578
  batch 400 loss: 0.9308502662181855
  batch 450 loss: 0.9317722535133361
  batch 500 loss: 0.9220403492450714
  batch 550 loss: 0.9659347081184387
  batch 600 loss: 0.9689209079742431
  batch 650 loss: 0.9388751709461212
  batch 700 loss: 0.9288545894622803
  batch 750 loss: 0.9332664239406586
  batch 800 loss: 0.9718135166168212
  batch 850 loss: 0.9490537285804749
  batch 900 loss: 0.9181287252902984
LOSS train 0.91813 valid 0.96116, valid PER 29.49%
EPOCH 6:
  batch 50 loss: 0.8850532448291779
  batch 100 loss: 0.8840395164489746
  batch 150 loss: 0.896788364648819
  batch 200 loss: 0.8595702672004699
  batch 250 loss: 0.8745440053939819
  batch 300 loss: 0.9016857004165649
  batch 350 loss: 0.9082100403308868
  batch 400 loss: 0.895458744764328
  batch 450 loss: 0.9037818622589111
  batch 500 loss: 0.8400486826896667
  batch 550 loss: 0.8625545370578765
  batch 600 loss: 0.868585821390152
  batch 650 loss: 0.8412571561336517
  batch 700 loss: 0.8550434541702271
  batch 750 loss: 0.8705518269538879
  batch 800 loss: 0.8701771438121796
  batch 850 loss: 0.9007005941867828
  batch 900 loss: 0.9033679676055908
LOSS train 0.90337 valid 0.91638, valid PER 28.83%
EPOCH 7:
  batch 50 loss: 0.817864031791687
  batch 100 loss: 0.8515783679485321
  batch 150 loss: 0.8027646803855896
  batch 200 loss: 0.8139462041854858
  batch 250 loss: 0.8618331587314606
  batch 300 loss: 0.832251924276352
  batch 350 loss: 0.8618623435497283
  batch 400 loss: 0.8166083908081054
  batch 450 loss: 0.8506436419486999
  batch 500 loss: 0.8043407273292541
  batch 550 loss: 0.8281379950046539
  batch 600 loss: 0.8449700677394867
  batch 650 loss: 0.8260463106632233
  batch 700 loss: 0.8306177413463592
  batch 750 loss: 0.8299996793270111
  batch 800 loss: 0.8035696971416474
  batch 850 loss: 0.8072723495960236
  batch 900 loss: 0.8367327916622161
LOSS train 0.83673 valid 0.91713, valid PER 28.84%
EPOCH 8:
  batch 50 loss: 0.8261524057388305
  batch 100 loss: 0.7813072121143341
  batch 150 loss: 0.8045880222320556
  batch 200 loss: 0.7756385421752929
  batch 250 loss: 0.7772595906257629
  batch 300 loss: 0.7437757897377014
  batch 350 loss: 0.78482666015625
  batch 400 loss: 0.7930539119243621
  batch 450 loss: 0.8157501316070557
  batch 500 loss: 0.7740204888582229
  batch 550 loss: 0.782873929142952
  batch 600 loss: 0.7526482796669006
  batch 650 loss: 0.7845018351078034
  batch 700 loss: 0.805403802394867
  batch 750 loss: 0.7952166831493378
  batch 800 loss: 0.8174121356010438
  batch 850 loss: 0.7665143239498139
  batch 900 loss: 0.7745825147628784
LOSS train 0.77458 valid 0.86964, valid PER 26.74%
EPOCH 9:
  batch 50 loss: 0.71770323574543
  batch 100 loss: 0.7303897964954377
  batch 150 loss: 0.753984911441803
  batch 200 loss: 0.7293774920701981
  batch 250 loss: 0.7072761672735214
  batch 300 loss: 0.7577067792415619
  batch 350 loss: 0.7205201321840287
  batch 400 loss: 0.7693945252895356
  batch 450 loss: 0.7823550164699554
  batch 500 loss: 0.7281288582086564
  batch 550 loss: 0.7423262143135071
  batch 600 loss: 0.7686041474342347
  batch 650 loss: 0.7585195636749268
  batch 700 loss: 0.7441834020614624
  batch 750 loss: 0.7483238792419433
  batch 800 loss: 0.7767012000083924
  batch 850 loss: 0.7743864846229553
  batch 900 loss: 0.7195836472511291
LOSS train 0.71958 valid 0.85437, valid PER 25.82%
EPOCH 10:
  batch 50 loss: 0.6998762059211731
  batch 100 loss: 0.7202832865715026
  batch 150 loss: 0.7220743596553802
  batch 200 loss: 0.6793887746334076
  batch 250 loss: 0.6991129422187805
  batch 300 loss: 0.7385625255107879
  batch 350 loss: 0.7012643837928771
  batch 400 loss: 0.6943435156345368
  batch 450 loss: 0.7338007813692093
  batch 500 loss: 0.6941384446620941
  batch 550 loss: 0.7351223850250244
  batch 600 loss: 0.7134052264690399
  batch 650 loss: 0.771404265165329
  batch 700 loss: 0.7313902527093887
  batch 750 loss: 0.7424554121494293
  batch 800 loss: 0.7197960591316224
  batch 850 loss: 0.7407900488376618
  batch 900 loss: 0.745644553899765
LOSS train 0.74564 valid 0.86775, valid PER 27.50%
EPOCH 11:
  batch 50 loss: 0.6935374528169632
  batch 100 loss: 0.6705282181501389
  batch 150 loss: 0.6755030107498169
  batch 200 loss: 0.6629435396194459
  batch 250 loss: 0.6782718139886856
  batch 300 loss: 0.6584657007455825
  batch 350 loss: 0.7173333805799484
  batch 400 loss: 0.6915610694885254
  batch 450 loss: 0.7178552341461182
  batch 500 loss: 0.7207173418998718
  batch 550 loss: 0.7546451663970948
  batch 600 loss: 0.7194537341594696
  batch 650 loss: 0.7109795796871186
  batch 700 loss: 0.7775807827711105
  batch 750 loss: 0.6860099637508392
  batch 800 loss: 0.7353717064857483
  batch 850 loss: 0.7155242925882339
  batch 900 loss: 0.7598976159095764
LOSS train 0.75990 valid 0.86281, valid PER 26.38%
EPOCH 12:
  batch 50 loss: 0.65034936606884
  batch 100 loss: 0.627455604672432
  batch 150 loss: 0.6646922242641449
  batch 200 loss: 0.6719246983528138
  batch 250 loss: 0.6776907032728196
  batch 300 loss: 0.7126792412996292
  batch 350 loss: 0.6699714940786362
  batch 400 loss: 0.7039890611171722
  batch 450 loss: 0.6794702613353729
  batch 500 loss: 0.7013635569810868
  batch 550 loss: 0.6912145489454269
  batch 600 loss: 0.6904184824228287
  batch 650 loss: 0.6833183377981186
  batch 700 loss: 0.6776606208086013
  batch 750 loss: 0.6795925086736679
  batch 800 loss: 0.6483680033683776
  batch 850 loss: 0.6721095836162567
  batch 900 loss: 0.7087719982862473
LOSS train 0.70877 valid 0.84163, valid PER 25.68%
EPOCH 13:
  batch 50 loss: 0.616065326333046
  batch 100 loss: 0.6113906306028366
  batch 150 loss: 0.6485197573900223
  batch 200 loss: 0.624230581521988
  batch 250 loss: 0.6415154004096985
  batch 300 loss: 0.6900286090373993
  batch 350 loss: 0.6167510324716567
  batch 400 loss: 0.6292641520500183
  batch 450 loss: 0.6704221206903458
  batch 500 loss: 0.6441278928518295
  batch 550 loss: 0.6892335665225983
  batch 600 loss: 0.6745692038536072
  batch 650 loss: 0.6509079480171204
  batch 700 loss: 0.6652262204885483
  batch 750 loss: 0.6124166136980057
  batch 800 loss: 0.6552218490839005
  batch 850 loss: 0.6388777416944503
  batch 900 loss: 0.6731833696365357
LOSS train 0.67318 valid 0.81916, valid PER 24.60%
EPOCH 14:
  batch 50 loss: 0.610034066438675
  batch 100 loss: 0.5893505579233169
  batch 150 loss: 0.6222125750780105
  batch 200 loss: 0.6361764138936996
  batch 250 loss: 0.6139188867807388
  batch 300 loss: 0.6108407866954804
  batch 350 loss: 0.6220371872186661
  batch 400 loss: 0.6521892166137695
  batch 450 loss: 0.6148798418045044
  batch 500 loss: 0.6386688351631165
  batch 550 loss: 0.6517959803342819
  batch 600 loss: 0.6338234227895737
  batch 650 loss: 0.6520584630966186
  batch 700 loss: 0.6424057120084763
  batch 750 loss: 0.6534786206483841
  batch 800 loss: 0.6279524666070938
  batch 850 loss: 0.6675627493858337
  batch 900 loss: 0.6801391786336899
LOSS train 0.68014 valid 0.84480, valid PER 24.83%
EPOCH 15:
  batch 50 loss: 0.5676683217287064
  batch 100 loss: 0.5781610560417175
  batch 150 loss: 0.6047340023517609
  batch 200 loss: 0.6238418757915497
  batch 250 loss: 0.6626919323205948
  batch 300 loss: 0.6364164650440216
  batch 350 loss: 0.675215734243393
  batch 400 loss: 0.6503229290246964
  batch 450 loss: 0.6463947689533234
  batch 500 loss: 0.6329947358369827
  batch 550 loss: 0.6624631732702255
  batch 600 loss: 0.685205642580986
  batch 650 loss: 0.638901121020317
  batch 700 loss: 0.6409206008911132
  batch 750 loss: 0.6624600833654404
  batch 800 loss: 0.6362778121232986
  batch 850 loss: 0.6353497976064681
  batch 900 loss: 0.6133296340703964
LOSS train 0.61333 valid 0.83256, valid PER 25.48%
EPOCH 16:
  batch 50 loss: 0.5991935324668884
  batch 100 loss: 0.5606862461566925
  batch 150 loss: 0.5802755689620972
  batch 200 loss: 0.6041999506950378
  batch 250 loss: 0.582396754026413
  batch 300 loss: 0.5961173141002655
  batch 350 loss: 0.5862978893518448
  batch 400 loss: 0.5770627582073211
  batch 450 loss: 0.5908668828010559
  batch 500 loss: 0.5819161534309387
  batch 550 loss: 0.5760217213630676
  batch 600 loss: 0.6221143788099289
  batch 650 loss: 0.6177406579256057
  batch 700 loss: 0.5732616180181503
  batch 750 loss: 0.5903984409570694
  batch 800 loss: 0.5977803063392639
  batch 850 loss: 0.6455141961574554
  batch 900 loss: 0.6478634703159333
LOSS train 0.64786 valid 0.83306, valid PER 24.71%
EPOCH 17:
  batch 50 loss: 0.5860757321119309
  batch 100 loss: 0.5387809091806411
  batch 150 loss: 0.5918988800048828
  batch 200 loss: 0.5585915875434876
  batch 250 loss: 0.5802431291341782
  batch 300 loss: 0.5530383211374282
  batch 350 loss: 0.6092754536867142
  batch 400 loss: 0.591179386973381
  batch 450 loss: 0.5971465027332306
  batch 500 loss: 0.5941168016195297
  batch 550 loss: 0.609471794962883
  batch 600 loss: 0.6278615546226501
  batch 650 loss: 0.5728904980421067
  batch 700 loss: 0.6239040750265121
  batch 750 loss: 0.5849761992692948
  batch 800 loss: 0.649057331085205
  batch 850 loss: 0.6205342292785645
  batch 900 loss: 0.6235121166706086
LOSS train 0.62351 valid 0.82958, valid PER 24.96%
EPOCH 18:
  batch 50 loss: 0.5672115176916123
  batch 100 loss: 0.5504489469528199
  batch 150 loss: 0.5799574047327042
  batch 200 loss: 0.5666690534353256
  batch 250 loss: 0.5428896063566208
  batch 300 loss: 0.5581586229801178
  batch 350 loss: 0.5451433169841766
  batch 400 loss: 0.5484235775470734
  batch 450 loss: 0.5843725234270096
  batch 500 loss: 0.5745586603879929
  batch 550 loss: 0.5858270126581192
  batch 600 loss: 0.5938900965452194
  batch 650 loss: 0.5825098407268524
  batch 700 loss: 0.571745838522911
  batch 750 loss: 0.5934709167480469
  batch 800 loss: 0.6769023948907852
  batch 850 loss: 0.6361067962646484
  batch 900 loss: 0.6248670023679733
LOSS train 0.62487 valid 0.81721, valid PER 25.10%
EPOCH 19:
  batch 50 loss: 0.5574893969297409
  batch 100 loss: 0.5498148500919342
  batch 150 loss: 0.5259730142354965
  batch 200 loss: 0.5430002850294113
  batch 250 loss: 0.5812809205055237
  batch 300 loss: 0.5893912774324417
  batch 350 loss: 0.5552404981851577
  batch 400 loss: 0.5603954887390137
  batch 450 loss: 0.534100626707077
  batch 500 loss: 0.5496251487731934
  batch 550 loss: 0.6409240674972534
  batch 600 loss: 0.596778829395771
  batch 650 loss: 0.5991459691524506
  batch 700 loss: 0.6211814987659454
  batch 750 loss: 0.5764202982187271
  batch 800 loss: 0.5603006845712661
  batch 850 loss: 0.5844951808452606
  batch 900 loss: 0.5766172999143601
LOSS train 0.57662 valid 0.81879, valid PER 24.60%
EPOCH 20:
  batch 50 loss: 0.4923467028141022
  batch 100 loss: 0.533833379149437
  batch 150 loss: 0.5180929136276246
  batch 200 loss: 0.5141731840372086
  batch 250 loss: 0.5622988063097
  batch 300 loss: 0.5608614218235016
  batch 350 loss: 0.567749896645546
  batch 400 loss: 0.5852049815654755
  batch 450 loss: 0.5639868468046189
  batch 500 loss: 0.5663953220844269
  batch 550 loss: 0.5646244692802429
  batch 600 loss: 0.5448838555812836
  batch 650 loss: 0.542230287194252
  batch 700 loss: 0.5333821505308152
  batch 750 loss: 0.528608781695366
  batch 800 loss: 0.5523937499523163
  batch 850 loss: 0.5596547967195511
  batch 900 loss: 0.5750030040740967
LOSS train 0.57500 valid 0.80592, valid PER 24.14%
Training finished in 5.0 minutes.
Model saved to checkpoints/20231205_073649/model_20
Loading model from checkpoints/20231205_073649/model_20
SUB: 15.42%, DEL: 8.09%, INS: 2.24%, COR: 76.49%, PER: 25.75%
