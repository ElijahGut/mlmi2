Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=2.0)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.39616174697876
  batch 100 loss: 3.0692514896392824
  batch 150 loss: 3.072232403755188
  batch 200 loss: 2.7578572607040406
  batch 250 loss: 2.589879240989685
  batch 300 loss: 2.411827173233032
  batch 350 loss: 2.3404771089553833
  batch 400 loss: 2.298295590877533
  batch 450 loss: 2.214338777065277
  batch 500 loss: 2.1183658242225647
  batch 550 loss: 2.082618522644043
  batch 600 loss: 2.0310178065299986
  batch 650 loss: 1.9552507185935974
  batch 700 loss: 1.9422678017616273
  batch 750 loss: 1.900794177055359
  batch 800 loss: 1.8756250262260437
  batch 850 loss: 1.83270681142807
  batch 900 loss: 1.8310470271110535
LOSS train 1.83105 valid 1.78322, valid PER 70.01%
EPOCH 2:
  batch 50 loss: 1.797824466228485
  batch 100 loss: 1.7247062134742737
  batch 150 loss: 1.7249008011817932
  batch 200 loss: 1.7200020265579223
  batch 250 loss: 1.7345733261108398
  batch 300 loss: 1.6747285866737365
  batch 350 loss: 1.601583161354065
  batch 400 loss: 1.626854259967804
  batch 450 loss: 1.5886668944358826
  batch 500 loss: 1.6118629717826842
  batch 550 loss: 1.6107535910606385
  batch 600 loss: 1.5763512969017028
  batch 650 loss: 1.6025196528434753
  batch 700 loss: 1.5883864235877991
  batch 750 loss: 1.551961681842804
  batch 800 loss: 1.4986262369155883
  batch 850 loss: 1.5120293474197388
  batch 900 loss: 1.5367462754249572
LOSS train 1.53675 valid 1.48514, valid PER 54.72%
EPOCH 3:
  batch 50 loss: 1.4903305220603942
  batch 100 loss: 1.4538473653793336
  batch 150 loss: 1.4600912737846374
  batch 200 loss: 1.4367705011367797
  batch 250 loss: 1.43692715883255
  batch 300 loss: 1.4419476723670959
  batch 350 loss: 1.4678919315338135
  batch 400 loss: 1.453057816028595
  batch 450 loss: 1.4285762977600098
  batch 500 loss: 1.4003374147415162
  batch 550 loss: 1.4290187644958496
  batch 600 loss: 1.3976275277137757
  batch 650 loss: 1.3711834836006165
  batch 700 loss: 1.3763019013404847
  batch 750 loss: 1.433978307247162
  batch 800 loss: 1.37194180727005
  batch 850 loss: 1.4022454977035523
  batch 900 loss: 1.3449671745300293
LOSS train 1.34497 valid 1.30476, valid PER 45.87%
EPOCH 4:
  batch 50 loss: 1.3328924870491028
  batch 100 loss: 1.365802183151245
  batch 150 loss: 1.3077639174461364
  batch 200 loss: 1.3337726736068725
  batch 250 loss: 1.3466419029235839
  batch 300 loss: 1.3555780911445618
  batch 350 loss: 1.2658009386062623
  batch 400 loss: 1.3275453209877015
  batch 450 loss: 1.3051980447769165
  batch 500 loss: 1.2857799816131592
  batch 550 loss: 1.3079248237609864
  batch 600 loss: 1.3376050305366516
  batch 650 loss: 1.308389413356781
  batch 700 loss: 1.2959870433807372
  batch 750 loss: 1.280096354484558
  batch 800 loss: 1.2167694532871247
  batch 850 loss: 1.282040638923645
  batch 900 loss: 1.322924336194992
LOSS train 1.32292 valid 1.23260, valid PER 42.17%
EPOCH 5:
  batch 50 loss: 1.2345072102546693
  batch 100 loss: 1.2455324232578278
  batch 150 loss: 1.2814169788360597
  batch 200 loss: 1.2127531921863557
  batch 250 loss: 1.2098865163326264
  batch 300 loss: 1.2386906886100768
  batch 350 loss: 1.2435386109352111
  batch 400 loss: 1.233417373895645
  batch 450 loss: 1.2264792394638062
  batch 500 loss: 1.2382602310180664
  batch 550 loss: 1.1974175727367402
  batch 600 loss: 1.2625020956993103
  batch 650 loss: 1.2048035085201263
  batch 700 loss: 1.2532999849319457
  batch 750 loss: 1.1724274373054504
  batch 800 loss: 1.226647708415985
  batch 850 loss: 1.2221203207969666
  batch 900 loss: 1.226895080804825
LOSS train 1.22690 valid 1.15450, valid PER 38.07%
EPOCH 6:
  batch 50 loss: 1.2123585772514343
  batch 100 loss: 1.1529661524295807
  batch 150 loss: 1.1541743969917297
  batch 200 loss: 1.156680271625519
  batch 250 loss: 1.1965660488605498
  batch 300 loss: 1.1641805362701416
  batch 350 loss: 1.1632773303985595
  batch 400 loss: 1.1466139018535615
  batch 450 loss: 1.179315342903137
  batch 500 loss: 1.1650483536720275
  batch 550 loss: 1.1896417129039765
  batch 600 loss: 1.163737884759903
  batch 650 loss: 1.1649506616592407
  batch 700 loss: 1.1566292607784272
  batch 750 loss: 1.1491309249401092
  batch 800 loss: 1.1452666485309602
  batch 850 loss: 1.1423282098770142
  batch 900 loss: 1.1564825451374054
LOSS train 1.15648 valid 1.12316, valid PER 37.51%
EPOCH 7:
  batch 50 loss: 1.134715702533722
  batch 100 loss: 1.1387598371505738
  batch 150 loss: 1.1199319958686829
  batch 200 loss: 1.109302898645401
  batch 250 loss: 1.1217267751693725
  batch 300 loss: 1.0915899980068207
  batch 350 loss: 1.119079225063324
  batch 400 loss: 1.1224155163764953
  batch 450 loss: 1.1148608791828156
  batch 500 loss: 1.115031542778015
  batch 550 loss: 1.0962356531620026
  batch 600 loss: 1.1099720990657806
  batch 650 loss: 1.1008620285987853
  batch 700 loss: 1.1219050180912018
  batch 750 loss: 1.0956483399868011
  batch 800 loss: 1.082997326850891
  batch 850 loss: 1.131443748474121
  batch 900 loss: 1.1451347541809083
LOSS train 1.14513 valid 1.08908, valid PER 35.98%
EPOCH 8:
  batch 50 loss: 1.088240202665329
  batch 100 loss: 1.0876369857788086
  batch 150 loss: 1.0746768510341644
  batch 200 loss: 1.049783467054367
  batch 250 loss: 1.0901131463050842
  batch 300 loss: 1.0048720383644103
  batch 350 loss: 1.1148085403442383
  batch 400 loss: 1.055251832008362
  batch 450 loss: 1.0895548951625824
  batch 500 loss: 1.1237459218502044
  batch 550 loss: 1.0413083267211913
  batch 600 loss: 1.0947143256664276
  batch 650 loss: 1.1113280713558198
  batch 700 loss: 1.0501900947093963
  batch 750 loss: 1.0523915827274322
  batch 800 loss: 1.073119978904724
  batch 850 loss: 1.0826650559902191
  batch 900 loss: 1.0764090192317963
LOSS train 1.07641 valid 1.04974, valid PER 33.74%
EPOCH 9:
  batch 50 loss: 0.9994503498077393
  batch 100 loss: 1.0434447586536408
  batch 150 loss: 1.0367590880393982
  batch 200 loss: 1.0121941900253295
  batch 250 loss: 1.0616057372093202
  batch 300 loss: 1.05013334274292
  batch 350 loss: 1.0774345254898072
  batch 400 loss: 1.0494982039928435
  batch 450 loss: 1.0400883805751802
  batch 500 loss: 1.0148260796070099
  batch 550 loss: 1.0437644922733307
  batch 600 loss: 1.0599927961826325
  batch 650 loss: 1.042251524925232
  batch 700 loss: 1.0062129473686219
  batch 750 loss: 1.0450249755382537
  batch 800 loss: 1.04425882935524
  batch 850 loss: 1.0654571485519408
  batch 900 loss: 1.0258718347549438
LOSS train 1.02587 valid 1.03023, valid PER 33.10%
EPOCH 10:
  batch 50 loss: 0.9736087119579315
  batch 100 loss: 0.9774996984004974
  batch 150 loss: 1.0289982092380523
  batch 200 loss: 1.01762268781662
  batch 250 loss: 1.0157937610149383
  batch 300 loss: 0.9958087134361268
  batch 350 loss: 1.0264354717731476
  batch 400 loss: 0.9784356021881103
  batch 450 loss: 0.9678469669818878
  batch 500 loss: 1.0203960740566254
  batch 550 loss: 1.038315349817276
  batch 600 loss: 0.9957187354564667
  batch 650 loss: 0.9869070637226105
  batch 700 loss: 1.0237556552886964
  batch 750 loss: 0.9992240655422211
  batch 800 loss: 1.0138092672824859
  batch 850 loss: 1.0351738405227662
  batch 900 loss: 1.0142307198047638
LOSS train 1.01423 valid 1.06688, valid PER 35.29%
EPOCH 11:
  batch 50 loss: 0.9570706415176392
  batch 100 loss: 0.9406054580211639
  batch 150 loss: 0.9651453566551208
  batch 200 loss: 1.0100050508975982
  batch 250 loss: 0.9932700359821319
  batch 300 loss: 0.9559436881542206
  batch 350 loss: 0.9682339489459991
  batch 400 loss: 0.9878147888183594
  batch 450 loss: 0.9930413663387299
  batch 500 loss: 0.9686761701107025
  batch 550 loss: 0.9707905614376068
  batch 600 loss: 0.9608155405521392
  batch 650 loss: 1.0323564732074737
  batch 700 loss: 0.9517206251621246
  batch 750 loss: 0.9698661851882935
  batch 800 loss: 1.0113678646087647
  batch 850 loss: 1.03056485414505
  batch 900 loss: 1.0118505036830903
LOSS train 1.01185 valid 1.00638, valid PER 32.26%
EPOCH 12:
  batch 50 loss: 0.9615663373470307
  batch 100 loss: 0.9514090204238892
  batch 150 loss: 0.9344014310836792
  batch 200 loss: 0.920003525018692
  batch 250 loss: 0.9599232649803162
  batch 300 loss: 0.9550770020484924
  batch 350 loss: 0.9483155846595764
  batch 400 loss: 0.980034773349762
  batch 450 loss: 0.9796715974807739
  batch 500 loss: 0.9855603241920471
  batch 550 loss: 0.9310729885101319
  batch 600 loss: 0.9310291028022766
  batch 650 loss: 0.9806938481330871
  batch 700 loss: 0.9632940506935119
  batch 750 loss: 0.9535025060176849
  batch 800 loss: 0.9473349249362946
  batch 850 loss: 0.9859928810596466
  batch 900 loss: 0.989569878578186
LOSS train 0.98957 valid 0.98221, valid PER 31.56%
EPOCH 13:
  batch 50 loss: 0.8980565798282624
  batch 100 loss: 0.9302881169319153
  batch 150 loss: 0.8996741998195649
  batch 200 loss: 0.9281974828243256
  batch 250 loss: 0.911829776763916
  batch 300 loss: 0.9066978085041046
  batch 350 loss: 0.9283199942111969
  batch 400 loss: 0.9458485805988311
  batch 450 loss: 0.9475040721893311
  batch 500 loss: 0.907192200422287
  batch 550 loss: 0.9575600636005401
  batch 600 loss: 0.9247036111354828
  batch 650 loss: 0.9705435466766358
  batch 700 loss: 0.9509696328639984
  batch 750 loss: 0.9081078743934632
  batch 800 loss: 0.9075660967826843
  batch 850 loss: 0.9644042944908142
  batch 900 loss: 0.9553172397613525
LOSS train 0.95532 valid 1.00608, valid PER 31.62%
EPOCH 14:
  batch 50 loss: 0.9052971601486206
  batch 100 loss: 0.9107829463481903
  batch 150 loss: 0.8864709198474884
  batch 200 loss: 0.9097968137264252
  batch 250 loss: 0.9082648348808289
  batch 300 loss: 0.940182341337204
  batch 350 loss: 0.8912663078308105
  batch 400 loss: 0.9066680717468262
  batch 450 loss: 0.9085769534111023
  batch 500 loss: 0.9245696449279786
  batch 550 loss: 0.9314867866039276
  batch 600 loss: 0.8867035698890686
  batch 650 loss: 0.9386653316020965
  batch 700 loss: 0.9648657274246216
  batch 750 loss: 0.9166378939151764
  batch 800 loss: 0.879849990606308
  batch 850 loss: 0.9215068352222443
  batch 900 loss: 0.9154551935195923
LOSS train 0.91546 valid 0.99734, valid PER 31.56%
EPOCH 15:
  batch 50 loss: 0.9046001207828521
  batch 100 loss: 0.8724617087841033
  batch 150 loss: 0.8750557172298431
  batch 200 loss: 0.9334545481204987
  batch 250 loss: 0.9100805366039276
  batch 300 loss: 0.8890787160396576
  batch 350 loss: 0.8985631263256073
  batch 400 loss: 0.8875259113311768
  batch 450 loss: 0.8906681096553802
  batch 500 loss: 0.8607056486606598
  batch 550 loss: 0.882762746810913
  batch 600 loss: 0.9334798216819763
  batch 650 loss: 0.9254048144817353
  batch 700 loss: 0.9067280077934265
  batch 750 loss: 0.9096701908111572
  batch 800 loss: 0.8898432576656341
  batch 850 loss: 0.8723091900348663
  batch 900 loss: 0.9098453724384308
LOSS train 0.90985 valid 0.99165, valid PER 31.14%
EPOCH 16:
  batch 50 loss: 0.914444317817688
  batch 100 loss: 0.8399189388751984
  batch 150 loss: 0.8561139297485352
  batch 200 loss: 0.8614623320102691
  batch 250 loss: 0.8813263213634491
  batch 300 loss: 0.8671993279457092
  batch 350 loss: 0.9078670120239258
  batch 400 loss: 0.9216424798965455
  batch 450 loss: 0.8951249265670777
  batch 500 loss: 0.8697825217247009
  batch 550 loss: 0.8847282111644745
  batch 600 loss: 0.8676795554161072
  batch 650 loss: 0.9008337473869323
  batch 700 loss: 0.8657198178768158
  batch 750 loss: 0.8806832659244538
  batch 800 loss: 0.8821799337863923
  batch 850 loss: 0.8750803816318512
  batch 900 loss: 0.8691567945480346
LOSS train 0.86916 valid 0.95902, valid PER 29.38%
EPOCH 17:
  batch 50 loss: 0.8618446451425552
  batch 100 loss: 0.8477286565303802
  batch 150 loss: 0.8562690699100495
  batch 200 loss: 0.850370432138443
  batch 250 loss: 0.882083922624588
  batch 300 loss: 0.8581074440479278
  batch 350 loss: 0.8267628514766693
  batch 400 loss: 0.9212120413780213
  batch 450 loss: 0.8769504415988922
  batch 500 loss: 0.8445482683181763
  batch 550 loss: 0.8656784284114838
  batch 600 loss: 0.9235613870620728
  batch 650 loss: 0.8673854780197143
  batch 700 loss: 0.8593417131900787
  batch 750 loss: 0.821079193353653
  batch 800 loss: 0.8711326193809509
  batch 850 loss: 0.8861933743953705
  batch 900 loss: 0.847773722410202
LOSS train 0.84777 valid 0.96238, valid PER 30.35%
EPOCH 18:
  batch 50 loss: 0.8421327888965606
  batch 100 loss: 0.8364670109748841
  batch 150 loss: 0.8646106874942779
  batch 200 loss: 0.8669422382116317
  batch 250 loss: 0.8440626955032349
  batch 300 loss: 0.845366467833519
  batch 350 loss: 0.8654181122779846
  batch 400 loss: 0.8256058955192566
  batch 450 loss: 0.9020950901508331
  batch 500 loss: 0.8667731523513794
  batch 550 loss: 0.8792578148841858
  batch 600 loss: 0.8185376858711243
  batch 650 loss: 0.8375137984752655
  batch 700 loss: 0.878794778585434
  batch 750 loss: 0.849748387336731
  batch 800 loss: 0.8590352296829223
  batch 850 loss: 0.8408558881282806
  batch 900 loss: 0.8701151549816132
LOSS train 0.87012 valid 0.98951, valid PER 31.20%
EPOCH 19:
  batch 50 loss: 0.7806796097755432
  batch 100 loss: 0.7887698662281036
  batch 150 loss: 0.8412621986865997
  batch 200 loss: 0.8232837879657745
  batch 250 loss: 0.8558144229650497
  batch 300 loss: 0.8414601147174835
  batch 350 loss: 0.8191965901851654
  batch 400 loss: 0.8567088866233825
  batch 450 loss: 0.8582096886634827
  batch 500 loss: 0.8385669434070587
  batch 550 loss: 0.8267244696617126
  batch 600 loss: 0.8419944608211517
  batch 650 loss: 0.9077223992347717
  batch 700 loss: 0.812359231710434
  batch 750 loss: 0.8255701935291291
  batch 800 loss: 0.8599780213832855
  batch 850 loss: 0.8556889462471008
  batch 900 loss: 0.8643161058425903
LOSS train 0.86432 valid 0.96102, valid PER 30.00%
EPOCH 20:
  batch 50 loss: 0.8155484414100647
  batch 100 loss: 0.7985481363534928
  batch 150 loss: 0.7989837610721588
  batch 200 loss: 0.8153965878486633
  batch 250 loss: 0.8324220156669617
  batch 300 loss: 0.8167657148838043
  batch 350 loss: 0.8099973332881928
  batch 400 loss: 0.8150202190876007
  batch 450 loss: 0.8311309170722961
  batch 500 loss: 0.7973521661758423
  batch 550 loss: 0.8715273487567902
  batch 600 loss: 0.810479416847229
  batch 650 loss: 0.8473392856121064
  batch 700 loss: 0.8366842699050904
  batch 750 loss: 0.8040023225545884
  batch 800 loss: 0.86621391415596
  batch 850 loss: 0.8533803582191467
  batch 900 loss: 0.8416039597988129
LOSS train 0.84160 valid 0.94685, valid PER 28.94%
Training finished in 4.0 minutes.
Model saved to checkpoints/20231206_213402/model_20
Loading model from checkpoints/20231206_213402/model_20
SUB: 14.60%, DEL: 14.25%, INS: 2.24%, COR: 71.15%, PER: 31.09%
