Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=2, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=0.5, is_bidir=True)
Total number of model parameters is 562216
EPOCH 1, Learning Rate: 0.5
  batch 50 loss: 5.930881180763245
  batch 100 loss: 3.3477425384521484
  batch 150 loss: 3.331959352493286
  batch 200 loss: 3.292570776939392
  batch 250 loss: 3.2226848363876344
  batch 300 loss: 3.1261202669143677
  batch 350 loss: 3.0085468912124633
  batch 400 loss: 2.8706309461593627
  batch 450 loss: 2.733879246711731
  batch 500 loss: 2.5782597827911378
  batch 550 loss: 2.4784210777282714
  batch 600 loss: 2.409700984954834
  batch 650 loss: 2.3065586519241332
  batch 700 loss: 2.2691298723220825
  batch 750 loss: 2.214483244419098
  batch 800 loss: 2.1654173827171324
  batch 850 loss: 2.1327513027191163
  batch 900 loss: 2.099637017250061
LOSS train 2.09964 valid 2.01646, valid PER 75.04%
EPOCH 2, Learning Rate: 0.5
  batch 50 loss: 2.052378408908844
  batch 100 loss: 1.9947480177879333
  batch 150 loss: 1.914774112701416
  batch 200 loss: 1.9054667115211488
  batch 250 loss: 1.8975689816474914
  batch 300 loss: 1.8503815150260925
  batch 350 loss: 1.8208363628387452
  batch 400 loss: 1.7962596440315246
  batch 450 loss: 1.7948430824279784
  batch 500 loss: 1.7597224020957947
  batch 550 loss: 1.745748484134674
  batch 600 loss: 1.721272587776184
  batch 650 loss: 1.6806109213829041
  batch 700 loss: 1.6921856474876404
  batch 750 loss: 1.6642474102973939
  batch 800 loss: 1.6278330540657044
  batch 850 loss: 1.6242528820037843
  batch 900 loss: 1.609717149734497
LOSS train 1.60972 valid 1.53959, valid PER 57.51%
EPOCH 3, Learning Rate: 0.5
  batch 50 loss: 1.5588430547714234
  batch 100 loss: 1.60717148065567
  batch 150 loss: 1.6018811345100403
  batch 200 loss: 1.5341318869590759
  batch 250 loss: 1.537984516620636
  batch 300 loss: 1.5282914805412293
  batch 350 loss: 1.541002402305603
  batch 400 loss: 1.5105380272865296
  batch 450 loss: 1.49087087392807
  batch 500 loss: 1.471525821685791
  batch 550 loss: 1.465021071434021
  batch 600 loss: 1.410300076007843
  batch 650 loss: 1.4389161992073058
  batch 700 loss: 1.42853013753891
  batch 750 loss: 1.4417579174041748
  batch 800 loss: 1.4471730875968933
  batch 850 loss: 1.4082736492156982
  batch 900 loss: 1.4057135128974914
LOSS train 1.40571 valid 1.31333, valid PER 43.58%
EPOCH 4, Learning Rate: 0.5
  batch 50 loss: 1.4215572881698608
  batch 100 loss: 1.3418538904190063
  batch 150 loss: 1.365886628627777
  batch 200 loss: 1.3493578243255615
  batch 250 loss: 1.355367612838745
  batch 300 loss: 1.3585825181007385
  batch 350 loss: 1.3271806836128235
  batch 400 loss: 1.3000216174125672
  batch 450 loss: 1.2908800077438354
  batch 500 loss: 1.3520975947380065
  batch 550 loss: 1.2785037326812745
  batch 600 loss: 1.2806581568717956
  batch 650 loss: 1.3217754697799682
  batch 700 loss: 1.332985270023346
  batch 750 loss: 1.2695322930812836
  batch 800 loss: 1.2746302711963653
  batch 850 loss: 1.2472864258289338
  batch 900 loss: 1.2496307921409606
LOSS train 1.24963 valid 1.20247, valid PER 37.55%
EPOCH 5, Learning Rate: 0.5
  batch 50 loss: 1.2545601081848146
  batch 100 loss: 1.2092180860042572
  batch 150 loss: 1.2534113013744355
  batch 200 loss: 1.2669298338890076
  batch 250 loss: 1.2050338387489319
  batch 300 loss: 1.2206424677371979
  batch 350 loss: 1.1966218376159667
  batch 400 loss: 1.17408069729805
  batch 450 loss: 1.1651774191856383
  batch 500 loss: 1.1572449803352356
  batch 550 loss: 1.206335688829422
  batch 600 loss: 1.193043328523636
  batch 650 loss: 1.192456247806549
  batch 700 loss: 1.1703090476989746
  batch 750 loss: 1.1788854205608368
  batch 800 loss: 1.1984084856510162
  batch 850 loss: 1.197840085029602
  batch 900 loss: 1.1569985127449036
LOSS train 1.15700 valid 1.11230, valid PER 34.69%
EPOCH 6, Learning Rate: 0.5
  batch 50 loss: 1.1361941301822662
  batch 100 loss: 1.1496466183662415
  batch 150 loss: 1.1424869191646576
  batch 200 loss: 1.1074947535991668
  batch 250 loss: 1.1262552058696746
  batch 300 loss: 1.1541288340091704
  batch 350 loss: 1.1353312849998474
  batch 400 loss: 1.1279225409030915
  batch 450 loss: 1.1474851775169372
  batch 500 loss: 1.0653840696811676
  batch 550 loss: 1.1391989231109618
  batch 600 loss: 1.1055707371234893
  batch 650 loss: 1.0913974130153656
  batch 700 loss: 1.0823627531528472
  batch 750 loss: 1.1262642562389373
  batch 800 loss: 1.1060624635219574
  batch 850 loss: 1.133916563987732
  batch 900 loss: 1.1468234860897064
LOSS train 1.14682 valid 1.03567, valid PER 32.88%
EPOCH 7, Learning Rate: 0.5
  batch 50 loss: 1.0810409307479858
  batch 100 loss: 1.1253144216537476
  batch 150 loss: 1.0624604105949402
  batch 200 loss: 1.0676394641399383
  batch 250 loss: 1.1120728325843812
  batch 300 loss: 1.0821757352352142
  batch 350 loss: 1.11386864900589
  batch 400 loss: 1.0568361175060272
  batch 450 loss: 1.0570891427993774
  batch 500 loss: 1.0658362793922425
  batch 550 loss: 1.0364277017116548
  batch 600 loss: 1.0376955592632293
  batch 650 loss: 1.0244050025939941
  batch 700 loss: 1.0877590560913086
  batch 750 loss: 1.0556737446784974
  batch 800 loss: 1.0465646767616272
  batch 850 loss: 1.0274861121177674
  batch 900 loss: 1.041970921754837
LOSS train 1.04197 valid 1.03443, valid PER 32.29%
EPOCH 8, Learning Rate: 0.5
  batch 50 loss: 1.0393379604816437
  batch 100 loss: 1.0198607110977174
  batch 150 loss: 1.0484784376621246
  batch 200 loss: 1.0102433907985686
  batch 250 loss: 1.014066377878189
  batch 300 loss: 0.9663069868087768
  batch 350 loss: 1.0089964520931245
  batch 400 loss: 0.9998472857475281
  batch 450 loss: 1.0560187113285064
  batch 500 loss: 1.0274210810661315
  batch 550 loss: 1.0091510581970216
  batch 600 loss: 0.9813799011707306
  batch 650 loss: 1.0026290905475617
  batch 700 loss: 1.030964993238449
  batch 750 loss: 1.0243145251274108
  batch 800 loss: 1.0195657193660737
  batch 850 loss: 1.0012205004692079
  batch 900 loss: 0.9926968884468078
LOSS train 0.99270 valid 0.95555, valid PER 29.56%
EPOCH 9, Learning Rate: 0.5
  batch 50 loss: 0.9800159668922425
  batch 100 loss: 0.9622221839427948
  batch 150 loss: 0.9775695621967315
  batch 200 loss: 0.9549372935295105
  batch 250 loss: 0.9388068497180939
  batch 300 loss: 0.9898070812225341
  batch 350 loss: 0.9486038327217102
  batch 400 loss: 0.9886613655090332
  batch 450 loss: 0.9891223192214966
  batch 500 loss: 0.9605791604518891
  batch 550 loss: 0.9947046136856079
  batch 600 loss: 0.9893870317935943
  batch 650 loss: 0.9967310631275177
  batch 700 loss: 0.966670835018158
  batch 750 loss: 0.9591174125671387
  batch 800 loss: 1.0020572125911713
  batch 850 loss: 0.9888717234134674
  batch 900 loss: 0.9514073634147644
LOSS train 0.95141 valid 0.92628, valid PER 28.98%
EPOCH 10, Learning Rate: 0.5
  batch 50 loss: 0.9367637348175049
  batch 100 loss: 0.9647771275043487
  batch 150 loss: 0.9631665897369385
  batch 200 loss: 0.9258062815666199
  batch 250 loss: 0.9286385118961334
  batch 300 loss: 0.9493189609050751
  batch 350 loss: 0.9215064108371734
  batch 400 loss: 0.9176860463619232
  batch 450 loss: 0.9182638430595398
  batch 500 loss: 0.9305063545703888
  batch 550 loss: 0.932632018327713
  batch 600 loss: 0.9449291634559631
  batch 650 loss: 0.9510840201377868
  batch 700 loss: 0.9716780686378479
  batch 750 loss: 0.9488731145858764
  batch 800 loss: 0.9327427172660827
  batch 850 loss: 0.9327026736736298
  batch 900 loss: 0.925601441860199
LOSS train 0.92560 valid 0.92408, valid PER 28.64%
EPOCH 11, Learning Rate: 0.5
  batch 50 loss: 0.9273250567913055
  batch 100 loss: 0.8984091138839722
  batch 150 loss: 0.9065316522121429
  batch 200 loss: 0.8591247570514678
  batch 250 loss: 0.8867362022399903
  batch 300 loss: 0.9020854115486145
  batch 350 loss: 0.9410257411003112
  batch 400 loss: 0.8987257111072541
  batch 450 loss: 0.9129366493225097
  batch 500 loss: 0.8994959282875061
  batch 550 loss: 0.9248425996303559
  batch 600 loss: 0.898166606426239
  batch 650 loss: 0.9240721333026886
  batch 700 loss: 0.9815086376667023
  batch 750 loss: 0.9050078618526459
  batch 800 loss: 0.9163827586174011
  batch 850 loss: 0.9017406237125397
  batch 900 loss: 0.9170089793205262
LOSS train 0.91701 valid 0.88730, valid PER 27.40%
EPOCH 12, Learning Rate: 0.5
  batch 50 loss: 0.8439773571491241
  batch 100 loss: 0.841234666109085
  batch 150 loss: 0.8617879831790924
  batch 200 loss: 0.8938186335563659
  batch 250 loss: 0.859725044965744
  batch 300 loss: 0.9127453339099884
  batch 350 loss: 0.871217588186264
  batch 400 loss: 0.9008823776245117
  batch 450 loss: 0.8807139801979065
  batch 500 loss: 0.9100472748279571
  batch 550 loss: 0.8946442353725433
  batch 600 loss: 0.9106448137760162
  batch 650 loss: 0.9026335549354553
  batch 700 loss: 0.8703691577911377
  batch 750 loss: 0.9041865479946136
  batch 800 loss: 0.8432387375831604
  batch 850 loss: 0.8738100790977478
  batch 900 loss: 0.8939127588272094
LOSS train 0.89391 valid 0.87540, valid PER 27.02%
EPOCH 13, Learning Rate: 0.5
  batch 50 loss: 0.8337337934970855
  batch 100 loss: 0.850961627960205
  batch 150 loss: 0.880116593837738
  batch 200 loss: 0.828577082157135
  batch 250 loss: 0.8385738956928254
  batch 300 loss: 0.8979846143722534
  batch 350 loss: 0.8336278879642487
  batch 400 loss: 0.8308868706226349
  batch 450 loss: 0.8615549612045288
  batch 500 loss: 0.8572245049476623
  batch 550 loss: 0.913180513381958
  batch 600 loss: 0.8838376939296723
  batch 650 loss: 0.8466811764240265
  batch 700 loss: 0.8743124294281006
  batch 750 loss: 0.8139490389823913
  batch 800 loss: 0.8589215159416199
  batch 850 loss: 0.8336170399188996
  batch 900 loss: 0.8514061522483826
LOSS train 0.85141 valid 0.85818, valid PER 26.28%
EPOCH 14, Learning Rate: 0.5
  batch 50 loss: 0.8125736725330353
  batch 100 loss: 0.8083309686183929
  batch 150 loss: 0.8400346505641937
  batch 200 loss: 0.8326999270915985
  batch 250 loss: 0.8304107820987702
  batch 300 loss: 0.822105507850647
  batch 350 loss: 0.8222362387180329
  batch 400 loss: 0.8473483288288116
  batch 450 loss: 0.815820404291153
  batch 500 loss: 0.825009331703186
  batch 550 loss: 0.8293038082122802
  batch 600 loss: 0.8198328626155853
  batch 650 loss: 0.8469922482967377
  batch 700 loss: 0.862321902513504
  batch 750 loss: 0.8141270244121551
  batch 800 loss: 0.8122880363464355
  batch 850 loss: 0.8823396360874176
  batch 900 loss: 0.8339839231967926
LOSS train 0.83398 valid 0.84651, valid PER 26.08%
EPOCH 15, Learning Rate: 0.5
  batch 50 loss: 0.7833599388599396
  batch 100 loss: 0.8048613476753235
  batch 150 loss: 0.8223842120170594
  batch 200 loss: 0.8247867107391358
  batch 250 loss: 0.8308502697944641
  batch 300 loss: 0.8314739763736725
  batch 350 loss: 0.8267611348628998
  batch 400 loss: 0.805837230682373
  batch 450 loss: 0.7984538555145264
  batch 500 loss: 0.7880826210975647
  batch 550 loss: 0.8576602447032928
  batch 600 loss: 0.8455383741855621
  batch 650 loss: 0.7998306655883789
  batch 700 loss: 0.7972029936313629
  batch 750 loss: 0.8337658739089966
  batch 800 loss: 0.8097461676597595
  batch 850 loss: 0.7966312754154206
  batch 900 loss: 0.7632332134246826
LOSS train 0.76323 valid 0.84750, valid PER 26.70%
EPOCH 16, Learning Rate: 0.5
  batch 50 loss: 0.7891012907028199
  batch 100 loss: 0.7743258917331696
  batch 150 loss: 0.7846061551570892
  batch 200 loss: 0.8176367127895355
  batch 250 loss: 0.7983095955848694
  batch 300 loss: 0.8063162386417388
  batch 350 loss: 0.8021207070350647
  batch 400 loss: 0.798323335647583
  batch 450 loss: 0.8168638825416565
  batch 500 loss: 0.80047248005867
  batch 550 loss: 0.7811155354976654
  batch 600 loss: 0.8249445939064026
  batch 650 loss: 0.8159399652481079
  batch 700 loss: 0.7910327541828156
  batch 750 loss: 0.8192197251319885
  batch 800 loss: 0.7797355854511261
  batch 850 loss: 0.7731336116790771
  batch 900 loss: 0.7870595180988311
LOSS train 0.78706 valid 0.81731, valid PER 25.13%
EPOCH 17, Learning Rate: 0.5
  batch 50 loss: 0.7858753025531768
  batch 100 loss: 0.734644073843956
  batch 150 loss: 0.8021188235282898
  batch 200 loss: 0.7558209943771362
  batch 250 loss: 0.7748544293642045
  batch 300 loss: 0.7489709550142288
  batch 350 loss: 0.7961832964420319
  batch 400 loss: 0.7789961242675781
  batch 450 loss: 0.7675631177425385
  batch 500 loss: 0.7963664948940277
  batch 550 loss: 0.7727198326587676
  batch 600 loss: 0.7804007601737976
  batch 650 loss: 0.7584605407714844
  batch 700 loss: 0.7796624445915222
  batch 750 loss: 0.7537486922740936
  batch 800 loss: 0.8078385472297669
  batch 850 loss: 0.7981198143959045
  batch 900 loss: 0.7867052119970321
LOSS train 0.78671 valid 0.82986, valid PER 25.52%
EPOCH 18, Learning Rate: 0.5
  batch 50 loss: 0.7468985533714294
  batch 100 loss: 0.7370431065559387
  batch 150 loss: 0.7624675035476685
  batch 200 loss: 0.7603932416439056
  batch 250 loss: 0.7415921711921691
  batch 300 loss: 0.734053925871849
  batch 350 loss: 0.7167710256576538
  batch 400 loss: 0.7559597361087799
  batch 450 loss: 0.7756184977293015
  batch 500 loss: 0.7642655348777772
  batch 550 loss: 0.7953195643424987
  batch 600 loss: 0.7585901099443436
  batch 650 loss: 0.733157353401184
  batch 700 loss: 0.7296890860795975
  batch 750 loss: 0.7647526645660401
  batch 800 loss: 0.7884239917993545
  batch 850 loss: 0.7805413424968719
  batch 900 loss: 0.76902173101902
LOSS train 0.76902 valid 0.81128, valid PER 25.24%
EPOCH 19, Learning Rate: 0.5
  batch 50 loss: 0.7417540711164474
  batch 100 loss: 0.7418036955595017
  batch 150 loss: 0.7253669500350952
  batch 200 loss: 0.7208548438549042
  batch 250 loss: 0.7611595624685288
  batch 300 loss: 0.7670169419050217
  batch 350 loss: 0.758600367307663
  batch 400 loss: 0.7577278172969818
  batch 450 loss: 0.6996499407291412
  batch 500 loss: 0.7046494793891906
  batch 550 loss: 0.7519302213191986
  batch 600 loss: 0.7590687000751495
  batch 650 loss: 0.7729638469219208
  batch 700 loss: 0.7643348801136017
  batch 750 loss: 0.7407024359703064
  batch 800 loss: 0.7071251189708709
  batch 850 loss: 0.7678128337860107
  batch 900 loss: 0.7349609702825546
LOSS train 0.73496 valid 0.80667, valid PER 24.96%
EPOCH 20, Learning Rate: 0.5
  batch 50 loss: 0.6983905512094498
  batch 100 loss: 0.7110504877567291
  batch 150 loss: 0.72508475959301
  batch 200 loss: 0.7186120128631592
  batch 250 loss: 0.7557142543792724
  batch 300 loss: 0.7382583999633789
  batch 350 loss: 0.706950803399086
  batch 400 loss: 0.724191097021103
  batch 450 loss: 0.7303379112482071
  batch 500 loss: 0.7352500855922699
  batch 550 loss: 0.7347942894697189
  batch 600 loss: 0.733421733379364
  batch 650 loss: 0.7269371736049652
  batch 700 loss: 0.7268393349647522
  batch 750 loss: 0.7054869621992111
  batch 800 loss: 0.7275569689273834
  batch 850 loss: 0.7378831726312637
  batch 900 loss: 0.716208871603012
LOSS train 0.71621 valid 0.80599, valid PER 24.78%
Training finished in 5.0 minutes.
Model saved to checkpoints/20231207_193515/model_20
Loading model from checkpoints/20231207_193515/model_20
SUB: 15.67%, DEL: 8.86%, INS: 2.35%, COR: 75.48%, PER: 26.87%
