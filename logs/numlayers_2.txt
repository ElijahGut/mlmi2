Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=2, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 562216
EPOCH 1, Learning Rate: 0.5
  batch 50 loss: 4.727740173339844
  batch 100 loss: 3.314740037918091
  batch 150 loss: 3.2488884496688843
  batch 200 loss: 3.140782189369202
  batch 250 loss: 3.0116112279891967
  batch 300 loss: 2.7735728549957277
  batch 350 loss: 2.6456764459609987
  batch 400 loss: 2.533956332206726
  batch 450 loss: 2.4505402708053587
  batch 500 loss: 2.32306161403656
  batch 550 loss: 2.2641466665267944
  batch 600 loss: 2.192946639060974
  batch 650 loss: 2.100422651767731
  batch 700 loss: 2.0849052453041077
  batch 750 loss: 2.018749454021454
  batch 800 loss: 1.9707487869262694
  batch 850 loss: 1.9121218156814574
  batch 900 loss: 1.881927683353424
LOSS train 1.88193 valid 1.77021, valid PER 69.13%
EPOCH 2, Learning Rate: 0.5
  batch 50 loss: 1.8393582963943482
  batch 100 loss: 1.787407808303833
  batch 150 loss: 1.679174611568451
  batch 200 loss: 1.688476960659027
  batch 250 loss: 1.6746016597747804
  batch 300 loss: 1.6430101943016053
  batch 350 loss: 1.6256960368156432
  batch 400 loss: 1.580082356929779
  batch 450 loss: 1.5599318981170653
  batch 500 loss: 1.5559288454055786
  batch 550 loss: 1.5367767739295959
  batch 600 loss: 1.5019483375549316
  batch 650 loss: 1.4487054395675658
  batch 700 loss: 1.4804369306564331
  batch 750 loss: 1.4462564539909364
  batch 800 loss: 1.3748841571807862
  batch 850 loss: 1.4130010557174684
  batch 900 loss: 1.360765872001648
LOSS train 1.36077 valid 1.26797, valid PER 41.21%
EPOCH 3, Learning Rate: 0.5
  batch 50 loss: 1.3000723004341126
  batch 100 loss: 1.363007755279541
  batch 150 loss: 1.3740758466720582
  batch 200 loss: 1.2812253665924072
  batch 250 loss: 1.3023001503944398
  batch 300 loss: 1.301866512298584
  batch 350 loss: 1.3246551752090454
  batch 400 loss: 1.2925393295288086
  batch 450 loss: 1.2701041674613953
  batch 500 loss: 1.2461905312538146
  batch 550 loss: 1.2400179719924926
  batch 600 loss: 1.1840830099582673
  batch 650 loss: 1.2210996866226196
  batch 700 loss: 1.2310713279247283
  batch 750 loss: 1.231259090900421
  batch 800 loss: 1.2418068623542786
  batch 850 loss: 1.2120034635066985
  batch 900 loss: 1.201973466873169
LOSS train 1.20197 valid 1.10483, valid PER 34.18%
EPOCH 4, Learning Rate: 0.5
  batch 50 loss: 1.2014659178256988
  batch 100 loss: 1.1353184914588927
  batch 150 loss: 1.1551881766319274
  batch 200 loss: 1.132426096200943
  batch 250 loss: 1.1344344639778137
  batch 300 loss: 1.150779845714569
  batch 350 loss: 1.1216526556015014
  batch 400 loss: 1.0892913377285003
  batch 450 loss: 1.1208249962329864
  batch 500 loss: 1.1714815723896026
  batch 550 loss: 1.0908192932605743
  batch 600 loss: 1.0897439801692963
  batch 650 loss: 1.1659540855884551
  batch 700 loss: 1.1647441637516023
  batch 750 loss: 1.1163517534732819
  batch 800 loss: 1.1038750278949738
  batch 850 loss: 1.0719056284427644
  batch 900 loss: 1.0829850924015045
LOSS train 1.08299 valid 1.03076, valid PER 32.12%
EPOCH 5, Learning Rate: 0.5
  batch 50 loss: 1.0667661154270172
  batch 100 loss: 1.0452066564559936
  batch 150 loss: 1.055234775543213
  batch 200 loss: 1.0904234218597413
  batch 250 loss: 1.0409143435955048
  batch 300 loss: 1.0636545670032502
  batch 350 loss: 1.024250375032425
  batch 400 loss: 1.0062939643859863
  batch 450 loss: 1.0094045627117156
  batch 500 loss: 0.9981947481632233
  batch 550 loss: 1.0521851205825805
  batch 600 loss: 1.037404282093048
  batch 650 loss: 1.0396889698505403
  batch 700 loss: 1.014669908285141
  batch 750 loss: 1.0254283368587493
  batch 800 loss: 1.0673638021945953
  batch 850 loss: 1.0487462723255156
  batch 900 loss: 1.010232765674591
LOSS train 1.01023 valid 0.95107, valid PER 29.73%
EPOCH 6, Learning Rate: 0.5
  batch 50 loss: 0.9641262245178223
  batch 100 loss: 1.0016274082660674
  batch 150 loss: 0.9748474669456482
  batch 200 loss: 0.9542132925987243
  batch 250 loss: 0.981914187669754
  batch 300 loss: 1.0067096519470216
  batch 350 loss: 0.9977150976657867
  batch 400 loss: 0.9665771806240082
  batch 450 loss: 0.9970155167579651
  batch 500 loss: 0.9484925079345703
  batch 550 loss: 0.966752690076828
  batch 600 loss: 0.9561407613754273
  batch 650 loss: 0.9500546956062317
  batch 700 loss: 0.9635089826583862
  batch 750 loss: 1.0137779676914216
  batch 800 loss: 0.9755005300045013
  batch 850 loss: 1.0015404105186463
  batch 900 loss: 1.0051215636730193
LOSS train 1.00512 valid 0.92033, valid PER 29.04%
EPOCH 7, Learning Rate: 0.5
  batch 50 loss: 0.9180897915363312
  batch 100 loss: 0.9733851718902587
  batch 150 loss: 0.903678925037384
  batch 200 loss: 0.9246434998512268
  batch 250 loss: 0.9693366587162018
  batch 300 loss: 0.9458464920520783
  batch 350 loss: 0.9697516965866089
  batch 400 loss: 0.906953866481781
  batch 450 loss: 0.9284523773193359
  batch 500 loss: 0.9051030731201172
  batch 550 loss: 0.9123644602298736
  batch 600 loss: 0.9170906901359558
  batch 650 loss: 0.8873714673519134
  batch 700 loss: 0.9698256802558899
  batch 750 loss: 0.9088786292076111
  batch 800 loss: 0.9037902009487152
  batch 850 loss: 0.8934387457370758
  batch 900 loss: 0.9069420647621155
LOSS train 0.90694 valid 0.91565, valid PER 28.84%
EPOCH 8, Learning Rate: 0.5
  batch 50 loss: 0.9012103700637817
  batch 100 loss: 0.8697852253913879
  batch 150 loss: 0.8872828447818756
  batch 200 loss: 0.8706080269813538
  batch 250 loss: 0.8721171391010284
  batch 300 loss: 0.841768786907196
  batch 350 loss: 0.8731420576572418
  batch 400 loss: 0.8554071962833405
  batch 450 loss: 0.9207946527004242
  batch 500 loss: 0.8942949795722961
  batch 550 loss: 0.8893092811107636
  batch 600 loss: 0.8555660963058471
  batch 650 loss: 0.8648669993877411
  batch 700 loss: 0.9101497232913971
  batch 750 loss: 0.8891388738155365
  batch 800 loss: 0.8949422419071198
  batch 850 loss: 0.8702728378772736
  batch 900 loss: 0.8893143689632416
LOSS train 0.88931 valid 0.88243, valid PER 27.14%
EPOCH 9, Learning Rate: 0.5
  batch 50 loss: 0.8325359392166137
  batch 100 loss: 0.8260523581504822
  batch 150 loss: 0.8524900341033935
  batch 200 loss: 0.865843408703804
  batch 250 loss: 0.8206637930870057
  batch 300 loss: 0.8794545650482177
  batch 350 loss: 0.8264660108089447
  batch 400 loss: 0.8503223884105683
  batch 450 loss: 0.869752185344696
  batch 500 loss: 0.8467741215229034
  batch 550 loss: 0.859109308719635
  batch 600 loss: 0.8737133133411408
  batch 650 loss: 0.8716746258735657
  batch 700 loss: 0.854631029367447
  batch 750 loss: 0.855228773355484
  batch 800 loss: 0.8888972568511962
  batch 850 loss: 0.8566396474838257
  batch 900 loss: 0.8314918029308319
LOSS train 0.83149 valid 0.84785, valid PER 26.44%
EPOCH 10, Learning Rate: 0.5
  batch 50 loss: 0.802427191734314
  batch 100 loss: 0.8140203201770783
  batch 150 loss: 0.8465143442153931
  batch 200 loss: 0.8001657092571258
  batch 250 loss: 0.8027261972427369
  batch 300 loss: 0.8208464241027832
  batch 350 loss: 0.7873727148771286
  batch 400 loss: 0.778971860408783
  batch 450 loss: 0.7929968696832657
  batch 500 loss: 0.7854436528682709
  batch 550 loss: 0.8246018135547638
  batch 600 loss: 0.809015656709671
  batch 650 loss: 0.838827748298645
  batch 700 loss: 0.8229292964935303
  batch 750 loss: 0.827732492685318
  batch 800 loss: 0.8349762952327728
  batch 850 loss: 0.8018734359741211
  batch 900 loss: 0.8030836391448974
LOSS train 0.80308 valid 0.85296, valid PER 26.70%
EPOCH 11, Learning Rate: 0.5
  batch 50 loss: 0.7686917650699615
  batch 100 loss: 0.7724978590011596
  batch 150 loss: 0.7749073421955108
  batch 200 loss: 0.7476064819097519
  batch 250 loss: 0.7627910280227661
  batch 300 loss: 0.7670814692974091
  batch 350 loss: 0.8085766506195068
  batch 400 loss: 0.7607713925838471
  batch 450 loss: 0.7971547609567642
  batch 500 loss: 0.7710360819101334
  batch 550 loss: 0.7870255219936371
  batch 600 loss: 0.7928689932823181
  batch 650 loss: 0.8055477917194367
  batch 700 loss: 0.8627812242507935
  batch 750 loss: 0.7849306726455688
  batch 800 loss: 0.7931142330169678
  batch 850 loss: 0.7912745946645736
  batch 900 loss: 0.8204714143276215
LOSS train 0.82047 valid 0.82172, valid PER 25.31%
EPOCH 12, Learning Rate: 0.5
  batch 50 loss: 0.7161683458089828
  batch 100 loss: 0.7385312247276307
  batch 150 loss: 0.7645353507995606
  batch 200 loss: 0.7553737491369248
  batch 250 loss: 0.7348169553279876
  batch 300 loss: 0.7894734370708466
  batch 350 loss: 0.7449691331386566
  batch 400 loss: 0.7860178047418594
  batch 450 loss: 0.7601656353473664
  batch 500 loss: 0.7803940355777741
  batch 550 loss: 0.7647778558731079
  batch 600 loss: 0.780193418264389
  batch 650 loss: 0.7593402844667435
  batch 700 loss: 0.7554025292396546
  batch 750 loss: 0.783664790391922
  batch 800 loss: 0.7431262636184692
  batch 850 loss: 0.763357138633728
  batch 900 loss: 0.7768466317653656
LOSS train 0.77685 valid 0.81288, valid PER 24.76%
EPOCH 13, Learning Rate: 0.5
  batch 50 loss: 0.699034731388092
  batch 100 loss: 0.712254005074501
  batch 150 loss: 0.732853392958641
  batch 200 loss: 0.6816592389345169
  batch 250 loss: 0.6978155243396759
  batch 300 loss: 0.7631170111894607
  batch 350 loss: 0.7211793142557145
  batch 400 loss: 0.7340928226709366
  batch 450 loss: 0.7507114923000335
  batch 500 loss: 0.7407153266668319
  batch 550 loss: 0.7804889798164367
  batch 600 loss: 0.7641800665855407
  batch 650 loss: 0.7358821022510529
  batch 700 loss: 0.7379811871051788
  batch 750 loss: 0.7125943797826767
  batch 800 loss: 0.7621030569076538
  batch 850 loss: 0.7182895368337632
  batch 900 loss: 0.7295326805114746
LOSS train 0.72953 valid 0.81927, valid PER 24.62%
EPOCH 14, Learning Rate: 0.5
  batch 50 loss: 0.700879852771759
  batch 100 loss: 0.6726680010557174
  batch 150 loss: 0.7216051018238068
  batch 200 loss: 0.7100075042247772
  batch 250 loss: 0.6926081830263138
  batch 300 loss: 0.7111719369888305
  batch 350 loss: 0.7029658448696137
  batch 400 loss: 0.7480198228359223
  batch 450 loss: 0.693138244152069
  batch 500 loss: 0.7176290357112884
  batch 550 loss: 0.7218077206611633
  batch 600 loss: 0.706829445362091
  batch 650 loss: 0.7257663154602051
  batch 700 loss: 0.7331551402807236
  batch 750 loss: 0.724462810754776
  batch 800 loss: 0.6984507238864899
  batch 850 loss: 0.738080952167511
  batch 900 loss: 0.7385276281833648
LOSS train 0.73853 valid 0.77546, valid PER 23.88%
EPOCH 15, Learning Rate: 0.5
  batch 50 loss: 0.6474640887975692
  batch 100 loss: 0.6684620654582978
  batch 150 loss: 0.6976352268457413
  batch 200 loss: 0.7137207639217377
  batch 250 loss: 0.7160165894031525
  batch 300 loss: 0.6926920151710511
  batch 350 loss: 0.6866769188642502
  batch 400 loss: 0.6874362248182296
  batch 450 loss: 0.6812220793962479
  batch 500 loss: 0.6888170945644378
  batch 550 loss: 0.7356969726085663
  batch 600 loss: 0.7125996941328049
  batch 650 loss: 0.6894812142848968
  batch 700 loss: 0.6881289947032928
  batch 750 loss: 0.7081515580415726
  batch 800 loss: 0.6837765282392502
  batch 850 loss: 0.6741053003072739
  batch 900 loss: 0.6722448551654816
LOSS train 0.67224 valid 0.78870, valid PER 24.62%
EPOCH 16, Learning Rate: 0.5
  batch 50 loss: 0.6841813373565674
  batch 100 loss: 0.6546898430585861
  batch 150 loss: 0.6552682054042817
  batch 200 loss: 0.6862687921524048
  batch 250 loss: 0.6698386818170547
  batch 300 loss: 0.6665485888719559
  batch 350 loss: 0.6979701578617096
  batch 400 loss: 0.6936642616987229
  batch 450 loss: 0.70015460729599
  batch 500 loss: 0.6511525863409042
  batch 550 loss: 0.6383116048574448
  batch 600 loss: 0.6803723788261413
  batch 650 loss: 0.699356073141098
  batch 700 loss: 0.643549588918686
  batch 750 loss: 0.7000233101844787
  batch 800 loss: 0.6822593116760254
  batch 850 loss: 0.675296025276184
  batch 900 loss: 0.6945449006557465
LOSS train 0.69454 valid 0.77372, valid PER 23.28%
EPOCH 17, Learning Rate: 0.5
  batch 50 loss: 0.6769343137741088
  batch 100 loss: 0.6160106301307678
  batch 150 loss: 0.6787937581539154
  batch 200 loss: 0.625821852684021
  batch 250 loss: 0.6469775456190109
  batch 300 loss: 0.6325542145967483
  batch 350 loss: 0.6818863373994827
  batch 400 loss: 0.6554201769828797
  batch 450 loss: 0.6393569511175156
  batch 500 loss: 0.6731664741039276
  batch 550 loss: 0.6667372459173202
  batch 600 loss: 0.6877176851034165
  batch 650 loss: 0.6715538316965103
  batch 700 loss: 0.6780507707595825
  batch 750 loss: 0.6408485668897629
  batch 800 loss: 0.6714511984586715
  batch 850 loss: 0.6667514622211457
  batch 900 loss: 0.6611746716499328
LOSS train 0.66117 valid 0.75351, valid PER 22.64%
EPOCH 18, Learning Rate: 0.5
  batch 50 loss: 0.6018430614471435
  batch 100 loss: 0.6062287670373917
  batch 150 loss: 0.6632276934385299
  batch 200 loss: 0.6254492527246476
  batch 250 loss: 0.6187837225198746
  batch 300 loss: 0.6173103904724121
  batch 350 loss: 0.5959863317012787
  batch 400 loss: 0.6405388784408569
  batch 450 loss: 0.6576024585962296
  batch 500 loss: 0.6343601793050766
  batch 550 loss: 0.6964305025339127
  batch 600 loss: 0.6402745991945267
  batch 650 loss: 0.6366310948133469
  batch 700 loss: 0.6475622588396073
  batch 750 loss: 0.649025347828865
  batch 800 loss: 0.6685591351985931
  batch 850 loss: 0.6606018596887588
  batch 900 loss: 0.6322254478931427
LOSS train 0.63223 valid 0.75751, valid PER 23.17%
EPOCH 19, Learning Rate: 0.5
  batch 50 loss: 0.6009782242774964
  batch 100 loss: 0.6127664428949356
  batch 150 loss: 0.6085234999656677
  batch 200 loss: 0.5937433242797852
  batch 250 loss: 0.6362679880857468
  batch 300 loss: 0.6309119719266891
  batch 350 loss: 0.6259068137407303
  batch 400 loss: 0.6187187576293945
  batch 450 loss: 0.5729661220312119
  batch 500 loss: 0.5992977249622345
  batch 550 loss: 0.6369751727581024
  batch 600 loss: 0.646798540353775
  batch 650 loss: 0.6417157834768296
  batch 700 loss: 0.6422941660881043
  batch 750 loss: 0.6005126601457595
  batch 800 loss: 0.609395831823349
  batch 850 loss: 0.6463216245174408
  batch 900 loss: 0.630536926984787
LOSS train 0.63054 valid 0.76524, valid PER 22.90%
EPOCH 20, Learning Rate: 0.5
  batch 50 loss: 0.565020654797554
  batch 100 loss: 0.5979965597391128
  batch 150 loss: 0.5865336966514587
  batch 200 loss: 0.6188972133398056
  batch 250 loss: 0.6135570818185806
  batch 300 loss: 0.6037363314628601
  batch 350 loss: 0.5986474466323852
  batch 400 loss: 0.6076022773981095
  batch 450 loss: 0.6022104662656784
  batch 500 loss: 0.6338509345054626
  batch 550 loss: 0.6204054486751557
  batch 600 loss: 0.6133564686775208
  batch 650 loss: 0.6294289231300354
  batch 700 loss: 0.5886063379049301
  batch 750 loss: 0.5999177694320679
  batch 800 loss: 0.6080473154783249
  batch 850 loss: 0.6501356208324433
  batch 900 loss: 0.6190716332197189
LOSS train 0.61907 valid 0.80019, valid PER 23.51%
Training finished in 5.0 minutes.
Model saved to checkpoints/20231210_002047/model_17
Loading model from checkpoints/20231210_002047/model_17
SUB: 14.78%, DEL: 8.88%, INS: 1.79%, COR: 76.34%, PER: 25.46%
