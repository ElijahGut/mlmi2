Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.001, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='adam', grad_clip=0.5, is_bidir=True)
Total number of model parameters is 166952
EPOCH 1, Learning Rate: 0.001
  batch 50 loss: 9.06240351676941
  batch 100 loss: 3.0863462495803833
  batch 150 loss: 2.936730828285217
  batch 200 loss: 2.7810316133499144
  batch 250 loss: 2.701765947341919
  batch 300 loss: 2.542498254776001
  batch 350 loss: 2.415041694641113
  batch 400 loss: 2.358227949142456
  batch 450 loss: 2.2692677998542785
  batch 500 loss: 2.1494507956504822
  batch 550 loss: 2.065229370594025
  batch 600 loss: 2.0296831822395323
  batch 650 loss: 1.9387594962120056
  batch 700 loss: 1.9492904114723206
  batch 750 loss: 1.8975093746185303
  batch 800 loss: 1.8862630558013915
  batch 850 loss: 1.8513550853729248
  batch 900 loss: 1.8295019507408141
LOSS train 1.82950 valid 1.76474, valid PER 67.69%
EPOCH 2, Learning Rate: 0.001
  batch 50 loss: 1.7902056050300599
  batch 100 loss: 1.7547413635253906
  batch 150 loss: 1.7091497921943664
  batch 200 loss: 1.7190219521522523
  batch 250 loss: 1.7271299433708192
  batch 300 loss: 1.7059199142456054
  batch 350 loss: 1.6196836590766908
  batch 400 loss: 1.6337465715408326
  batch 450 loss: 1.5901840615272522
  batch 500 loss: 1.6123162841796874
  batch 550 loss: 1.630107867717743
  batch 600 loss: 1.5782916402816773
  batch 650 loss: 1.5924538087844848
  batch 700 loss: 1.5940891051292418
  batch 750 loss: 1.5679398155212403
  batch 800 loss: 1.5047028493881225
  batch 850 loss: 1.5158866477012634
  batch 900 loss: 1.546263608932495
LOSS train 1.54626 valid 1.50462, valid PER 55.55%
EPOCH 3, Learning Rate: 0.001
  batch 50 loss: 1.5094951677322388
  batch 100 loss: 1.4927501034736634
  batch 150 loss: 1.4650988698005676
  batch 200 loss: 1.4456300354003906
  batch 250 loss: 1.4592919540405274
  batch 300 loss: 1.426175196170807
  batch 350 loss: 1.4770411014556886
  batch 400 loss: 1.4335727882385254
  batch 450 loss: 1.4266863679885864
  batch 500 loss: 1.4040935826301575
  batch 550 loss: 1.4052424621582031
  batch 600 loss: 1.379623863697052
  batch 650 loss: 1.3697246360778808
  batch 700 loss: 1.3736847150325775
  batch 750 loss: 1.420364179611206
  batch 800 loss: 1.355332179069519
  batch 850 loss: 1.4041845774650574
  batch 900 loss: 1.3484151315689088
LOSS train 1.34842 valid 1.31004, valid PER 43.83%
EPOCH 4, Learning Rate: 0.001
  batch 50 loss: 1.3158191990852357
  batch 100 loss: 1.3444038605690003
  batch 150 loss: 1.282500158548355
  batch 200 loss: 1.3346595978736877
  batch 250 loss: 1.3443909788131714
  batch 300 loss: 1.332990846633911
  batch 350 loss: 1.2646422564983368
  batch 400 loss: 1.3215182137489319
  batch 450 loss: 1.3060906314849854
  batch 500 loss: 1.2736463928222657
  batch 550 loss: 1.321405634880066
  batch 600 loss: 1.3208259797096253
  batch 650 loss: 1.2958705163002013
  batch 700 loss: 1.2870885920524597
  batch 750 loss: 1.2520885837078095
  batch 800 loss: 1.2335996305942536
  batch 850 loss: 1.2738959288597107
  batch 900 loss: 1.2885289359092713
LOSS train 1.28853 valid 1.20566, valid PER 40.33%
EPOCH 5, Learning Rate: 0.001
  batch 50 loss: 1.223037029504776
  batch 100 loss: 1.2185108113288878
  batch 150 loss: 1.2539625787734985
  batch 200 loss: 1.1753424680233002
  batch 250 loss: 1.2147119343280792
  batch 300 loss: 1.2319107615947724
  batch 350 loss: 1.2154268753528594
  batch 400 loss: 1.2321878731250764
  batch 450 loss: 1.2048884546756744
  batch 500 loss: 1.2228509628772735
  batch 550 loss: 1.2015523731708526
  batch 600 loss: 1.2521308660507202
  batch 650 loss: 1.1802709269523621
  batch 700 loss: 1.2387714934349061
  batch 750 loss: 1.166583070755005
  batch 800 loss: 1.1985277318954468
  batch 850 loss: 1.1962959969043732
  batch 900 loss: 1.2108075881004334
LOSS train 1.21081 valid 1.16082, valid PER 38.70%
EPOCH 6, Learning Rate: 0.001
  batch 50 loss: 1.206114627122879
  batch 100 loss: 1.1394747376441956
  batch 150 loss: 1.1319947385787963
  batch 200 loss: 1.1556456184387207
  batch 250 loss: 1.1796103739738464
  batch 300 loss: 1.183104511499405
  batch 350 loss: 1.1665257573127747
  batch 400 loss: 1.1518210649490357
  batch 450 loss: 1.1867912924289703
  batch 500 loss: 1.1537174987792969
  batch 550 loss: 1.1849512887001037
  batch 600 loss: 1.1420010638237
  batch 650 loss: 1.1761832165718078
  batch 700 loss: 1.1723101437091827
  batch 750 loss: 1.137066103219986
  batch 800 loss: 1.1373385274410248
  batch 850 loss: 1.1352952778339387
  batch 900 loss: 1.1831168186664582
LOSS train 1.18312 valid 1.10633, valid PER 36.38%
EPOCH 7, Learning Rate: 0.001
  batch 50 loss: 1.1182005167007447
  batch 100 loss: 1.139962763786316
  batch 150 loss: 1.1115342497825622
  batch 200 loss: 1.119364469051361
  batch 250 loss: 1.1201829600334168
  batch 300 loss: 1.1150132179260255
  batch 350 loss: 1.0948998022079468
  batch 400 loss: 1.1205814671516419
  batch 450 loss: 1.1190657699108124
  batch 500 loss: 1.1257111024856568
  batch 550 loss: 1.1170773494243622
  batch 600 loss: 1.1128207576274871
  batch 650 loss: 1.1070203113555908
  batch 700 loss: 1.1267441713809967
  batch 750 loss: 1.0803576338291168
  batch 800 loss: 1.0978723931312562
  batch 850 loss: 1.1327529299259185
  batch 900 loss: 1.1518787050247192
LOSS train 1.15188 valid 1.12138, valid PER 37.09%
EPOCH 8, Learning Rate: 0.001
  batch 50 loss: 1.115575225353241
  batch 100 loss: 1.090034565925598
  batch 150 loss: 1.0765725207328796
  batch 200 loss: 1.05709166765213
  batch 250 loss: 1.078829528093338
  batch 300 loss: 0.9951281881332398
  batch 350 loss: 1.086056340932846
  batch 400 loss: 1.0399892807006836
  batch 450 loss: 1.098205054998398
  batch 500 loss: 1.1202213442325593
  batch 550 loss: 1.0693998634815216
  batch 600 loss: 1.1042500090599061
  batch 650 loss: 1.1224372494220733
  batch 700 loss: 1.0584732210636139
  batch 750 loss: 1.0834278452396393
  batch 800 loss: 1.0893012404441833
  batch 850 loss: 1.079070053100586
  batch 900 loss: 1.0459371817111969
LOSS train 1.04594 valid 1.07289, valid PER 35.24%
EPOCH 9, Learning Rate: 0.001
  batch 50 loss: 1.0088710045814515
  batch 100 loss: 1.0477435445785523
  batch 150 loss: 1.0597244668006898
  batch 200 loss: 0.9941558647155762
  batch 250 loss: 1.024631336927414
  batch 300 loss: 1.0604433560371398
  batch 350 loss: 1.053855184316635
  batch 400 loss: 1.028640763759613
  batch 450 loss: 1.04954261302948
  batch 500 loss: 1.01077943444252
  batch 550 loss: 1.0494096457958222
  batch 600 loss: 1.0634288704395294
  batch 650 loss: 1.0401358258724214
  batch 700 loss: 1.0384050738811492
  batch 750 loss: 1.042544003725052
  batch 800 loss: 1.0397615551948547
  batch 850 loss: 1.0577784848213196
  batch 900 loss: 1.030754554271698
LOSS train 1.03075 valid 1.03454, valid PER 33.74%
EPOCH 10, Learning Rate: 0.001
  batch 50 loss: 0.9648619019985198
  batch 100 loss: 1.009509459733963
  batch 150 loss: 1.0314674770832062
  batch 200 loss: 1.0550133621692657
  batch 250 loss: 1.0103254091739655
  batch 300 loss: 0.9832286393642425
  batch 350 loss: 1.0178768014907837
  batch 400 loss: 0.9768908476829529
  batch 450 loss: 0.9744209015369415
  batch 500 loss: 1.0368752658367157
  batch 550 loss: 1.0233940970897675
  batch 600 loss: 0.9990682113170624
  batch 650 loss: 0.9931599795818329
  batch 700 loss: 1.0266644597053527
  batch 750 loss: 1.0109342432022095
  batch 800 loss: 1.0640110290050506
  batch 850 loss: 1.0090508210659026
  batch 900 loss: 1.0321068692207336
LOSS train 1.03211 valid 1.00797, valid PER 32.64%
EPOCH 11, Learning Rate: 0.001
  batch 50 loss: 0.9613586783409118
  batch 100 loss: 0.947924165725708
  batch 150 loss: 0.9587044262886047
  batch 200 loss: 1.0297511863708495
  batch 250 loss: 0.9933887541294097
  batch 300 loss: 0.9702722072601319
  batch 350 loss: 0.9945888662338257
  batch 400 loss: 1.0119673478603364
  batch 450 loss: 0.9938821864128112
  batch 500 loss: 0.9680741167068482
  batch 550 loss: 0.989200576543808
  batch 600 loss: 0.9528658378124237
  batch 650 loss: 1.036313247680664
  batch 700 loss: 0.9761035144329071
  batch 750 loss: 0.9877615082263946
  batch 800 loss: 1.0219025778770447
  batch 850 loss: 1.0324174821376801
  batch 900 loss: 1.006462845802307
LOSS train 1.00646 valid 0.98917, valid PER 31.59%
EPOCH 12, Learning Rate: 0.001
  batch 50 loss: 0.9836152136325836
  batch 100 loss: 0.9685702836513519
  batch 150 loss: 0.9738714146614075
  batch 200 loss: 0.952359174489975
  batch 250 loss: 0.9850973272323609
  batch 300 loss: 0.9488803088665009
  batch 350 loss: 0.9605365037918091
  batch 400 loss: 0.9775471580028534
  batch 450 loss: 0.9752079975605011
  batch 500 loss: 1.0182994353771209
  batch 550 loss: 0.9301786696910859
  batch 600 loss: 0.9300726234912873
  batch 650 loss: 1.0007539665699006
  batch 700 loss: 0.9731054615974426
  batch 750 loss: 0.9473456978797913
  batch 800 loss: 0.9414401125907897
  batch 850 loss: 0.9834732747077942
  batch 900 loss: 0.9872067439556121
LOSS train 0.98721 valid 0.98832, valid PER 32.26%
EPOCH 13, Learning Rate: 0.001
  batch 50 loss: 0.9250783562660218
  batch 100 loss: 0.9572002553939819
  batch 150 loss: 0.9307824838161468
  batch 200 loss: 0.9564520943164826
  batch 250 loss: 0.940011556148529
  batch 300 loss: 0.943157696723938
  batch 350 loss: 0.9567861342430115
  batch 400 loss: 0.9580424845218658
  batch 450 loss: 0.935525426864624
  batch 500 loss: 0.9265220057964325
  batch 550 loss: 0.9381689918041229
  batch 600 loss: 0.9345144820213318
  batch 650 loss: 0.9545387363433838
  batch 700 loss: 0.9537101531028748
  batch 750 loss: 0.9218641972541809
  batch 800 loss: 0.9536892211437226
  batch 850 loss: 0.9628392910957336
  batch 900 loss: 0.9651294517517089
LOSS train 0.96513 valid 0.97808, valid PER 31.72%
EPOCH 14, Learning Rate: 0.001
  batch 50 loss: 0.9166108787059783
  batch 100 loss: 0.9171376156806946
  batch 150 loss: 0.9223924267292023
  batch 200 loss: 0.9147282874584198
  batch 250 loss: 0.9294365429878235
  batch 300 loss: 0.931876335144043
  batch 350 loss: 0.8968876218795776
  batch 400 loss: 0.9098417353630066
  batch 450 loss: 0.9115779542922974
  batch 500 loss: 0.9600535213947297
  batch 550 loss: 0.965524959564209
  batch 600 loss: 0.8998138904571533
  batch 650 loss: 0.953515567779541
  batch 700 loss: 0.9669608843326568
  batch 750 loss: 0.9289919078350067
  batch 800 loss: 0.895081228017807
  batch 850 loss: 0.9289888119697571
  batch 900 loss: 0.9323761892318726
LOSS train 0.93238 valid 0.97488, valid PER 31.74%
EPOCH 15, Learning Rate: 0.001
  batch 50 loss: 0.9132463669776917
  batch 100 loss: 0.9084709131717682
  batch 150 loss: 0.9094706511497498
  batch 200 loss: 0.9740988898277283
  batch 250 loss: 0.9537746274471283
  batch 300 loss: 0.8790488529205323
  batch 350 loss: 0.9011362588405609
  batch 400 loss: 0.8873485291004181
  batch 450 loss: 0.9017905747890472
  batch 500 loss: 0.8592170441150665
  batch 550 loss: 0.8916433477401733
  batch 600 loss: 0.9244416046142578
  batch 650 loss: 0.9320414865016937
  batch 700 loss: 0.9170383095741272
  batch 750 loss: 0.9304105401039123
  batch 800 loss: 0.9284554576873779
  batch 850 loss: 0.8976242458820343
  batch 900 loss: 0.9271029174327851
LOSS train 0.92710 valid 0.96004, valid PER 30.66%
EPOCH 16, Learning Rate: 0.001
  batch 50 loss: 0.9175353908538818
  batch 100 loss: 0.8561308360099793
  batch 150 loss: 0.8768646240234375
  batch 200 loss: 0.8811832916736603
  batch 250 loss: 0.9119397675991059
  batch 300 loss: 0.8878886461257934
  batch 350 loss: 0.9137542104721069
  batch 400 loss: 0.9064257740974426
  batch 450 loss: 0.9176903653144837
  batch 500 loss: 0.8734371995925904
  batch 550 loss: 0.9300824844837189
  batch 600 loss: 0.8841856563091278
  batch 650 loss: 0.9099382519721985
  batch 700 loss: 0.8943325448036193
  batch 750 loss: 0.9043966126441956
  batch 800 loss: 0.9090087056159973
  batch 850 loss: 0.8919177067279815
  batch 900 loss: 0.8953133678436279
LOSS train 0.89531 valid 0.95056, valid PER 30.87%
EPOCH 17, Learning Rate: 0.001
  batch 50 loss: 0.8828831440210343
  batch 100 loss: 0.8904474234580994
  batch 150 loss: 0.8771393990516663
  batch 200 loss: 0.8754821491241455
  batch 250 loss: 0.8737822294235229
  batch 300 loss: 0.875424691438675
  batch 350 loss: 0.8480022084712983
  batch 400 loss: 0.9104540991783142
  batch 450 loss: 0.8914790046215058
  batch 500 loss: 0.8803182530403137
  batch 550 loss: 0.8841140174865723
  batch 600 loss: 0.9497538077831268
  batch 650 loss: 0.8630368173122406
  batch 700 loss: 0.8544778513908386
  batch 750 loss: 0.8585913562774659
  batch 800 loss: 0.8520119261741638
  batch 850 loss: 0.870724538564682
  batch 900 loss: 0.8695703828334809
LOSS train 0.86957 valid 0.94768, valid PER 30.34%
EPOCH 18, Learning Rate: 0.001
  batch 50 loss: 0.8544490575790405
  batch 100 loss: 0.8798415875434875
  batch 150 loss: 0.8753348076343537
  batch 200 loss: 0.8658748376369476
  batch 250 loss: 0.8661897659301758
  batch 300 loss: 0.8364891219139099
  batch 350 loss: 0.8735861515998841
  batch 400 loss: 0.8498959159851074
  batch 450 loss: 0.9153701841831208
  batch 500 loss: 0.8651809680461884
  batch 550 loss: 0.8626332223415375
  batch 600 loss: 0.8493494760990142
  batch 650 loss: 0.8703942811489105
  batch 700 loss: 0.8903520119190216
  batch 750 loss: 0.8535944604873658
  batch 800 loss: 0.8470753383636475
  batch 850 loss: 0.8397924661636352
  batch 900 loss: 0.893846046924591
LOSS train 0.89385 valid 0.95290, valid PER 29.89%
EPOCH 19, Learning Rate: 0.001
  batch 50 loss: 0.8217599391937256
  batch 100 loss: 0.8122389566898346
  batch 150 loss: 0.8346615111827851
  batch 200 loss: 0.8383908641338348
  batch 250 loss: 0.8593070924282074
  batch 300 loss: 0.8351390957832336
  batch 350 loss: 0.8362113320827484
  batch 400 loss: 0.8595316529273986
  batch 450 loss: 0.8778975927829742
  batch 500 loss: 0.870294657945633
  batch 550 loss: 0.8247731375694275
  batch 600 loss: 0.8663441967964173
  batch 650 loss: 0.8864652323722839
  batch 700 loss: 0.8315201675891877
  batch 750 loss: 0.8178671252727509
  batch 800 loss: 0.8771602499485016
  batch 850 loss: 0.8677460634708405
  batch 900 loss: 0.8305701684951782
LOSS train 0.83057 valid 0.92731, valid PER 29.56%
EPOCH 20, Learning Rate: 0.001
  batch 50 loss: 0.8055439794063568
  batch 100 loss: 0.8156419467926025
  batch 150 loss: 0.8047240674495697
  batch 200 loss: 0.8206427347660065
  batch 250 loss: 0.8286340272426606
  batch 300 loss: 0.8608252108097076
  batch 350 loss: 0.814619402885437
  batch 400 loss: 0.8425533509254456
  batch 450 loss: 0.8362654948234558
  batch 500 loss: 0.8140159130096436
  batch 550 loss: 0.8706744956970215
  batch 600 loss: 0.8218059146404266
  batch 650 loss: 0.8754861295223236
  batch 700 loss: 0.843593887090683
  batch 750 loss: 0.8307336974143982
  batch 800 loss: 0.8547571301460266
  batch 850 loss: 0.8573259460926056
  batch 900 loss: 0.8558564448356628
LOSS train 0.85586 valid 0.92687, valid PER 29.46%
Training finished in 4.0 minutes.
Model saved to checkpoints/20231207_184742/model_20
Loading model from checkpoints/20231207_184742/model_20
SUB: 15.93%, DEL: 13.51%, INS: 1.57%, COR: 70.55%, PER: 31.02%
