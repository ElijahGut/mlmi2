Namespace(seed=123, train_json='train_fbank61.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=2, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_61.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=0.5, is_bidir=True)
Total number of model parameters is 218559
EPOCH 1, Learning Rate: 0.5
  batch 50 loss: 7.238559160232544
  batch 100 loss: 4.024539184570313
  batch 150 loss: 3.986020178794861
  batch 200 loss: 3.91457546710968
  batch 250 loss: 3.864237117767334
  batch 300 loss: 3.8393171977996827
  batch 350 loss: 3.82524384021759
  batch 400 loss: 3.8254240131378174
  batch 450 loss: 3.8029038810729983
  batch 500 loss: 3.769434680938721
  batch 550 loss: 3.7096943950653074
  batch 600 loss: 3.6285454177856447
  batch 650 loss: 3.5270422649383546
  batch 700 loss: 3.450354480743408
  batch 750 loss: 3.3802035331726072
  batch 800 loss: 3.291783504486084
  batch 850 loss: 3.2387072134017942
  batch 900 loss: 3.16736864566803
LOSS train 3.16737 valid 3.58120, valid PER 100.00%
EPOCH 2, Learning Rate: 0.5
  batch 50 loss: 3.0683317041397093
  batch 100 loss: 2.9907697534561155
  batch 150 loss: 2.8727637767791747
  batch 200 loss: 2.8511757707595824
  batch 250 loss: 2.8208941507339476
  batch 300 loss: 2.7580553102493286
  batch 350 loss: 2.715171513557434
  batch 400 loss: 2.6529805850982666
  batch 450 loss: 2.65929301738739
  batch 500 loss: 2.6084722757339476
  batch 550 loss: 2.55592435836792
  batch 600 loss: 2.5281496953964235
  batch 650 loss: 2.4828846645355225
  batch 700 loss: 2.461725535392761
  batch 750 loss: 2.43364040851593
  batch 800 loss: 2.387286787033081
  batch 850 loss: 2.3916630363464355
  batch 900 loss: 2.350756678581238
LOSS train 2.35076 valid 3.37691, valid PER 88.53%
EPOCH 3, Learning Rate: 0.5
  batch 50 loss: 2.2553165197372436
  batch 100 loss: 2.315688865184784
  batch 150 loss: 2.326907305717468
  batch 200 loss: 2.2105819368362427
  batch 250 loss: 2.2231288504600526
  batch 300 loss: 2.22573175907135
  batch 350 loss: 2.218695960044861
  batch 400 loss: 2.188103582859039
  batch 450 loss: 2.1655329275131225
  batch 500 loss: 2.1290524625778198
  batch 550 loss: 2.1207778525352476
  batch 600 loss: 2.0639400267601014
  batch 650 loss: 2.080326142311096
  batch 700 loss: 2.0473910093307497
  batch 750 loss: 2.074670295715332
  batch 800 loss: 2.0890230417251585
  batch 850 loss: 2.019034750461578
  batch 900 loss: 2.0493243312835694
LOSS train 2.04932 valid 3.24461, valid PER 77.52%
EPOCH 4, Learning Rate: 0.5
  batch 50 loss: 2.0273755240440368
  batch 100 loss: 1.952547583580017
  batch 150 loss: 1.9817073655128479
  batch 200 loss: 1.9892590427398682
  batch 250 loss: 1.9673897767066955
  batch 300 loss: 1.954581959247589
  batch 350 loss: 1.9259550189971923
  batch 400 loss: 1.8895109152793885
  batch 450 loss: 1.9100933241844178
  batch 500 loss: 1.9364021396636963
  batch 550 loss: 1.8841242980957031
  batch 600 loss: 1.8918394660949707
  batch 650 loss: 1.9209307861328124
  batch 700 loss: 1.9372995281219483
  batch 750 loss: 1.8517856860160828
  batch 800 loss: 1.8602527809143066
  batch 850 loss: 1.850921869277954
  batch 900 loss: 1.8210726118087768
LOSS train 1.82107 valid 3.25435, valid PER 69.94%
EPOCH 5, Learning Rate: 0.5
  batch 50 loss: 1.8369372987747192
  batch 100 loss: 1.7944625878334046
  batch 150 loss: 1.8124843549728393
  batch 200 loss: 1.81841148853302
  batch 250 loss: 1.784034857749939
  batch 300 loss: 1.796921603679657
  batch 350 loss: 1.7693443608283996
  batch 400 loss: 1.7522676420211791
  batch 450 loss: 1.7433296871185302
  batch 500 loss: 1.734737718105316
  batch 550 loss: 1.7709784650802611
  batch 600 loss: 1.7574266839027404
  batch 650 loss: 1.7742001843452453
  batch 700 loss: 1.7388493061065673
  batch 750 loss: 1.7471220803260803
  batch 800 loss: 1.769400188922882
  batch 850 loss: 1.7592972612380982
  batch 900 loss: 1.6894663977622986
LOSS train 1.68947 valid 3.26546, valid PER 66.73%
EPOCH 6, Learning Rate: 0.5
  batch 50 loss: 1.6774723052978515
  batch 100 loss: 1.722444975376129
  batch 150 loss: 1.6715399050712585
  batch 200 loss: 1.6661839294433594
  batch 250 loss: 1.7160881853103638
  batch 300 loss: 1.7091116428375244
  batch 350 loss: 1.6876874446868897
  batch 400 loss: 1.6469540739059447
  batch 450 loss: 1.6829553031921387
  batch 500 loss: 1.6350202083587646
  batch 550 loss: 1.6474545884132386
  batch 600 loss: 1.644510736465454
  batch 650 loss: 1.6212108135223389
  batch 700 loss: 1.6254794001579285
  batch 750 loss: 1.667250680923462
  batch 800 loss: 1.626140420436859
  batch 850 loss: 1.6527861905097962
  batch 900 loss: 1.6383298802375794
LOSS train 1.63833 valid 3.24972, valid PER 62.22%
EPOCH 7, Learning Rate: 0.5
  batch 50 loss: 1.6320793581008912
  batch 100 loss: 1.6512034368515014
  batch 150 loss: 1.574952733516693
  batch 200 loss: 1.6018797659873962
  batch 250 loss: 1.6261414790153503
  batch 300 loss: 1.5852385807037352
  batch 350 loss: 1.622047336101532
  batch 400 loss: 1.5755775094032287
  batch 450 loss: 1.5602635145187378
  batch 500 loss: 1.578098349571228
  batch 550 loss: 1.5620622634887695
  batch 600 loss: 1.5798852038383484
  batch 650 loss: 1.555721528530121
  batch 700 loss: 1.6024667310714722
  batch 750 loss: 1.5628732466697692
  batch 800 loss: 1.561806559562683
  batch 850 loss: 1.558447115421295
  batch 900 loss: 1.5597874450683593
LOSS train 1.55979 valid 3.16472, valid PER 61.35%
EPOCH 8, Learning Rate: 0.5
  batch 50 loss: 1.5376910471916199
  batch 100 loss: 1.5459573888778686
  batch 150 loss: 1.5856068778038024
  batch 200 loss: 1.557991144657135
  batch 250 loss: 1.5223401260375977
  batch 300 loss: 1.5072024512290954
  batch 350 loss: 1.4980618286132812
  batch 400 loss: 1.5128851795196534
  batch 450 loss: 1.5815268230438233
  batch 500 loss: 1.5497007846832276
  batch 550 loss: 1.5347298526763915
  batch 600 loss: 1.4969011211395264
  batch 650 loss: 1.5221802639961242
  batch 700 loss: 1.5223829174041748
  batch 750 loss: 1.5153244304656983
  batch 800 loss: 1.5024927926063538
  batch 850 loss: 1.4933454060554505
  batch 900 loss: 1.512248318195343
LOSS train 1.51225 valid 3.13980, valid PER 58.26%
EPOCH 9, Learning Rate: 0.5
  batch 50 loss: 1.4718750762939452
  batch 100 loss: 1.4476425814628602
  batch 150 loss: 1.4621564316749573
  batch 200 loss: 1.4627754998207092
  batch 250 loss: 1.4408693051338195
  batch 300 loss: 1.4902532887458801
  batch 350 loss: 1.4670827722549438
  batch 400 loss: 1.5040734553337096
  batch 450 loss: 1.4728606462478637
  batch 500 loss: 1.4596206855773926
  batch 550 loss: 1.4686221432685853
  batch 600 loss: 1.5142236590385436
  batch 650 loss: 1.5000511908531189
  batch 700 loss: 1.457956199645996
  batch 750 loss: 1.466218476295471
  batch 800 loss: 1.520606656074524
  batch 850 loss: 1.468585295677185
  batch 900 loss: 1.45898854970932
LOSS train 1.45899 valid 3.18673, valid PER 58.22%
EPOCH 10, Learning Rate: 0.5
  batch 50 loss: 1.4388060307502746
  batch 100 loss: 1.4446443104743958
  batch 150 loss: 1.4674238467216492
  batch 200 loss: 1.455018665790558
  batch 250 loss: 1.4219344091415405
  batch 300 loss: 1.4247521591186523
  batch 350 loss: 1.4363292360305786
  batch 400 loss: 1.4021312189102173
  batch 450 loss: 1.3963128757476806
  batch 500 loss: 1.4350949597358704
  batch 550 loss: 1.4352110600471497
  batch 600 loss: 1.4202077102661133
  batch 650 loss: 1.4327826261520387
  batch 700 loss: 1.4623523926734925
  batch 750 loss: 1.4349149131774903
  batch 800 loss: 1.481381618976593
  batch 850 loss: 1.40690194606781
  batch 900 loss: 1.380076961517334
LOSS train 1.38008 valid 3.18509, valid PER 58.27%
EPOCH 11, Learning Rate: 0.5
  batch 50 loss: 1.4185220503807068
  batch 100 loss: 1.4058235716819762
  batch 150 loss: 1.3868457293510437
  batch 200 loss: 1.3595116436481476
  batch 250 loss: 1.3787926864624023
  batch 300 loss: 1.4121779537200927
  batch 350 loss: 1.4354790616035462
  batch 400 loss: 1.3884636092185973
  batch 450 loss: 1.393582763671875
  batch 500 loss: 1.3755461645126343
  batch 550 loss: 1.3972569108009338
  batch 600 loss: 1.3801387643814087
  batch 650 loss: 1.3913507914543153
  batch 700 loss: 1.5116224455833436
  batch 750 loss: 1.3770862650871276
  batch 800 loss: 1.4058962631225587
  batch 850 loss: 1.3832291316986085
  batch 900 loss: 1.3792781138420105
LOSS train 1.37928 valid 3.16113, valid PER 56.65%
EPOCH 12, Learning Rate: 0.5
  batch 50 loss: 1.3129017972946166
  batch 100 loss: 1.326952784061432
  batch 150 loss: 1.3732054829597473
  batch 200 loss: 1.3907588875293733
  batch 250 loss: 1.3431630754470825
  batch 300 loss: 1.4116699409484863
  batch 350 loss: 1.3567834877967835
  batch 400 loss: 1.3956743550300599
  batch 450 loss: 1.3636269736289979
  batch 500 loss: 1.3869383025169373
  batch 550 loss: 1.37876238822937
  batch 600 loss: 1.376412682533264
  batch 650 loss: 1.3564498770236968
  batch 700 loss: 1.3449088668823241
  batch 750 loss: 1.4018260264396667
  batch 800 loss: 1.3302382731437683
  batch 850 loss: 1.3565037441253662
  batch 900 loss: 1.3570574641227722
LOSS train 1.35706 valid 3.26830, valid PER 57.52%
EPOCH 13, Learning Rate: 0.5
  batch 50 loss: 1.3087817525863648
  batch 100 loss: 1.3538946104049683
  batch 150 loss: 1.3953001022338867
  batch 200 loss: 1.3082725310325622
  batch 250 loss: 1.3363346672058105
  batch 300 loss: 1.3742886328697204
  batch 350 loss: 1.3595736289024354
  batch 400 loss: 1.3538719344139098
  batch 450 loss: 1.3496205830574035
  batch 500 loss: 1.3385466623306275
  batch 550 loss: 1.41619069814682
  batch 600 loss: 1.3544786953926087
  batch 650 loss: 1.3540127325057982
  batch 700 loss: 1.4117072105407715
  batch 750 loss: 1.3245229053497314
  batch 800 loss: 1.3104697346687317
  batch 850 loss: 1.3481228971481323
  batch 900 loss: 1.3404292261600494
LOSS train 1.34043 valid 3.17211, valid PER 55.82%
EPOCH 14, Learning Rate: 0.5
  batch 50 loss: 1.338482084274292
  batch 100 loss: 1.2699063301086426
  batch 150 loss: 1.338521910905838
  batch 200 loss: 1.308821828365326
  batch 250 loss: 1.3038684463500976
  batch 300 loss: 1.2664297008514405
  batch 350 loss: 1.316479296684265
  batch 400 loss: 1.3388622832298278
  batch 450 loss: 1.2963328123092652
  batch 500 loss: 1.3140553975105285
  batch 550 loss: 1.3175735759735108
  batch 600 loss: 1.3429615426063537
  batch 650 loss: 1.3548773670196532
  batch 700 loss: 1.346374442577362
  batch 750 loss: 1.3109493279457092
  batch 800 loss: 1.3310255312919617
  batch 850 loss: 1.3887239265441895
  batch 900 loss: 1.3320845866203308
LOSS train 1.33208 valid 3.06276, valid PER 54.97%
EPOCH 15, Learning Rate: 0.5
  batch 50 loss: 1.26023503780365
  batch 100 loss: 1.3242187929153442
  batch 150 loss: 1.2851717507839202
  batch 200 loss: 1.3459870982170106
  batch 250 loss: 1.333373577594757
  batch 300 loss: 1.3299017357826233
  batch 350 loss: 1.2845671153068543
  batch 400 loss: 1.3023568212985992
  batch 450 loss: 1.2871396219730378
  batch 500 loss: 1.29390522480011
  batch 550 loss: 1.334314317703247
  batch 600 loss: 1.3424179637432099
  batch 650 loss: 1.2999199891090394
  batch 700 loss: 1.2811155366897582
  batch 750 loss: 1.318232206106186
  batch 800 loss: 1.2668186521530151
  batch 850 loss: 1.2662978875637054
  batch 900 loss: 1.2742824459075928
LOSS train 1.27428 valid 3.16584, valid PER 55.77%
EPOCH 16, Learning Rate: 0.5
  batch 50 loss: 1.2836747467517853
  batch 100 loss: 1.2694873809814453
  batch 150 loss: 1.289486095905304
  batch 200 loss: 1.3049502730369569
  batch 250 loss: 1.2751642704010009
  batch 300 loss: 1.3144386386871338
  batch 350 loss: 1.2646415114402771
  batch 400 loss: 1.2574975907802581
  batch 450 loss: 1.283294177055359
  batch 500 loss: 1.3121546494960785
  batch 550 loss: 1.2684727585315705
  batch 600 loss: 1.284315173625946
  batch 650 loss: 1.259154896736145
  batch 700 loss: 1.276403340101242
  batch 750 loss: 1.278760129213333
  batch 800 loss: 1.258452639579773
  batch 850 loss: 1.2415904641151427
  batch 900 loss: 1.2797176492214204
LOSS train 1.27972 valid 3.15079, valid PER 54.81%
EPOCH 17, Learning Rate: 0.5
  batch 50 loss: 1.2770549535751343
  batch 100 loss: 1.214647661447525
  batch 150 loss: 1.2958101797103883
  batch 200 loss: 1.2348840427398682
  batch 250 loss: 1.2819574415683745
  batch 300 loss: 1.2284823310375215
  batch 350 loss: 1.2577961683273315
  batch 400 loss: 1.2484218883514404
  batch 450 loss: 1.255532054901123
  batch 500 loss: 1.2693173480033875
  batch 550 loss: 1.2381326735019684
  batch 600 loss: 1.3248524129390717
  batch 650 loss: 1.2428288543224335
  batch 700 loss: 1.2557499301433563
  batch 750 loss: 1.2055220520496368
  batch 800 loss: 1.2452241146564484
  batch 850 loss: 1.2443376517295837
  batch 900 loss: 1.2808089709281922
LOSS train 1.28081 valid 3.23675, valid PER 54.90%
EPOCH 18, Learning Rate: 0.5
  batch 50 loss: 1.2378451085090638
  batch 100 loss: 1.2167018556594849
  batch 150 loss: 1.3010179114341736
  batch 200 loss: 1.2388078105449676
  batch 250 loss: 1.2288246536254883
  batch 300 loss: 1.2214930152893066
  batch 350 loss: 1.2212633574008942
  batch 400 loss: 1.2004606699943543
  batch 450 loss: 1.2414165723323822
  batch 500 loss: 1.2493144190311432
  batch 550 loss: 1.2449305760860443
  batch 600 loss: 1.2309519410133363
  batch 650 loss: 1.2074868774414063
  batch 700 loss: 1.2045810675621034
  batch 750 loss: 1.2259017288684846
  batch 800 loss: 1.2402301502227784
  batch 850 loss: 1.2692913687229157
  batch 900 loss: 1.273470025062561
LOSS train 1.27347 valid 3.09980, valid PER 54.36%
EPOCH 19, Learning Rate: 0.5
  batch 50 loss: 1.2648221719264985
  batch 100 loss: 1.224870549440384
  batch 150 loss: 1.2276722407341003
  batch 200 loss: 1.210766214132309
  batch 250 loss: 1.2452852749824523
  batch 300 loss: 1.2260911571979523
  batch 350 loss: 1.2538369357585908
  batch 400 loss: 1.215149437189102
  batch 450 loss: 1.1772700977325439
  batch 500 loss: 1.2107822179794312
  batch 550 loss: 1.2402215600013733
  batch 600 loss: 1.2444907748699188
  batch 650 loss: 1.2191693091392517
  batch 700 loss: 1.2454229831695556
  batch 750 loss: 1.2102029407024384
  batch 800 loss: 1.2183211398124696
  batch 850 loss: 1.22161172747612
  batch 900 loss: 1.23671404838562
LOSS train 1.23671 valid 3.09139, valid PER 54.04%
EPOCH 20, Learning Rate: 0.5
  batch 50 loss: 1.175287356376648
  batch 100 loss: 1.2078753745555877
  batch 150 loss: 1.2040099680423737
  batch 200 loss: 1.204031490087509
  batch 250 loss: 1.271760996580124
  batch 300 loss: 1.2223542928695679
  batch 350 loss: 1.2098579144477843
  batch 400 loss: 1.1495273768901826
  batch 450 loss: 1.203195606470108
  batch 500 loss: 1.2191021060943603
  batch 550 loss: 1.2152161860466004
  batch 600 loss: 1.1872311866283416
  batch 650 loss: 1.2063969230651856
  batch 700 loss: 1.2154041945934295
  batch 750 loss: 1.2024631917476654
  batch 800 loss: 1.2189069426059722
  batch 850 loss: 1.2090716326236726
  batch 900 loss: 1.1854059624671935
LOSS train 1.18541 valid 3.23140, valid PER 54.87%
Training finished in 4.0 minutes.
Model saved to checkpoints/20231209_105942/model_14
Loading model from checkpoints/20231209_105942/model_14
SUB: 41.57%, DEL: 12.90%, INS: 1.90%, COR: 45.53%, PER: 56.37%
