Namespace(seed=123, train_json='train_fbank61.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=2, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_61.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=0.5)
Total number of model parameters is 568127
EPOCH 1, Learning Rate: 0.5
  batch 50 loss: 7.026053609848023
  batch 100 loss: 3.9752517461776735
  batch 150 loss: 3.9007491493225097
  batch 200 loss: 3.877654552459717
  batch 250 loss: 3.8522442197799682
  batch 300 loss: 3.8122823286056517
  batch 350 loss: 3.773044590950012
  batch 400 loss: 3.742739896774292
  batch 450 loss: 3.6852571773529053
  batch 500 loss: 3.5927062606811524
  batch 550 loss: 3.486220874786377
  batch 600 loss: 3.371885967254639
  batch 650 loss: 3.2717123079299926
  batch 700 loss: 3.2048315095901487
  batch 750 loss: 3.1312626457214354
  batch 800 loss: 3.048762845993042
  batch 850 loss: 2.9885329103469847
  batch 900 loss: 2.9245494270324706
LOSS train 2.92455 valid 3.53737, valid PER 96.55%
EPOCH 2, Learning Rate: 0.5
  batch 50 loss: 2.839281897544861
  batch 100 loss: 2.775942440032959
  batch 150 loss: 2.6715914011001587
  batch 200 loss: 2.649922423362732
  batch 250 loss: 2.600636568069458
  batch 300 loss: 2.5337667274475097
  batch 350 loss: 2.4664173555374145
  batch 400 loss: 2.4178205585479735
  batch 450 loss: 2.412200541496277
  batch 500 loss: 2.377483730316162
  batch 550 loss: 2.326812560558319
  batch 600 loss: 2.2947815990448
  batch 650 loss: 2.2349229669570922
  batch 700 loss: 2.2072132635116577
  batch 750 loss: 2.1915100240707397
  batch 800 loss: 2.1514303517341613
  batch 850 loss: 2.133877453804016
  batch 900 loss: 2.095297269821167
LOSS train 2.09530 valid 3.42940, valid PER 68.98%
EPOCH 3, Learning Rate: 0.5
  batch 50 loss: 2.01379492521286
  batch 100 loss: 2.0721610593795776
  batch 150 loss: 2.066177761554718
  batch 200 loss: 1.980912251472473
  batch 250 loss: 1.9625610709190369
  batch 300 loss: 1.9981522536277772
  batch 350 loss: 1.9798073005676269
  batch 400 loss: 1.961172001361847
  batch 450 loss: 1.9139045548439027
  batch 500 loss: 1.879215965270996
  batch 550 loss: 1.8732745027542115
  batch 600 loss: 1.8311454343795777
  batch 650 loss: 1.8422143363952637
  batch 700 loss: 1.8199478101730346
  batch 750 loss: 1.83502094745636
  batch 800 loss: 1.8483913564682006
  batch 850 loss: 1.7795110869407653
  batch 900 loss: 1.8079154300689697
LOSS train 1.80792 valid 3.31443, valid PER 55.53%
EPOCH 4, Learning Rate: 0.5
  batch 50 loss: 1.8055401372909545
  batch 100 loss: 1.713869879245758
  batch 150 loss: 1.739112467765808
  batch 200 loss: 1.7516316366195679
  batch 250 loss: 1.7277951312065125
  batch 300 loss: 1.7068260073661805
  batch 350 loss: 1.7151960587501527
  batch 400 loss: 1.659996678829193
  batch 450 loss: 1.6720019555091858
  batch 500 loss: 1.7202238512039185
  batch 550 loss: 1.6536832499504088
  batch 600 loss: 1.6491147255897523
  batch 650 loss: 1.687172656059265
  batch 700 loss: 1.700322415828705
  batch 750 loss: 1.6489971780776977
  batch 800 loss: 1.62957706451416
  batch 850 loss: 1.6050391936302184
  batch 900 loss: 1.5844155406951905
LOSS train 1.58442 valid 3.35788, valid PER 43.73%
EPOCH 5, Learning Rate: 0.5
  batch 50 loss: 1.5956169509887694
  batch 100 loss: 1.5764326190948486
  batch 150 loss: 1.5601902866363526
  batch 200 loss: 1.606674587726593
  batch 250 loss: 1.5295948386192322
  batch 300 loss: 1.5783126974105834
  batch 350 loss: 1.5329960584640503
  batch 400 loss: 1.5209944224357606
  batch 450 loss: 1.4943255257606507
  batch 500 loss: 1.5032120418548585
  batch 550 loss: 1.5208118271827697
  batch 600 loss: 1.5257970070838929
  batch 650 loss: 1.5193521928787233
  batch 700 loss: 1.5093740296363831
  batch 750 loss: 1.4849540066719056
  batch 800 loss: 1.5149075675010681
  batch 850 loss: 1.512921073436737
  batch 900 loss: 1.4770184564590454
LOSS train 1.47702 valid 3.31093, valid PER 38.63%
EPOCH 6, Learning Rate: 0.5
  batch 50 loss: 1.4340091347694397
  batch 100 loss: 1.4617347478866578
  batch 150 loss: 1.457954318523407
  batch 200 loss: 1.412525463104248
  batch 250 loss: 1.4531604981422424
  batch 300 loss: 1.4696018028259277
  batch 350 loss: 1.4563572645187377
  batch 400 loss: 1.401055006980896
  batch 450 loss: 1.431083152294159
  batch 500 loss: 1.375106430053711
  batch 550 loss: 1.4093630719184875
  batch 600 loss: 1.409904522895813
  batch 650 loss: 1.3960908246040344
  batch 700 loss: 1.4005529808998107
  batch 750 loss: 1.4275366926193238
  batch 800 loss: 1.3924595093727112
  batch 850 loss: 1.4423466515541077
  batch 900 loss: 1.4248720407485962
LOSS train 1.42487 valid 3.26120, valid PER 36.70%
EPOCH 7, Learning Rate: 0.5
  batch 50 loss: 1.3486963105201721
  batch 100 loss: 1.4066981148719788
  batch 150 loss: 1.3282316470146178
  batch 200 loss: 1.3707925629615785
  batch 250 loss: 1.3919591522216797
  batch 300 loss: 1.359334546327591
  batch 350 loss: 1.3844602417945862
  batch 400 loss: 1.3391889905929566
  batch 450 loss: 1.3303116619586945
  batch 500 loss: 1.3491558599472047
  batch 550 loss: 1.3164429795742034
  batch 600 loss: 1.3327978873252868
  batch 650 loss: 1.30996808052063
  batch 700 loss: 1.3692484736442565
  batch 750 loss: 1.3219330668449403
  batch 800 loss: 1.310646529197693
  batch 850 loss: 1.3147722387313843
  batch 900 loss: 1.3088087368011474
LOSS train 1.30881 valid 3.41304, valid PER 34.36%
EPOCH 8, Learning Rate: 0.5
  batch 50 loss: 1.3230347561836242
  batch 100 loss: 1.267252322435379
  batch 150 loss: 1.3173344182968139
  batch 200 loss: 1.3198353409767152
  batch 250 loss: 1.2717320787906647
  batch 300 loss: 1.244703949689865
  batch 350 loss: 1.2776368927955628
  batch 400 loss: 1.2746348142623902
  batch 450 loss: 1.34353933095932
  batch 500 loss: 1.2945501172542573
  batch 550 loss: 1.278406172990799
  batch 600 loss: 1.2490013659000396
  batch 650 loss: 1.2781469476222993
  batch 700 loss: 1.2735976266860962
  batch 750 loss: 1.291494572162628
  batch 800 loss: 1.2817041730880738
  batch 850 loss: 1.2654147458076477
  batch 900 loss: 1.2610882496833802
LOSS train 1.26109 valid 3.29334, valid PER 32.60%
EPOCH 9, Learning Rate: 0.5
  batch 50 loss: 1.231653757095337
  batch 100 loss: 1.2083969354629516
  batch 150 loss: 1.21945197224617
  batch 200 loss: 1.221662392616272
  batch 250 loss: 1.2086860477924346
  batch 300 loss: 1.2040240001678466
  batch 350 loss: 1.2169146430492401
  batch 400 loss: 1.2437663853168488
  batch 450 loss: 1.252392691373825
  batch 500 loss: 1.2137844145298005
  batch 550 loss: 1.2217546379566193
  batch 600 loss: 1.2798029255867005
  batch 650 loss: 1.2467118942737578
  batch 700 loss: 1.2166703832149506
  batch 750 loss: 1.2204706025123597
  batch 800 loss: 1.2871425116062165
  batch 850 loss: 1.2413465297222137
  batch 900 loss: 1.2099053382873535
LOSS train 1.20991 valid 3.35966, valid PER 30.53%
EPOCH 10, Learning Rate: 0.5
  batch 50 loss: 1.2154175591468812
  batch 100 loss: 1.2192782855033875
  batch 150 loss: 1.2267580211162568
  batch 200 loss: 1.2050388383865356
  batch 250 loss: 1.18174889087677
  batch 300 loss: 1.1977730333805083
  batch 350 loss: 1.1699509370326995
  batch 400 loss: 1.1644496285915376
  batch 450 loss: 1.1528130042552949
  batch 500 loss: 1.1713278496265411
  batch 550 loss: 1.1887869441509247
  batch 600 loss: 1.1759453225135803
  batch 650 loss: 1.2211116981506347
  batch 700 loss: 1.1986342453956604
  batch 750 loss: 1.2081683838367463
  batch 800 loss: 1.211589620113373
  batch 850 loss: 1.1739641642570495
  batch 900 loss: 1.1603287398815154
LOSS train 1.16033 valid 3.40137, valid PER 31.28%
EPOCH 11, Learning Rate: 0.5
  batch 50 loss: 1.1819969141483306
  batch 100 loss: 1.1693959796428681
  batch 150 loss: 1.14899640083313
  batch 200 loss: 1.117664077281952
  batch 250 loss: 1.1409942352771758
  batch 300 loss: 1.1281325173377992
  batch 350 loss: 1.1801094603538513
  batch 400 loss: 1.146227754354477
  batch 450 loss: 1.1373534214496612
  batch 500 loss: 1.1471620273590089
  batch 550 loss: 1.1522482669353484
  batch 600 loss: 1.177044723033905
  batch 650 loss: 1.143780916929245
  batch 700 loss: 1.2458676600456238
  batch 750 loss: 1.1673850297927857
  batch 800 loss: 1.1451450967788697
  batch 850 loss: 1.1639712071418762
  batch 900 loss: 1.1814353239536286
LOSS train 1.18144 valid 3.36455, valid PER 29.84%
EPOCH 12, Learning Rate: 0.5
  batch 50 loss: 1.0914726567268371
  batch 100 loss: 1.0825888884067536
  batch 150 loss: 1.117333562374115
  batch 200 loss: 1.1518226158618927
  batch 250 loss: 1.1139919531345368
  batch 300 loss: 1.1507221412658692
  batch 350 loss: 1.1306907200813294
  batch 400 loss: 1.1550138545036317
  batch 450 loss: 1.1229689300060273
  batch 500 loss: 1.1345727229118348
  batch 550 loss: 1.1393379974365234
  batch 600 loss: 1.121695306301117
  batch 650 loss: 1.1109275448322296
  batch 700 loss: 1.147217231988907
  batch 750 loss: 1.1526715385913848
  batch 800 loss: 1.0781230866909026
  batch 850 loss: 1.1138796937465667
  batch 900 loss: 1.1431544947624206
LOSS train 1.14315 valid 3.46671, valid PER 29.21%
EPOCH 13, Learning Rate: 0.5
  batch 50 loss: 1.0703862619400024
  batch 100 loss: 1.0867511451244354
  batch 150 loss: 1.1201135861873626
  batch 200 loss: 1.055071586370468
  batch 250 loss: 1.0887592136859894
  batch 300 loss: 1.1238733589649201
  batch 350 loss: 1.0610796618461609
  batch 400 loss: 1.0831981074810029
  batch 450 loss: 1.1019196045398711
  batch 500 loss: 1.0920148408412933
  batch 550 loss: 1.1496626031398773
  batch 600 loss: 1.1210860621929168
  batch 650 loss: 1.105781753063202
  batch 700 loss: 1.118805124759674
  batch 750 loss: 1.063561578989029
  batch 800 loss: 1.089876880645752
  batch 850 loss: 1.0932469773292541
  batch 900 loss: 1.109034889936447
LOSS train 1.10903 valid 3.49564, valid PER 28.62%
EPOCH 14, Learning Rate: 0.5
  batch 50 loss: 1.0673550736904145
  batch 100 loss: 1.0351081991195679
  batch 150 loss: 1.0800613677501678
  batch 200 loss: 1.0581358206272125
  batch 250 loss: 1.0711477053165437
  batch 300 loss: 1.03093670129776
  batch 350 loss: 1.0578119587898254
  batch 400 loss: 1.0864345729351044
  batch 450 loss: 1.049652512073517
  batch 500 loss: 1.047279051542282
  batch 550 loss: 1.0691441679000855
  batch 600 loss: 1.0457216560840608
  batch 650 loss: 1.0874529719352721
  batch 700 loss: 1.0930577147006988
  batch 750 loss: 1.0704166662693024
  batch 800 loss: 1.0623035001754761
  batch 850 loss: 1.1098183131217956
  batch 900 loss: 1.0707270526885986
LOSS train 1.07073 valid 3.39973, valid PER 27.86%
EPOCH 15, Learning Rate: 0.5
  batch 50 loss: 0.9983985662460327
  batch 100 loss: 1.035803985595703
  batch 150 loss: 1.0327408826351165
  batch 200 loss: 1.075087537765503
  batch 250 loss: 1.052253257036209
  batch 300 loss: 1.0415233051776887
  batch 350 loss: 1.0311606597900391
  batch 400 loss: 1.0481398916244506
  batch 450 loss: 1.0321486139297484
  batch 500 loss: 1.039398181438446
  batch 550 loss: 1.0971221113204956
  batch 600 loss: 1.0748680186271669
  batch 650 loss: 1.025349154472351
  batch 700 loss: 1.0292847669124603
  batch 750 loss: 1.0635119986534118
  batch 800 loss: 1.031267418861389
  batch 850 loss: 1.0256075322628022
  batch 900 loss: 1.0013318526744843
LOSS train 1.00133 valid 3.39609, valid PER 27.47%
EPOCH 16, Learning Rate: 0.5
  batch 50 loss: 1.01535595536232
  batch 100 loss: 1.0090800285339356
  batch 150 loss: 1.0315821778774261
  batch 200 loss: 1.0495525228977203
  batch 250 loss: 1.0181322240829467
  batch 300 loss: 1.0290229630470276
  batch 350 loss: 1.015071896314621
  batch 400 loss: 1.0045556890964509
  batch 450 loss: 1.0204256081581116
  batch 500 loss: 1.0231441342830658
  batch 550 loss: 0.9937810873985291
  batch 600 loss: 1.0381170868873597
  batch 650 loss: 1.0289088082313538
  batch 700 loss: 1.0135767996311187
  batch 750 loss: 1.0248249971866608
  batch 800 loss: 1.0119423401355743
  batch 850 loss: 1.0070483577251435
  batch 900 loss: 1.030289855003357
LOSS train 1.03029 valid 3.47614, valid PER 27.33%
EPOCH 17, Learning Rate: 0.5
  batch 50 loss: 1.0394751954078674
  batch 100 loss: 0.9442394471168518
  batch 150 loss: 1.023736721277237
  batch 200 loss: 0.9653845429420471
  batch 250 loss: 1.0216841530799865
  batch 300 loss: 0.99129523396492
  batch 350 loss: 1.0088015103340149
  batch 400 loss: 0.9920714962482452
  batch 450 loss: 0.986828590631485
  batch 500 loss: 1.0033409643173217
  batch 550 loss: 0.9829907310009003
  batch 600 loss: 1.0499313819408416
  batch 650 loss: 0.9891193759441376
  batch 700 loss: 0.9955972492694855
  batch 750 loss: 0.9540563929080963
  batch 800 loss: 1.0214073264598846
  batch 850 loss: 0.9982897579669953
  batch 900 loss: 1.0105783879756927
LOSS train 1.01058 valid 3.49742, valid PER 26.79%
EPOCH 18, Learning Rate: 0.5
  batch 50 loss: 0.9653433156013489
  batch 100 loss: 0.9760160720348359
  batch 150 loss: 1.0211690938472748
  batch 200 loss: 0.9730399203300476
  batch 250 loss: 0.937970484495163
  batch 300 loss: 0.9626199436187745
  batch 350 loss: 0.9403381574153901
  batch 400 loss: 0.9500197112560272
  batch 450 loss: 1.0007890903949737
  batch 500 loss: 0.9933908081054688
  batch 550 loss: 1.0073162186145783
  batch 600 loss: 0.9910415935516358
  batch 650 loss: 0.9795033156871795
  batch 700 loss: 0.9658850181102753
  batch 750 loss: 0.9925890028476715
  batch 800 loss: 0.9983805406093598
  batch 850 loss: 1.0001951718330384
  batch 900 loss: 0.9886316633224488
LOSS train 0.98863 valid 3.38103, valid PER 26.61%
EPOCH 19, Learning Rate: 0.5
  batch 50 loss: 0.9571721863746643
  batch 100 loss: 0.9723582518100738
  batch 150 loss: 0.9495702922344208
  batch 200 loss: 0.9604765689373016
  batch 250 loss: 0.9788655650615692
  batch 300 loss: 0.9850446736812591
  batch 350 loss: 0.9529481387138367
  batch 400 loss: 0.9538274836540223
  batch 450 loss: 0.9243854260444642
  batch 500 loss: 0.9603609752655029
  batch 550 loss: 0.9737413704395295
  batch 600 loss: 0.9941953039169311
  batch 650 loss: 0.9997963237762452
  batch 700 loss: 0.9851405417919159
  batch 750 loss: 0.9248756790161132
  batch 800 loss: 0.9473901438713074
  batch 850 loss: 0.9946715712547303
  batch 900 loss: 0.9557861292362213
LOSS train 0.95579 valid 3.39573, valid PER 26.03%
EPOCH 20, Learning Rate: 0.5
  batch 50 loss: 0.8884919369220734
  batch 100 loss: 0.9333491909503937
  batch 150 loss: 0.9221826136112213
  batch 200 loss: 0.932759827375412
  batch 250 loss: 0.9606947100162506
  batch 300 loss: 0.9626254594326019
  batch 350 loss: 0.9374542117118836
  batch 400 loss: 0.9423670375347137
  batch 450 loss: 0.9339106047153473
  batch 500 loss: 0.9602673840522766
  batch 550 loss: 0.9651889526844024
  batch 600 loss: 0.942059645652771
  batch 650 loss: 0.9521439599990845
  batch 700 loss: 0.9426699292659759
  batch 750 loss: 0.9347123861312866
  batch 800 loss: 0.9552938961982727
  batch 850 loss: 0.9620060908794403
  batch 900 loss: 0.9333122146129608
LOSS train 0.93331 valid 3.59905, valid PER 25.78%
Training finished in 6.0 minutes.
Model saved to checkpoints/20231209_162647/model_6
Loading model from checkpoints/20231209_162647/model_6
SUB: 17.36%, DEL: 19.59%, INS: 1.10%, COR: 63.05%, PER: 38.05%
