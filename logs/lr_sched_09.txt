Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.9, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 166952
EPOCH 1, Learning Rate: 0.9
  batch 50 loss: 4.3107725572586055
  batch 100 loss: 3.0659221696853636
  batch 150 loss: 2.8584711265563967
  batch 200 loss: 2.712234282493591
  batch 250 loss: 2.680891637802124
  batch 300 loss: 2.5145944452285764
  batch 350 loss: 2.3051070189476013
  batch 400 loss: 2.2243217539787294
  batch 450 loss: 2.112117254734039
  batch 500 loss: 2.060435674190521
  batch 550 loss: 1.9875634479522706
  batch 600 loss: 1.9331533026695251
  batch 650 loss: 1.8552847003936768
  batch 700 loss: 1.8572390389442444
  batch 750 loss: 1.8048618841171264
  batch 800 loss: 1.7642785215377808
  batch 850 loss: 1.7427554798126221
  batch 900 loss: 1.7133480358123778
avg val loss: 1.627693772315979
LOSS train 1.71335 valid 1.62769, valid PER 62.28%
EPOCH 2, Learning Rate: 0.9
  batch 50 loss: 1.6864711809158326
  batch 100 loss: 1.6261856150627136
  batch 150 loss: 1.6088057613372804
  batch 200 loss: 1.6414454698562622
  batch 250 loss: 1.6316103768348693
  batch 300 loss: 1.5849945974349975
  batch 350 loss: 1.5008884000778198
  batch 400 loss: 1.5433165907859803
  batch 450 loss: 1.480406346321106
  batch 500 loss: 1.5122869658470153
  batch 550 loss: 1.5112978076934815
  batch 600 loss: 1.440139434337616
  batch 650 loss: 1.4691038870811461
  batch 700 loss: 1.447001383304596
  batch 750 loss: 1.4309325432777404
  batch 800 loss: 1.3984707140922545
  batch 850 loss: 1.3978560519218446
  batch 900 loss: 1.4550533199310303
avg val loss: 1.3122596740722656
LOSS train 1.45505 valid 1.31226, valid PER 42.63%
EPOCH 3, Learning Rate: 0.9
  batch 50 loss: 1.399793930053711
  batch 100 loss: 1.3461738753318786
  batch 150 loss: 1.372511830329895
  batch 200 loss: 1.3361678004264832
  batch 250 loss: 1.3578875017166139
  batch 300 loss: 1.3339702463150025
  batch 350 loss: 1.3711723184585571
  batch 400 loss: 1.3451206731796264
  batch 450 loss: 1.3291885948181152
  batch 500 loss: 1.303242585659027
  batch 550 loss: 1.3064681553840638
  batch 600 loss: 1.2879473233222962
  batch 650 loss: 1.2373098266124725
  batch 700 loss: 1.2981296586990356
  batch 750 loss: 1.3524699532985687
  batch 800 loss: 1.2846140122413636
  batch 850 loss: 1.2947396802902222
  batch 900 loss: 1.2456932115554809
avg val loss: 1.2151691913604736
LOSS train 1.24569 valid 1.21517, valid PER 37.64%
EPOCH 4, Learning Rate: 0.9
  batch 50 loss: 1.2680642342567443
  batch 100 loss: 1.2665049052238464
  batch 150 loss: 1.2512006676197052
  batch 200 loss: 1.2875742959976195
  batch 250 loss: 1.3013727784156799
  batch 300 loss: 1.2981819438934326
  batch 350 loss: 1.2224397945404053
  batch 400 loss: 1.297307561635971
  batch 450 loss: 1.2134098100662232
  batch 500 loss: 1.2025345158576966
  batch 550 loss: 1.2345876812934875
  batch 600 loss: 1.2773035728931428
  batch 650 loss: 1.2352946746349334
  batch 700 loss: 1.2317349767684938
  batch 750 loss: 1.1806918132305144
  batch 800 loss: 1.156140809059143
  batch 850 loss: 1.2060410261154175
  batch 900 loss: 1.2429353189468384
avg val loss: 1.155539631843567
LOSS train 1.24294 valid 1.15554, valid PER 36.90%
EPOCH 5, Learning Rate: 0.9
  batch 50 loss: 1.167441761493683
  batch 100 loss: 1.1786795628070832
  batch 150 loss: 1.2311802506446838
  batch 200 loss: 1.1466951727867127
  batch 250 loss: 1.1734918189048766
  batch 300 loss: 1.1845140624046326
  batch 350 loss: 1.2083428943157195
  batch 400 loss: 1.1665376019477844
  batch 450 loss: 1.1653997611999511
  batch 500 loss: 1.1779221832752227
  batch 550 loss: 1.154323867559433
  batch 600 loss: 1.24341210603714
  batch 650 loss: 1.167712014913559
  batch 700 loss: 1.2055696475505828
  batch 750 loss: 1.1261403834819794
  batch 800 loss: 1.1565322721004485
  batch 850 loss: 1.1672291195392608
  batch 900 loss: 1.185703444480896
avg val loss: 1.1063653230667114
LOSS train 1.18570 valid 1.10637, valid PER 35.68%
EPOCH 6, Learning Rate: 0.9
  batch 50 loss: 1.175099742412567
  batch 100 loss: 1.156243703365326
  batch 150 loss: 1.1175312626361846
  batch 200 loss: 1.1287526595592499
  batch 250 loss: 1.1302384972572326
  batch 300 loss: 1.140381360054016
  batch 350 loss: 1.1285699999332428
  batch 400 loss: 1.1310624969005585
  batch 450 loss: 1.1536158573627473
  batch 500 loss: 1.121009556055069
  batch 550 loss: 1.1404072439670563
  batch 600 loss: 1.0950912153720855
  batch 650 loss: 1.1097813189029693
  batch 700 loss: 1.1120821928977966
  batch 750 loss: 1.1185652375221253
  batch 800 loss: 1.0895765244960784
  batch 850 loss: 1.0801478254795074
  batch 900 loss: 1.1253407537937163
avg val loss: 1.0814772844314575
LOSS train 1.12534 valid 1.08148, valid PER 34.44%
EPOCH 7, Learning Rate: 0.9
  batch 50 loss: 1.0869616627693177
  batch 100 loss: 1.0941359984874726
  batch 150 loss: 1.0822434628009796
  batch 200 loss: 1.0686626887321473
  batch 250 loss: 1.07815385222435
  batch 300 loss: 1.0779834640026094
  batch 350 loss: 1.0946220934391022
  batch 400 loss: 1.0949607646465302
  batch 450 loss: 1.0780448722839355
  batch 500 loss: 1.0780056262016295
  batch 550 loss: 1.0740014791488648
  batch 600 loss: 1.0781013834476472
  batch 650 loss: 1.0583592534065247
  batch 700 loss: 1.0870401060581207
  batch 750 loss: 1.079136095046997
  batch 800 loss: 1.0773165941238403
  batch 850 loss: 1.0750300812721252
  batch 900 loss: 1.1207770943641662
avg val loss: 1.0395166873931885
LOSS train 1.12078 valid 1.03952, valid PER 33.95%
EPOCH 8, Learning Rate: 0.9
  batch 50 loss: 1.03868443608284
  batch 100 loss: 1.0529338598251343
  batch 150 loss: 1.0382932794094086
  batch 200 loss: 1.0335876393318175
  batch 250 loss: 1.0463007855415345
  batch 300 loss: 0.9895391488075256
  batch 350 loss: 1.0736142086982727
  batch 400 loss: 1.0363628673553467
  batch 450 loss: 1.070872564315796
  batch 500 loss: 1.090353125333786
  batch 550 loss: 1.0239504051208497
  batch 600 loss: 1.0519164180755616
  batch 650 loss: 1.0874280881881715
  batch 700 loss: 1.0220255005359649
  batch 750 loss: 1.05735484957695
  batch 800 loss: 1.060697194337845
  batch 850 loss: 1.0340019929409028
  batch 900 loss: 1.0388613486289977
avg val loss: 1.0257847309112549
LOSS train 1.03886 valid 1.02578, valid PER 32.69%
EPOCH 9, Learning Rate: 0.9
  batch 50 loss: 0.9796264493465423
  batch 100 loss: 1.040086041688919
  batch 150 loss: 1.0562712502479554
  batch 200 loss: 1.0119530618190766
  batch 250 loss: 1.045384041070938
  batch 300 loss: 1.0307603394985199
  batch 350 loss: 1.0749893951416016
  batch 400 loss: 1.012242897748947
  batch 450 loss: 1.0394499838352202
  batch 500 loss: 1.012319144010544
  batch 550 loss: 1.0672245490550996
  batch 600 loss: 1.0602023386955262
  batch 650 loss: 1.0436788237094878
  batch 700 loss: 1.0456908297538758
  batch 750 loss: 1.023006238937378
  batch 800 loss: 1.033931441307068
  batch 850 loss: 1.0572396743297576
  batch 900 loss: 1.0298954153060913
avg val loss: 1.0388455390930176
LOSS train 1.02990 valid 1.03885, valid PER 33.56%
EPOCH 10, Learning Rate: 0.45
  batch 50 loss: 0.9395591437816619
  batch 100 loss: 0.9414259541034699
  batch 150 loss: 0.9519520616531372
  batch 200 loss: 0.9585463809967041
  batch 250 loss: 0.9397094821929932
  batch 300 loss: 0.8968183958530426
  batch 350 loss: 0.9361345851421357
  batch 400 loss: 0.8863147580623627
  batch 450 loss: 0.9012433516979218
  batch 500 loss: 0.9329814052581787
  batch 550 loss: 0.933016700744629
  batch 600 loss: 0.9056173634529113
  batch 650 loss: 0.9192631113529205
  batch 700 loss: 0.9307037556171417
  batch 750 loss: 0.8855901825428009
  batch 800 loss: 0.9176299023628235
  batch 850 loss: 0.9333893847465515
  batch 900 loss: 0.9165170705318451
avg val loss: 0.9596813917160034
LOSS train 0.91652 valid 0.95968, valid PER 31.19%
EPOCH 11, Learning Rate: 0.45
  batch 50 loss: 0.8804800248146057
  batch 100 loss: 0.8474414455890655
  batch 150 loss: 0.8578050661087037
  batch 200 loss: 0.9091357278823853
  batch 250 loss: 0.9251330161094665
  batch 300 loss: 0.8660281860828399
  batch 350 loss: 0.8849163424968719
  batch 400 loss: 0.893470231294632
  batch 450 loss: 0.8916435348987579
  batch 500 loss: 0.8707292342185974
  batch 550 loss: 0.8948356246948242
  batch 600 loss: 0.8817117238044738
  batch 650 loss: 0.9382759082317352
  batch 700 loss: 0.854706678390503
  batch 750 loss: 0.8634843230247498
  batch 800 loss: 0.9145084834098816
  batch 850 loss: 0.9418462884426116
  batch 900 loss: 0.9181622803211212
avg val loss: 0.9282714128494263
LOSS train 0.91816 valid 0.92827, valid PER 30.05%
EPOCH 12, Learning Rate: 0.45
  batch 50 loss: 0.867970095872879
  batch 100 loss: 0.8587658965587616
  batch 150 loss: 0.8243957996368408
  batch 200 loss: 0.8444362634420395
  batch 250 loss: 0.8884356570243835
  batch 300 loss: 0.8757275629043579
  batch 350 loss: 0.8779557323455811
  batch 400 loss: 0.9084533417224884
  batch 450 loss: 0.8737841522693635
  batch 500 loss: 0.9065589535236359
  batch 550 loss: 0.8300452959537507
  batch 600 loss: 0.8471022093296051
  batch 650 loss: 0.9060651409626007
  batch 700 loss: 0.9030006074905396
  batch 750 loss: 0.8581553840637207
  batch 800 loss: 0.8654394793510437
  batch 850 loss: 0.9106004607677459
  batch 900 loss: 0.915591629743576
avg val loss: 0.940090000629425
LOSS train 0.91559 valid 0.94009, valid PER 30.84%
EPOCH 13, Learning Rate: 0.225
  batch 50 loss: 0.8256743741035462
  batch 100 loss: 0.85420539021492
  batch 150 loss: 0.7977034175395965
  batch 200 loss: 0.821782773733139
  batch 250 loss: 0.8147606998682022
  batch 300 loss: 0.8063443493843079
  batch 350 loss: 0.8245710265636444
  batch 400 loss: 0.8323282718658447
  batch 450 loss: 0.8339042329788208
  batch 500 loss: 0.8064810264110566
  batch 550 loss: 0.8350140166282654
  batch 600 loss: 0.8178826284408569
  batch 650 loss: 0.8244878709316253
  batch 700 loss: 0.828088014125824
  batch 750 loss: 0.7903226912021637
  batch 800 loss: 0.8015976119041442
  batch 850 loss: 0.8374530887603759
  batch 900 loss: 0.8351985788345337
avg val loss: 0.9023391604423523
LOSS train 0.83520 valid 0.90234, valid PER 29.11%
EPOCH 14, Learning Rate: 0.225
  batch 50 loss: 0.8050882136821746
  batch 100 loss: 0.7895062494277955
  batch 150 loss: 0.7937907922267914
  batch 200 loss: 0.79318066239357
  batch 250 loss: 0.8010762512683869
  batch 300 loss: 0.8408313310146331
  batch 350 loss: 0.7682012283802032
  batch 400 loss: 0.8021137356758118
  batch 450 loss: 0.8070000386238099
  batch 500 loss: 0.8140649831295014
  batch 550 loss: 0.8183200645446778
  batch 600 loss: 0.7784482729434967
  batch 650 loss: 0.8192408311367035
  batch 700 loss: 0.8235274398326874
  batch 750 loss: 0.7962823140621186
  batch 800 loss: 0.7665627944469452
  batch 850 loss: 0.8135241007804871
  batch 900 loss: 0.7935826206207275
avg val loss: 0.8937966823577881
LOSS train 0.79358 valid 0.89380, valid PER 28.82%
EPOCH 15, Learning Rate: 0.225
  batch 50 loss: 0.7884116423130035
  batch 100 loss: 0.7659654033184051
  batch 150 loss: 0.7784043645858765
  batch 200 loss: 0.8249468231201171
  batch 250 loss: 0.7954655492305756
  batch 300 loss: 0.7813295567035675
  batch 350 loss: 0.7704023838043212
  batch 400 loss: 0.7794216358661652
  batch 450 loss: 0.7863425767421722
  batch 500 loss: 0.7476213014125824
  batch 550 loss: 0.8071942555904389
  batch 600 loss: 0.8244956266880036
  batch 650 loss: 0.8072134959697723
  batch 700 loss: 0.806627858877182
  batch 750 loss: 0.8101595735549927
  batch 800 loss: 0.7784158754348754
  batch 850 loss: 0.7709464871883392
  batch 900 loss: 0.8079220223426818
avg val loss: 0.9014883041381836
LOSS train 0.80792 valid 0.90149, valid PER 29.24%
EPOCH 16, Learning Rate: 0.1125
  batch 50 loss: 0.785934089422226
  batch 100 loss: 0.7653609251976013
  batch 150 loss: 0.7657733565568924
  batch 200 loss: 0.7453111135959625
  batch 250 loss: 0.7693744122982025
  batch 300 loss: 0.7528587824106217
  batch 350 loss: 0.7773017954826354
  batch 400 loss: 0.7845050001144409
  batch 450 loss: 0.786390733718872
  batch 500 loss: 0.7403709018230438
  batch 550 loss: 0.7631018185615539
  batch 600 loss: 0.7495680975914002
  batch 650 loss: 0.7638761270046234
  batch 700 loss: 0.7476165294647217
  batch 750 loss: 0.7594872164726257
  batch 800 loss: 0.7559518873691559
  batch 850 loss: 0.748942518234253
  batch 900 loss: 0.760601316690445
avg val loss: 0.89581298828125
LOSS train 0.76060 valid 0.89581, valid PER 28.49%
EPOCH 17, Learning Rate: 0.05625
  batch 50 loss: 0.7786819416284562
  batch 100 loss: 0.7567741906642914
  batch 150 loss: 0.7372252374887467
  batch 200 loss: 0.7296762609481812
  batch 250 loss: 0.7514467358589172
  batch 300 loss: 0.7603975546360016
  batch 350 loss: 0.735215607881546
  batch 400 loss: 0.7698595583438873
  batch 450 loss: 0.7549097847938537
  batch 500 loss: 0.7202959835529328
  batch 550 loss: 0.731183757185936
  batch 600 loss: 0.7727933371067047
  batch 650 loss: 0.7315207523107529
  batch 700 loss: 0.7206727284193039
  batch 750 loss: 0.7198531579971313
  batch 800 loss: 0.7147578686475754
  batch 850 loss: 0.74396908223629
  batch 900 loss: 0.7126423275470734
avg val loss: 0.8823912739753723
LOSS train 0.71264 valid 0.88239, valid PER 28.22%
EPOCH 18, Learning Rate: 0.05625
  batch 50 loss: 0.727094212770462
  batch 100 loss: 0.7377171802520752
  batch 150 loss: 0.7462620139122009
  batch 200 loss: 0.7245979017019272
  batch 250 loss: 0.7498725938796997
  batch 300 loss: 0.71154694378376
  batch 350 loss: 0.7474061858654022
  batch 400 loss: 0.7366161811351776
  batch 450 loss: 0.760882511138916
  batch 500 loss: 0.7264491903781891
  batch 550 loss: 0.7426095545291901
  batch 600 loss: 0.7128483408689499
  batch 650 loss: 0.7143522626161576
  batch 700 loss: 0.7630318343639374
  batch 750 loss: 0.7050641733407974
  batch 800 loss: 0.726182758808136
  batch 850 loss: 0.7308971494436264
  batch 900 loss: 0.7523973727226257
avg val loss: 0.88157057762146
LOSS train 0.75240 valid 0.88157, valid PER 27.95%
EPOCH 19, Learning Rate: 0.05625
  batch 50 loss: 0.7091266918182373
  batch 100 loss: 0.6985637772083283
  batch 150 loss: 0.7243316161632538
  batch 200 loss: 0.7148342216014862
  batch 250 loss: 0.7392812049388886
  batch 300 loss: 0.7387593770027161
  batch 350 loss: 0.7189702612161636
  batch 400 loss: 0.7388601732254029
  batch 450 loss: 0.7328235280513763
  batch 500 loss: 0.7289248532056809
  batch 550 loss: 0.722825801372528
  batch 600 loss: 0.733120813369751
  batch 650 loss: 0.7728137934207916
  batch 700 loss: 0.703693472146988
  batch 750 loss: 0.7074351960420608
  batch 800 loss: 0.7288041365146637
  batch 850 loss: 0.7321943831443787
  batch 900 loss: 0.7251995062828064
avg val loss: 0.8827840089797974
LOSS train 0.72520 valid 0.88278, valid PER 27.85%
EPOCH 20, Learning Rate: 0.028125
  batch 50 loss: 0.716850563287735
  batch 100 loss: 0.7232488638162613
  batch 150 loss: 0.7017352354526519
  batch 200 loss: 0.7021778291463852
  batch 250 loss: 0.7166930651664734
  batch 300 loss: 0.7243030023574829
  batch 350 loss: 0.7070553463697433
  batch 400 loss: 0.7229796648025513
  batch 450 loss: 0.7331988942623139
  batch 500 loss: 0.6988116788864136
  batch 550 loss: 0.7587266957759857
  batch 600 loss: 0.6904007130861283
  batch 650 loss: 0.7259414321184159
  batch 700 loss: 0.7144382965564727
  batch 750 loss: 0.7016101837158203
  batch 800 loss: 0.7298393356800079
  batch 850 loss: 0.7296381688117981
  batch 900 loss: 0.7142496681213379
avg val loss: 0.8809306621551514
LOSS train 0.71425 valid 0.88093, valid PER 27.73%
Training finished in 4.0 minutes.
Model saved to checkpoints/20231210_132654/model_20
Loading model from checkpoints/20231210_132654/model_20
SUB: 15.42%, DEL: 12.80%, INS: 1.63%, COR: 71.78%, PER: 29.85%
