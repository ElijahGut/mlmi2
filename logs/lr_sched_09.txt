Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.9, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 166952
EPOCH 1, Learning Rate: 0.9
  batch 50 loss: 4.3107119035720824
  batch 100 loss: 3.032239809036255
  batch 150 loss: 2.854074950218201
  batch 200 loss: 2.71101505279541
  batch 250 loss: 2.689599299430847
  batch 300 loss: 2.5429110527038574
  batch 350 loss: 2.348143820762634
  batch 400 loss: 2.242761912345886
  batch 450 loss: 2.1357297778129576
  batch 500 loss: 2.041023087501526
  batch 550 loss: 1.9893026280403137
  batch 600 loss: 1.9281310534477234
  batch 650 loss: 1.84493887424469
  batch 700 loss: 1.8478145098686218
  batch 750 loss: 1.7911250543594361
  batch 800 loss: 1.7625317764282227
  batch 850 loss: 1.7380422520637513
  batch 900 loss: 1.7233777332305908
running loss: 40.2562530040741
LOSS train 1.72338 valid 1.66710, valid PER 63.82%
EPOCH 2, Learning Rate: 0.9
  batch 50 loss: 1.6881764221191407
  batch 100 loss: 1.6207221698760987
  batch 150 loss: 1.6255516386032105
  batch 200 loss: 1.6435530757904053
  batch 250 loss: 1.618068549633026
  batch 300 loss: 1.5972283959388733
  batch 350 loss: 1.5061629700660706
  batch 400 loss: 1.5526876330375672
  batch 450 loss: 1.471558222770691
  batch 500 loss: 1.5109070825576782
  batch 550 loss: 1.5049269914627075
  batch 600 loss: 1.4410807442665101
  batch 650 loss: 1.4820857167243957
  batch 700 loss: 1.4776114058494567
  batch 750 loss: 1.4363798141479491
  batch 800 loss: 1.3953398251533509
  batch 850 loss: 1.3946534180641175
  batch 900 loss: 1.4710082936286926
running loss: 33.20873773097992
LOSS train 1.47101 valid 1.31003, valid PER 41.52%
EPOCH 3, Learning Rate: 0.9
  batch 50 loss: 1.4111774563789368
  batch 100 loss: 1.356723234653473
  batch 150 loss: 1.3674010801315308
  batch 200 loss: 1.3462173461914062
  batch 250 loss: 1.3562829720973968
  batch 300 loss: 1.327036556005478
  batch 350 loss: 1.370610795021057
  batch 400 loss: 1.3353431296348572
  batch 450 loss: 1.3412210297584535
  batch 500 loss: 1.3031595706939698
  batch 550 loss: 1.3179583489894866
  batch 600 loss: 1.2975957083702088
  batch 650 loss: 1.2597075867652894
  batch 700 loss: 1.2807504630088806
  batch 750 loss: 1.3402127361297607
  batch 800 loss: 1.248576500415802
  batch 850 loss: 1.2880510997772217
  batch 900 loss: 1.239708857536316
running loss: 30.230223298072815
LOSS train 1.23971 valid 1.22123, valid PER 38.05%
EPOCH 4, Learning Rate: 0.9
  batch 50 loss: 1.2152816212177278
  batch 100 loss: 1.225405216217041
  batch 150 loss: 1.2091783201694488
  batch 200 loss: 1.2408679497241974
  batch 250 loss: 1.2463756060600282
  batch 300 loss: 1.261883443593979
  batch 350 loss: 1.2024279844760895
  batch 400 loss: 1.237309716939926
  batch 450 loss: 1.2350205290317535
  batch 500 loss: 1.2137320256233215
  batch 550 loss: 1.2200513565540314
  batch 600 loss: 1.243989840745926
  batch 650 loss: 1.234544403553009
  batch 700 loss: 1.2035067510604858
  batch 750 loss: 1.186072552204132
  batch 800 loss: 1.140318969488144
  batch 850 loss: 1.1905401051044464
  batch 900 loss: 1.2458811116218567
running loss: 28.26539546251297
LOSS train 1.24588 valid 1.14708, valid PER 36.25%
EPOCH 5, Learning Rate: 0.9
  batch 50 loss: 1.1557960259914397
  batch 100 loss: 1.1509052658081054
  batch 150 loss: 1.200731784105301
  batch 200 loss: 1.125036140680313
  batch 250 loss: 1.1618452382087707
  batch 300 loss: 1.1453041911125184
  batch 350 loss: 1.1624085760116578
  batch 400 loss: 1.1575566279888152
  batch 450 loss: 1.1428948795795442
  batch 500 loss: 1.1625277304649353
  batch 550 loss: 1.1334943878650665
  batch 600 loss: 1.2076909685134887
  batch 650 loss: 1.1502786803245544
  batch 700 loss: 1.225174343585968
  batch 750 loss: 1.1193690359592439
  batch 800 loss: 1.1385906863212585
  batch 850 loss: 1.1668371808528901
  batch 900 loss: 1.1729178154468536
running loss: 28.66446977853775
LOSS train 1.17292 valid 1.13003, valid PER 35.66%
EPOCH 6, Learning Rate: 0.45
  batch 50 loss: 1.1122997617721557
  batch 100 loss: 1.0374605870246887
  batch 150 loss: 1.021397808790207
  batch 200 loss: 1.0490230739116668
  batch 250 loss: 1.0644739174842834
  batch 300 loss: 1.0558647155761718
  batch 350 loss: 1.0409506034851075
  batch 400 loss: 1.0629122281074523
  batch 450 loss: 1.0327984929084777
  batch 500 loss: 1.0395417320728302
  batch 550 loss: 1.0495650744438172
  batch 600 loss: 1.0122721815109252
  batch 650 loss: 1.0375868701934814
  batch 700 loss: 1.0331628727912903
  batch 750 loss: 1.0193754398822785
  batch 800 loss: 1.00729425907135
  batch 850 loss: 0.9889553129673004
  batch 900 loss: 1.0209386456012726
running loss: 24.143598556518555
LOSS train 1.02094 valid 1.00811, valid PER 32.31%
EPOCH 7, Learning Rate: 0.45
  batch 50 loss: 1.0010386741161346
  batch 100 loss: 1.0108170223236084
  batch 150 loss: 0.9908583927154541
  batch 200 loss: 0.9781367886066437
  batch 250 loss: 0.9815830683708191
  batch 300 loss: 0.9809061753749847
  batch 350 loss: 0.9758124256134033
  batch 400 loss: 0.9889303708076477
  batch 450 loss: 1.0103925609588622
  batch 500 loss: 0.9695798480510711
  batch 550 loss: 0.9701944398880005
  batch 600 loss: 0.9799435114860535
  batch 650 loss: 0.9640835130214691
  batch 700 loss: 1.014921362400055
  batch 750 loss: 0.9929001605510712
  batch 800 loss: 0.9904931366443634
  batch 850 loss: 0.9915133798122406
  batch 900 loss: 1.0387049305438996
running loss: 23.17083889245987
LOSS train 1.03870 valid 1.00281, valid PER 32.30%
EPOCH 8, Learning Rate: 0.45
  batch 50 loss: 0.9525998663902283
  batch 100 loss: 0.9684066188335418
  batch 150 loss: 0.9652698791027069
  batch 200 loss: 0.9611877858638763
  batch 250 loss: 0.9586496651172638
  batch 300 loss: 0.9126977896690369
  batch 350 loss: 0.9960912358760834
  batch 400 loss: 0.952991567850113
  batch 450 loss: 0.9890511548519134
  batch 500 loss: 1.0086797535419465
  batch 550 loss: 0.936455501317978
  batch 600 loss: 0.9742277991771698
  batch 650 loss: 1.0013215231895447
  batch 700 loss: 0.9496785819530487
  batch 750 loss: 0.969252210855484
  batch 800 loss: 0.9713178777694702
  batch 850 loss: 0.9861959493160248
  batch 900 loss: 0.9843007004261017
running loss: 23.375499665737152
LOSS train 0.98430 valid 0.98662, valid PER 31.84%
EPOCH 9, Learning Rate: 0.225
  batch 50 loss: 0.901132093667984
  batch 100 loss: 0.91385946393013
  batch 150 loss: 0.9268426406383514
  batch 200 loss: 0.8748357319831848
  batch 250 loss: 0.907805358171463
  batch 300 loss: 0.9083181834220886
  batch 350 loss: 0.932849634885788
  batch 400 loss: 0.9007363283634185
  batch 450 loss: 0.9146060872077942
  batch 500 loss: 0.8762264168262481
  batch 550 loss: 0.9139320290088654
  batch 600 loss: 0.9192658424377441
  batch 650 loss: 0.9121464014053344
  batch 700 loss: 0.8710606491565704
  batch 750 loss: 0.8890940546989441
  batch 800 loss: 0.9201828563213348
  batch 850 loss: 0.9134697473049164
  batch 900 loss: 0.8793306565284729
running loss: 22.526248514652252
LOSS train 0.87933 valid 0.95034, valid PER 30.36%
EPOCH 10, Learning Rate: 0.225
  batch 50 loss: 0.867741996049881
  batch 100 loss: 0.8937922096252442
  batch 150 loss: 0.9125181210041046
  batch 200 loss: 0.9111227333545685
  batch 250 loss: 0.8828775799274444
  batch 300 loss: 0.86308558344841
  batch 350 loss: 0.8908108329772949
  batch 400 loss: 0.8644443774223327
  batch 450 loss: 0.8581502830982208
  batch 500 loss: 0.9004513514041901
  batch 550 loss: 0.9095672392845153
  batch 600 loss: 0.8899132239818573
  batch 650 loss: 0.8796488511562347
  batch 700 loss: 0.9008403623104095
  batch 750 loss: 0.872083967924118
  batch 800 loss: 0.893416724205017
  batch 850 loss: 0.9026459991931916
  batch 900 loss: 0.9010964202880859
running loss: 20.676546037197113
LOSS train 0.90110 valid 0.93963, valid PER 30.41%
EPOCH 11, Learning Rate: 0.225
  batch 50 loss: 0.8575075483322143
  batch 100 loss: 0.8294117987155915
  batch 150 loss: 0.8395683145523072
  batch 200 loss: 0.8952922010421753
  batch 250 loss: 0.8777432811260223
  batch 300 loss: 0.8509408044815063
  batch 350 loss: 0.8653374409675598
  batch 400 loss: 0.8689558815956115
  batch 450 loss: 0.8793089032173157
  batch 500 loss: 0.8487234854698181
  batch 550 loss: 0.869755951166153
  batch 600 loss: 0.8548399579524993
  batch 650 loss: 0.9049969696998597
  batch 700 loss: 0.8423489224910736
  batch 750 loss: 0.848292988538742
  batch 800 loss: 0.8898969030380249
  batch 850 loss: 0.9081609177589417
  batch 900 loss: 0.8938536190986633
running loss: 20.66192501783371
LOSS train 0.89385 valid 0.93306, valid PER 29.76%
EPOCH 12, Learning Rate: 0.225
  batch 50 loss: 0.8621232402324677
  batch 100 loss: 0.8389817321300507
  batch 150 loss: 0.8238280093669892
  batch 200 loss: 0.8571982777118683
  batch 250 loss: 0.8576603305339813
  batch 300 loss: 0.864788681268692
  batch 350 loss: 0.8675964677333832
  batch 400 loss: 0.8706137835979462
  batch 450 loss: 0.8639667963981629
  batch 500 loss: 0.8872372162342071
  batch 550 loss: 0.8213831210136413
  batch 600 loss: 0.8511564218997956
  batch 650 loss: 0.9018621873855591
  batch 700 loss: 0.8754387187957764
  batch 750 loss: 0.8426228427886963
  batch 800 loss: 0.8450709176063538
  batch 850 loss: 0.8827881741523743
  batch 900 loss: 0.8899341499805451
running loss: 19.910393595695496
LOSS train 0.88993 valid 0.92026, valid PER 29.77%
EPOCH 13, Learning Rate: 0.225
  batch 50 loss: 0.819173104763031
  batch 100 loss: 0.8523282182216644
  batch 150 loss: 0.8281788659095765
  batch 200 loss: 0.857930701971054
  batch 250 loss: 0.852393981218338
  batch 300 loss: 0.8464391934871673
  batch 350 loss: 0.8400183880329132
  batch 400 loss: 0.8607895720005035
  batch 450 loss: 0.8415619504451751
  batch 500 loss: 0.8219340717792512
  batch 550 loss: 0.8563623547554016
  batch 600 loss: 0.8374793434143066
  batch 650 loss: 0.8566974008083343
  batch 700 loss: 0.8854401051998139
  batch 750 loss: 0.8312131643295289
  batch 800 loss: 0.8556928718090058
  batch 850 loss: 0.8738458037376404
  batch 900 loss: 0.8813941216468811
running loss: 21.00915491580963
LOSS train 0.88139 valid 0.92718, valid PER 29.66%
EPOCH 14, Learning Rate: 0.1125
  batch 50 loss: 0.8309796786308289
  batch 100 loss: 0.8199676179885864
  batch 150 loss: 0.8110579693317413
  batch 200 loss: 0.8080197322368622
  batch 250 loss: 0.8166191446781158
  batch 300 loss: 0.8479647237062454
  batch 350 loss: 0.7910754692554474
  batch 400 loss: 0.8222458171844482
  batch 450 loss: 0.8107933831214905
  batch 500 loss: 0.8170115315914154
  batch 550 loss: 0.8370177686214447
  batch 600 loss: 0.7854202449321747
  batch 650 loss: 0.8136812746524811
  batch 700 loss: 0.8352662336826324
  batch 750 loss: 0.7971880173683167
  batch 800 loss: 0.7597996664047241
  batch 850 loss: 0.8124385690689087
  batch 900 loss: 0.81182208776474
running loss: 20.166227757930756
LOSS train 0.81182 valid 0.90833, valid PER 29.12%
EPOCH 15, Learning Rate: 0.05625
  batch 50 loss: 0.8099290478229523
  batch 100 loss: 0.7789970433712006
  batch 150 loss: 0.7921501755714416
  batch 200 loss: 0.8239959990978241
  batch 250 loss: 0.8113332402706146
  batch 300 loss: 0.784428391456604
  batch 350 loss: 0.7798303854465485
  batch 400 loss: 0.7798096734285355
  batch 450 loss: 0.7750390923023224
  batch 500 loss: 0.7501453685760499
  batch 550 loss: 0.7862878984212875
  batch 600 loss: 0.8073174548149109
  batch 650 loss: 0.8007582533359527
  batch 700 loss: 0.8052164912223816
  batch 750 loss: 0.7979255580902099
  batch 800 loss: 0.7755265653133392
  batch 850 loss: 0.7689423632621765
  batch 900 loss: 0.7925209307670593
running loss: 18.55610901117325
LOSS train 0.79252 valid 0.89999, valid PER 28.62%
EPOCH 16, Learning Rate: 0.05625
  batch 50 loss: 0.8032979536056518
  batch 100 loss: 0.7612673193216324
  batch 150 loss: 0.7878464353084564
  batch 200 loss: 0.7710429835319519
  batch 250 loss: 0.8000914216041565
  batch 300 loss: 0.7827607309818267
  batch 350 loss: 0.800302208662033
  batch 400 loss: 0.802017912864685
  batch 450 loss: 0.8078295958042144
  batch 500 loss: 0.7539721930027008
  batch 550 loss: 0.7906301164627075
  batch 600 loss: 0.7637246680259705
  batch 650 loss: 0.7788483440876007
  batch 700 loss: 0.7649726939201354
  batch 750 loss: 0.7743089973926545
  batch 800 loss: 0.771256959438324
  batch 850 loss: 0.7722619849443436
  batch 900 loss: 0.7883033430576325
running loss: 18.07347184419632
LOSS train 0.78830 valid 0.89633, valid PER 28.66%
EPOCH 17, Learning Rate: 0.05625
  batch 50 loss: 0.7915066254138946
  batch 100 loss: 0.7824438828229904
  batch 150 loss: 0.7707802683115006
  batch 200 loss: 0.7648186123371125
  batch 250 loss: 0.7928069663047791
  batch 300 loss: 0.7747155541181564
  batch 350 loss: 0.7532740044593811
  batch 400 loss: 0.8028355956077575
  batch 450 loss: 0.7835876846313476
  batch 500 loss: 0.7623743212223053
  batch 550 loss: 0.7743042492866516
  batch 600 loss: 0.8126531648635864
  batch 650 loss: 0.7656172060966492
  batch 700 loss: 0.7749635970592499
  batch 750 loss: 0.7475510883331299
  batch 800 loss: 0.7643336582183838
  batch 850 loss: 0.7710820937156677
  batch 900 loss: 0.7520539528131485
running loss: 19.349671125411987
LOSS train 0.75205 valid 0.89423, valid PER 28.34%
EPOCH 18, Learning Rate: 0.028125
  batch 50 loss: 0.7738437509536743
  batch 100 loss: 0.7728217041492462
  batch 150 loss: 0.7797258090972901
  batch 200 loss: 0.7642368793487548
  batch 250 loss: 0.7818081831932068
  batch 300 loss: 0.7419246661663056
  batch 350 loss: 0.7660270321369171
  batch 400 loss: 0.747138442993164
  batch 450 loss: 0.7967571520805359
  batch 500 loss: 0.7550233966112136
  batch 550 loss: 0.7720711410045624
  batch 600 loss: 0.7484932196140289
  batch 650 loss: 0.7499173152446746
  batch 700 loss: 0.7988150012493134
  batch 750 loss: 0.7571766650676728
  batch 800 loss: 0.7421982318162919
  batch 850 loss: 0.7374593663215637
  batch 900 loss: 0.7854746019840241
running loss: 17.746538996696472
LOSS train 0.78547 valid 0.89123, valid PER 28.40%
EPOCH 19, Learning Rate: 0.028125
  batch 50 loss: 0.7491305387020111
  batch 100 loss: 0.7276692759990692
  batch 150 loss: 0.7619477128982544
  batch 200 loss: 0.7588545000553131
  batch 250 loss: 0.775554775595665
  batch 300 loss: 0.7773666787147522
  batch 350 loss: 0.7546132016181946
  batch 400 loss: 0.7597286164760589
  batch 450 loss: 0.7712672102451325
  batch 500 loss: 0.7609633004665375
  batch 550 loss: 0.7514953255653382
  batch 600 loss: 0.7536570918560028
  batch 650 loss: 0.8109186446666717
  batch 700 loss: 0.7392375147342682
  batch 750 loss: 0.7408681505918503
  batch 800 loss: 0.7714010322093964
  batch 850 loss: 0.7687077105045319
  batch 900 loss: 0.7584543907642365
running loss: 18.102386116981506
LOSS train 0.75845 valid 0.89455, valid PER 28.25%
EPOCH 20, Learning Rate: 0.0140625
  batch 50 loss: 0.7462224447727204
  batch 100 loss: 0.7488273656368256
  batch 150 loss: 0.7377137231826782
  batch 200 loss: 0.7411812794208527
  batch 250 loss: 0.7505625128746033
  batch 300 loss: 0.7563638663291932
  batch 350 loss: 0.7376316797733307
  batch 400 loss: 0.7572606730461121
  batch 450 loss: 0.7672168254852295
  batch 500 loss: 0.732723639011383
  batch 550 loss: 0.8002996516227722
  batch 600 loss: 0.7313555896282196
  batch 650 loss: 0.751501990556717
  batch 700 loss: 0.7593840897083283
  batch 750 loss: 0.7454861623048782
  batch 800 loss: 0.7830434632301331
  batch 850 loss: 0.765762392282486
  batch 900 loss: 0.7655789506435394
running loss: 18.32010108232498
LOSS train 0.76558 valid 0.89049, valid PER 28.12%
Training finished in 4.0 minutes.
Model saved to checkpoints/20231210_030534/model_20
Loading model from checkpoints/20231210_030534/model_20
SUB: 15.88%, DEL: 12.97%, INS: 1.64%, COR: 71.15%, PER: 30.49%
