Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.1, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 83496
EPOCH 1, Learning Rate: 0.1
  batch 50 loss: 7.930461525917053
  batch 100 loss: 3.324196982383728
  batch 150 loss: 3.2840675592422484
  batch 200 loss: 3.262001805305481
  batch 250 loss: 3.240431661605835
  batch 300 loss: 3.204443163871765
  batch 350 loss: 3.1850431108474733
  batch 400 loss: 3.1739808177947997
  batch 450 loss: 3.1433409738540647
  batch 500 loss: 3.094227318763733
  batch 550 loss: 3.041850004196167
  batch 600 loss: 2.97547842502594
  batch 650 loss: 2.8960157346725466
  batch 700 loss: 2.851239495277405
  batch 750 loss: 2.786358871459961
  batch 800 loss: 2.7416978454589844
  batch 850 loss: 2.7125687408447265
  batch 900 loss: 2.645939950942993
avg val loss: 2.6128005981445312
LOSS train 2.64594 valid 2.61280, valid PER 85.74%
EPOCH 2, Learning Rate: 0.1
  batch 50 loss: 2.6122821235656737
  batch 100 loss: 2.554519510269165
  batch 150 loss: 2.5080649137496946
  batch 200 loss: 2.4988253021240237
  batch 250 loss: 2.4754131555557253
  batch 300 loss: 2.453633770942688
  batch 350 loss: 2.388643355369568
  batch 400 loss: 2.4042818927764893
  batch 450 loss: 2.3587827348709105
  batch 500 loss: 2.34314555644989
  batch 550 loss: 2.3331552648544314
  batch 600 loss: 2.2834271574020386
  batch 650 loss: 2.289764232635498
  batch 700 loss: 2.2529156041145324
  batch 750 loss: 2.258949353694916
  batch 800 loss: 2.1932998132705688
  batch 850 loss: 2.177720634937286
  batch 900 loss: 2.188515319824219
avg val loss: 2.1325535774230957
LOSS train 2.18852 valid 2.13255, valid PER 80.00%
EPOCH 3, Learning Rate: 0.1
  batch 50 loss: 2.162374017238617
  batch 100 loss: 2.1184671449661256
  batch 150 loss: 2.122237033843994
  batch 200 loss: 2.1008570742607118
  batch 250 loss: 2.079116883277893
  batch 300 loss: 2.0704974699020386
  batch 350 loss: 2.0968653845787046
  batch 400 loss: 2.060224199295044
  batch 450 loss: 2.023406925201416
  batch 500 loss: 2.0218452167510987
  batch 550 loss: 1.9928473210334778
  batch 600 loss: 1.9659567093849182
  batch 650 loss: 1.9396899580955504
  batch 700 loss: 1.9702645325660706
  batch 750 loss: 1.9936825728416443
  batch 800 loss: 1.913501808643341
  batch 850 loss: 1.9523921990394593
  batch 900 loss: 1.8994140362739562
avg val loss: 1.8806275129318237
LOSS train 1.89941 valid 1.88063, valid PER 72.64%
EPOCH 4, Learning Rate: 0.1
  batch 50 loss: 1.9051442432403565
  batch 100 loss: 1.9113623046875
  batch 150 loss: 1.8389829516410827
  batch 200 loss: 1.883174500465393
  batch 250 loss: 1.882622697353363
  batch 300 loss: 1.9024671459197997
  batch 350 loss: 1.8028656792640687
  batch 400 loss: 1.8352352976799011
  batch 450 loss: 1.8348180198669433
  batch 500 loss: 1.8030636024475097
  batch 550 loss: 1.832858326435089
  batch 600 loss: 1.830328722000122
  batch 650 loss: 1.8272150444984436
  batch 700 loss: 1.769680597782135
  batch 750 loss: 1.7564523315429688
  batch 800 loss: 1.7491620039939881
  batch 850 loss: 1.7604821228981018
  batch 900 loss: 1.7972731113433837
avg val loss: 1.7056018114089966
LOSS train 1.79727 valid 1.70560, valid PER 63.64%
EPOCH 5, Learning Rate: 0.1
  batch 50 loss: 1.7374823665618897
  batch 100 loss: 1.7176789736747742
  batch 150 loss: 1.7528520560264587
  batch 200 loss: 1.6991111087799071
  batch 250 loss: 1.6945876145362855
  batch 300 loss: 1.7220778632164002
  batch 350 loss: 1.712220742702484
  batch 400 loss: 1.7202225852012634
  batch 450 loss: 1.6901614785194397
  batch 500 loss: 1.7103903794288635
  batch 550 loss: 1.6520634651184083
  batch 600 loss: 1.707797131538391
  batch 650 loss: 1.6639908480644225
  batch 700 loss: 1.6933585929870605
  batch 750 loss: 1.644218327999115
  batch 800 loss: 1.6778188157081604
  batch 850 loss: 1.6818528008460998
  batch 900 loss: 1.6582917952537537
avg val loss: 1.6081429719924927
LOSS train 1.65829 valid 1.60814, valid PER 58.94%
EPOCH 6, Learning Rate: 0.1
  batch 50 loss: 1.6654360866546631
  batch 100 loss: 1.6265296316146851
  batch 150 loss: 1.60517493724823
  batch 200 loss: 1.6303182387351989
  batch 250 loss: 1.643764729499817
  batch 300 loss: 1.596472852230072
  batch 350 loss: 1.6085339760780335
  batch 400 loss: 1.6096499466896057
  batch 450 loss: 1.6341608309745788
  batch 500 loss: 1.5935505747795105
  batch 550 loss: 1.6043516302108765
  batch 600 loss: 1.6115932941436768
  batch 650 loss: 1.6120157599449159
  batch 700 loss: 1.5929261541366577
  batch 750 loss: 1.5669882822036743
  batch 800 loss: 1.573516969680786
  batch 850 loss: 1.5677059006690979
  batch 900 loss: 1.5934435939788818
avg val loss: 1.5355052947998047
LOSS train 1.59344 valid 1.53551, valid PER 54.28%
EPOCH 7, Learning Rate: 0.1
  batch 50 loss: 1.5791744470596314
  batch 100 loss: 1.566609673500061
  batch 150 loss: 1.567494330406189
  batch 200 loss: 1.5513881731033325
  batch 250 loss: 1.5597542858123778
  batch 300 loss: 1.5216419625282287
  batch 350 loss: 1.5345506548881531
  batch 400 loss: 1.5552143621444703
  batch 450 loss: 1.5376764559745788
  batch 500 loss: 1.5206047892570496
  batch 550 loss: 1.5267604184150696
  batch 600 loss: 1.5304783248901368
  batch 650 loss: 1.5142684555053711
  batch 700 loss: 1.523649594783783
  batch 750 loss: 1.50897705078125
  batch 800 loss: 1.4928083562850951
  batch 850 loss: 1.535756070613861
  batch 900 loss: 1.551859073638916
avg val loss: 1.4626281261444092
LOSS train 1.55186 valid 1.46263, valid PER 50.96%
EPOCH 8, Learning Rate: 0.1
  batch 50 loss: 1.516807689666748
  batch 100 loss: 1.5049391651153565
  batch 150 loss: 1.503097493648529
  batch 200 loss: 1.4576081609725953
  batch 250 loss: 1.5128962516784668
  batch 300 loss: 1.436712203025818
  batch 350 loss: 1.5224317836761474
  batch 400 loss: 1.4569290590286255
  batch 450 loss: 1.5082470989227295
  batch 500 loss: 1.524851086139679
  batch 550 loss: 1.4681518268585205
  batch 600 loss: 1.4876230835914612
  batch 650 loss: 1.5227816128730773
  batch 700 loss: 1.4522737240791321
  batch 750 loss: 1.463076846599579
  batch 800 loss: 1.468176579475403
  batch 850 loss: 1.4829191851615906
  batch 900 loss: 1.4523888802528382
avg val loss: 1.4327579736709595
LOSS train 1.45239 valid 1.43276, valid PER 47.83%
EPOCH 9, Learning Rate: 0.1
  batch 50 loss: 1.3986107468605042
  batch 100 loss: 1.4752950882911682
  batch 150 loss: 1.4612822794914246
  batch 200 loss: 1.4004669427871703
  batch 250 loss: 1.4474227643013
  batch 300 loss: 1.4413019323349
  batch 350 loss: 1.4351204919815064
  batch 400 loss: 1.4381032872200012
  batch 450 loss: 1.4393405079841615
  batch 500 loss: 1.4221487164497375
  batch 550 loss: 1.4723508524894715
  batch 600 loss: 1.4383682680130006
  batch 650 loss: 1.4344091653823852
  batch 700 loss: 1.4406707048416139
  batch 750 loss: 1.4298047280311585
  batch 800 loss: 1.431631498336792
  batch 850 loss: 1.4694286060333253
  batch 900 loss: 1.4067391848564148
avg val loss: 1.3510546684265137
LOSS train 1.40674 valid 1.35105, valid PER 44.47%
EPOCH 10, Learning Rate: 0.1
  batch 50 loss: 1.3652200198173523
  batch 100 loss: 1.3809911262989045
  batch 150 loss: 1.4299686884880065
  batch 200 loss: 1.4369438648223878
  batch 250 loss: 1.4294520783424378
  batch 300 loss: 1.360480945110321
  batch 350 loss: 1.4122014236450195
  batch 400 loss: 1.3828770112991333
  batch 450 loss: 1.3758394408226013
  batch 500 loss: 1.4192625653743745
  batch 550 loss: 1.4210597395896911
  batch 600 loss: 1.3883388757705688
  batch 650 loss: 1.3666173839569091
  batch 700 loss: 1.396401629447937
  batch 750 loss: 1.3757028341293336
  batch 800 loss: 1.385560281276703
  batch 850 loss: 1.4162088298797608
  batch 900 loss: 1.410132429599762
avg val loss: 1.3468263149261475
LOSS train 1.41013 valid 1.34683, valid PER 45.24%
EPOCH 11, Learning Rate: 0.1
  batch 50 loss: 1.3547177171707154
  batch 100 loss: 1.3393822860717775
  batch 150 loss: 1.347704997062683
  batch 200 loss: 1.389716510772705
  batch 250 loss: 1.3871207594871522
  batch 300 loss: 1.3328937697410583
  batch 350 loss: 1.3616690063476562
  batch 400 loss: 1.3736090302467345
  batch 450 loss: 1.3697659373283386
  batch 500 loss: 1.3414831411838533
  batch 550 loss: 1.3635891604423522
  batch 600 loss: 1.321971867084503
  batch 650 loss: 1.4131251239776612
  batch 700 loss: 1.330276539325714
  batch 750 loss: 1.3453663492202759
  batch 800 loss: 1.3804046201705933
  batch 850 loss: 1.3980519115924834
  batch 900 loss: 1.3684191942214965
avg val loss: 1.3094102144241333
LOSS train 1.36842 valid 1.30941, valid PER 42.71%
EPOCH 12, Learning Rate: 0.1
  batch 50 loss: 1.3530942606925964
  batch 100 loss: 1.3242886924743653
  batch 150 loss: 1.3223794889450073
  batch 200 loss: 1.3504619479179383
  batch 250 loss: 1.3593001079559326
  batch 300 loss: 1.324221866130829
  batch 350 loss: 1.3486977219581604
  batch 400 loss: 1.368019573688507
  batch 450 loss: 1.3424311590194702
  batch 500 loss: 1.355385057926178
  batch 550 loss: 1.2758010029792786
  batch 600 loss: 1.2984234857559205
  batch 650 loss: 1.3729847419261931
  batch 700 loss: 1.3207308053970337
  batch 750 loss: 1.3234886574745177
  batch 800 loss: 1.2847426164150237
  batch 850 loss: 1.3584384059906005
  batch 900 loss: 1.3623278069496154
avg val loss: 1.2815442085266113
LOSS train 1.36233 valid 1.28154, valid PER 43.09%
EPOCH 13, Learning Rate: 0.1
  batch 50 loss: 1.2911101269721985
  batch 100 loss: 1.3259160423278809
  batch 150 loss: 1.2844740998744966
  batch 200 loss: 1.3279909253120423
  batch 250 loss: 1.3167457914352416
  batch 300 loss: 1.2800177669525146
  batch 350 loss: 1.3119727611541747
  batch 400 loss: 1.3274000692367554
  batch 450 loss: 1.3506373357772827
  batch 500 loss: 1.253983463048935
  batch 550 loss: 1.2948584485054015
  batch 600 loss: 1.3033590245246887
  batch 650 loss: 1.2951199078559876
  batch 700 loss: 1.3135885000228882
  batch 750 loss: 1.2915237152576446
  batch 800 loss: 1.3045632100105287
  batch 850 loss: 1.3254733753204346
  batch 900 loss: 1.318516616821289
avg val loss: 1.2730528116226196
LOSS train 1.31852 valid 1.27305, valid PER 41.37%
EPOCH 14, Learning Rate: 0.1
  batch 50 loss: 1.279923391342163
  batch 100 loss: 1.290735766887665
  batch 150 loss: 1.2869647908210755
  batch 200 loss: 1.3067931413650513
  batch 250 loss: 1.2837837076187133
  batch 300 loss: 1.3065366196632384
  batch 350 loss: 1.2645923268795014
  batch 400 loss: 1.2740777826309204
  batch 450 loss: 1.2694306397438049
  batch 500 loss: 1.3268759751319885
  batch 550 loss: 1.312202513217926
  batch 600 loss: 1.2880844008922576
  batch 650 loss: 1.32074401140213
  batch 700 loss: 1.2932993078231811
  batch 750 loss: 1.26367782831192
  batch 800 loss: 1.23100919008255
  batch 850 loss: 1.2978172135353088
  batch 900 loss: 1.2919697165489197
avg val loss: 1.2666771411895752
LOSS train 1.29197 valid 1.26668, valid PER 41.37%
EPOCH 15, Learning Rate: 0.1
  batch 50 loss: 1.2896681201457978
  batch 100 loss: 1.2547117781639099
  batch 150 loss: 1.258329826593399
  batch 200 loss: 1.3013553404808045
  batch 250 loss: 1.2532037711143493
  batch 300 loss: 1.2543793869018556
  batch 350 loss: 1.2517097890377045
  batch 400 loss: 1.249868265390396
  batch 450 loss: 1.2520331501960755
  batch 500 loss: 1.225410952568054
  batch 550 loss: 1.2682247591018676
  batch 600 loss: 1.2730223155021667
  batch 650 loss: 1.2960408449172973
  batch 700 loss: 1.2962935221195222
  batch 750 loss: 1.2641185629367828
  batch 800 loss: 1.2497399747371674
  batch 850 loss: 1.2514216363430024
  batch 900 loss: 1.2772949397563935
avg val loss: 1.2268823385238647
LOSS train 1.27729 valid 1.22688, valid PER 40.49%
EPOCH 16, Learning Rate: 0.1
  batch 50 loss: 1.2932871770858765
  batch 100 loss: 1.2198976385593414
  batch 150 loss: 1.2518098270893097
  batch 200 loss: 1.2526752209663392
  batch 250 loss: 1.2609761142730713
  batch 300 loss: 1.2511832201480866
  batch 350 loss: 1.267194585800171
  batch 400 loss: 1.2557865059375763
  batch 450 loss: 1.2788811707496643
  batch 500 loss: 1.2252275454998016
  batch 550 loss: 1.2778210949897766
  batch 600 loss: 1.2670304822921752
  batch 650 loss: 1.273653781414032
  batch 700 loss: 1.2270278525352478
  batch 750 loss: 1.2316665303707124
  batch 800 loss: 1.2317414271831513
  batch 850 loss: 1.2280973041057586
  batch 900 loss: 1.2415470468997956
avg val loss: 1.2133657932281494
LOSS train 1.24155 valid 1.21337, valid PER 39.26%
EPOCH 17, Learning Rate: 0.1
  batch 50 loss: 1.237840507030487
  batch 100 loss: 1.2520313799381255
  batch 150 loss: 1.2131100630760192
  batch 200 loss: 1.2179069638252258
  batch 250 loss: 1.2340468418598176
  batch 300 loss: 1.232191379070282
  batch 350 loss: 1.2237897384166718
  batch 400 loss: 1.309379253387451
  batch 450 loss: 1.2306517732143403
  batch 500 loss: 1.2351898407936097
  batch 550 loss: 1.2278473198413848
  batch 600 loss: 1.291226875782013
  batch 650 loss: 1.2193184351921083
  batch 700 loss: 1.251286927461624
  batch 750 loss: 1.2111656963825226
  batch 800 loss: 1.207826735973358
  batch 850 loss: 1.2214918303489686
  batch 900 loss: 1.2023052752017975
avg val loss: 1.1958019733428955
LOSS train 1.20231 valid 1.19580, valid PER 39.09%
EPOCH 18, Learning Rate: 0.1
  batch 50 loss: 1.2092782258987427
  batch 100 loss: 1.2418210196495056
  batch 150 loss: 1.2583001852035522
  batch 200 loss: 1.2070281600952149
  batch 250 loss: 1.2151261067390442
  batch 300 loss: 1.2038856387138366
  batch 350 loss: 1.2558998847007752
  batch 400 loss: 1.1881429553031921
  batch 450 loss: 1.2648747158050537
  batch 500 loss: 1.2182405185699463
  batch 550 loss: 1.1952618587017059
  batch 600 loss: 1.1942079174518585
  batch 650 loss: 1.1987109088897705
  batch 700 loss: 1.2390364587306977
  batch 750 loss: 1.2012585616111755
  batch 800 loss: 1.213290718793869
  batch 850 loss: 1.1997202610969544
  batch 900 loss: 1.221100125312805
avg val loss: 1.1923413276672363
LOSS train 1.22110 valid 1.19234, valid PER 39.33%
EPOCH 19, Learning Rate: 0.1
  batch 50 loss: 1.1659621739387511
  batch 100 loss: 1.159862198829651
  batch 150 loss: 1.2006096231937409
  batch 200 loss: 1.2168344640731812
  batch 250 loss: 1.2423083651065827
  batch 300 loss: 1.212726366519928
  batch 350 loss: 1.1842132925987243
  batch 400 loss: 1.2104197788238524
  batch 450 loss: 1.2119101440906526
  batch 500 loss: 1.2133954298496246
  batch 550 loss: 1.1994241297245025
  batch 600 loss: 1.1884340143203735
  batch 650 loss: 1.231805787086487
  batch 700 loss: 1.174113610982895
  batch 750 loss: 1.1567287290096282
  batch 800 loss: 1.2168266427516938
  batch 850 loss: 1.2325072717666625
  batch 900 loss: 1.2048658168315887
avg val loss: 1.2068140506744385
LOSS train 1.20487 valid 1.20681, valid PER 39.55%
EPOCH 20, Learning Rate: 0.05
  batch 50 loss: 1.1655331945419312
  batch 100 loss: 1.1475583124160766
  batch 150 loss: 1.1470612955093384
  batch 200 loss: 1.1566128861904144
  batch 250 loss: 1.1569296050071716
  batch 300 loss: 1.1750798630714416
  batch 350 loss: 1.127482032775879
  batch 400 loss: 1.1504106879234315
  batch 450 loss: 1.1407559061050414
  batch 500 loss: 1.1466800999641418
  batch 550 loss: 1.1884850692749023
  batch 600 loss: 1.1250219607353211
  batch 650 loss: 1.15247047662735
  batch 700 loss: 1.150143256187439
  batch 750 loss: 1.1476806247234344
  batch 800 loss: 1.1755244290828706
  batch 850 loss: 1.1806567406654358
  batch 900 loss: 1.1617857480049134
avg val loss: 1.1414793729782104
LOSS train 1.16179 valid 1.14148, valid PER 37.80%
Training finished in 2.0 minutes.
Model saved to checkpoints/20231210_141058/model_20
Loading model from checkpoints/20231210_141058/model_20
SUB: 19.30%, DEL: 18.57%, INS: 1.00%, COR: 62.13%, PER: 38.87%
