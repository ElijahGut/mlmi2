Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.1, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 83496
EPOCH 1, Learning Rate: 0.1
  batch 50 loss: 7.930463690757751
  batch 100 loss: 3.324197244644165
  batch 150 loss: 3.2840676736831664
  batch 200 loss: 3.2620020961761473
  batch 250 loss: 3.2404317474365234
  batch 300 loss: 3.2044434547424316
  batch 350 loss: 3.1850433588027953
  batch 400 loss: 3.1739811372756956
  batch 450 loss: 3.143341374397278
  batch 500 loss: 3.09422788143158
  batch 550 loss: 3.0418509912490843
  batch 600 loss: 2.975479803085327
  batch 650 loss: 2.8960172653198244
  batch 700 loss: 2.8512405061721804
  batch 750 loss: 2.7863599634170533
  batch 800 loss: 2.7416985988616944
  batch 850 loss: 2.7125691413879394
  batch 900 loss: 2.6459406805038452
running loss: 62.987815856933594
LOSS train 2.64594 valid 2.61280, valid PER 85.74%
EPOCH 2, Learning Rate: 0.1
  batch 50 loss: 2.6122825002670287
  batch 100 loss: 2.554519867897034
  batch 150 loss: 2.5080651092529296
  batch 200 loss: 2.4988254737854003
  batch 250 loss: 2.475413107872009
  batch 300 loss: 2.4536340188980104
  batch 350 loss: 2.38864381313324
  batch 400 loss: 2.4042817068099978
  batch 450 loss: 2.358782901763916
  batch 500 loss: 2.343145489692688
  batch 550 loss: 2.3331550979614257
  batch 600 loss: 2.2834274673461916
  batch 650 loss: 2.289764316082001
  batch 700 loss: 2.252915163040161
  batch 750 loss: 2.258948998451233
  batch 800 loss: 2.1932991886138917
  batch 850 loss: 2.1777204704284667
  batch 900 loss: 2.188515224456787
running loss: 51.98915994167328
LOSS train 2.18852 valid 2.13255, valid PER 80.00%
EPOCH 3, Learning Rate: 0.1
  batch 50 loss: 2.1623735141754152
  batch 100 loss: 2.118467071056366
  batch 150 loss: 2.1222367763519285
  batch 200 loss: 2.1008570194244385
  batch 250 loss: 2.0791162705421447
  batch 300 loss: 2.0704971742630005
  batch 350 loss: 2.096864664554596
  batch 400 loss: 2.060223774909973
  batch 450 loss: 2.0234068775177003
  batch 500 loss: 2.0218451476097106
  batch 550 loss: 1.9928466987609863
  batch 600 loss: 1.9659564089775086
  batch 650 loss: 1.9396897530555726
  batch 700 loss: 1.970264377593994
  batch 750 loss: 1.9936823105812074
  batch 800 loss: 1.9135013675689698
  batch 850 loss: 1.9523916816711426
  batch 900 loss: 1.8994132661819458
running loss: 45.47657084465027
LOSS train 1.89941 valid 1.88063, valid PER 72.64%
EPOCH 4, Learning Rate: 0.1
  batch 50 loss: 1.9051440954208374
  batch 100 loss: 1.911361644268036
  batch 150 loss: 1.8389832639694215
  batch 200 loss: 1.8831736874580383
  batch 250 loss: 1.882621591091156
  batch 300 loss: 1.9024669003486634
  batch 350 loss: 1.8028644609451294
  batch 400 loss: 1.8352348160743714
  batch 450 loss: 1.8348170113563538
  batch 500 loss: 1.8030628156661987
  batch 550 loss: 1.832858006954193
  batch 600 loss: 1.8303279423713683
  batch 650 loss: 1.827214241027832
  batch 700 loss: 1.7696799445152283
  batch 750 loss: 1.756451840400696
  batch 800 loss: 1.7491617727279662
  batch 850 loss: 1.7604801678657531
  batch 900 loss: 1.7972757768630983
running loss: 41.62948751449585
LOSS train 1.79728 valid 1.70560, valid PER 63.64%
EPOCH 5, Learning Rate: 0.1
  batch 50 loss: 1.7374817848205566
  batch 100 loss: 1.7176780366897584
  batch 150 loss: 1.7528514099121093
  batch 200 loss: 1.699111089706421
  batch 250 loss: 1.6945883488655091
  batch 300 loss: 1.7220773768424988
  batch 350 loss: 1.7122208952903748
  batch 400 loss: 1.7202222967147827
  batch 450 loss: 1.6901603651046753
  batch 500 loss: 1.7103900337219238
  batch 550 loss: 1.6520624613761903
  batch 600 loss: 1.7077967500686646
  batch 650 loss: 1.6639905452728272
  batch 700 loss: 1.6933566164970397
  batch 750 loss: 1.6442190194129944
  batch 800 loss: 1.6778171491622924
  batch 850 loss: 1.6818512511253356
  batch 900 loss: 1.6582913398742676
running loss: 39.32460379600525
LOSS train 1.65829 valid 1.60814, valid PER 58.94%
EPOCH 6, Learning Rate: 0.1
  batch 50 loss: 1.665436899662018
  batch 100 loss: 1.6265303468704224
  batch 150 loss: 1.6051744508743286
  batch 200 loss: 1.6303152894973756
  batch 250 loss: 1.6437498927116394
  batch 300 loss: 1.5964837169647217
  batch 350 loss: 1.6085299229621888
  batch 400 loss: 1.609648289680481
  batch 450 loss: 1.6341604685783386
  batch 500 loss: 1.5935486125946046
  batch 550 loss: 1.6043531322479248
  batch 600 loss: 1.6114079928398133
  batch 650 loss: 1.6135816311836242
  batch 700 loss: 1.5928071999549867
  batch 750 loss: 1.56631338596344
  batch 800 loss: 1.5738474225997925
  batch 850 loss: 1.567526659965515
  batch 900 loss: 1.5928702139854432
running loss: 37.52969753742218
LOSS train 1.59287 valid 1.53642, valid PER 54.23%
EPOCH 7, Learning Rate: 0.1
  batch 50 loss: 1.5794824194908141
  batch 100 loss: 1.5669216871261598
  batch 150 loss: 1.5677135252952576
  batch 200 loss: 1.5508499002456666
  batch 250 loss: 1.5597722458839416
  batch 300 loss: 1.5217168736457825
  batch 350 loss: 1.53419410943985
  batch 400 loss: 1.5540905404090881
  batch 450 loss: 1.538994436264038
  batch 500 loss: 1.520355405807495
  batch 550 loss: 1.5260835695266723
  batch 600 loss: 1.5307054281234742
  batch 650 loss: 1.5156696844100952
  batch 700 loss: 1.5218794131278992
  batch 750 loss: 1.5088007020950318
  batch 800 loss: 1.493442142009735
  batch 850 loss: 1.5377797436714173
  batch 900 loss: 1.5510563564300537
running loss: 35.61183965206146
LOSS train 1.55106 valid 1.46270, valid PER 50.74%
EPOCH 8, Learning Rate: 0.1
  batch 50 loss: 1.518898251056671
  batch 100 loss: 1.502721061706543
  batch 150 loss: 1.5042475986480712
  batch 200 loss: 1.4567607998847962
  batch 250 loss: 1.512425298690796
  batch 300 loss: 1.4359839630126954
  batch 350 loss: 1.5208541464805603
  batch 400 loss: 1.4616162300109863
  batch 450 loss: 1.5064027667045594
  batch 500 loss: 1.5184525322914124
  batch 550 loss: 1.4664216184616088
  batch 600 loss: 1.4872287559509276
  batch 650 loss: 1.528651225566864
  batch 700 loss: 1.4546329998970031
  batch 750 loss: 1.4609829378128052
  batch 800 loss: 1.463046817779541
  batch 850 loss: 1.4812044644355773
  batch 900 loss: 1.4464160990715027
running loss: 34.3377845287323
LOSS train 1.44642 valid 1.42848, valid PER 47.83%
EPOCH 9, Learning Rate: 0.1
  batch 50 loss: 1.4031592202186585
  batch 100 loss: 1.4757683181762695
  batch 150 loss: 1.4558845019340516
  batch 200 loss: 1.4031694221496582
  batch 250 loss: 1.4463186407089232
  batch 300 loss: 1.4371308588981628
  batch 350 loss: 1.4331837797164917
  batch 400 loss: 1.432310197353363
  batch 450 loss: 1.4449703979492188
  batch 500 loss: 1.421794831752777
  batch 550 loss: 1.4667986559867858
  batch 600 loss: 1.4356394839286803
  batch 650 loss: 1.4316299271583557
  batch 700 loss: 1.436529107093811
  batch 750 loss: 1.4323621916770934
  batch 800 loss: 1.4320184755325318
  batch 850 loss: 1.467143609523773
  batch 900 loss: 1.4125819396972656
running loss: 34.48404026031494
LOSS train 1.41258 valid 1.35163, valid PER 44.57%
EPOCH 10, Learning Rate: 0.05
  batch 50 loss: 1.3520893692970275
  batch 100 loss: 1.3681729900836945
  batch 150 loss: 1.4017190265655517
  batch 200 loss: 1.409180256128311
  batch 250 loss: 1.3895740628242492
  batch 300 loss: 1.3365809774398805
  batch 350 loss: 1.3857528686523437
  batch 400 loss: 1.3497010397911071
  batch 450 loss: 1.3380134308338165
  batch 500 loss: 1.385701835155487
  batch 550 loss: 1.3846371507644653
  batch 600 loss: 1.3620621001720428
  batch 650 loss: 1.3403787398338318
  batch 700 loss: 1.367789101600647
  batch 750 loss: 1.3508570098876953
  batch 800 loss: 1.3618015432357788
  batch 850 loss: 1.3905230951309204
  batch 900 loss: 1.3961087894439697
running loss: 32.55501115322113
LOSS train 1.39611 valid 1.31972, valid PER 43.85%
EPOCH 11, Learning Rate: 0.05
  batch 50 loss: 1.3402362561225891
  batch 100 loss: 1.3148680543899536
  batch 150 loss: 1.3238516330718995
  batch 200 loss: 1.3660361135005952
  batch 250 loss: 1.3596147918701171
  batch 300 loss: 1.3158787453174592
  batch 350 loss: 1.3363801765441894
  batch 400 loss: 1.3603058624267579
  batch 450 loss: 1.3595314621925354
  batch 500 loss: 1.3242320358753203
  batch 550 loss: 1.3465155172348022
  batch 600 loss: 1.3013974797725678
  batch 650 loss: 1.3864500141143798
  batch 700 loss: 1.3187282156944276
  batch 750 loss: 1.3331060934066772
  batch 800 loss: 1.3656962132453918
  batch 850 loss: 1.3838407254219056
  batch 900 loss: 1.3507839179039
running loss: 31.74718415737152
LOSS train 1.35078 valid 1.32671, valid PER 44.01%
EPOCH 12, Learning Rate: 0.05
  batch 50 loss: 1.3328206729888916
  batch 100 loss: 1.3272369408607483
  batch 150 loss: 1.31567143201828
  batch 200 loss: 1.3333305406570435
  batch 250 loss: 1.3496257710456847
  batch 300 loss: 1.3235097432136536
  batch 350 loss: 1.3353101682662964
  batch 400 loss: 1.3533190417289733
  batch 450 loss: 1.3441369485855104
  batch 500 loss: 1.3316770577430725
  batch 550 loss: 1.270952124595642
  batch 600 loss: 1.2944169020652772
  batch 650 loss: 1.3660708475112915
  batch 700 loss: 1.3247723078727722
  batch 750 loss: 1.3143704652786254
  batch 800 loss: 1.2761281430721283
  batch 850 loss: 1.3444216752052307
  batch 900 loss: 1.3519564938545228
running loss: 31.577557921409607
LOSS train 1.35196 valid 1.28315, valid PER 42.70%
EPOCH 13, Learning Rate: 0.05
  batch 50 loss: 1.285175747871399
  batch 100 loss: 1.3289388251304626
  batch 150 loss: 1.2874069142341613
  batch 200 loss: 1.334141480922699
  batch 250 loss: 1.3100654423236846
  batch 300 loss: 1.2831522297859193
  batch 350 loss: 1.3143895053863526
  batch 400 loss: 1.320652015209198
  batch 450 loss: 1.3548935508728028
  batch 500 loss: 1.2609186017513274
  batch 550 loss: 1.2897465658187866
  batch 600 loss: 1.3092894840240479
  batch 650 loss: 1.2983594572544097
  batch 700 loss: 1.2997957062721253
  batch 750 loss: 1.2961864614486693
  batch 800 loss: 1.3026266503334045
  batch 850 loss: 1.3302806091308594
  batch 900 loss: 1.326200773715973
running loss: 31.503526091575623
LOSS train 1.32620 valid 1.27553, valid PER 41.58%
EPOCH 14, Learning Rate: 0.05
  batch 50 loss: 1.2944371676445008
  batch 100 loss: 1.303987877368927
  batch 150 loss: 1.289895634651184
  batch 200 loss: 1.301131534576416
  batch 250 loss: 1.2865074825286866
  batch 300 loss: 1.302597622871399
  batch 350 loss: 1.2766388511657716
  batch 400 loss: 1.294521882534027
  batch 450 loss: 1.2826514911651612
  batch 500 loss: 1.2948139476776124
  batch 550 loss: 1.315921552181244
  batch 600 loss: 1.2940703845024109
  batch 650 loss: 1.3303661251068115
  batch 700 loss: 1.3140759754180908
  batch 750 loss: 1.2697215700149536
  batch 800 loss: 1.2339573645591735
  batch 850 loss: 1.299215211868286
  batch 900 loss: 1.2861642146110535
running loss: 31.678610801696777
LOSS train 1.28616 valid 1.26010, valid PER 41.07%
EPOCH 15, Learning Rate: 0.025
  batch 50 loss: 1.2916348099708557
  batch 100 loss: 1.2471538996696472
  batch 150 loss: 1.2464220368862151
  batch 200 loss: 1.283411592245102
  batch 250 loss: 1.2646974086761475
  batch 300 loss: 1.2520860147476196
  batch 350 loss: 1.2488145554065704
  batch 400 loss: 1.2460366940498353
  batch 450 loss: 1.2444809079170227
  batch 500 loss: 1.2187921166419984
  batch 550 loss: 1.2735025024414062
  batch 600 loss: 1.2716125690937041
  batch 650 loss: 1.2902415156364442
  batch 700 loss: 1.2894038259983063
  batch 750 loss: 1.2619795835018157
  batch 800 loss: 1.2535087609291076
  batch 850 loss: 1.2420855724811555
  batch 900 loss: 1.2807146751880645
running loss: 30.407746493816376
LOSS train 1.28071 valid 1.24160, valid PER 41.26%
EPOCH 16, Learning Rate: 0.025
  batch 50 loss: 1.293257896900177
  batch 100 loss: 1.2178526258468627
  batch 150 loss: 1.2413618338108063
  batch 200 loss: 1.2365811455249787
  batch 250 loss: 1.2762918150424958
  batch 300 loss: 1.2654501271247864
  batch 350 loss: 1.2605551385879517
  batch 400 loss: 1.2633095800876617
  batch 450 loss: 1.2741506111621856
  batch 500 loss: 1.2290821766853333
  batch 550 loss: 1.2676399850845337
  batch 600 loss: 1.2680220818519592
  batch 650 loss: 1.2738923025131226
  batch 700 loss: 1.2312409341335298
  batch 750 loss: 1.2539218842983246
  batch 800 loss: 1.2309188008308412
  batch 850 loss: 1.231922425031662
  batch 900 loss: 1.243248541355133
running loss: 29.65007185935974
LOSS train 1.24325 valid 1.23647, valid PER 40.39%
EPOCH 17, Learning Rate: 0.025
  batch 50 loss: 1.2433734500408173
  batch 100 loss: 1.2455384159088134
  batch 150 loss: 1.226059000492096
  batch 200 loss: 1.2398749768733979
  batch 250 loss: 1.243511483669281
  batch 300 loss: 1.2328685688972474
  batch 350 loss: 1.2300372588634492
  batch 400 loss: 1.3166810655593872
  batch 450 loss: 1.2471255230903626
  batch 500 loss: 1.2305896937847138
  batch 550 loss: 1.2513798820972442
  batch 600 loss: 1.2888072156906127
  batch 650 loss: 1.2245713567733765
  batch 700 loss: 1.2479739964008332
  batch 750 loss: 1.2261308181285857
  batch 800 loss: 1.220307834148407
  batch 850 loss: 1.241298804283142
  batch 900 loss: 1.21901123046875
running loss: 30.719217240810394
LOSS train 1.21901 valid 1.24311, valid PER 40.75%
EPOCH 18, Learning Rate: 0.0125
  batch 50 loss: 1.2311030685901643
  batch 100 loss: 1.2500218772888183
  batch 150 loss: 1.2521288633346557
  batch 200 loss: 1.2328436422348021
  batch 250 loss: 1.2399384820461272
  batch 300 loss: 1.2173090052604676
  batch 350 loss: 1.2648597693443298
  batch 400 loss: 1.210701323747635
  batch 450 loss: 1.269755070209503
  batch 500 loss: 1.224864776134491
  batch 550 loss: 1.2018770956993103
  batch 600 loss: 1.2026664638519287
  batch 650 loss: 1.2007541108131408
  batch 700 loss: 1.2554366827011108
  batch 750 loss: 1.2232459425926208
  batch 800 loss: 1.2314852702617645
  batch 850 loss: 1.1981550133228303
  batch 900 loss: 1.2365567898750305
running loss: 28.505538880825043
LOSS train 1.23656 valid 1.22818, valid PER 40.09%
EPOCH 19, Learning Rate: 0.0125
  batch 50 loss: 1.1883710551261901
  batch 100 loss: 1.1771346139907837
  batch 150 loss: 1.231375106573105
  batch 200 loss: 1.223871853351593
  batch 250 loss: 1.2529564464092255
  batch 300 loss: 1.2227256321907043
  batch 350 loss: 1.2018573379516602
  batch 400 loss: 1.2590795159339905
  batch 450 loss: 1.2307184517383576
  batch 500 loss: 1.235206413269043
  batch 550 loss: 1.21015460729599
  batch 600 loss: 1.2175002741813659
  batch 650 loss: 1.241133918762207
  batch 700 loss: 1.1868488168716431
  batch 750 loss: 1.1924285984039307
  batch 800 loss: 1.226293958425522
  batch 850 loss: 1.253521763086319
  batch 900 loss: 1.2028308749198913
running loss: 30.245448112487793
LOSS train 1.20283 valid 1.22230, valid PER 39.73%
EPOCH 20, Learning Rate: 0.00625
  batch 50 loss: 1.206174771785736
  batch 100 loss: 1.2097619342803956
  batch 150 loss: 1.2108116328716279
  batch 200 loss: 1.2206901872158051
  batch 250 loss: 1.2159685933589934
  batch 300 loss: 1.224089217185974
  batch 350 loss: 1.1808385789394378
  batch 400 loss: 1.2021417129039764
  batch 450 loss: 1.2029628801345824
  batch 500 loss: 1.1986069571971893
  batch 550 loss: 1.2433226251602172
  batch 600 loss: 1.1780491411685943
  batch 650 loss: 1.2177133870124817
  batch 700 loss: 1.2179742550849915
  batch 750 loss: 1.2041963863372802
  batch 800 loss: 1.2210418498516082
  batch 850 loss: 1.2333155167102814
  batch 900 loss: 1.2181625056266785
running loss: 28.897594153881073
LOSS train 1.21816 valid 1.21581, valid PER 39.73%
Training finished in 2.0 minutes.
Model saved to checkpoints/20231210_041725/model_20
Loading model from checkpoints/20231210_041725/model_20
SUB: 19.76%, DEL: 21.05%, INS: 1.13%, COR: 59.20%, PER: 41.93%
