Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.005, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='adam', grad_clip=0.5)
Total number of model parameters is 166952
EPOCH 1, Learning Rate: 0.005
  batch 50 loss: 4.891500558853149
  batch 100 loss: 2.8733468437194825
  batch 150 loss: 2.621232771873474
  batch 200 loss: 2.3933087301254274
  batch 250 loss: 2.2711526918411256
  batch 300 loss: 2.1138069868087768
  batch 350 loss: 1.972266628742218
  batch 400 loss: 1.9554994678497315
  batch 450 loss: 1.9383325052261353
  batch 500 loss: 1.837584946155548
  batch 550 loss: 1.7713471889495849
  batch 600 loss: 1.7568433308601379
  batch 650 loss: 1.6994821977615358
  batch 700 loss: 1.7079272174835205
  batch 750 loss: 1.6699364042282105
  batch 800 loss: 1.6763240170478821
  batch 850 loss: 1.6430956196784974
  batch 900 loss: 1.5999498820304872
LOSS train 1.59995 valid 1.53448, valid PER 52.94%
EPOCH 2, Learning Rate: 0.005
  batch 50 loss: 1.5633625459671021
  batch 100 loss: 1.5601657915115357
  batch 150 loss: 1.5017035126686096
  batch 200 loss: 1.5428737711906433
  batch 250 loss: 1.5440668392181396
  batch 300 loss: 1.5473966336250304
  batch 350 loss: 1.4608888483047486
  batch 400 loss: 1.4789175868034363
  batch 450 loss: 1.4416556811332704
  batch 500 loss: 1.4709568285942078
  batch 550 loss: 1.48290625333786
  batch 600 loss: 1.451026237010956
  batch 650 loss: 1.479504358768463
  batch 700 loss: 1.4447771143913268
  batch 750 loss: 1.432999277114868
  batch 800 loss: 1.3966255187988281
  batch 850 loss: 1.40375324010849
  batch 900 loss: 1.4574698686599732
LOSS train 1.45747 valid 1.38631, valid PER 45.25%
EPOCH 3, Learning Rate: 0.005
  batch 50 loss: 1.3878307056427002
  batch 100 loss: 1.3612618398666383
  batch 150 loss: 1.413923053741455
  batch 200 loss: 1.4133847117424012
  batch 250 loss: 1.3569814944267273
  batch 300 loss: 1.335911178588867
  batch 350 loss: 1.4279972505569458
  batch 400 loss: 1.3811479496955872
  batch 450 loss: 1.4851053071022033
  batch 500 loss: 1.3901369524002076
  batch 550 loss: 1.3608968245983124
  batch 600 loss: 1.3650981903076171
  batch 650 loss: 1.3217058539390565
  batch 700 loss: 1.3188767552375793
  batch 750 loss: 1.393586847782135
  batch 800 loss: 1.3162710452079773
  batch 850 loss: 1.3588415861129761
  batch 900 loss: 1.2804898500442505
LOSS train 1.28049 valid 1.28209, valid PER 41.86%
EPOCH 4, Learning Rate: 0.005
  batch 50 loss: 1.2601085638999938
  batch 100 loss: 1.2964884114265443
  batch 150 loss: 1.2516401529312133
  batch 200 loss: 1.3133614587783813
  batch 250 loss: 1.3098135459423066
  batch 300 loss: 1.3093360996246337
  batch 350 loss: 1.244748704433441
  batch 400 loss: 1.3273768186569215
  batch 450 loss: 1.279806705713272
  batch 500 loss: 1.2475580370426178
  batch 550 loss: 1.2994350099563599
  batch 600 loss: 1.3049585974216462
  batch 650 loss: 1.2747379374504089
  batch 700 loss: 1.2628104162216187
  batch 750 loss: 1.2335628128051759
  batch 800 loss: 1.2280436861515045
  batch 850 loss: 1.2932200455665588
  batch 900 loss: 1.3287359535694123
LOSS train 1.32874 valid 1.21657, valid PER 39.59%
EPOCH 5, Learning Rate: 0.005
  batch 50 loss: 1.2319751787185669
  batch 100 loss: 1.2345031023025512
  batch 150 loss: 1.302338671684265
  batch 200 loss: 1.1987393617630004
  batch 250 loss: 1.2197185611724854
  batch 300 loss: 1.2565448212623596
  batch 350 loss: 1.2473592972755432
  batch 400 loss: 1.252382116317749
  batch 450 loss: 1.2299542069435119
  batch 500 loss: 1.2680928409099579
  batch 550 loss: 1.1613267660140991
  batch 600 loss: 1.2672635757923125
  batch 650 loss: 1.2545683145523072
  batch 700 loss: 1.2651537811756135
  batch 750 loss: 1.2035932052135467
  batch 800 loss: 1.2472724223136902
  batch 850 loss: 1.2451237535476685
  batch 900 loss: 1.2549524068832398
LOSS train 1.25495 valid 1.19133, valid PER 38.44%
EPOCH 6, Learning Rate: 0.005
  batch 50 loss: 1.221850596666336
  batch 100 loss: 1.1817697966098786
  batch 150 loss: 1.1357501828670502
  batch 200 loss: 1.2033622169494629
  batch 250 loss: 1.220311745405197
  batch 300 loss: 1.1903105115890502
  batch 350 loss: 1.1657469749450684
  batch 400 loss: 1.197582551240921
  batch 450 loss: 1.2351427900791168
  batch 500 loss: 1.2080898106098175
  batch 550 loss: 1.2096387839317322
  batch 600 loss: 1.177913337945938
  batch 650 loss: 1.2123712527751922
  batch 700 loss: 1.2021212542057038
  batch 750 loss: 1.1722639787197113
  batch 800 loss: 1.1688962984085083
  batch 850 loss: 1.1771713137626647
  batch 900 loss: 1.2085198473930359
LOSS train 1.20852 valid 1.14351, valid PER 37.57%
EPOCH 7, Learning Rate: 0.005
  batch 50 loss: 1.1810815477371215
  batch 100 loss: 1.1793057584762574
  batch 150 loss: 1.1940858793258666
  batch 200 loss: 1.1732107758522035
  batch 250 loss: 1.1614817655086518
  batch 300 loss: 1.1702792954444885
  batch 350 loss: 1.1816122591495515
  batch 400 loss: 1.1798882675170899
  batch 450 loss: 1.1859162521362305
  batch 500 loss: 1.1656326603889466
  batch 550 loss: 1.185736962556839
  batch 600 loss: 1.149940996170044
  batch 650 loss: 1.1547694063186646
  batch 700 loss: 1.2088279771804809
  batch 750 loss: 1.1487276804447175
  batch 800 loss: 1.1405316746234895
  batch 850 loss: 1.1645719921588897
  batch 900 loss: 1.2323260164260865
LOSS train 1.23233 valid 1.15132, valid PER 37.98%
EPOCH 8, Learning Rate: 0.005
  batch 50 loss: 1.136506017446518
  batch 100 loss: 1.1363654696941377
  batch 150 loss: 1.123253207206726
  batch 200 loss: 1.1642710709571837
  batch 250 loss: 1.1803010177612305
  batch 300 loss: 1.0853715801239014
  batch 350 loss: 1.1845117545127868
  batch 400 loss: 1.1777335011959076
  batch 450 loss: 1.196873950958252
  batch 500 loss: 1.2617817175388337
  batch 550 loss: 1.2033330416679382
  batch 600 loss: 1.1934751617908477
  batch 650 loss: 1.201891143321991
  batch 700 loss: 1.1356056177616118
  batch 750 loss: 1.167529842853546
  batch 800 loss: 1.1656031429767608
  batch 850 loss: 1.162900756597519
  batch 900 loss: 1.1662472224235534
LOSS train 1.16625 valid 1.12642, valid PER 36.81%
EPOCH 9, Learning Rate: 0.005
  batch 50 loss: 1.0972912049293517
  batch 100 loss: 1.1464994084835052
  batch 150 loss: 1.1539878606796266
  batch 200 loss: 1.0856380867958069
  batch 250 loss: 1.1415693986415862
  batch 300 loss: 1.1632843542098998
  batch 350 loss: 1.1726316154003142
  batch 400 loss: 1.1594907462596893
  batch 450 loss: 1.1453364944458009
  batch 500 loss: 1.1187122571468353
  batch 550 loss: 1.1411468291282654
  batch 600 loss: 1.1975559842586518
  batch 650 loss: 1.1194666147232055
  batch 700 loss: 1.1401599478721618
  batch 750 loss: 1.1348755955696106
  batch 800 loss: 1.13880326628685
  batch 850 loss: 1.149935973882675
  batch 900 loss: 1.1193631947040559
LOSS train 1.11936 valid 1.11723, valid PER 36.60%
EPOCH 10, Learning Rate: 0.005
  batch 50 loss: 1.1011387991905213
  batch 100 loss: 1.1028995311260223
  batch 150 loss: 1.13109849691391
  batch 200 loss: 1.1412801945209503
  batch 250 loss: 1.1173752558231353
  batch 300 loss: 1.0802962982654571
  batch 350 loss: 1.124132993221283
  batch 400 loss: 1.0991287112236023
  batch 450 loss: 1.1063307321071625
  batch 500 loss: 1.1364318025112152
  batch 550 loss: 1.143193793296814
  batch 600 loss: 1.1259452879428864
  batch 650 loss: 1.1179405856132507
  batch 700 loss: 1.1253134202957153
  batch 750 loss: 1.106277368068695
  batch 800 loss: 1.1710858273506164
  batch 850 loss: 1.1258778965473175
  batch 900 loss: 1.0981432330608367
LOSS train 1.09814 valid 1.10218, valid PER 34.80%
EPOCH 11, Learning Rate: 0.005
  batch 50 loss: 1.0784630298614502
  batch 100 loss: 1.0845909488201142
  batch 150 loss: 1.09058620095253
  batch 200 loss: 1.1239671123027801
  batch 250 loss: 1.12258523106575
  batch 300 loss: 1.0997933292388915
  batch 350 loss: 1.1203664135932923
  batch 400 loss: 1.119787038564682
  batch 450 loss: 1.1160237741470338
  batch 500 loss: 1.1035535252094268
  batch 550 loss: 1.0885311102867126
  batch 600 loss: 1.081892831325531
  batch 650 loss: 1.135007073879242
  batch 700 loss: 1.0824738729000092
  batch 750 loss: 1.116621264219284
  batch 800 loss: 1.136584620475769
  batch 850 loss: 1.1603820240497589
  batch 900 loss: 1.1458349418640137
LOSS train 1.14583 valid 1.09543, valid PER 34.28%
EPOCH 12, Learning Rate: 0.005
  batch 50 loss: 1.1080344796180726
  batch 100 loss: 1.0780917870998383
  batch 150 loss: 1.0631546580791473
  batch 200 loss: 1.090544629096985
  batch 250 loss: 1.113308608531952
  batch 300 loss: 1.104053955078125
  batch 350 loss: 1.0824081134796142
  batch 400 loss: 1.1104597198963164
  batch 450 loss: 1.0973844826221466
  batch 500 loss: 1.1077957260608673
  batch 550 loss: 1.055843939781189
  batch 600 loss: 1.0815623342990874
  batch 650 loss: 1.1239979839324952
  batch 700 loss: 1.1074262177944183
  batch 750 loss: 1.1287920928001405
  batch 800 loss: 1.1131632137298584
  batch 850 loss: 1.1317849266529083
  batch 900 loss: 1.1313416409492492
LOSS train 1.13134 valid 1.10830, valid PER 35.76%
EPOCH 13, Learning Rate: 0.005
  batch 50 loss: 1.068723109960556
  batch 100 loss: 1.110064159631729
  batch 150 loss: 1.0785316109657288
  batch 200 loss: 1.0792853701114655
  batch 250 loss: 1.1081433749198915
  batch 300 loss: 1.0945237565040589
  batch 350 loss: 1.0987616789340973
  batch 400 loss: 1.1284168899059295
  batch 450 loss: 1.115863355398178
  batch 500 loss: 1.0956862020492553
  batch 550 loss: 1.0975393986701965
  batch 600 loss: 1.1046720004081727
  batch 650 loss: 1.0999546730518341
  batch 700 loss: 1.135314061641693
  batch 750 loss: 1.08171887755394
  batch 800 loss: 1.0864053320884706
  batch 850 loss: 1.1136201524734497
  batch 900 loss: 1.1305363547801972
LOSS train 1.13054 valid 1.10104, valid PER 36.02%
EPOCH 14, Learning Rate: 0.005
  batch 50 loss: 1.067570505142212
  batch 100 loss: 1.075262315273285
  batch 150 loss: 1.0822634661197663
  batch 200 loss: 1.0808476495742798
  batch 250 loss: 1.084773690700531
  batch 300 loss: 1.1010025227069855
  batch 350 loss: 1.0761276042461396
  batch 400 loss: 1.095772396326065
  batch 450 loss: 1.0891659200191497
  batch 500 loss: 1.090661919116974
  batch 550 loss: 1.0993601787090301
  batch 600 loss: 1.0401740157604218
  batch 650 loss: 1.0992657542228699
  batch 700 loss: 1.1087645041942595
  batch 750 loss: 1.0664409136772155
  batch 800 loss: 1.0634641814231873
  batch 850 loss: 1.1024687838554383
  batch 900 loss: 1.0829120826721192
LOSS train 1.08291 valid 1.10614, valid PER 36.00%
EPOCH 15, Learning Rate: 0.005
  batch 50 loss: 1.048139319419861
  batch 100 loss: 1.0485375344753265
  batch 150 loss: 1.0538495326042174
  batch 200 loss: 1.1061436760425567
  batch 250 loss: 1.0759334194660186
  batch 300 loss: 1.0622961664199828
  batch 350 loss: 1.048753844499588
  batch 400 loss: 1.0503408324718475
  batch 450 loss: 1.0669625902175903
  batch 500 loss: 1.0433436286449433
  batch 550 loss: 1.0616700267791748
  batch 600 loss: 1.1191673374176025
  batch 650 loss: 1.1050810146331786
  batch 700 loss: 1.0877035343647004
  batch 750 loss: 1.085014454126358
  batch 800 loss: 1.0727547371387482
  batch 850 loss: 1.0539441347122191
  batch 900 loss: 1.0913756799697876
LOSS train 1.09138 valid 1.09240, valid PER 34.71%
EPOCH 16, Learning Rate: 0.005
  batch 50 loss: 1.0731884014606476
  batch 100 loss: 1.057290735244751
  batch 150 loss: 1.07154465675354
  batch 200 loss: 1.0439095425605773
  batch 250 loss: 1.075229935646057
  batch 300 loss: 1.075813378095627
  batch 350 loss: 1.118602033853531
  batch 400 loss: 1.0889176368713378
  batch 450 loss: 1.0954540622234346
  batch 500 loss: 1.0419155287742614
  batch 550 loss: 1.1070168280601502
  batch 600 loss: 1.0779630768299102
  batch 650 loss: 1.0849445676803589
  batch 700 loss: 1.0593670475482941
  batch 750 loss: 1.0783263099193574
  batch 800 loss: 1.054973316192627
  batch 850 loss: 1.0655245876312256
  batch 900 loss: 1.064900848865509
LOSS train 1.06490 valid 1.10043, valid PER 35.00%
EPOCH 17, Learning Rate: 0.005
  batch 50 loss: 1.0561738884449006
  batch 100 loss: 1.0430819809436798
  batch 150 loss: 1.0329500484466552
  batch 200 loss: 1.0517927014827728
  batch 250 loss: 1.0686767971515656
  batch 300 loss: 1.0707506549358368
  batch 350 loss: 1.0270677995681763
  batch 400 loss: 1.077815203666687
  batch 450 loss: 1.0843165409564972
  batch 500 loss: 1.0405138325691223
  batch 550 loss: 1.0713638699054717
  batch 600 loss: 1.106474804878235
  batch 650 loss: 1.0449559533596038
  batch 700 loss: 1.069030168056488
  batch 750 loss: 1.024061794281006
  batch 800 loss: 1.0594498360157012
  batch 850 loss: 1.060374423265457
  batch 900 loss: 1.0098859286308288
LOSS train 1.00989 valid 1.10528, valid PER 34.88%
EPOCH 18, Learning Rate: 0.005
  batch 50 loss: 1.0520651757717132
  batch 100 loss: 1.053510992527008
  batch 150 loss: 1.0669639563560487
  batch 200 loss: 1.0444187760353087
  batch 250 loss: 1.0439958429336549
  batch 300 loss: 1.0159316504001616
  batch 350 loss: 1.089853743314743
  batch 400 loss: 1.0519284236431121
  batch 450 loss: 1.10329141497612
  batch 500 loss: 1.0676673769950866
  batch 550 loss: 1.0564947235584259
  batch 600 loss: 1.0492817401885985
  batch 650 loss: 1.0588375556468963
  batch 700 loss: 1.0803523981571197
  batch 750 loss: 1.0470555818080902
  batch 800 loss: 1.0350580728054046
  batch 850 loss: 1.0447279250621795
  batch 900 loss: 1.0823770809173583
LOSS train 1.08238 valid 1.07779, valid PER 33.65%
EPOCH 19, Learning Rate: 0.005
  batch 50 loss: 1.000784785747528
  batch 100 loss: 1.0354792368412018
  batch 150 loss: 1.0293428087234497
  batch 200 loss: 1.0309556913375855
  batch 250 loss: 1.0736460864543915
  batch 300 loss: 1.0638819468021392
  batch 350 loss: 1.0314697921276093
  batch 400 loss: 1.0435960125923156
  batch 450 loss: 1.063202476501465
  batch 500 loss: 1.041683815717697
  batch 550 loss: 1.0597044825553894
  batch 600 loss: 1.0504359889030457
  batch 650 loss: 1.0943176770210266
  batch 700 loss: 1.056399916410446
  batch 750 loss: 1.053285721540451
  batch 800 loss: 1.0788507676124572
  batch 850 loss: 1.0642070257663727
  batch 900 loss: 1.0561540114879608
LOSS train 1.05615 valid 1.08212, valid PER 34.30%
EPOCH 20, Learning Rate: 0.005
  batch 50 loss: 1.0065835189819337
  batch 100 loss: 1.0320771276950835
  batch 150 loss: 1.0085846090316772
  batch 200 loss: 1.02101459980011
  batch 250 loss: 1.0121874976158143
  batch 300 loss: 1.0764773845672608
  batch 350 loss: 1.0024659466743469
  batch 400 loss: 1.039528920650482
  batch 450 loss: 1.0225136005878448
  batch 500 loss: 1.0560628437995911
  batch 550 loss: 1.1575179731845855
  batch 600 loss: 1.018457201719284
  batch 650 loss: 1.065302392244339
  batch 700 loss: 1.060314792394638
  batch 750 loss: 1.0525675511360169
  batch 800 loss: 1.0799616909027099
  batch 850 loss: 1.0675396144390106
  batch 900 loss: 1.080342447757721
LOSS train 1.08034 valid 1.08468, valid PER 34.75%
Training finished in 4.0 minutes.
Model saved to checkpoints/20231210_004706/model_18
Loading model from checkpoints/20231210_004706/model_18
SUB: 17.31%, DEL: 17.42%, INS: 1.15%, COR: 65.27%, PER: 35.88%
