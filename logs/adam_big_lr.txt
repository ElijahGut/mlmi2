Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.11, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='adam', grad_clip=1.5)
Total number of model parameters is 166952
EPOCH 1, Learning Rate: 0.11
  batch 50 loss: 20.46008511543274
  batch 100 loss: 4.86986768245697
  batch 150 loss: 4.353694128990173
  batch 200 loss: 4.293101778030396
  batch 250 loss: 4.497047853469849
  batch 300 loss: 4.144521174430847
  batch 350 loss: 4.14977988243103
  batch 400 loss: 4.14347888469696
  batch 450 loss: 4.095347900390625
  batch 500 loss: 4.116847934722901
  batch 550 loss: 4.100408754348755
  batch 600 loss: 4.070722460746765
  batch 650 loss: 4.123805241584778
  batch 700 loss: 4.18443229675293
  batch 750 loss: 4.195675239562989
  batch 800 loss: 4.080066552162171
  batch 850 loss: 4.256671380996704
  batch 900 loss: 4.0918142080307005
avg val loss: 3.997982978820801
LOSS train 4.09181 valid 3.99798, valid PER 97.31%
EPOCH 2, Learning Rate: 0.11
  batch 50 loss: 4.127774357795715
  batch 100 loss: 4.058313002586365
  batch 150 loss: 4.110312633514404
  batch 200 loss: 4.1683745861053465
  batch 250 loss: 4.141847367286682
  batch 300 loss: 4.137888307571411
  batch 350 loss: 4.133069434165955
  batch 400 loss: 4.2278355741500855
  batch 450 loss: 4.112654247283936
  batch 500 loss: 3.997197790145874
  batch 550 loss: 4.093598217964172
  batch 600 loss: 4.003978705406189
  batch 650 loss: 4.113470301628113
  batch 700 loss: 4.179280042648315
  batch 750 loss: 4.06250216960907
  batch 800 loss: 4.129764275550842
  batch 850 loss: 4.055497207641602
  batch 900 loss: 4.080129566192627
avg val loss: 4.151195049285889
LOSS train 4.08013 valid 4.15120, valid PER 97.33%
EPOCH 3, Learning Rate: 0.055
  batch 50 loss: 3.833691806793213
  batch 100 loss: 3.7644582080841062
  batch 150 loss: 3.794561152458191
  batch 200 loss: 3.8098098182678224
  batch 250 loss: 3.7946802377700806
  batch 300 loss: 3.7720412826538086
  batch 350 loss: 3.7761377573013304
  batch 400 loss: 3.8089121532440187
  batch 450 loss: 3.8348461246490477
  batch 500 loss: 3.7410999011993407
  batch 550 loss: 3.8319692516326906
  batch 600 loss: 3.7719601821899413
  batch 650 loss: 3.6589145851135254
  batch 700 loss: 3.695089592933655
  batch 750 loss: 3.800771870613098
  batch 800 loss: 3.7085008573532106
  batch 850 loss: 3.6343753290176393
  batch 900 loss: 3.7549268627166748
avg val loss: 3.754936933517456
LOSS train 3.75493 valid 3.75494, valid PER 88.55%
EPOCH 4, Learning Rate: 0.055
  batch 50 loss: 3.8686513662338258
  batch 100 loss: 3.878562078475952
  batch 150 loss: 3.688316192626953
  batch 200 loss: 3.8506268739700316
  batch 250 loss: 3.80589891910553
  batch 300 loss: 3.7245800161361693
  batch 350 loss: 3.706234698295593
  batch 400 loss: 3.727773275375366
  batch 450 loss: 3.7686905384063722
  batch 500 loss: 3.6379437017440797
  batch 550 loss: 3.7419963026046754
  batch 600 loss: 3.727349190711975
  batch 650 loss: 3.6963164091110228
  batch 700 loss: 3.6509092330932615
  batch 750 loss: 3.712478675842285
  batch 800 loss: 3.6605528736114503
  batch 850 loss: 3.6390098905563355
  batch 900 loss: 3.733614320755005
avg val loss: 3.7974984645843506
LOSS train 3.73361 valid 3.79750, valid PER 92.99%
EPOCH 5, Learning Rate: 0.0275
  batch 50 loss: 3.6513123083114625
  batch 100 loss: 3.575817346572876
  batch 150 loss: 3.5959255790710447
  batch 200 loss: 3.612497706413269
  batch 250 loss: 3.5731220054626465
  batch 300 loss: 3.5588603019714355
  batch 350 loss: 3.641622042655945
  batch 400 loss: 3.6350120401382444
  batch 450 loss: 3.5674073696136475
  batch 500 loss: 3.606018261909485
  batch 550 loss: 3.567865252494812
  batch 600 loss: 3.5478055477142334
  batch 650 loss: 3.5871127223968506
  batch 700 loss: 3.5787733793258667
  batch 750 loss: 3.5524920845031738
  batch 800 loss: 3.577920427322388
  batch 850 loss: 3.5777839374542237
  batch 900 loss: 3.6097822904586794
avg val loss: 3.568516731262207
LOSS train 3.60978 valid 3.56852, valid PER 86.63%
EPOCH 6, Learning Rate: 0.0275
  batch 50 loss: 3.5150033760070802
  batch 100 loss: 3.5784507179260254
  batch 150 loss: 3.548441519737244
  batch 200 loss: 3.538656873703003
  batch 250 loss: 3.588488063812256
  batch 300 loss: 3.5064044046401976
  batch 350 loss: 3.5377554368972777
  batch 400 loss: 3.5636147546768187
  batch 450 loss: 3.5605845499038695
  batch 500 loss: 3.5082466125488283
  batch 550 loss: 3.560488142967224
  batch 600 loss: 3.548137331008911
  batch 650 loss: 3.539921021461487
  batch 700 loss: 3.575205512046814
  batch 750 loss: 3.5318352222442626
  batch 800 loss: 3.5116555738449096
  batch 850 loss: 3.521034698486328
  batch 900 loss: 3.675456166267395
avg val loss: 3.589735746383667
LOSS train 3.67546 valid 3.58974, valid PER 88.79%
EPOCH 7, Learning Rate: 0.01375
  batch 50 loss: 3.5273484277725218
  batch 100 loss: 3.492734489440918
  batch 150 loss: 3.4721416521072386
  batch 200 loss: 3.5172765684127807
  batch 250 loss: 3.4756211185455324
  batch 300 loss: 3.4522870445251463
  batch 350 loss: 3.448229455947876
  batch 400 loss: 3.4205023288726806
  batch 450 loss: 3.506578650474548
  batch 500 loss: 3.4266065692901613
  batch 550 loss: 3.4700349283218386
  batch 600 loss: 3.462956876754761
  batch 650 loss: 3.4975164985656737
  batch 700 loss: 3.4803311014175415
  batch 750 loss: 3.4708985900878906
  batch 800 loss: 3.480878963470459
  batch 850 loss: 3.4470205783843992
  batch 900 loss: 3.507770199775696
avg val loss: 3.4539053440093994
LOSS train 3.50777 valid 3.45391, valid PER 87.23%
EPOCH 8, Learning Rate: 0.01375
  batch 50 loss: 3.4748109579086304
  batch 100 loss: 3.473315052986145
  batch 150 loss: 3.455869264602661
  batch 200 loss: 3.4884652280807495
  batch 250 loss: 3.4753741216659546
  batch 300 loss: 3.410487341880798
  batch 350 loss: 3.3979684638977052
  batch 400 loss: 3.502785940170288
  batch 450 loss: 3.470812358856201
  batch 500 loss: 3.4546143198013306
  batch 550 loss: 3.4096957445144653
  batch 600 loss: 3.4213488483428955
  batch 650 loss: 3.416902532577515
  batch 700 loss: 3.4395315504074095
  batch 750 loss: 3.4137153005599976
  batch 800 loss: 3.371310625076294
  batch 850 loss: 3.4324765014648437
  batch 900 loss: 3.529865069389343
avg val loss: 3.5284228324890137
LOSS train 3.52987 valid 3.52842, valid PER 87.19%
EPOCH 9, Learning Rate: 0.006875
  batch 50 loss: 3.4797838163375854
  batch 100 loss: 3.4683149242401123
  batch 150 loss: 3.446720814704895
  batch 200 loss: 3.4711830186843873
  batch 250 loss: 3.4492920207977296
  batch 300 loss: 3.3861793899536132
  batch 350 loss: 3.418240795135498
  batch 400 loss: 3.348537492752075
  batch 450 loss: 3.355752778053284
  batch 500 loss: 3.4165901374816894
  batch 550 loss: 3.366313877105713
  batch 600 loss: 3.433056993484497
  batch 650 loss: 3.4395722770690917
  batch 700 loss: 3.420596857070923
  batch 750 loss: 3.5099719285964968
  batch 800 loss: 3.41859977722168
  batch 850 loss: 3.4175221729278564
  batch 900 loss: 3.4148069858551025
avg val loss: 3.3838260173797607
LOSS train 3.41481 valid 3.38383, valid PER 86.84%
EPOCH 10, Learning Rate: 0.006875
  batch 50 loss: 3.365482668876648
  batch 100 loss: 3.419867687225342
  batch 150 loss: 3.3728464365005495
  batch 200 loss: 3.362724947929382
  batch 250 loss: 3.392040042877197
  batch 300 loss: 3.4089932203292848
  batch 350 loss: 3.3899820613861085
  batch 400 loss: 3.389456205368042
  batch 450 loss: 3.3877711963653563
  batch 500 loss: 3.4487479066848756
  batch 550 loss: 3.37450834274292
  batch 600 loss: 3.4178326320648194
  batch 650 loss: 3.3630530405044556
  batch 700 loss: 3.369855661392212
  batch 750 loss: 3.332177186012268
  batch 800 loss: 3.396783571243286
  batch 850 loss: 3.3650318956375123
  batch 900 loss: 3.3588125038146974
avg val loss: 3.379528522491455
LOSS train 3.35881 valid 3.37953, valid PER 86.30%
EPOCH 11, Learning Rate: 0.006875
  batch 50 loss: 3.38670202255249
  batch 100 loss: 3.3660143613815308
  batch 150 loss: 3.350531415939331
  batch 200 loss: 3.356158666610718
  batch 250 loss: 3.3860870218276977
  batch 300 loss: 3.3741014432907104
  batch 350 loss: 3.342739062309265
  batch 400 loss: 3.3878426790237426
  batch 450 loss: 3.3719207286834716
  batch 500 loss: 3.356085066795349
  batch 550 loss: 3.3665635347366334
  batch 600 loss: 3.3222816801071167
  batch 650 loss: 3.4894919061660765
  batch 700 loss: 3.3905006742477415
  batch 750 loss: 3.3553739547729493
  batch 800 loss: 3.3432376623153686
  batch 850 loss: 3.373624773025513
  batch 900 loss: 3.365554666519165
avg val loss: 3.352294921875
LOSS train 3.36555 valid 3.35229, valid PER 85.68%
EPOCH 12, Learning Rate: 0.006875
  batch 50 loss: 3.3601153802871706
  batch 100 loss: 3.3384565544128417
  batch 150 loss: 3.3238665676116943
  batch 200 loss: 3.359702272415161
  batch 250 loss: 3.3811035537719727
  batch 300 loss: 3.324375967979431
  batch 350 loss: 3.3825633192062377
  batch 400 loss: 3.4079144334793092
  batch 450 loss: 3.321074342727661
  batch 500 loss: 3.3541384172439574
  batch 550 loss: 3.3212179565429687
  batch 600 loss: 3.327840266227722
  batch 650 loss: 3.3359040784835816
  batch 700 loss: 3.323458933830261
  batch 750 loss: 3.3401270055770875
  batch 800 loss: 3.30712797164917
  batch 850 loss: 3.3546145248413084
  batch 900 loss: 3.355183272361755
avg val loss: 3.3341493606567383
LOSS train 3.35518 valid 3.33415, valid PER 85.40%
EPOCH 13, Learning Rate: 0.006875
  batch 50 loss: 3.324381470680237
  batch 100 loss: 3.3466412734985354
  batch 150 loss: 3.308952598571777
  batch 200 loss: 3.34341082572937
  batch 250 loss: 3.324789113998413
  batch 300 loss: 3.39166720867157
  batch 350 loss: 3.377276453971863
  batch 400 loss: 3.3915367460250856
  batch 450 loss: 3.2895034742355347
  batch 500 loss: 3.2975398206710818
  batch 550 loss: 3.3248377084732055
  batch 600 loss: 3.3514278984069823
  batch 650 loss: 3.3208096408843994
  batch 700 loss: 3.3529489183425905
  batch 750 loss: 3.3448037338256835
  batch 800 loss: 3.297893762588501
  batch 850 loss: 3.3691917753219602
  batch 900 loss: 3.4310375213623048
avg val loss: 3.3354592323303223
LOSS train 3.43104 valid 3.33546, valid PER 85.09%
EPOCH 14, Learning Rate: 0.0034375
  batch 50 loss: 3.2883652353286745
  batch 100 loss: 3.3687571144104003
  batch 150 loss: 3.3033785820007324
  batch 200 loss: 3.4043006563186644
  batch 250 loss: 3.3239149713516234
  batch 300 loss: 3.3357294464111327
  batch 350 loss: 3.360367865562439
  batch 400 loss: 3.3134074544906618
  batch 450 loss: 3.31728657245636
  batch 500 loss: 3.313136410713196
  batch 550 loss: 3.3037553787231446
  batch 600 loss: 3.2973901796340943
  batch 650 loss: 3.3440517711639406
  batch 700 loss: 3.3203840827941895
  batch 750 loss: 3.293530235290527
  batch 800 loss: 3.2823577308654786
  batch 850 loss: 3.2824782514572144
  batch 900 loss: 3.2976293992996215
avg val loss: 3.3308308124542236
LOSS train 3.29763 valid 3.33083, valid PER 84.55%
EPOCH 15, Learning Rate: 0.0034375
  batch 50 loss: 3.3669374990463257
  batch 100 loss: 3.301052465438843
  batch 150 loss: 3.3571239805221555
  batch 200 loss: 3.3003373336791992
  batch 250 loss: 3.277970914840698
  batch 300 loss: 3.3379055070877075
  batch 350 loss: 3.2797436332702636
  batch 400 loss: 3.294794316291809
  batch 450 loss: 3.333731255531311
  batch 500 loss: 3.2897378826141357
  batch 550 loss: 3.3486504650115965
  batch 600 loss: 3.347499008178711
  batch 650 loss: 3.320164666175842
  batch 700 loss: 3.312600088119507
  batch 750 loss: 3.3008919954299927
  batch 800 loss: 3.283992581367493
  batch 850 loss: 3.3209132719039918
  batch 900 loss: 3.30404296875
avg val loss: 3.3513081073760986
LOSS train 3.30404 valid 3.35131, valid PER 84.98%
EPOCH 16, Learning Rate: 0.00171875
  batch 50 loss: 3.3476850938797
  batch 100 loss: 3.2925296783447267
  batch 150 loss: 3.3204852867126466
  batch 200 loss: 3.4018935775756836
  batch 250 loss: 3.355028500556946
  batch 300 loss: 3.3150738096237182
  batch 350 loss: 3.3887537908554077
  batch 400 loss: 3.3395323753356934
  batch 450 loss: 3.389897313117981
  batch 500 loss: 3.2776791143417356
  batch 550 loss: 3.3404297113418577
  batch 600 loss: 3.318021774291992
  batch 650 loss: 3.3056513833999634
  batch 700 loss: 3.272181315422058
  batch 750 loss: 3.311800708770752
  batch 800 loss: 3.318289771080017
  batch 850 loss: 3.328888359069824
  batch 900 loss: 3.278861336708069
avg val loss: 3.325700044631958
LOSS train 3.27886 valid 3.32570, valid PER 84.95%
EPOCH 17, Learning Rate: 0.00171875
  batch 50 loss: 3.2937378978729246
  batch 100 loss: 3.314626669883728
  batch 150 loss: 3.2501655769348146
  batch 200 loss: 3.3271797704696655
  batch 250 loss: 3.2993358469009397
  batch 300 loss: 3.3041845464706423
  batch 350 loss: 3.2476471614837648
  batch 400 loss: 3.3070477724075316
  batch 450 loss: 3.276969051361084
  batch 500 loss: 3.33582582950592
  batch 550 loss: 3.3180915451049806
  batch 600 loss: 3.3419265794754027
  batch 650 loss: 3.2918469190597532
  batch 700 loss: 3.295189070701599
  batch 750 loss: 3.276046271324158
  batch 800 loss: 3.321473741531372
  batch 850 loss: 3.2859200286865233
  batch 900 loss: 3.2867765855789184
avg val loss: 3.310214042663574
LOSS train 3.28678 valid 3.31021, valid PER 84.79%
EPOCH 18, Learning Rate: 0.00171875
  batch 50 loss: 3.2507966470718386
  batch 100 loss: 3.2604763555526732
  batch 150 loss: 3.300495104789734
  batch 200 loss: 3.3061555862426757
  batch 250 loss: 3.333100733757019
  batch 300 loss: 3.2903759479522705
  batch 350 loss: 3.3286895179748535
  batch 400 loss: 3.286935806274414
  batch 450 loss: 3.3295162773132323
  batch 500 loss: 3.2951323795318603
  batch 550 loss: 3.264836049079895
  batch 600 loss: 3.2958020162582398
  batch 650 loss: 3.263115782737732
  batch 700 loss: 3.3150227165222166
  batch 750 loss: 3.2866891860961913
  batch 800 loss: 3.301740140914917
  batch 850 loss: 3.261915192604065
  batch 900 loss: 3.321303896903992
avg val loss: 3.3063924312591553
LOSS train 3.32130 valid 3.30639, valid PER 84.63%
EPOCH 19, Learning Rate: 0.00171875
  batch 50 loss: 3.2396516942977907
  batch 100 loss: 3.2514427518844604
  batch 150 loss: 3.2594207429885866
  batch 200 loss: 3.2689387607574463
  batch 250 loss: 3.286073760986328
  batch 300 loss: 3.297770299911499
  batch 350 loss: 3.289634418487549
  batch 400 loss: 3.260942368507385
  batch 450 loss: 3.3186251401901243
  batch 500 loss: 3.354574999809265
  batch 550 loss: 3.275283989906311
  batch 600 loss: 3.2452344369888304
  batch 650 loss: 3.31004777431488
  batch 700 loss: 3.3035382270812987
  batch 750 loss: 3.2688191986083983
  batch 800 loss: 3.2956178092956545
  batch 850 loss: 3.313526749610901
  batch 900 loss: 3.2609226083755494
avg val loss: 3.298159599304199
LOSS train 3.26092 valid 3.29816, valid PER 84.68%
EPOCH 20, Learning Rate: 0.00171875
  batch 50 loss: 3.256359548568726
  batch 100 loss: 3.3104798460006712
  batch 150 loss: 3.3275954341888427
  batch 200 loss: 3.274905343055725
  batch 250 loss: 3.2762898778915406
  batch 300 loss: 3.233140468597412
  batch 350 loss: 3.283175015449524
  batch 400 loss: 3.265430111885071
  batch 450 loss: 3.2530457592010498
  batch 500 loss: 3.2702144384384155
  batch 550 loss: 3.3061233806610106
  batch 600 loss: 3.2357627391815185
  batch 650 loss: 3.252226376533508
  batch 700 loss: 3.291738224029541
  batch 750 loss: 3.2690046310424803
  batch 800 loss: 3.2770245265960694
  batch 850 loss: 3.281060161590576
  batch 900 loss: 3.359004325866699
avg val loss: 3.3037424087524414
LOSS train 3.35900 valid 3.30374, valid PER 84.58%
Training finished in 4.0 minutes.
Model saved to checkpoints/20231211_100501/model_19
Loading model from checkpoints/20231211_100501/model_19
SUB: 4.00%, DEL: 80.95%, INS: 0.00%, COR: 15.04%, PER: 84.96%
