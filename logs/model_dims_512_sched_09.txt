Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=512, concat=1, lr=0.9, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 2240552
EPOCH 1, Learning Rate: 0.9
  batch 50 loss: 4.312786893844605
  batch 100 loss: 3.201427960395813
  batch 150 loss: 3.0206061506271364
  batch 200 loss: 2.8323876953125
  batch 250 loss: 2.761294164657593
  batch 300 loss: 2.5079710197448732
  batch 350 loss: 2.3908666276931765
  batch 400 loss: 2.3197443962097166
  batch 450 loss: 2.216753306388855
  batch 500 loss: 2.1293934178352356
  batch 550 loss: 2.0678270840644837
  batch 600 loss: 2.0105364179611205
  batch 650 loss: 1.93106032371521
  batch 700 loss: 1.918644642829895
  batch 750 loss: 1.8290073537826539
  batch 800 loss: 1.8089466619491577
  batch 850 loss: 1.787430136203766
  batch 900 loss: 1.7438755130767822
running loss: 40.59558892250061
LOSS train 1.74388 valid 1.75068, valid PER 60.35%
EPOCH 2, Learning Rate: 0.9
  batch 50 loss: 1.6968199586868287
  batch 100 loss: 1.635751678943634
  batch 150 loss: 1.610403940677643
  batch 200 loss: 1.5891266322135926
  batch 250 loss: 1.5978348064422607
  batch 300 loss: 1.5279279470443725
  batch 350 loss: 1.4576569938659667
  batch 400 loss: 1.4776596069335937
  batch 450 loss: 1.412990174293518
  batch 500 loss: 1.4686219549179078
  batch 550 loss: 1.4434579086303712
  batch 600 loss: 1.372701325416565
  batch 650 loss: 1.4017033243179322
  batch 700 loss: 1.407001438140869
  batch 750 loss: 1.3820291090011596
  batch 800 loss: 1.3442785823345185
  batch 850 loss: 1.3434169960021973
  batch 900 loss: 1.359700026512146
running loss: 31.140034198760986
LOSS train 1.35970 valid 1.27691, valid PER 37.98%
EPOCH 3, Learning Rate: 0.9
  batch 50 loss: 1.2957775139808654
  batch 100 loss: 1.2985977637767792
  batch 150 loss: 1.2827094674110413
  batch 200 loss: 1.2656198680400848
  batch 250 loss: 1.26246417760849
  batch 300 loss: 1.2388727796077728
  batch 350 loss: 1.2942872798442842
  batch 400 loss: 1.2540974009037018
  batch 450 loss: 1.2458371829986572
  batch 500 loss: 1.2084851491451263
  batch 550 loss: 1.2507022511959076
  batch 600 loss: 1.2218675363063811
  batch 650 loss: 1.1780730235576629
  batch 700 loss: 1.2221953630447389
  batch 750 loss: 1.2474529349803924
  batch 800 loss: 1.1716588366031646
  batch 850 loss: 1.211125682592392
  batch 900 loss: 1.140735045671463
running loss: 27.960672855377197
LOSS train 1.14074 valid 1.18500, valid PER 36.12%
EPOCH 4, Learning Rate: 0.9
  batch 50 loss: 1.1420880711078645
  batch 100 loss: 1.1913983929157257
  batch 150 loss: 1.1260755801200866
  batch 200 loss: 1.1542795026302337
  batch 250 loss: 1.1574772584438324
  batch 300 loss: 1.1545418310165405
  batch 350 loss: 1.0778888034820557
  batch 400 loss: 1.141452499628067
  batch 450 loss: 1.11016761302948
  batch 500 loss: 1.106654816865921
  batch 550 loss: 1.1409020328521728
  batch 600 loss: 1.1644290161132813
  batch 650 loss: 1.1124126291275025
  batch 700 loss: 1.1094136536121368
  batch 750 loss: 1.0767601788043977
  batch 800 loss: 1.0566812241077423
  batch 850 loss: 1.1020462620258331
  batch 900 loss: 1.1456450808048249
running loss: 25.445847511291504
LOSS train 1.14565 valid 1.06891, valid PER 33.77%
EPOCH 5, Learning Rate: 0.9
  batch 50 loss: 1.0306837284564971
  batch 100 loss: 1.0363123667240144
  batch 150 loss: 1.0877798533439635
  batch 200 loss: 1.0056265950202943
  batch 250 loss: 1.0433987283706665
  batch 300 loss: 1.04728107213974
  batch 350 loss: 1.0182794797420502
  batch 400 loss: 1.0262824189662934
  batch 450 loss: 1.0250436103343963
  batch 500 loss: 1.009918122291565
  batch 550 loss: 1.018240385055542
  batch 600 loss: 1.0933531177043916
  batch 650 loss: 1.0429894518852234
  batch 700 loss: 1.0611408519744874
  batch 750 loss: 0.9967277336120606
  batch 800 loss: 1.0185604417324066
  batch 850 loss: 1.060097771883011
  batch 900 loss: 1.0547856056690217
running loss: 24.406760036945343
LOSS train 1.05479 valid 1.02962, valid PER 32.55%
EPOCH 6, Learning Rate: 0.9
  batch 50 loss: 1.0099224245548248
  batch 100 loss: 0.9481111097335816
  batch 150 loss: 0.9299343121051789
  batch 200 loss: 0.963654990196228
  batch 250 loss: 1.0144344568252563
  batch 300 loss: 1.0012391519546509
  batch 350 loss: 0.9857256460189819
  batch 400 loss: 0.9703915345668793
  batch 450 loss: 0.9975286185741424
  batch 500 loss: 0.9961087346076966
  batch 550 loss: 1.0109911048412323
  batch 600 loss: 0.9744614243507386
  batch 650 loss: 0.9739650809764862
  batch 700 loss: 0.9801984393596649
  batch 750 loss: 0.9699084210395813
  batch 800 loss: 0.9615350329875946
  batch 850 loss: 0.9362228095531464
  batch 900 loss: 0.9646356523036956
running loss: 23.224234759807587
LOSS train 0.96464 valid 1.03980, valid PER 31.68%
EPOCH 7, Learning Rate: 0.9
  batch 50 loss: 0.9726296937465668
  batch 100 loss: 0.9605519843101501
  batch 150 loss: 0.9362215650081634
  batch 200 loss: 0.9242810881137848
  batch 250 loss: 0.9306237161159515
  batch 300 loss: 0.9233229649066925
  batch 350 loss: 0.9194019305706024
  batch 400 loss: 0.9443611514568329
  batch 450 loss: 0.9561556005477905
  batch 500 loss: 0.9476775884628296
  batch 550 loss: 0.9199963998794556
  batch 600 loss: 0.9160776340961456
  batch 650 loss: 0.9166662859916687
  batch 700 loss: 0.9258442533016205
  batch 750 loss: 0.9075446844100952
  batch 800 loss: 0.9067341876029968
  batch 850 loss: 0.9266673076152802
  batch 900 loss: 0.9543571150302887
running loss: 21.34597736597061
LOSS train 0.95436 valid 0.95870, valid PER 30.46%
EPOCH 8, Learning Rate: 0.9
  batch 50 loss: 0.8700891327857971
  batch 100 loss: 0.9308874642848969
  batch 150 loss: 0.8904222512245178
  batch 200 loss: 0.8562160074710846
  batch 250 loss: 0.8619321835041046
  batch 300 loss: 0.8338567018508911
  batch 350 loss: 0.9067153108119964
  batch 400 loss: 0.8702379381656646
  batch 450 loss: 0.9171830475330353
  batch 500 loss: 0.9623603677749634
  batch 550 loss: 0.8486659908294678
  batch 600 loss: 0.9059211349487305
  batch 650 loss: 0.9282172036170959
  batch 700 loss: 0.8712093305587768
  batch 750 loss: 0.8860565960407257
  batch 800 loss: 0.900249058008194
  batch 850 loss: 0.8760101962089538
  batch 900 loss: 0.8967616641521454
running loss: 21.277601659297943
LOSS train 0.89676 valid 0.92994, valid PER 29.44%
EPOCH 9, Learning Rate: 0.9
  batch 50 loss: 0.800287526845932
  batch 100 loss: 0.8258143293857575
  batch 150 loss: 0.8625351464748383
  batch 200 loss: 0.7968648034334183
  batch 250 loss: 0.839628963470459
  batch 300 loss: 0.8568289840221405
  batch 350 loss: 0.8657024705410004
  batch 400 loss: 0.8583478575944901
  batch 450 loss: 0.8502106416225433
  batch 500 loss: 0.830430303812027
  batch 550 loss: 0.8656740891933441
  batch 600 loss: 0.8862310910224914
  batch 650 loss: 0.8642792415618896
  batch 700 loss: 0.8183820462226867
  batch 750 loss: 0.8429526913166047
  batch 800 loss: 0.8376184129714965
  batch 850 loss: 0.8791720461845398
  batch 900 loss: 0.8268163067102432
running loss: 20.41131979227066
LOSS train 0.82682 valid 0.93558, valid PER 29.88%
EPOCH 10, Learning Rate: 0.9
  batch 50 loss: 0.7540228474140167
  batch 100 loss: 0.7826713943481445
  batch 150 loss: 0.8129743766784668
  batch 200 loss: 0.841799213886261
  batch 250 loss: 0.8329888451099395
  batch 300 loss: 0.770420960187912
  batch 350 loss: 0.8330671632289887
  batch 400 loss: 0.8025748085975647
  batch 450 loss: 0.79974569439888
  batch 500 loss: 0.8270427131652832
  batch 550 loss: 0.8266642737388611
  batch 600 loss: 0.7985003972053528
  batch 650 loss: 0.7874782454967498
  batch 700 loss: 0.8090896892547608
  batch 750 loss: 0.7917909514904022
  batch 800 loss: 0.7898826098442078
  batch 850 loss: 0.8230819809436798
  batch 900 loss: 0.8145353424549103
running loss: 19.29612648487091
LOSS train 0.81454 valid 0.95128, valid PER 29.44%
EPOCH 11, Learning Rate: 0.9
  batch 50 loss: 0.7148596215248108
  batch 100 loss: 0.7221013844013214
  batch 150 loss: 0.7289681339263916
  batch 200 loss: 0.7784630239009858
  batch 250 loss: 0.7821511596441268
  batch 300 loss: 0.7336493825912476
  batch 350 loss: 0.7532768952846527
  batch 400 loss: 0.7739438927173614
  batch 450 loss: 0.7770397090911865
  batch 500 loss: 0.7720992505550385
  batch 550 loss: 0.7929947531223297
  batch 600 loss: 0.7567452150583267
  batch 650 loss: 0.8349393701553345
  batch 700 loss: 0.7464140677452087
  batch 750 loss: 0.7523592436313629
  batch 800 loss: 0.7861042797565461
  batch 850 loss: 0.8387136363983154
  batch 900 loss: 0.810797564983368
running loss: 18.583406269550323
LOSS train 0.81080 valid 0.93259, valid PER 28.80%
EPOCH 12, Learning Rate: 0.9
  batch 50 loss: 0.7370317173004151
  batch 100 loss: 0.7058017122745514
  batch 150 loss: 0.7117865306138992
  batch 200 loss: 0.7353628724813461
  batch 250 loss: 0.7624779284000397
  batch 300 loss: 0.734503099322319
  batch 350 loss: 0.7353609812259674
  batch 400 loss: 0.7532903784513474
  batch 450 loss: 0.7588187098503113
  batch 500 loss: 0.7764288306236267
  batch 550 loss: 0.7186664974689484
  batch 600 loss: 0.7270153886079789
  batch 650 loss: 0.755734743475914
  batch 700 loss: 0.7482164120674133
  batch 750 loss: 0.7436967939138412
  batch 800 loss: 0.7288663327693939
  batch 850 loss: 0.7855226588249207
  batch 900 loss: 0.7567891776561737
running loss: 17.279194980859756
LOSS train 0.75679 valid 0.88130, valid PER 27.34%
EPOCH 13, Learning Rate: 0.9
  batch 50 loss: 0.6561730968952179
  batch 100 loss: 0.6892270582914353
  batch 150 loss: 0.6517907190322876
  batch 200 loss: 0.6911584740877151
  batch 250 loss: 0.6809841102361679
  batch 300 loss: 0.6748313790559769
  batch 350 loss: 0.6815522128343582
  batch 400 loss: 0.7120597928762435
  batch 450 loss: 0.7036907607316971
  batch 500 loss: 0.6629140865802765
  batch 550 loss: 0.7147057485580445
  batch 600 loss: 0.7093028217554093
  batch 650 loss: 0.7259032320976258
  batch 700 loss: 0.7821783882379532
  batch 750 loss: 0.7191060882806778
  batch 800 loss: 0.7130914813280106
  batch 850 loss: 0.7368641346693039
  batch 900 loss: 0.7381587785482406
running loss: 18.10965058207512
LOSS train 0.73816 valid 0.89868, valid PER 27.07%
EPOCH 14, Learning Rate: 0.45
  batch 50 loss: 0.6017237389087677
  batch 100 loss: 0.6014046835899353
  batch 150 loss: 0.5675394678115845
  batch 200 loss: 0.5795815712213517
  batch 250 loss: 0.586552397608757
  batch 300 loss: 0.5908736521005631
  batch 350 loss: 0.5640301418304443
  batch 400 loss: 0.567045488357544
  batch 450 loss: 0.5742970061302185
  batch 500 loss: 0.5720085775852204
  batch 550 loss: 0.5775177866220474
  batch 600 loss: 0.5452276062965393
  batch 650 loss: 0.5771321207284927
  batch 700 loss: 0.5916765958070755
  batch 750 loss: 0.5720874893665314
  batch 800 loss: 0.5393902486562729
  batch 850 loss: 0.5631435996294022
  batch 900 loss: 0.5749716877937316
running loss: 14.546869456768036
LOSS train 0.57497 valid 0.84124, valid PER 25.69%
EPOCH 15, Learning Rate: 0.45
  batch 50 loss: 0.5032756406068802
  batch 100 loss: 0.49546534538269044
  batch 150 loss: 0.5115106344223023
  batch 200 loss: 0.533517792224884
  batch 250 loss: 0.542629565000534
  batch 300 loss: 0.5110032969713211
  batch 350 loss: 0.5090326279401779
  batch 400 loss: 0.5093917208909988
  batch 450 loss: 0.5148819535970688
  batch 500 loss: 0.49621642887592315
  batch 550 loss: 0.520822668671608
  batch 600 loss: 0.5375251698493958
  batch 650 loss: 0.5398507034778595
  batch 700 loss: 0.5442556542158127
  batch 750 loss: 0.5339631474018097
  batch 800 loss: 0.5346059578657151
  batch 850 loss: 0.5141974025964737
  batch 900 loss: 0.5319364368915558
running loss: 11.856085330247879
LOSS train 0.53194 valid 0.83530, valid PER 24.84%
EPOCH 16, Learning Rate: 0.45
  batch 50 loss: 0.49064503371715545
  batch 100 loss: 0.47693139016628266
  batch 150 loss: 0.4697046849131584
  batch 200 loss: 0.4792067420482635
  batch 250 loss: 0.494921972155571
  batch 300 loss: 0.48457962572574614
  batch 350 loss: 0.5010881662368775
  batch 400 loss: 0.49552425384521487
  batch 450 loss: 0.5077203673124313
  batch 500 loss: 0.4735381692647934
  batch 550 loss: 0.499244219660759
  batch 600 loss: 0.4721963512897491
  batch 650 loss: 0.5206274545192718
  batch 700 loss: 0.5039819031953812
  batch 750 loss: 0.5183702272176742
  batch 800 loss: 0.5208394336700439
  batch 850 loss: 0.5063132601976394
  batch 900 loss: 0.5118328994512558
running loss: 12.243469268083572
LOSS train 0.51183 valid 0.85255, valid PER 24.76%
EPOCH 17, Learning Rate: 0.225
  batch 50 loss: 0.44089926600456236
  batch 100 loss: 0.4353423431515694
  batch 150 loss: 0.4197899740934372
  batch 200 loss: 0.4129714781045914
  batch 250 loss: 0.4439924043416977
  batch 300 loss: 0.41929165065288543
  batch 350 loss: 0.40017114967107775
  batch 400 loss: 0.440817606151104
  batch 450 loss: 0.4329197162389755
  batch 500 loss: 0.39965544521808627
  batch 550 loss: 0.41345495522022246
  batch 600 loss: 0.4329184228181839
  batch 650 loss: 0.4163488060235977
  batch 700 loss: 0.41554067373275755
  batch 750 loss: 0.41125067323446274
  batch 800 loss: 0.4059408590197563
  batch 850 loss: 0.4253784245252609
  batch 900 loss: 0.402397421002388
running loss: 10.265329271554947
LOSS train 0.40240 valid 0.85559, valid PER 24.30%
EPOCH 18, Learning Rate: 0.225
  batch 50 loss: 0.3758473640680313
  batch 100 loss: 0.38864473909139635
  batch 150 loss: 0.38673004508018494
  batch 200 loss: 0.4021457415819168
  batch 250 loss: 0.3961732548475265
  batch 300 loss: 0.37414459109306336
  batch 350 loss: 0.37907192051410676
  batch 400 loss: 0.388541579246521
  batch 450 loss: 0.40199841022491456
  batch 500 loss: 0.3936084032058716
  batch 550 loss: 0.404287029504776
  batch 600 loss: 0.3874922966957092
  batch 650 loss: 0.3742639124393463
  batch 700 loss: 0.4049126660823822
  batch 750 loss: 0.37947879046201705
  batch 800 loss: 0.38769917875528337
  batch 850 loss: 0.3909947922825813
  batch 900 loss: 0.39869908064603804
running loss: 9.337830364704132
LOSS train 0.39870 valid 0.87036, valid PER 24.19%
EPOCH 19, Learning Rate: 0.225
  batch 50 loss: 0.3647504451870918
  batch 100 loss: 0.34116883754730226
  batch 150 loss: 0.3630782866477966
  batch 200 loss: 0.3608050152659416
  batch 250 loss: 0.36441013246774673
  batch 300 loss: 0.3740555277466774
  batch 350 loss: 0.3590984669327736
  batch 400 loss: 0.3634897521138191
  batch 450 loss: 0.38424650847911834
  batch 500 loss: 0.3607206293940544
  batch 550 loss: 0.35584451973438264
  batch 600 loss: 0.3537483549118042
  batch 650 loss: 0.38683143377304074
  batch 700 loss: 0.36627160727977753
  batch 750 loss: 0.35501940816640853
  batch 800 loss: 0.37718009650707246
  batch 850 loss: 0.3722731477022171
  batch 900 loss: 0.3766656744480133
running loss: 8.975320994853973
LOSS train 0.37667 valid 0.89113, valid PER 24.47%
EPOCH 20, Learning Rate: 0.225
  batch 50 loss: 0.3442317169904709
  batch 100 loss: 0.32887033551931383
  batch 150 loss: 0.3293972361087799
  batch 200 loss: 0.33140451401472093
  batch 250 loss: 0.3449726250767708
  batch 300 loss: 0.3552935367822647
  batch 350 loss: 0.32022650182247164
  batch 400 loss: 0.34369004279375076
  batch 450 loss: 0.34662668615579606
  batch 500 loss: 0.3256080812215805
  batch 550 loss: 0.3783260577917099
  batch 600 loss: 0.33651191979646683
  batch 650 loss: 0.3461392864584923
  batch 700 loss: 0.34597651243209837
  batch 750 loss: 0.3393385609984398
  batch 800 loss: 0.3605591303110123
  batch 850 loss: 0.3594541144371033
  batch 900 loss: 0.3425625717639923
running loss: 8.346243098378181
LOSS train 0.34256 valid 0.90471, valid PER 24.18%
Training finished in 8.0 minutes.
Model saved to checkpoints/20231210_043328/model_15
Loading model from checkpoints/20231210_043328/model_15
SUB: 14.82%, DEL: 9.65%, INS: 2.09%, COR: 75.53%, PER: 26.55%
