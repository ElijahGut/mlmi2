Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=512, concat=1, lr=0.9, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 2240552
EPOCH 1, Learning Rate: 0.9
  batch 50 loss: 4.312831244468689
  batch 100 loss: 3.2013282918930055
  batch 150 loss: 2.999775424003601
  batch 200 loss: 2.7944188356399535
  batch 250 loss: 2.849874982833862
  batch 300 loss: 2.7100490522384644
  batch 350 loss: 2.496878252029419
  batch 400 loss: 2.418719992637634
  batch 450 loss: 2.3618585443496705
  batch 500 loss: 2.3146008539199827
  batch 550 loss: 2.179961986541748
  batch 600 loss: 2.050920205116272
  batch 650 loss: 1.9670436930656434
  batch 700 loss: 1.9290629053115844
  batch 750 loss: 1.854549262523651
  batch 800 loss: 1.8366947650909424
  batch 850 loss: 1.7850230407714844
  batch 900 loss: 1.7733475875854492
avg val loss: 1.7654920816421509
LOSS train 1.77335 valid 1.76549, valid PER 61.96%
EPOCH 2, Learning Rate: 0.9
  batch 50 loss: 1.6983609795570374
  batch 100 loss: 1.6394928765296937
  batch 150 loss: 1.6205507183074952
  batch 200 loss: 1.6352217864990235
  batch 250 loss: 1.643043749332428
  batch 300 loss: 1.5872959208488464
  batch 350 loss: 1.4827395057678223
  batch 400 loss: 1.5000584888458253
  batch 450 loss: 1.4373326182365418
  batch 500 loss: 1.5089432454109193
  batch 550 loss: 1.4453789615631103
  batch 600 loss: 1.3960929942131042
  batch 650 loss: 1.4243040084838867
  batch 700 loss: 1.4158084762096406
  batch 750 loss: 1.3775770854949951
  batch 800 loss: 1.3352311432361603
  batch 850 loss: 1.3094513595104218
  batch 900 loss: 1.3765693426132202
avg val loss: 1.2469667196273804
LOSS train 1.37657 valid 1.24697, valid PER 39.21%
EPOCH 3, Learning Rate: 0.9
  batch 50 loss: 1.2957123351097106
  batch 100 loss: 1.286380388736725
  batch 150 loss: 1.2890510487556457
  batch 200 loss: 1.2689965915679933
  batch 250 loss: 1.2675793313980102
  batch 300 loss: 1.239403450489044
  batch 350 loss: 1.334488208293915
  batch 400 loss: 1.2721948659420013
  batch 450 loss: 1.2537048947811127
  batch 500 loss: 1.2031944382190705
  batch 550 loss: 1.2563986563682557
  batch 600 loss: 1.2102940261363984
  batch 650 loss: 1.1688472461700439
  batch 700 loss: 1.2330131387710572
  batch 750 loss: 1.2696646201610564
  batch 800 loss: 1.1773276233673096
  batch 850 loss: 1.222951555252075
  batch 900 loss: 1.1647732472419738
avg val loss: 1.2028506994247437
LOSS train 1.16477 valid 1.20285, valid PER 36.88%
EPOCH 4, Learning Rate: 0.9
  batch 50 loss: 1.1294535136222839
  batch 100 loss: 1.1552731156349183
  batch 150 loss: 1.1147480356693267
  batch 200 loss: 1.1826191222667695
  batch 250 loss: 1.180286194086075
  batch 300 loss: 1.1663581895828248
  batch 350 loss: 1.0876362907886505
  batch 400 loss: 1.1155474078655243
  batch 450 loss: 1.113142410516739
  batch 500 loss: 1.1120468544960023
  batch 550 loss: 1.1445250046253204
  batch 600 loss: 1.1531097042560576
  batch 650 loss: 1.1391240286827087
  batch 700 loss: 1.1240014553070068
  batch 750 loss: 1.0967599511146546
  batch 800 loss: 1.048910130262375
  batch 850 loss: 1.0948729372024537
  batch 900 loss: 1.1537352299690247
avg val loss: 1.0739390850067139
LOSS train 1.15374 valid 1.07394, valid PER 34.07%
EPOCH 5, Learning Rate: 0.9
  batch 50 loss: 1.0313784217834472
  batch 100 loss: 1.0512285673618316
  batch 150 loss: 1.0885551464557648
  batch 200 loss: 1.0269934689998628
  batch 250 loss: 1.0330986201763153
  batch 300 loss: 1.0570845675468445
  batch 350 loss: 1.0391719627380371
  batch 400 loss: 1.023537313938141
  batch 450 loss: 1.0466990160942078
  batch 500 loss: 1.0629137325286866
  batch 550 loss: 1.0121820878982544
  batch 600 loss: 1.1002839601039887
  batch 650 loss: 1.0419660806655884
  batch 700 loss: 1.0650571084022522
  batch 750 loss: 0.9934629571437835
  batch 800 loss: 1.0364962244033813
  batch 850 loss: 1.0617537260055543
  batch 900 loss: 1.0515321862697602
avg val loss: 1.0581146478652954
LOSS train 1.05153 valid 1.05811, valid PER 32.73%
EPOCH 6, Learning Rate: 0.9
  batch 50 loss: 1.0669754564762115
  batch 100 loss: 0.9841639494895935
  batch 150 loss: 0.9561972916126251
  batch 200 loss: 1.0134047627449037
  batch 250 loss: 1.0213933825492858
  batch 300 loss: 0.9908088362216949
  batch 350 loss: 0.9885073506832123
  batch 400 loss: 0.9653963327407837
  batch 450 loss: 1.0240878570079803
  batch 500 loss: 1.0212360894680024
  batch 550 loss: 1.012284916639328
  batch 600 loss: 0.947446277141571
  batch 650 loss: 0.9752775764465332
  batch 700 loss: 0.9953039813041688
  batch 750 loss: 0.9808910179138184
  batch 800 loss: 0.9851863610744477
  batch 850 loss: 0.9658799815177918
  batch 900 loss: 0.9881449103355407
avg val loss: 1.0404059886932373
LOSS train 0.98814 valid 1.04041, valid PER 32.04%
EPOCH 7, Learning Rate: 0.9
  batch 50 loss: 0.9588670301437378
  batch 100 loss: 0.9480862867832184
  batch 150 loss: 0.9133285009860992
  batch 200 loss: 0.9065848958492279
  batch 250 loss: 0.9242037677764893
  batch 300 loss: 0.9000526440143585
  batch 350 loss: 0.9099383115768432
  batch 400 loss: 0.9451573920249939
  batch 450 loss: 0.9392907011508942
  batch 500 loss: 0.930361419916153
  batch 550 loss: 0.9160214424133301
  batch 600 loss: 0.9239911031723023
  batch 650 loss: 0.908346791267395
  batch 700 loss: 0.9390449416637421
  batch 750 loss: 0.8988138353824615
  batch 800 loss: 0.9138245403766632
  batch 850 loss: 0.9276458597183228
  batch 900 loss: 0.978611478805542
avg val loss: 0.9794870018959045
LOSS train 0.97861 valid 0.97949, valid PER 30.98%
EPOCH 8, Learning Rate: 0.9
  batch 50 loss: 0.8598929738998413
  batch 100 loss: 0.8702757620811462
  batch 150 loss: 0.882844021320343
  batch 200 loss: 0.8619817268848419
  batch 250 loss: 0.8716892099380493
  batch 300 loss: 0.82627934217453
  batch 350 loss: 0.9195870935916901
  batch 400 loss: 0.8650146532058716
  batch 450 loss: 0.8879170453548432
  batch 500 loss: 0.9264880490303039
  batch 550 loss: 0.853936597108841
  batch 600 loss: 0.9048601126670838
  batch 650 loss: 0.9370205998420715
  batch 700 loss: 0.863784909248352
  batch 750 loss: 0.8802484083175659
  batch 800 loss: 0.9046735560894013
  batch 850 loss: 0.874742032289505
  batch 900 loss: 0.8890674448013306
avg val loss: 0.9391145706176758
LOSS train 0.88907 valid 0.93911, valid PER 29.24%
EPOCH 9, Learning Rate: 0.9
  batch 50 loss: 0.7938283514976502
  batch 100 loss: 0.8505551636219024
  batch 150 loss: 0.8639631605148316
  batch 200 loss: 0.8136807560920716
  batch 250 loss: 0.8416173028945922
  batch 300 loss: 0.8344572353363037
  batch 350 loss: 0.8721352016925812
  batch 400 loss: 0.8509962892532349
  batch 450 loss: 0.8353749465942383
  batch 500 loss: 0.8059709370136261
  batch 550 loss: 0.8513348668813705
  batch 600 loss: 0.8626964545249939
  batch 650 loss: 0.8498914694786072
  batch 700 loss: 0.810662122964859
  batch 750 loss: 0.8279605627059936
  batch 800 loss: 0.8390752804279328
  batch 850 loss: 0.8540717470645904
  batch 900 loss: 0.8161944961547851
avg val loss: 0.9229224324226379
LOSS train 0.81619 valid 0.92292, valid PER 29.03%
EPOCH 10, Learning Rate: 0.9
  batch 50 loss: 0.7306199139356613
  batch 100 loss: 0.7773589432239533
  batch 150 loss: 0.8205395448207855
  batch 200 loss: 0.8310908335447311
  batch 250 loss: 0.8033698606491089
  batch 300 loss: 0.7599376511573791
  batch 350 loss: 0.8197238850593567
  batch 400 loss: 0.7827448439598084
  batch 450 loss: 0.7720000970363617
  batch 500 loss: 0.8071715122461319
  batch 550 loss: 0.8181927871704101
  batch 600 loss: 0.7922686076164246
  batch 650 loss: 0.7739652562141418
  batch 700 loss: 0.8083263444900513
  batch 750 loss: 0.7867683565616608
  batch 800 loss: 0.8063586521148681
  batch 850 loss: 0.8168987345695495
  batch 900 loss: 0.8045769822597504
avg val loss: 0.9163727164268494
LOSS train 0.80458 valid 0.91637, valid PER 28.93%
EPOCH 11, Learning Rate: 0.9
  batch 50 loss: 0.7223660361766815
  batch 100 loss: 0.6840637767314911
  batch 150 loss: 0.7160923475027084
  batch 200 loss: 0.7683947384357452
  batch 250 loss: 0.7557726764678955
  batch 300 loss: 0.7267138838768006
  batch 350 loss: 0.7502871692180634
  batch 400 loss: 0.7716441619396209
  batch 450 loss: 0.7564443385601044
  batch 500 loss: 0.7193995642662049
  batch 550 loss: 0.7691394138336182
  batch 600 loss: 0.7272435414791107
  batch 650 loss: 0.8122092545032501
  batch 700 loss: 0.7408740568161011
  batch 750 loss: 0.7817839014530182
  batch 800 loss: 0.7798046296834946
  batch 850 loss: 0.8072457981109619
  batch 900 loss: 0.7992303502559662
avg val loss: 0.9120700359344482
LOSS train 0.79923 valid 0.91207, valid PER 27.60%
EPOCH 12, Learning Rate: 0.9
  batch 50 loss: 0.7002102988958359
  batch 100 loss: 0.6868027770519256
  batch 150 loss: 0.6860901963710785
  batch 200 loss: 0.6953889793157577
  batch 250 loss: 0.7344488835334778
  batch 300 loss: 0.7098428446054459
  batch 350 loss: 0.7063800531625748
  batch 400 loss: 0.7347836345434189
  batch 450 loss: 0.7424524497985839
  batch 500 loss: 0.7541178226470947
  batch 550 loss: 0.7116155540943145
  batch 600 loss: 0.7224715995788574
  batch 650 loss: 0.7723559010028839
  batch 700 loss: 0.7344174098968506
  batch 750 loss: 0.7396251052618027
  batch 800 loss: 0.7452903795242309
  batch 850 loss: 0.8010963767766952
  batch 900 loss: 0.7767675065994263
avg val loss: 0.8841610550880432
LOSS train 0.77677 valid 0.88416, valid PER 28.25%
EPOCH 13, Learning Rate: 0.9
  batch 50 loss: 0.6661723572015762
  batch 100 loss: 0.6946612000465393
  batch 150 loss: 0.6690337610244751
  batch 200 loss: 0.7098572307825088
  batch 250 loss: 0.6806653183698654
  batch 300 loss: 0.6654311168193817
  batch 350 loss: 0.6790234696865082
  batch 400 loss: 0.7128604239225388
  batch 450 loss: 0.7013611268997192
  batch 500 loss: 0.6841774785518646
  batch 550 loss: 0.7283863872289658
  batch 600 loss: 0.7019377362728119
  batch 650 loss: 0.7297914791107177
  batch 700 loss: 0.7354479461908341
  batch 750 loss: 0.6727539020776748
  batch 800 loss: 0.6862334883213044
  batch 850 loss: 0.7294178229570388
  batch 900 loss: 0.7218118906021118
avg val loss: 0.8735422492027283
LOSS train 0.72181 valid 0.87354, valid PER 26.13%
EPOCH 14, Learning Rate: 0.9
  batch 50 loss: 0.6334248596429825
  batch 100 loss: 0.6276856106519699
  batch 150 loss: 0.6385989463329316
  batch 200 loss: 0.6377810740470886
  batch 250 loss: 0.6584346187114716
  batch 300 loss: 0.6773109465837479
  batch 350 loss: 0.6314815610647202
  batch 400 loss: 0.6394210118055343
  batch 450 loss: 0.6850078129768371
  batch 500 loss: 0.6814456766843796
  batch 550 loss: 0.704152466058731
  batch 600 loss: 0.6530313533544541
  batch 650 loss: 0.6982642513513565
  batch 700 loss: 0.7181759011745453
  batch 750 loss: 0.6844872397184372
  batch 800 loss: 0.654690962433815
  batch 850 loss: 0.6969765853881836
  batch 900 loss: 0.69336550116539
avg val loss: 0.9041021466255188
LOSS train 0.69337 valid 0.90410, valid PER 27.06%
EPOCH 15, Learning Rate: 0.45
  batch 50 loss: 0.576123663187027
  batch 100 loss: 0.558627262711525
  batch 150 loss: 0.5534429717063903
  batch 200 loss: 0.5602553308010101
  batch 250 loss: 0.5399172902107239
  batch 300 loss: 0.5200729250907898
  batch 350 loss: 0.5386951327323913
  batch 400 loss: 0.5190915936231613
  batch 450 loss: 0.5147432494163513
  batch 500 loss: 0.5054908084869385
  batch 550 loss: 0.5374243605136871
  batch 600 loss: 0.5285474848747254
  batch 650 loss: 0.5457198017835617
  batch 700 loss: 0.5518362796306611
  batch 750 loss: 0.5521546542644501
  batch 800 loss: 0.5221132624149323
  batch 850 loss: 0.5081062281131744
  batch 900 loss: 0.5309026288986206
avg val loss: 0.8388808369636536
LOSS train 0.53090 valid 0.83888, valid PER 25.20%
EPOCH 16, Learning Rate: 0.45
  batch 50 loss: 0.48672949850559233
  batch 100 loss: 0.4738501864671707
  batch 150 loss: 0.4691084212064743
  batch 200 loss: 0.47519963622093203
  batch 250 loss: 0.48909413933753965
  batch 300 loss: 0.48658361911773684
  batch 350 loss: 0.4897957998514175
  batch 400 loss: 0.4904631793498993
  batch 450 loss: 0.49117264807224276
  batch 500 loss: 0.4793388897180557
  batch 550 loss: 0.48152326583862304
  batch 600 loss: 0.47668504357337954
  batch 650 loss: 0.5071235209703445
  batch 700 loss: 0.4898260581493378
  batch 750 loss: 0.4997057902812958
  batch 800 loss: 0.5044432502985
  batch 850 loss: 0.5064880895614624
  batch 900 loss: 0.5081689912080765
avg val loss: 0.8508316874504089
LOSS train 0.50817 valid 0.85083, valid PER 24.52%
EPOCH 17, Learning Rate: 0.225
  batch 50 loss: 0.426150341629982
  batch 100 loss: 0.4311115378141403
  batch 150 loss: 0.4157101029157639
  batch 200 loss: 0.40692791491746905
  batch 250 loss: 0.42214050620794297
  batch 300 loss: 0.4090029525756836
  batch 350 loss: 0.40008448988199236
  batch 400 loss: 0.4331249833106995
  batch 450 loss: 0.414844009578228
  batch 500 loss: 0.38997030586004255
  batch 550 loss: 0.4106853711605072
  batch 600 loss: 0.4201228791475296
  batch 650 loss: 0.41877975553274155
  batch 700 loss: 0.3959856674075127
  batch 750 loss: 0.3966018655896187
  batch 800 loss: 0.3953010523319244
  batch 850 loss: 0.42505635172128675
  batch 900 loss: 0.4058439520001411
avg val loss: 0.8582521677017212
LOSS train 0.40584 valid 0.85825, valid PER 24.03%
EPOCH 18, Learning Rate: 0.1125
  batch 50 loss: 0.36554348886013033
  batch 100 loss: 0.3745719891786575
  batch 150 loss: 0.38473082065582276
  batch 200 loss: 0.37462506294250486
  batch 250 loss: 0.38234665751457214
  batch 300 loss: 0.3407665628194809
  batch 350 loss: 0.36405562967061994
  batch 400 loss: 0.36521466583013534
  batch 450 loss: 0.3689484640955925
  batch 500 loss: 0.3784498864412308
  batch 550 loss: 0.38286799609661104
  batch 600 loss: 0.34592268377542496
  batch 650 loss: 0.33637058138847353
  batch 700 loss: 0.37951901972293856
  batch 750 loss: 0.34876187533140185
  batch 800 loss: 0.3635612350702286
  batch 850 loss: 0.3561682778596878
  batch 900 loss: 0.3658548867702484
avg val loss: 0.8577557802200317
LOSS train 0.36585 valid 0.85776, valid PER 24.06%
EPOCH 19, Learning Rate: 0.05625
  batch 50 loss: 0.35007242769002916
  batch 100 loss: 0.33726017326116564
  batch 150 loss: 0.3331586739420891
  batch 200 loss: 0.34353856116533277
  batch 250 loss: 0.3388786298036575
  batch 300 loss: 0.3478424349427223
  batch 350 loss: 0.3280832362174988
  batch 400 loss: 0.335722581744194
  batch 450 loss: 0.35226175636053086
  batch 500 loss: 0.34774151533842085
  batch 550 loss: 0.3358090463280678
  batch 600 loss: 0.3306842604279518
  batch 650 loss: 0.36312404453754427
  batch 700 loss: 0.33473231732845304
  batch 750 loss: 0.331057288646698
  batch 800 loss: 0.34588192522525785
  batch 850 loss: 0.3479489916563034
  batch 900 loss: 0.3385073646903038
avg val loss: 0.8629842400550842
LOSS train 0.33851 valid 0.86298, valid PER 24.05%
EPOCH 20, Learning Rate: 0.028125
  batch 50 loss: 0.3326591464877129
  batch 100 loss: 0.3289711010456085
  batch 150 loss: 0.30884565114974977
  batch 200 loss: 0.31503087550401687
  batch 250 loss: 0.3313026094436646
  batch 300 loss: 0.34496989905834197
  batch 350 loss: 0.3058868035674095
  batch 400 loss: 0.332463259100914
  batch 450 loss: 0.33456466227769854
  batch 500 loss: 0.31492990761995315
  batch 550 loss: 0.35973855048418046
  batch 600 loss: 0.3097064533829689
  batch 650 loss: 0.3221197059750557
  batch 700 loss: 0.3293854650855064
  batch 750 loss: 0.31853226095438003
  batch 800 loss: 0.3393398833274841
  batch 850 loss: 0.3357683426141739
  batch 900 loss: 0.3255827933549881
avg val loss: 0.8658754825592041
LOSS train 0.32558 valid 0.86588, valid PER 23.88%
Training finished in 8.0 minutes.
Model saved to checkpoints/20231210_135508/model_15
Loading model from checkpoints/20231210_135508/model_15
SUB: 14.35%, DEL: 10.27%, INS: 2.00%, COR: 75.38%, PER: 26.62%
