Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.01, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='adam', grad_clip=None)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.369481439590454
  batch 100 loss: 2.9310345220565797
  batch 150 loss: 2.7592675065994263
  batch 200 loss: 2.4534643125534057
  batch 250 loss: 2.307114849090576
  batch 300 loss: 2.1780739784240724
  batch 350 loss: 2.065168950557709
  batch 400 loss: 2.052043125629425
  batch 450 loss: 1.9821794033050537
  batch 500 loss: 1.895154414176941
  batch 550 loss: 1.8534828948974609
  batch 600 loss: 1.8126202416419983
  batch 650 loss: 1.7536333990097046
  batch 700 loss: 1.777459614276886
  batch 750 loss: 1.7239399433135987
  batch 800 loss: 1.7094726061820984
  batch 850 loss: 1.6977263188362122
  batch 900 loss: 1.6716623067855836
LOSS train 1.67166 valid 1.60389, valid PER 60.01%
EPOCH 2:
  batch 50 loss: 1.6106983947753906
  batch 100 loss: 1.600618724822998
  batch 150 loss: 1.57426376581192
  batch 200 loss: 1.615017864704132
  batch 250 loss: 1.6138196778297424
  batch 300 loss: 1.5887485313415528
  batch 350 loss: 1.5186777329444885
  batch 400 loss: 1.5438168621063233
  batch 450 loss: 1.5024278926849366
  batch 500 loss: 1.5357667541503905
  batch 550 loss: 1.5392300391197205
  batch 600 loss: 1.4938504076004029
  batch 650 loss: 1.525883765220642
  batch 700 loss: 1.5532521104812622
  batch 750 loss: 1.5099606966972352
  batch 800 loss: 1.4362113618850707
  batch 850 loss: 1.4750485157966613
  batch 900 loss: 1.475815622806549
LOSS train 1.47582 valid 1.34405, valid PER 45.12%
EPOCH 3:
  batch 50 loss: 1.4509428715705872
  batch 100 loss: 1.4262276816368102
  batch 150 loss: 1.4160098242759704
  batch 200 loss: 1.4044251823425293
  batch 250 loss: 1.412239842414856
  batch 300 loss: 1.3693246674537658
  batch 350 loss: 1.4755595922470093
  batch 400 loss: 1.4104622101783753
  batch 450 loss: 1.4544760966300965
  batch 500 loss: 1.404777181148529
  batch 550 loss: 1.3949627733230592
  batch 600 loss: 1.4087050485610961
  batch 650 loss: 1.4046310329437255
  batch 700 loss: 1.3969161009788513
  batch 750 loss: 1.4755646252632142
  batch 800 loss: 1.3815614867210388
  batch 850 loss: 1.4155901670455933
  batch 900 loss: 1.4124904823303224
LOSS train 1.41249 valid 1.36152, valid PER 43.85%
EPOCH 4:
  batch 50 loss: 1.34332040309906
  batch 100 loss: 1.404697024822235
  batch 150 loss: 1.3865169525146483
  batch 200 loss: 1.4464684295654298
  batch 250 loss: 1.4801246547698974
  batch 300 loss: 1.4493203902244567
  batch 350 loss: 1.3885635113716126
  batch 400 loss: 1.5496646666526794
  batch 450 loss: 1.4918594574928283
  batch 500 loss: 1.4528643012046814
  batch 550 loss: 1.4594183588027954
  batch 600 loss: 1.4597478199005127
  batch 650 loss: 1.4215190744400024
  batch 700 loss: 1.4049588203430177
  batch 750 loss: 1.404981427192688
  batch 800 loss: 1.357281470298767
  batch 850 loss: 1.4303047490119933
  batch 900 loss: 1.4412163782119751
LOSS train 1.44122 valid 1.31690, valid PER 42.67%
EPOCH 5:
  batch 50 loss: 1.380028862953186
  batch 100 loss: 1.3555551767349243
  batch 150 loss: 1.425913655757904
  batch 200 loss: 1.514057228565216
  batch 250 loss: 1.4309659957885743
  batch 300 loss: 1.419854350090027
  batch 350 loss: 1.4250646996498109
  batch 400 loss: 1.3883376097679139
  batch 450 loss: 1.3791798067092895
  batch 500 loss: 1.3719901180267333
  batch 550 loss: 1.3083132553100585
  batch 600 loss: 1.3784848737716675
  batch 650 loss: 1.4031138634681701
  batch 700 loss: 1.4157033967971802
  batch 750 loss: 1.3728577995300293
  batch 800 loss: 1.3815848660469054
  batch 850 loss: 1.389810609817505
  batch 900 loss: 1.473771653175354
LOSS train 1.47377 valid 1.35166, valid PER 42.57%
EPOCH 6:
  batch 50 loss: 1.4162199068069459
  batch 100 loss: 1.3495581471920013
  batch 150 loss: 1.3328285455703734
  batch 200 loss: 1.3724669122695923
  batch 250 loss: 1.3760633182525634
  batch 300 loss: 1.3441002798080444
  batch 350 loss: 1.3384213042259216
  batch 400 loss: 1.3378740894794463
  batch 450 loss: 1.3546571171283721
  batch 500 loss: 1.3073702454566956
  batch 550 loss: 1.3461556315422059
  batch 600 loss: 1.3089694023132323
  batch 650 loss: 1.3545465087890625
  batch 700 loss: 1.3081485629081726
  batch 750 loss: 1.3158816230297088
  batch 800 loss: 1.283763062953949
  batch 850 loss: 1.309498779773712
  batch 900 loss: 1.3315620231628418
LOSS train 1.33156 valid 1.23939, valid PER 40.69%
EPOCH 7:
  batch 50 loss: 1.2804233312606812
  batch 100 loss: 1.324653046131134
  batch 150 loss: 1.3749713945388793
  batch 200 loss: 1.3447033882141113
  batch 250 loss: 1.3099119007587432
  batch 300 loss: 1.3234217715263368
  batch 350 loss: 1.3023605680465697
  batch 400 loss: 1.3709230470657348
  batch 450 loss: 1.3509866189956665
  batch 500 loss: 1.3212630701065065
  batch 550 loss: 1.3027885484695434
  batch 600 loss: 1.3478606534004212
  batch 650 loss: 1.3687731575965882
  batch 700 loss: 1.3612882280349732
  batch 750 loss: 1.323735020160675
  batch 800 loss: 1.3277603769302369
  batch 850 loss: 1.3220730805397034
  batch 900 loss: 1.3566175246238708
LOSS train 1.35662 valid 1.28635, valid PER 41.51%
EPOCH 8:
  batch 50 loss: 1.2934813284873963
  batch 100 loss: 1.2863333773612977
  batch 150 loss: 1.259241533279419
  batch 200 loss: 1.2832507932186126
  batch 250 loss: 1.3194765257835388
  batch 300 loss: 1.2575252091884612
  batch 350 loss: 1.3086193203926086
  batch 400 loss: 1.258255616426468
  batch 450 loss: 1.3328900671005248
  batch 500 loss: 1.3429425072669983
  batch 550 loss: 1.3133882832527162
  batch 600 loss: 1.3455047130584716
  batch 650 loss: 1.353157787322998
  batch 700 loss: 1.330044322013855
  batch 750 loss: 1.3798003029823303
  batch 800 loss: 1.336134341955185
  batch 850 loss: 1.345192129611969
  batch 900 loss: 1.364851212501526
LOSS train 1.36485 valid 1.29420, valid PER 41.44%
EPOCH 9:
  batch 50 loss: 1.3727787804603577
  batch 100 loss: 1.3670787048339843
  batch 150 loss: 1.3394350576400758
  batch 200 loss: 1.271107122898102
  batch 250 loss: 1.3168206346035003
  batch 300 loss: 1.3354094099998475
  batch 350 loss: 1.3151362895965577
  batch 400 loss: 1.3126649522781373
  batch 450 loss: 1.3142009353637696
  batch 500 loss: 1.2907322001457215
  batch 550 loss: 1.3490728092193605
  batch 600 loss: 1.3780410814285278
  batch 650 loss: 1.2958722066879274
  batch 700 loss: 1.2966766929626465
  batch 750 loss: 1.294754034280777
  batch 800 loss: 1.2883614778518677
  batch 850 loss: 1.3241371154785155
  batch 900 loss: 1.2955560302734375
LOSS train 1.29556 valid 1.23070, valid PER 39.97%
EPOCH 10:
  batch 50 loss: 1.2487760949134827
  batch 100 loss: 1.2768951845169068
  batch 150 loss: 1.3188733458518982
  batch 200 loss: 1.330283581018448
  batch 250 loss: 1.314288582801819
  batch 300 loss: 1.2430781459808349
  batch 350 loss: 1.4439663219451904
  batch 400 loss: 1.4352015089988708
  batch 450 loss: 1.3994107937812805
  batch 500 loss: 1.3998413228988646
  batch 550 loss: 1.3885895562171937
  batch 600 loss: 1.3470183992385865
  batch 650 loss: 1.3389438676834107
  batch 700 loss: 1.3550503158569336
  batch 750 loss: 1.309253613948822
  batch 800 loss: 1.335264494419098
  batch 850 loss: 1.3667592453956603
  batch 900 loss: 1.4749265789985657
LOSS train 1.47493 valid 1.32373, valid PER 43.30%
EPOCH 11:
  batch 50 loss: 1.3382133364677429
  batch 100 loss: 1.3551261591911317
  batch 150 loss: 1.3361316776275636
  batch 200 loss: 1.3641891407966613
  batch 250 loss: 1.3787793922424316
  batch 300 loss: 1.3187077951431274
  batch 350 loss: 1.3219700539112091
  batch 400 loss: 1.3974976253509521
  batch 450 loss: 1.3427962577342987
  batch 500 loss: 1.3681319856643677
  batch 550 loss: 1.487657744884491
  batch 600 loss: 1.4363792324066162
  batch 650 loss: 1.4514550018310546
  batch 700 loss: 1.3336305332183838
  batch 750 loss: 1.3245815312862397
  batch 800 loss: 1.3884605884552002
  batch 850 loss: 1.4349937868118285
  batch 900 loss: 1.3877769494056702
LOSS train 1.38778 valid 1.31293, valid PER 43.30%
EPOCH 12:
  batch 50 loss: 1.3588017177581788
  batch 100 loss: 1.3387335705757142
  batch 150 loss: 1.3227597558498383
  batch 200 loss: 1.3513489770889282
  batch 250 loss: 1.3803417325019836
  batch 300 loss: 1.3439317774772643
  batch 350 loss: 1.3148310661315918
  batch 400 loss: 1.3307467746734618
  batch 450 loss: 1.3228293919563294
  batch 500 loss: 1.3380803596973418
  batch 550 loss: 1.2681328630447388
  batch 600 loss: 1.258018934726715
  batch 650 loss: 1.3115541410446168
  batch 700 loss: 1.299916867017746
  batch 750 loss: 1.2753027069568634
  batch 800 loss: 1.2418965005874634
  batch 850 loss: 1.293728106021881
  batch 900 loss: 1.3032465922832488
LOSS train 1.30325 valid 1.25501, valid PER 41.22%
EPOCH 13:
  batch 50 loss: 1.2432712507247925
  batch 100 loss: 1.2472254574298858
  batch 150 loss: 1.254429497718811
  batch 200 loss: 1.2545729613304137
  batch 250 loss: 1.2579775762557983
  batch 300 loss: 1.2523244428634643
  batch 350 loss: 1.2570992088317872
  batch 400 loss: 1.2838940584659577
  batch 450 loss: 1.2874030315876006
  batch 500 loss: 1.3799019956588745
  batch 550 loss: 1.4410112047195434
  batch 600 loss: 1.309089970588684
  batch 650 loss: 1.2918798685073853
  batch 700 loss: 1.2768205857276917
  batch 750 loss: 1.2470677065849305
  batch 800 loss: 1.2514183402061463
  batch 850 loss: 1.3377782487869263
  batch 900 loss: 1.3426752543449403
LOSS train 1.34268 valid 1.29937, valid PER 42.48%
EPOCH 14:
  batch 50 loss: 1.2799289727210998
  batch 100 loss: 1.2773647964000703
  batch 150 loss: 1.2900202107429504
  batch 200 loss: 1.2777000641822815
  batch 250 loss: 1.3170672965049743
  batch 300 loss: 1.3142935049533844
  batch 350 loss: 1.256756763458252
  batch 400 loss: 1.270430427789688
  batch 450 loss: 1.2982805120944976
  batch 500 loss: 1.3024947905540467
  batch 550 loss: 1.3006156301498413
  batch 600 loss: 1.2617183196544648
  batch 650 loss: 1.521643626689911
  batch 700 loss: 1.5154944038391114
  batch 750 loss: 1.4197000455856323
  batch 800 loss: 1.3767468380928038
  batch 850 loss: 1.3921484088897704
  batch 900 loss: 1.3694346594810485
LOSS train 1.36943 valid 1.39396, valid PER 47.85%
EPOCH 15:
  batch 50 loss: 1.397550059556961
  batch 100 loss: 1.3274262809753419
  batch 150 loss: 1.3154777312278747
  batch 200 loss: 1.5221051979064941
  batch 250 loss: 1.4864980602264404
  batch 300 loss: 1.4741345500946046
  batch 350 loss: 1.4583051800727844
  batch 400 loss: 1.4902758741378783
  batch 450 loss: 1.458356442451477
  batch 500 loss: 1.4056680631637573
  batch 550 loss: 1.4365641975402832
  batch 600 loss: 1.4577787661552428
  batch 650 loss: 1.4575634908676147
  batch 700 loss: 1.42845960855484
  batch 750 loss: 1.3943894743919372
  batch 800 loss: 1.3812333369255065
  batch 850 loss: 1.376563048362732
  batch 900 loss: 1.3857564806938172
LOSS train 1.38576 valid 1.39338, valid PER 47.01%
EPOCH 16:
  batch 50 loss: 1.376873037815094
  batch 100 loss: 1.3128904175758362
  batch 150 loss: 1.3227791285514832
  batch 200 loss: 1.3495988941192627
  batch 250 loss: 1.3646425747871398
  batch 300 loss: 1.3305614972114563
  batch 350 loss: 1.3493747186660767
  batch 400 loss: 1.3497882604598999
  batch 450 loss: 1.3584512066841126
  batch 500 loss: 1.2859439063072204
  batch 550 loss: 1.333515570163727
  batch 600 loss: 1.3410831427574157
  batch 650 loss: 1.3611770629882813
  batch 700 loss: 1.3401310789585112
  batch 750 loss: 1.3237067759037018
  batch 800 loss: 1.3711789107322694
  batch 850 loss: 1.4316773176193238
  batch 900 loss: 1.4509922528266908
LOSS train 1.45099 valid 1.40480, valid PER 45.77%
EPOCH 17:
  batch 50 loss: 1.404675612449646
  batch 100 loss: 1.3705168414115905
  batch 150 loss: 1.3947241020202636
  batch 200 loss: 1.3872538280487061
  batch 250 loss: 1.3904505276679993
  batch 300 loss: 1.372564218044281
  batch 350 loss: 1.3436449003219604
  batch 400 loss: 1.399915816783905
  batch 450 loss: 1.3928623962402344
  batch 500 loss: 1.3559269046783446
  batch 550 loss: 1.3360070300102234
  batch 600 loss: 1.3884380888938903
  batch 650 loss: 1.3223848295211793
  batch 700 loss: 1.3416806483268737
  batch 750 loss: 1.2964792442321778
  batch 800 loss: 1.3274359178543091
  batch 850 loss: 1.3076960587501525
  batch 900 loss: 1.3007054913043976
LOSS train 1.30071 valid 1.48856, valid PER 45.68%
EPOCH 18:
  batch 50 loss: 1.404238796234131
  batch 100 loss: 1.3703102111816405
  batch 150 loss: 1.3665678763389588
  batch 200 loss: 1.3350605857372284
  batch 250 loss: 1.3182118558883666
  batch 300 loss: 1.3145850968360902
  batch 350 loss: 1.3439284110069274
  batch 400 loss: 1.3441539239883422
  batch 450 loss: 1.4113722109794617
  batch 500 loss: 1.3762478590011598
  batch 550 loss: 1.3748469519615174
  batch 600 loss: 1.3532134795188904
  batch 650 loss: 1.3579630208015443
  batch 700 loss: 1.425192847251892
  batch 750 loss: 1.3488207590579986
  batch 800 loss: 1.3636162209510803
  batch 850 loss: 1.3276960742473602
  batch 900 loss: 1.3824498653411865
LOSS train 1.38245 valid 1.36605, valid PER 43.95%
EPOCH 19:
  batch 50 loss: 1.273554265499115
  batch 100 loss: 1.2990271174907684
  batch 150 loss: 1.326974458694458
  batch 200 loss: 1.3315212523937225
  batch 250 loss: 1.3473114895820617
  batch 300 loss: 1.365913405418396
  batch 350 loss: 1.341616928577423
  batch 400 loss: 1.3833428192138673
  batch 450 loss: 1.3740559864044188
  batch 500 loss: 1.4068750476837157
  batch 550 loss: 1.3646760654449464
  batch 600 loss: 1.3372677373886108
  batch 650 loss: 1.3636009120941162
  batch 700 loss: 1.3236291313171387
  batch 750 loss: 1.2746508264541625
  batch 800 loss: 1.347062178850174
  batch 850 loss: 1.3248656916618347
  batch 900 loss: 1.318894853591919
LOSS train 1.31889 valid 1.31801, valid PER 41.57%
EPOCH 20:
  batch 50 loss: 1.2666607928276061
  batch 100 loss: 1.2819885778427125
  batch 150 loss: 1.2559462118148803
  batch 200 loss: 1.2788969492912292
  batch 250 loss: 1.284401626586914
  batch 300 loss: 1.3036467266082763
  batch 350 loss: 1.2506220173835754
  batch 400 loss: 1.287672953605652
  batch 450 loss: 1.2966138410568238
  batch 500 loss: 1.2678503131866454
  batch 550 loss: 1.3446992540359497
  batch 600 loss: 1.2575744676589966
  batch 650 loss: 1.3075396406650543
  batch 700 loss: 1.3130713891983032
  batch 750 loss: 1.3197376263141631
  batch 800 loss: 1.3644292759895325
  batch 850 loss: 1.3482400608062743
  batch 900 loss: 1.3233328330516816
LOSS train 1.32333 valid 1.31088, valid PER 40.45%
Training finished in 5.0 minutes.
Model saved to checkpoints/20231207_045333/model_9
Loading model from checkpoints/20231207_045333/model_9
SUB: 18.43%, DEL: 23.20%, INS: 1.08%, COR: 58.36%, PER: 42.72%
