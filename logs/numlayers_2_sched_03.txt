Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=2, fbank_dims=23, model_dims=128, concat=1, lr=0.3, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 562216
EPOCH 1, Learning Rate: 0.3
  batch 50 loss: 5.101613283157349
  batch 100 loss: 3.348761100769043
  batch 150 loss: 3.291650862693787
  batch 200 loss: 3.2430586576461793
  batch 250 loss: 3.1970808553695678
  batch 300 loss: 3.0880456924438477
  batch 350 loss: 3.0052032899856567
  batch 400 loss: 2.8833165550231934
  batch 450 loss: 2.7598992156982423
  batch 500 loss: 2.6207848405838012
  batch 550 loss: 2.530788655281067
  batch 600 loss: 2.4643775749206545
  batch 650 loss: 2.359233088493347
  batch 700 loss: 2.3222977256774904
  batch 750 loss: 2.261130883693695
  batch 800 loss: 2.207070574760437
  batch 850 loss: 2.176314902305603
  batch 900 loss: 2.134443252086639
avg val loss: 2.128591775894165
LOSS train 2.13444 valid 2.12859, valid PER 77.78%
EPOCH 2, Learning Rate: 0.3
  batch 50 loss: 2.0934803104400634
  batch 100 loss: 2.026003029346466
  batch 150 loss: 1.9254378652572632
  batch 200 loss: 1.9189842796325685
  batch 250 loss: 1.8975609397888185
  batch 300 loss: 1.854160099029541
  batch 350 loss: 1.8244935250282288
  batch 400 loss: 1.7901622676849365
  batch 450 loss: 1.7793604040145874
  batch 500 loss: 1.736631565093994
  batch 550 loss: 1.72541588306427
  batch 600 loss: 1.7023973345756531
  batch 650 loss: 1.6470529437065125
  batch 700 loss: 1.6689657878875732
  batch 750 loss: 1.6329164218902588
  batch 800 loss: 1.5829305696487426
  batch 850 loss: 1.583313636779785
  batch 900 loss: 1.5621846652030944
avg val loss: 1.4704434871673584
LOSS train 1.56218 valid 1.47044, valid PER 53.31%
EPOCH 3, Learning Rate: 0.3
  batch 50 loss: 1.5036961078643798
  batch 100 loss: 1.5575909662246703
  batch 150 loss: 1.550130615234375
  batch 200 loss: 1.4806572580337525
  batch 250 loss: 1.488159544467926
  batch 300 loss: 1.467949182987213
  batch 350 loss: 1.4854777336120606
  batch 400 loss: 1.4525710272789
  batch 450 loss: 1.4237710094451905
  batch 500 loss: 1.3977028036117554
  batch 550 loss: 1.3969189393520356
  batch 600 loss: 1.336817672252655
  batch 650 loss: 1.3631744694709778
  batch 700 loss: 1.3451900696754455
  batch 750 loss: 1.362074682712555
  batch 800 loss: 1.3667609906196594
  batch 850 loss: 1.3379748487472534
  batch 900 loss: 1.3248718249797822
avg val loss: 1.2243118286132812
LOSS train 1.32487 valid 1.22431, valid PER 38.70%
EPOCH 4, Learning Rate: 0.3
  batch 50 loss: 1.3209562122821807
  batch 100 loss: 1.2619042670726777
  batch 150 loss: 1.2727826166152953
  batch 200 loss: 1.254955265522003
  batch 250 loss: 1.2457967066764832
  batch 300 loss: 1.2700793993473054
  batch 350 loss: 1.2510369229316711
  batch 400 loss: 1.1990159893035888
  batch 450 loss: 1.212562427520752
  batch 500 loss: 1.2808055448532105
  batch 550 loss: 1.1913481533527375
  batch 600 loss: 1.1874408161640166
  batch 650 loss: 1.2505561482906342
  batch 700 loss: 1.260323543548584
  batch 750 loss: 1.1922899222373962
  batch 800 loss: 1.1890561664104462
  batch 850 loss: 1.166477621793747
  batch 900 loss: 1.1775495529174804
avg val loss: 1.1159698963165283
LOSS train 1.17755 valid 1.11597, valid PER 34.68%
EPOCH 5, Learning Rate: 0.3
  batch 50 loss: 1.1728460884094238
  batch 100 loss: 1.1493132627010345
  batch 150 loss: 1.1735581636428833
  batch 200 loss: 1.193539457321167
  batch 250 loss: 1.1354871273040772
  batch 300 loss: 1.1537553644180298
  batch 350 loss: 1.112857232093811
  batch 400 loss: 1.094062374830246
  batch 450 loss: 1.0991051840782164
  batch 500 loss: 1.0992558419704437
  batch 550 loss: 1.141511126756668
  batch 600 loss: 1.1176799786090852
  batch 650 loss: 1.1313322079181671
  batch 700 loss: 1.1058926105499267
  batch 750 loss: 1.115613203048706
  batch 800 loss: 1.1367312252521515
  batch 850 loss: 1.1256498110294342
  batch 900 loss: 1.0933944880962372
avg val loss: 1.0208845138549805
LOSS train 1.09339 valid 1.02088, valid PER 32.34%
EPOCH 6, Learning Rate: 0.3
  batch 50 loss: 1.0530963242053986
  batch 100 loss: 1.0720417034626006
  batch 150 loss: 1.0651251900196075
  batch 200 loss: 1.0426114547252654
  batch 250 loss: 1.051327543258667
  batch 300 loss: 1.0623224890232086
  batch 350 loss: 1.0706054484844207
  batch 400 loss: 1.0566702473163605
  batch 450 loss: 1.078078694343567
  batch 500 loss: 1.008394844532013
  batch 550 loss: 1.0598132872581483
  batch 600 loss: 1.0412934279441834
  batch 650 loss: 1.025452972650528
  batch 700 loss: 1.0180715143680572
  batch 750 loss: 1.0480056035518646
  batch 800 loss: 1.04113401055336
  batch 850 loss: 1.0673320019245147
  batch 900 loss: 1.0869820046424865
avg val loss: 0.9724550247192383
LOSS train 1.08698 valid 0.97246, valid PER 30.52%
EPOCH 7, Learning Rate: 0.3
  batch 50 loss: 1.014106148481369
  batch 100 loss: 1.048728631734848
  batch 150 loss: 0.9806552147865295
  batch 200 loss: 1.0007707631587983
  batch 250 loss: 1.0389646208286285
  batch 300 loss: 1.0380116879940033
  batch 350 loss: 1.0414314126968385
  batch 400 loss: 0.9979599380493164
  batch 450 loss: 0.9976198923587799
  batch 500 loss: 0.9847121071815491
  batch 550 loss: 0.9939950156211853
  batch 600 loss: 0.9889254868030548
  batch 650 loss: 0.9707581877708436
  batch 700 loss: 1.0274405145645142
  batch 750 loss: 0.9949897038936615
  batch 800 loss: 0.9743292033672333
  batch 850 loss: 0.974229986667633
  batch 900 loss: 0.9699991452693939
avg val loss: 0.9367197155952454
LOSS train 0.97000 valid 0.93672, valid PER 29.53%
EPOCH 8, Learning Rate: 0.3
  batch 50 loss: 0.9657716464996338
  batch 100 loss: 0.9352458906173706
  batch 150 loss: 0.9724882829189301
  batch 200 loss: 0.9556755471229553
  batch 250 loss: 0.9364903664588928
  batch 300 loss: 0.9244518554210663
  batch 350 loss: 0.9415312767028808
  batch 400 loss: 0.943426548242569
  batch 450 loss: 0.9823946416378021
  batch 500 loss: 0.9599676311016083
  batch 550 loss: 0.9573662734031677
  batch 600 loss: 0.932119392156601
  batch 650 loss: 0.9567881691455841
  batch 700 loss: 0.9758829522132874
  batch 750 loss: 0.9595398867130279
  batch 800 loss: 0.9702903687953949
  batch 850 loss: 0.9296348476409912
  batch 900 loss: 0.9396276080608368
avg val loss: 0.9140271544456482
LOSS train 0.93963 valid 0.91403, valid PER 28.00%
EPOCH 9, Learning Rate: 0.3
  batch 50 loss: 0.9033678317070007
  batch 100 loss: 0.885112681388855
  batch 150 loss: 0.9192136764526367
  batch 200 loss: 0.8893720090389252
  batch 250 loss: 0.8818446958065033
  batch 300 loss: 0.9238431060314178
  batch 350 loss: 0.8865633189678193
  batch 400 loss: 0.9270678591728211
  batch 450 loss: 0.9253358650207519
  batch 500 loss: 0.8952481460571289
  batch 550 loss: 0.9135964667797088
  batch 600 loss: 0.9451712572574615
  batch 650 loss: 0.934423772096634
  batch 700 loss: 0.9033559477329254
  batch 750 loss: 0.8898272883892059
  batch 800 loss: 0.9370127856731415
  batch 850 loss: 0.925527024269104
  batch 900 loss: 0.8748203790187836
avg val loss: 0.8762469291687012
LOSS train 0.87482 valid 0.87625, valid PER 27.01%
EPOCH 10, Learning Rate: 0.3
  batch 50 loss: 0.8625853538513184
  batch 100 loss: 0.9048447692394257
  batch 150 loss: 0.8936279273033142
  batch 200 loss: 0.8624970889091492
  batch 250 loss: 0.8618794882297516
  batch 300 loss: 0.8810099482536315
  batch 350 loss: 0.8444385838508606
  batch 400 loss: 0.8530342233181
  batch 450 loss: 0.8578171670436859
  batch 500 loss: 0.8609654664993286
  batch 550 loss: 0.8887457263469696
  batch 600 loss: 0.8732999038696289
  batch 650 loss: 0.9019733965396881
  batch 700 loss: 0.8967331254482269
  batch 750 loss: 0.8802654588222504
  batch 800 loss: 0.8873339641094208
  batch 850 loss: 0.85561101436615
  batch 900 loss: 0.8653059232234955
avg val loss: 0.887215256690979
LOSS train 0.86531 valid 0.88722, valid PER 28.00%
EPOCH 11, Learning Rate: 0.15
  batch 50 loss: 0.8377031552791595
  batch 100 loss: 0.7952859556674957
  batch 150 loss: 0.7840885293483734
  batch 200 loss: 0.7451467800140381
  batch 250 loss: 0.7777415490150452
  batch 300 loss: 0.7633429586887359
  batch 350 loss: 0.8265275883674622
  batch 400 loss: 0.7922452926635742
  batch 450 loss: 0.7773098111152649
  batch 500 loss: 0.7767225682735444
  batch 550 loss: 0.800624430179596
  batch 600 loss: 0.8010473877191544
  batch 650 loss: 0.8036733675003052
  batch 700 loss: 0.8506024253368377
  batch 750 loss: 0.8063070893287658
  batch 800 loss: 0.793028918504715
  batch 850 loss: 0.7777673876285554
  batch 900 loss: 0.7900144684314728
avg val loss: 0.8077782988548279
LOSS train 0.79001 valid 0.80778, valid PER 25.13%
EPOCH 12, Learning Rate: 0.15
  batch 50 loss: 0.7286361110210419
  batch 100 loss: 0.7353138333559036
  batch 150 loss: 0.7503993785381318
  batch 200 loss: 0.7725922179222107
  batch 250 loss: 0.7451610314846039
  batch 300 loss: 0.7759493124485016
  batch 350 loss: 0.7467976069450378
  batch 400 loss: 0.7845536124706268
  batch 450 loss: 0.7475776553153992
  batch 500 loss: 0.7794717955589294
  batch 550 loss: 0.7795986986160278
  batch 600 loss: 0.7825107824802399
  batch 650 loss: 0.7672054648399353
  batch 700 loss: 0.7489198684692383
  batch 750 loss: 0.7810110872983933
  batch 800 loss: 0.739440541267395
  batch 850 loss: 0.7602388846874237
  batch 900 loss: 0.7768239486217499
avg val loss: 0.8147254586219788
LOSS train 0.77682 valid 0.81473, valid PER 24.94%
EPOCH 13, Learning Rate: 0.075
  batch 50 loss: 0.7156009328365326
  batch 100 loss: 0.7165524876117706
  batch 150 loss: 0.7489751315116883
  batch 200 loss: 0.689249439239502
  batch 250 loss: 0.7076859676837921
  batch 300 loss: 0.754922439455986
  batch 350 loss: 0.6959453505277634
  batch 400 loss: 0.7015578043460846
  batch 450 loss: 0.7204955959320068
  batch 500 loss: 0.7200074487924576
  batch 550 loss: 0.76317227602005
  batch 600 loss: 0.7389970993995667
  batch 650 loss: 0.7147026014328003
  batch 700 loss: 0.7279454910755158
  batch 750 loss: 0.6831937384605408
  batch 800 loss: 0.7102204942703247
  batch 850 loss: 0.6827078467607498
  batch 900 loss: 0.7206863564252853
avg val loss: 0.7846654057502747
LOSS train 0.72069 valid 0.78467, valid PER 24.19%
EPOCH 14, Learning Rate: 0.075
  batch 50 loss: 0.6945719558000565
  batch 100 loss: 0.6840828394889832
  batch 150 loss: 0.6969476217031478
  batch 200 loss: 0.7043853336572647
  batch 250 loss: 0.6935676109790802
  batch 300 loss: 0.688060290813446
  batch 350 loss: 0.6894558465480805
  batch 400 loss: 0.7183923280239105
  batch 450 loss: 0.673282059431076
  batch 500 loss: 0.685706896185875
  batch 550 loss: 0.7052328228950501
  batch 600 loss: 0.7010216969251633
  batch 650 loss: 0.7177176147699356
  batch 700 loss: 0.7234002375602722
  batch 750 loss: 0.6960123634338379
  batch 800 loss: 0.6898021811246872
  batch 850 loss: 0.7322920989990235
  batch 900 loss: 0.7159799981117249
avg val loss: 0.7796911597251892
LOSS train 0.71598 valid 0.77969, valid PER 24.08%
EPOCH 15, Learning Rate: 0.075
  batch 50 loss: 0.6555484390258789
  batch 100 loss: 0.6874054235219955
  batch 150 loss: 0.7087731844186783
  batch 200 loss: 0.7022422969341278
  batch 250 loss: 0.7057045948505402
  batch 300 loss: 0.7024532347917557
  batch 350 loss: 0.6921006262302398
  batch 400 loss: 0.6979971921443939
  batch 450 loss: 0.6758063101768493
  batch 500 loss: 0.6720097267627716
  batch 550 loss: 0.7340425646305084
  batch 600 loss: 0.7281996804475784
  batch 650 loss: 0.6838556575775147
  batch 700 loss: 0.6716669845581055
  batch 750 loss: 0.7132201063632965
  batch 800 loss: 0.675204718708992
  batch 850 loss: 0.6650623852014541
  batch 900 loss: 0.6497113609313965
avg val loss: 0.7864816784858704
LOSS train 0.64971 valid 0.78648, valid PER 24.27%
EPOCH 16, Learning Rate: 0.0375
  batch 50 loss: 0.6852627390623093
  batch 100 loss: 0.6579589188098908
  batch 150 loss: 0.6698375630378723
  batch 200 loss: 0.6934664046764374
  batch 250 loss: 0.6654660385847092
  batch 300 loss: 0.6757799673080445
  batch 350 loss: 0.678042898774147
  batch 400 loss: 0.6667087179422378
  batch 450 loss: 0.691040711402893
  batch 500 loss: 0.6760788387060166
  batch 550 loss: 0.6473995238542557
  batch 600 loss: 0.6936865729093552
  batch 650 loss: 0.6819779366254807
  batch 700 loss: 0.6413296335935592
  batch 750 loss: 0.6548799234628677
  batch 800 loss: 0.6396714478731156
  batch 850 loss: 0.6553754484653473
  batch 900 loss: 0.6577511048316955
avg val loss: 0.7703027725219727
LOSS train 0.65775 valid 0.77030, valid PER 23.84%
EPOCH 17, Learning Rate: 0.0375
  batch 50 loss: 0.6887167519330979
  batch 100 loss: 0.6265873217582703
  batch 150 loss: 0.686281812787056
  batch 200 loss: 0.6395718842744827
  batch 250 loss: 0.6505880433321
  batch 300 loss: 0.6504357314109802
  batch 350 loss: 0.6815790957212449
  batch 400 loss: 0.655071668624878
  batch 450 loss: 0.6504847025871277
  batch 500 loss: 0.6727166122198105
  batch 550 loss: 0.6585555565357208
  batch 600 loss: 0.677956815958023
  batch 650 loss: 0.6524990922212601
  batch 700 loss: 0.6774263125658035
  batch 750 loss: 0.6433172959089279
  batch 800 loss: 0.6781065583229064
  batch 850 loss: 0.6616199469566345
  batch 900 loss: 0.6547796183824539
avg val loss: 0.7738716006278992
LOSS train 0.65478 valid 0.77387, valid PER 23.82%
EPOCH 18, Learning Rate: 0.01875
  batch 50 loss: 0.6637671530246735
  batch 100 loss: 0.6263250064849853
  batch 150 loss: 0.6806356632709503
  batch 200 loss: 0.6446206194162368
  batch 250 loss: 0.6299546009302139
  batch 300 loss: 0.6397083592414856
  batch 350 loss: 0.6208213716745377
  batch 400 loss: 0.637006573677063
  batch 450 loss: 0.6415339720249176
  batch 500 loss: 0.6330805110931397
  batch 550 loss: 0.6932289755344391
  batch 600 loss: 0.6557638776302338
  batch 650 loss: 0.6353803324699402
  batch 700 loss: 0.6395369648933411
  batch 750 loss: 0.6532101303339004
  batch 800 loss: 0.6623429208993912
  batch 850 loss: 0.659703488945961
  batch 900 loss: 0.6434355330467224
avg val loss: 0.7709122896194458
LOSS train 0.64344 valid 0.77091, valid PER 23.64%
EPOCH 19, Learning Rate: 0.009375
  batch 50 loss: 0.6531834822893142
  batch 100 loss: 0.646399763226509
  batch 150 loss: 0.6424481070041657
  batch 200 loss: 0.6227007848024368
  batch 250 loss: 0.663155545592308
  batch 300 loss: 0.6603390443325042
  batch 350 loss: 0.6533198136091233
  batch 400 loss: 0.6324092400074005
  batch 450 loss: 0.6007610315084457
  batch 500 loss: 0.6255544066429138
  batch 550 loss: 0.6438584649562835
  batch 600 loss: 0.6634915095567703
  batch 650 loss: 0.6524315029382706
  batch 700 loss: 0.6513031095266342
  batch 750 loss: 0.641063140630722
  batch 800 loss: 0.6029870402812958
  batch 850 loss: 0.6457216155529022
  batch 900 loss: 0.6388784343004227
avg val loss: 0.7675013542175293
LOSS train 0.63888 valid 0.76750, valid PER 23.52%
EPOCH 20, Learning Rate: 0.009375
  batch 50 loss: 0.6002479028701783
  batch 100 loss: 0.6527561926841736
  batch 150 loss: 0.6347077536582947
  batch 200 loss: 0.6333080661296845
  batch 250 loss: 0.667781503200531
  batch 300 loss: 0.6499213570356369
  batch 350 loss: 0.6110335385799408
  batch 400 loss: 0.6296990579366684
  batch 450 loss: 0.6230396288633346
  batch 500 loss: 0.6621865248680114
  batch 550 loss: 0.6495868569612503
  batch 600 loss: 0.6329509609937668
  batch 650 loss: 0.6496143758296966
  batch 700 loss: 0.6151176279783249
  batch 750 loss: 0.610156552195549
  batch 800 loss: 0.6408593893051148
  batch 850 loss: 0.6439694571495056
  batch 900 loss: 0.6299238270521164
avg val loss: 0.765758752822876
LOSS train 0.62992 valid 0.76576, valid PER 23.44%
Training finished in 5.0 minutes.
Model saved to checkpoints/20231210_135211/model_20
Loading model from checkpoints/20231210_135211/model_20
SUB: 14.79%, DEL: 8.51%, INS: 2.09%, COR: 76.70%, PER: 25.39%
