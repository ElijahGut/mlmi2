Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=2, fbank_dims=23, model_dims=128, concat=1, lr=0.3, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 562216
EPOCH 1, Learning Rate: 0.3
  batch 50 loss: 5.101618337631225
  batch 100 loss: 3.3487600088119507
  batch 150 loss: 3.291649923324585
  batch 200 loss: 3.2430570220947263
  batch 250 loss: 3.197080206871033
  batch 300 loss: 3.0880439853668213
  batch 350 loss: 3.0052032375335695
  batch 400 loss: 2.883317575454712
  batch 450 loss: 2.7598986291885375
  batch 500 loss: 2.620784726142883
  batch 550 loss: 2.5307874631881715
  batch 600 loss: 2.4643769454956055
  batch 650 loss: 2.3592334461212157
  batch 700 loss: 2.3222982406616213
  batch 750 loss: 2.2611305594444273
  batch 800 loss: 2.207070436477661
  batch 850 loss: 2.176314606666565
  batch 900 loss: 2.13444452047348
running loss: 49.67482554912567
LOSS train 2.13444 valid 2.12858, valid PER 77.78%
EPOCH 2, Learning Rate: 0.3
  batch 50 loss: 2.093480088710785
  batch 100 loss: 2.0260031986236573
  batch 150 loss: 1.9254401683807374
  batch 200 loss: 1.9189836049079896
  batch 250 loss: 1.8975636100769042
  batch 300 loss: 1.8541611409187317
  batch 350 loss: 1.8244912910461426
  batch 400 loss: 1.7901588582992554
  batch 450 loss: 1.7793628120422362
  batch 500 loss: 1.7366294288635253
  batch 550 loss: 1.7254205965995788
  batch 600 loss: 1.7023770380020142
  batch 650 loss: 1.6470501494407654
  batch 700 loss: 1.6689671468734741
  batch 750 loss: 1.6329093670845032
  batch 800 loss: 1.5830119156837463
  batch 850 loss: 1.583307249546051
  batch 900 loss: 1.5621677207946778
running loss: 37.31906867027283
LOSS train 1.56217 valid 1.47033, valid PER 53.30%
EPOCH 3, Learning Rate: 0.3
  batch 50 loss: 1.503667802810669
  batch 100 loss: 1.5575634884834288
  batch 150 loss: 1.5500649118423462
  batch 200 loss: 1.4806513452529908
  batch 250 loss: 1.4880340051651002
  batch 300 loss: 1.4678648400306702
  batch 350 loss: 1.4856038546562196
  batch 400 loss: 1.4525041317939758
  batch 450 loss: 1.423873131275177
  batch 500 loss: 1.397678825855255
  batch 550 loss: 1.396622166633606
  batch 600 loss: 1.336917178630829
  batch 650 loss: 1.3615640640258788
  batch 700 loss: 1.3452438044548034
  batch 750 loss: 1.3639902424812318
  batch 800 loss: 1.3691624736785888
  batch 850 loss: 1.331645863056183
  batch 900 loss: 1.324230134487152
running loss: 33.01576519012451
LOSS train 1.32423 valid 1.22929, valid PER 38.91%
EPOCH 4, Learning Rate: 0.3
  batch 50 loss: 1.32634446144104
  batch 100 loss: 1.259044132232666
  batch 150 loss: 1.2705933582782745
  batch 200 loss: 1.2573447585105897
  batch 250 loss: 1.248410588502884
  batch 300 loss: 1.2703417682647704
  batch 350 loss: 1.248128855228424
  batch 400 loss: 1.2065914213657378
  batch 450 loss: 1.210386596918106
  batch 500 loss: 1.2770779466629028
  batch 550 loss: 1.1949855506420135
  batch 600 loss: 1.1920292425155639
  batch 650 loss: 1.2508898186683655
  batch 700 loss: 1.2651283323764801
  batch 750 loss: 1.2040262126922607
  batch 800 loss: 1.197592661380768
  batch 850 loss: 1.1691517066955566
  batch 900 loss: 1.1800384891033173
running loss: 28.830980896949768
LOSS train 1.18004 valid 1.11135, valid PER 34.69%
EPOCH 5, Learning Rate: 0.3
  batch 50 loss: 1.1796608924865724
  batch 100 loss: 1.1354095578193664
  batch 150 loss: 1.1612064087390899
  batch 200 loss: 1.193695386648178
  batch 250 loss: 1.1259445321559907
  batch 300 loss: 1.1387001276016235
  batch 350 loss: 1.1140713047981263
  batch 400 loss: 1.0942567324638366
  batch 450 loss: 1.097483398914337
  batch 500 loss: 1.102253246307373
  batch 550 loss: 1.1406515657901763
  batch 600 loss: 1.1250586020946503
  batch 650 loss: 1.1264320230484008
  batch 700 loss: 1.1133185815811157
  batch 750 loss: 1.1078923189640044
  batch 800 loss: 1.1368703258037567
  batch 850 loss: 1.1221315014362334
  batch 900 loss: 1.0906890773773192
running loss: 26.116189301013947
LOSS train 1.09069 valid 1.03617, valid PER 32.50%
EPOCH 6, Learning Rate: 0.3
  batch 50 loss: 1.0616011750698089
  batch 100 loss: 1.0640990400314332
  batch 150 loss: 1.0718996500968934
  batch 200 loss: 1.023415904045105
  batch 250 loss: 1.0595104336738586
  batch 300 loss: 1.080398063659668
  batch 350 loss: 1.0699036014080048
  batch 400 loss: 1.0538591742515564
  batch 450 loss: 1.0804996633529662
  batch 500 loss: 1.0270486402511596
  batch 550 loss: 1.065658949613571
  batch 600 loss: 1.022479830980301
  batch 650 loss: 1.0264664208889007
  batch 700 loss: 1.0082166957855225
  batch 750 loss: 1.0665001380443573
  batch 800 loss: 1.033806688785553
  batch 850 loss: 1.0662237918376922
  batch 900 loss: 1.0649881565570831
running loss: 23.917184829711914
LOSS train 1.06499 valid 0.96482, valid PER 30.39%
EPOCH 7, Learning Rate: 0.3
  batch 50 loss: 1.002472618818283
  batch 100 loss: 1.0475485026836395
  batch 150 loss: 0.9816292309761048
  batch 200 loss: 0.9821680927276611
  batch 250 loss: 1.021784029006958
  batch 300 loss: 1.0303714084625244
  batch 350 loss: 1.0280252528190612
  batch 400 loss: 0.9706536483764648
  batch 450 loss: 0.9810747170448303
  batch 500 loss: 0.9776443982124329
  batch 550 loss: 0.9709039068222046
  batch 600 loss: 0.9759439885616302
  batch 650 loss: 0.9566139447689056
  batch 700 loss: 1.0171264672279359
  batch 750 loss: 0.9833775818347931
  batch 800 loss: 0.977351520061493
  batch 850 loss: 0.9652566516399383
  batch 900 loss: 0.9682257306575776
running loss: 22.234910130500793
LOSS train 0.96823 valid 0.94111, valid PER 29.63%
EPOCH 8, Learning Rate: 0.3
  batch 50 loss: 0.9540717816352844
  batch 100 loss: 0.9374429726600647
  batch 150 loss: 0.980765655040741
  batch 200 loss: 0.9477933204174042
  batch 250 loss: 0.9422527241706848
  batch 300 loss: 0.915508531332016
  batch 350 loss: 0.9515090715885163
  batch 400 loss: 0.9451496315002441
  batch 450 loss: 0.9823783802986145
  batch 500 loss: 0.9623898231983185
  batch 550 loss: 0.9443720650672912
  batch 600 loss: 0.9129883086681366
  batch 650 loss: 0.9567697477340699
  batch 700 loss: 0.968443193435669
  batch 750 loss: 0.9557893133163452
  batch 800 loss: 0.9569750916957855
  batch 850 loss: 0.9151095449924469
  batch 900 loss: 0.9297593879699707
running loss: 22.04479992389679
LOSS train 0.92976 valid 0.93726, valid PER 29.29%
EPOCH 9, Learning Rate: 0.3
  batch 50 loss: 0.8948840868473052
  batch 100 loss: 0.8758210301399231
  batch 150 loss: 0.9082466614246368
  batch 200 loss: 0.8820463562011719
  batch 250 loss: 0.8912678861618042
  batch 300 loss: 0.9276138734817505
  batch 350 loss: 0.8756533825397491
  batch 400 loss: 0.9152528762817382
  batch 450 loss: 0.9285341501235962
  batch 500 loss: 0.8909389626979828
  batch 550 loss: 0.9052399814128875
  batch 600 loss: 0.9275410842895507
  batch 650 loss: 0.9423736071586609
  batch 700 loss: 0.9187481093406678
  batch 750 loss: 0.8912047755718231
  batch 800 loss: 0.9360493922233581
  batch 850 loss: 0.9262229621410369
  batch 900 loss: 0.8818407583236695
running loss: 22.063596367836
LOSS train 0.88184 valid 0.88183, valid PER 27.90%
EPOCH 10, Learning Rate: 0.15
  batch 50 loss: 0.8529915571212768
  batch 100 loss: 0.858296058177948
  batch 150 loss: 0.8430569016933441
  batch 200 loss: 0.8045376396179199
  batch 250 loss: 0.8039564919471741
  batch 300 loss: 0.8307490253448486
  batch 350 loss: 0.7969057106971741
  batch 400 loss: 0.7973938655853271
  batch 450 loss: 0.8006654870510101
  batch 500 loss: 0.8109559226036072
  batch 550 loss: 0.8159686052799224
  batch 600 loss: 0.8151949775218964
  batch 650 loss: 0.8476669025421143
  batch 700 loss: 0.8227946490049363
  batch 750 loss: 0.8270876693725586
  batch 800 loss: 0.8258440971374512
  batch 850 loss: 0.814066778421402
  batch 900 loss: 0.8137416446208954
running loss: 20.438903212547302
LOSS train 0.81374 valid 0.83426, valid PER 26.13%
EPOCH 11, Learning Rate: 0.15
  batch 50 loss: 0.8040439879894257
  batch 100 loss: 0.7873113167285919
  batch 150 loss: 0.781242526769638
  batch 200 loss: 0.7410307288169861
  batch 250 loss: 0.7789052772521973
  batch 300 loss: 0.7712533819675446
  batch 350 loss: 0.8156540429592133
  batch 400 loss: 0.7852328968048096
  batch 450 loss: 0.772203117609024
  batch 500 loss: 0.7838882851600647
  batch 550 loss: 0.8044875121116638
  batch 600 loss: 0.8061129033565522
  batch 650 loss: 0.8059706628322602
  batch 700 loss: 0.8471505868434906
  batch 750 loss: 0.7938662266731262
  batch 800 loss: 0.8045654213428497
  batch 850 loss: 0.7800460541248322
  batch 900 loss: 0.8044215333461762
running loss: 18.23268836736679
LOSS train 0.80442 valid 0.81868, valid PER 25.70%
EPOCH 12, Learning Rate: 0.15
  batch 50 loss: 0.7427593672275543
  batch 100 loss: 0.7364819496870041
  batch 150 loss: 0.7476092922687531
  batch 200 loss: 0.7790525662899017
  batch 250 loss: 0.7549989938735961
  batch 300 loss: 0.8028753364086151
  batch 350 loss: 0.7610025429725646
  batch 400 loss: 0.8045180225372315
  batch 450 loss: 0.7721962451934814
  batch 500 loss: 0.7850312638282776
  batch 550 loss: 0.7812436747550965
  batch 600 loss: 0.7884203779697418
  batch 650 loss: 0.7836079370975494
  batch 700 loss: 0.7596026557683945
  batch 750 loss: 0.7815951693058014
  batch 800 loss: 0.7573865747451782
  batch 850 loss: 0.7572563767433167
  batch 900 loss: 0.7817847895622253
running loss: 18.78030753135681
LOSS train 0.78178 valid 0.80853, valid PER 24.80%
EPOCH 13, Learning Rate: 0.075
  batch 50 loss: 0.7242772352695465
  batch 100 loss: 0.7268980914354324
  batch 150 loss: 0.7447544580698013
  batch 200 loss: 0.6986618137359619
  batch 250 loss: 0.7108415889739991
  batch 300 loss: 0.761239310503006
  batch 350 loss: 0.6949441349506378
  batch 400 loss: 0.7022298526763916
  batch 450 loss: 0.719240373969078
  batch 500 loss: 0.724631872177124
  batch 550 loss: 0.7679850924015045
  batch 600 loss: 0.7485277938842774
  batch 650 loss: 0.7259222817420959
  batch 700 loss: 0.7249177551269531
  batch 750 loss: 0.6962322217226028
  batch 800 loss: 0.7345922464132308
  batch 850 loss: 0.7016937279701233
  batch 900 loss: 0.7288580393791199
running loss: 17.210078805685043
LOSS train 0.72886 valid 0.78548, valid PER 24.48%
EPOCH 14, Learning Rate: 0.075
  batch 50 loss: 0.7073439526557922
  batch 100 loss: 0.6976859664916992
  batch 150 loss: 0.7079940259456634
  batch 200 loss: 0.7057876896858215
  batch 250 loss: 0.7169928187131882
  batch 300 loss: 0.6890613567829132
  batch 350 loss: 0.7019602000713349
  batch 400 loss: 0.7288248825073242
  batch 450 loss: 0.6886621153354645
  batch 500 loss: 0.7046133863925934
  batch 550 loss: 0.7217629909515381
  batch 600 loss: 0.7093087875843048
  batch 650 loss: 0.7202426028251648
  batch 700 loss: 0.7343425011634827
  batch 750 loss: 0.7046226918697357
  batch 800 loss: 0.7017985171079636
  batch 850 loss: 0.7420640218257905
  batch 900 loss: 0.7154031497240066
running loss: 17.06644606590271
LOSS train 0.71540 valid 0.78419, valid PER 24.54%
EPOCH 15, Learning Rate: 0.075
  batch 50 loss: 0.6677487516403198
  batch 100 loss: 0.6908945119380951
  batch 150 loss: 0.7095111620426178
  batch 200 loss: 0.713536623120308
  batch 250 loss: 0.7030674785375595
  batch 300 loss: 0.7078266644477844
  batch 350 loss: 0.707237285375595
  batch 400 loss: 0.7141007483005524
  batch 450 loss: 0.6931180542707444
  batch 500 loss: 0.6732548570632935
  batch 550 loss: 0.7438205659389496
  batch 600 loss: 0.7320160049200058
  batch 650 loss: 0.694490596652031
  batch 700 loss: 0.6910148000717163
  batch 750 loss: 0.7179895448684692
  batch 800 loss: 0.6799178332090378
  batch 850 loss: 0.6785270726680755
  batch 900 loss: 0.6716716814041138
running loss: 17.22239601612091
LOSS train 0.67167 valid 0.78538, valid PER 24.70%
EPOCH 16, Learning Rate: 0.0375
  batch 50 loss: 0.6858745086193084
  batch 100 loss: 0.6574897104501725
  batch 150 loss: 0.6761504101753235
  batch 200 loss: 0.7021456801891327
  batch 250 loss: 0.6703162389993668
  batch 300 loss: 0.6857454836368561
  batch 350 loss: 0.6762868189811706
  batch 400 loss: 0.6780323135852814
  batch 450 loss: 0.6969588458538055
  batch 500 loss: 0.6761390370130539
  batch 550 loss: 0.651822292804718
  batch 600 loss: 0.6988149487972259
  batch 650 loss: 0.6837652534246444
  batch 700 loss: 0.6599984681606292
  batch 750 loss: 0.6808905065059662
  batch 800 loss: 0.6417358422279358
  batch 850 loss: 0.6613229763507843
  batch 900 loss: 0.6662156069278717
running loss: 16.000150114297867
LOSS train 0.66622 valid 0.76656, valid PER 23.87%
EPOCH 17, Learning Rate: 0.0375
  batch 50 loss: 0.6905021238327026
  batch 100 loss: 0.6312325447797775
  batch 150 loss: 0.6934513580799103
  batch 200 loss: 0.6425387132167816
  batch 250 loss: 0.6609305685758591
  batch 300 loss: 0.656583508849144
  batch 350 loss: 0.680057374238968
  batch 400 loss: 0.6676684641838073
  batch 450 loss: 0.6583468985557556
  batch 500 loss: 0.6723994129896164
  batch 550 loss: 0.6654604786634445
  batch 600 loss: 0.6841031539440156
  batch 650 loss: 0.6639259040355683
  batch 700 loss: 0.6803955084085465
  batch 750 loss: 0.6382042235136032
  batch 800 loss: 0.6887246841192245
  batch 850 loss: 0.6709714949131012
  batch 900 loss: 0.6636229759454727
running loss: 14.700290739536285
LOSS train 0.66362 valid 0.77084, valid PER 23.86%
EPOCH 18, Learning Rate: 0.0375
  batch 50 loss: 0.657781891822815
  batch 100 loss: 0.6371300345659257
  batch 150 loss: 0.6870665383338929
  batch 200 loss: 0.656710855960846
  batch 250 loss: 0.6408656626939774
  batch 300 loss: 0.6488397860527039
  batch 350 loss: 0.634721120595932
  batch 400 loss: 0.6532588666677475
  batch 450 loss: 0.6704003894329071
  batch 500 loss: 0.6496631252765656
  batch 550 loss: 0.6995965921878815
  batch 600 loss: 0.6711217558383942
  batch 650 loss: 0.6559370863437652
  batch 700 loss: 0.6505708438158035
  batch 750 loss: 0.663310416340828
  batch 800 loss: 0.6722935795783996
  batch 850 loss: 0.6845717173814774
  batch 900 loss: 0.6589780670404434
running loss: 16.097403079271317
LOSS train 0.65898 valid 0.76964, valid PER 23.89%
EPOCH 19, Learning Rate: 0.01875
  batch 50 loss: 0.6535364717245102
  batch 100 loss: 0.6582963526248932
  batch 150 loss: 0.6537185299396515
  batch 200 loss: 0.6188232505321503
  batch 250 loss: 0.6617929643392563
  batch 300 loss: 0.6754876095056533
  batch 350 loss: 0.663414209485054
  batch 400 loss: 0.6553243148326874
  batch 450 loss: 0.6130491030216217
  batch 500 loss: 0.6355449748039246
  batch 550 loss: 0.6565221309661865
  batch 600 loss: 0.6648490554094315
  batch 650 loss: 0.6661039578914643
  batch 700 loss: 0.6706751781702042
  batch 750 loss: 0.6413528919219971
  batch 800 loss: 0.6080679881572724
  batch 850 loss: 0.6522426635026932
  batch 900 loss: 0.6508988016843795
running loss: 15.35207685828209
LOSS train 0.65090 valid 0.76398, valid PER 23.60%
EPOCH 20, Learning Rate: 0.009375
  batch 50 loss: 0.6156158840656281
  batch 100 loss: 0.6503900527954102
  batch 150 loss: 0.634339788556099
  batch 200 loss: 0.6460846823453903
  batch 250 loss: 0.673454812169075
  batch 300 loss: 0.6516879051923752
  batch 350 loss: 0.6213900011777878
  batch 400 loss: 0.6420894223451614
  batch 450 loss: 0.6299149417877197
  batch 500 loss: 0.6609816598892212
  batch 550 loss: 0.6535835361480713
  batch 600 loss: 0.6304044449329376
  batch 650 loss: 0.6438810759782791
  batch 700 loss: 0.6260737800598144
  batch 750 loss: 0.6055227041244506
  batch 800 loss: 0.6451206141710282
  batch 850 loss: 0.6517229223251343
  batch 900 loss: 0.6311022317409516
running loss: 16.434795767068863
LOSS train 0.63110 valid 0.76090, valid PER 23.40%
Training finished in 6.0 minutes.
Model saved to checkpoints/20231210_043033/model_20
Loading model from checkpoints/20231210_043033/model_20
SUB: 14.75%, DEL: 8.13%, INS: 2.14%, COR: 77.12%, PER: 25.02%
