Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=None)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.1683362150192265
  batch 100 loss: 3.171694836616516
  batch 150 loss: 3.033505654335022
  batch 200 loss: 2.9206793212890627
  batch 250 loss: 2.8457581567764283
  batch 300 loss: 2.6672167348861695
  batch 350 loss: 2.5329700040817262
  batch 400 loss: 2.519623908996582
  batch 450 loss: 2.4393224477767945
  batch 500 loss: 2.363879961967468
  batch 550 loss: 2.3536850237846374
  batch 600 loss: 2.1575311183929444
  batch 650 loss: 2.0675918841362
  batch 700 loss: 2.055745153427124
  batch 750 loss: 1.9875312066078186
  batch 800 loss: 1.9582078528404236
  batch 850 loss: 1.9061190533638
  batch 900 loss: 1.8888773465156554
LOSS train 1.88888 valid 1.80978, valid PER 69.74%
EPOCH 2:
  batch 50 loss: 1.8317590022087098
  batch 100 loss: 1.7783291268348693
  batch 150 loss: 1.7705392408370972
  batch 200 loss: 1.7404200696945191
  batch 250 loss: 1.77968013048172
  batch 300 loss: 1.7159079313278198
  batch 350 loss: 1.6270511364936828
  batch 400 loss: 1.6500612807273864
  batch 450 loss: 1.5995493745803833
  batch 500 loss: 1.61805588722229
  batch 550 loss: 1.6403079676628112
  batch 600 loss: 1.5752610182762146
  batch 650 loss: 1.6032906651496888
  batch 700 loss: 1.5944498348236085
  batch 750 loss: 1.562953643798828
  batch 800 loss: 1.4970679831504823
  batch 850 loss: 1.5100657272338867
  batch 900 loss: 1.5223189115524292
LOSS train 1.52232 valid 1.40435, valid PER 47.41%
EPOCH 3:
  batch 50 loss: 1.4749218583106996
  batch 100 loss: 1.4604794144630433
  batch 150 loss: 1.446890561580658
  batch 200 loss: 1.4205143141746521
  batch 250 loss: 1.4030084323883056
  batch 300 loss: 1.4217448592185975
  batch 350 loss: 1.4466225361824037
  batch 400 loss: 1.4246155643463134
  batch 450 loss: 1.4015133666992188
  batch 500 loss: 1.3717609000205995
  batch 550 loss: 1.3846477603912353
  batch 600 loss: 1.3640738868713378
  batch 650 loss: 1.3186358714103699
  batch 700 loss: 1.3733613872528077
  batch 750 loss: 1.4019140529632568
  batch 800 loss: 1.3303304815292358
  batch 850 loss: 1.3830122542381287
  batch 900 loss: 1.3050242066383362
LOSS train 1.30502 valid 1.28295, valid PER 40.69%
EPOCH 4:
  batch 50 loss: 1.3014521741867064
  batch 100 loss: 1.321208906173706
  batch 150 loss: 1.2804972350597381
  batch 200 loss: 1.2892077040672303
  batch 250 loss: 1.3084678912162782
  batch 300 loss: 1.31897483587265
  batch 350 loss: 1.2490086984634399
  batch 400 loss: 1.2968468809127807
  batch 450 loss: 1.2745350563526154
  batch 500 loss: 1.2777737116813659
  batch 550 loss: 1.2901988351345062
  batch 600 loss: 1.3021712601184845
  batch 650 loss: 1.2848139929771423
  batch 700 loss: 1.2476266288757325
  batch 750 loss: 1.223031541109085
  batch 800 loss: 1.212207534313202
  batch 850 loss: 1.2308503115177154
  batch 900 loss: 1.2811183905601502
LOSS train 1.28112 valid 1.18681, valid PER 38.92%
EPOCH 5:
  batch 50 loss: 1.1908103895187379
  batch 100 loss: 1.198778920173645
  batch 150 loss: 1.253272430896759
  batch 200 loss: 1.1837838125228881
  batch 250 loss: 1.1898043441772461
  batch 300 loss: 1.2138216269016266
  batch 350 loss: 1.204047737121582
  batch 400 loss: 1.2007638323307037
  batch 450 loss: 1.2018651902675628
  batch 500 loss: 1.2010103416442872
  batch 550 loss: 1.1521613800525665
  batch 600 loss: 1.2256131100654601
  batch 650 loss: 1.1922010111808776
  batch 700 loss: 1.2213526833057404
  batch 750 loss: 1.1656885182857513
  batch 800 loss: 1.179622576236725
  batch 850 loss: 1.1924552476406098
  batch 900 loss: 1.1887133395671845
LOSS train 1.18871 valid 1.11666, valid PER 36.59%
EPOCH 6:
  batch 50 loss: 1.2036494266986848
  batch 100 loss: 1.1358521473407746
  batch 150 loss: 1.1340980803966523
  batch 200 loss: 1.1337071478366851
  batch 250 loss: 1.1598911464214325
  batch 300 loss: 1.1412012135982514
  batch 350 loss: 1.136889511346817
  batch 400 loss: 1.1312798571586609
  batch 450 loss: 1.1550489664077759
  batch 500 loss: 1.1633454954624176
  batch 550 loss: 1.1658164799213409
  batch 600 loss: 1.1253744268417358
  batch 650 loss: 1.1481223511695862
  batch 700 loss: 1.1482328498363494
  batch 750 loss: 1.1209970498085022
  batch 800 loss: 1.1248691713809966
  batch 850 loss: 1.1206915915012359
  batch 900 loss: 1.5889865064620972
LOSS train 1.58899 valid 1.38440, valid PER 45.02%
EPOCH 7:
  batch 50 loss: 1.2577191638946532
  batch 100 loss: 1.2168805336952209
  batch 150 loss: 1.1835432291030883
  batch 200 loss: 1.1431106185913087
  batch 250 loss: 1.1975105285644532
  batch 300 loss: 1.1448641288280488
  batch 350 loss: 1.1379855835437775
  batch 400 loss: 1.1533383822441101
  batch 450 loss: 1.135916337966919
  batch 500 loss: 1.1217316031455993
  batch 550 loss: 1.1169184732437134
  batch 600 loss: 1.1215619826316834
  batch 650 loss: 1.1044719159603118
  batch 700 loss: 1.1448052740097046
  batch 750 loss: 1.1327029263973236
  batch 800 loss: 1.1076777517795562
  batch 850 loss: 1.140889949798584
  batch 900 loss: 1.1557566654682159
LOSS train 1.15576 valid 1.10219, valid PER 36.15%
EPOCH 8:
  batch 50 loss: 1.105013120174408
  batch 100 loss: 1.1070303809642792
  batch 150 loss: 1.0846836733818055
  batch 200 loss: 1.0796128940582275
  batch 250 loss: 1.0908325827121734
  batch 300 loss: 1.0468071794509888
  batch 350 loss: 1.1226434469223023
  batch 400 loss: 1.0706493711471559
  batch 450 loss: 1.1025830650329589
  batch 500 loss: 1.112217378616333
  batch 550 loss: 1.0655133712291718
  batch 600 loss: 1.1081542301177978
  batch 650 loss: 1.133097666501999
  batch 700 loss: 1.0769916951656342
  batch 750 loss: 1.0871187162399292
  batch 800 loss: 1.1058584892749785
  batch 850 loss: 1.1089397513866424
  batch 900 loss: 1.0803742396831513
LOSS train 1.08037 valid 1.05135, valid PER 33.73%
EPOCH 9:
  batch 50 loss: 1.0308992779254913
  batch 100 loss: 1.0568515646457672
  batch 150 loss: 1.0746757531166076
  batch 200 loss: 1.0270974361896514
  batch 250 loss: 1.0562839615345
  batch 300 loss: 1.072574064731598
  batch 350 loss: 1.0860315299034118
  batch 400 loss: 1.0603750121593476
  batch 450 loss: 1.0618303048610687
  batch 500 loss: 1.0253955447673797
  batch 550 loss: 1.0866994702816009
  batch 600 loss: 1.102265317440033
  batch 650 loss: 1.0628525280952454
  batch 700 loss: 1.0533085465431213
  batch 750 loss: 1.0468078589439391
  batch 800 loss: 1.059967439174652
  batch 850 loss: 1.0783965706825256
  batch 900 loss: 1.0332244980335235
LOSS train 1.03322 valid 1.03584, valid PER 34.06%
EPOCH 10:
  batch 50 loss: 0.9905777728557587
  batch 100 loss: 1.0274335837364197
  batch 150 loss: 1.0350070762634278
  batch 200 loss: 1.0650628316402435
  batch 250 loss: 1.0404305016994477
  batch 300 loss: 0.9935979127883912
  batch 350 loss: 1.043597835302353
  batch 400 loss: 1.002220833301544
  batch 450 loss: 1.0024743616580962
  batch 500 loss: 1.0299274146556854
  batch 550 loss: 1.0408458280563355
  batch 600 loss: 1.0138071417808532
  batch 650 loss: 1.0027636802196502
  batch 700 loss: 1.0287411439418792
  batch 750 loss: 1.0133490788936614
  batch 800 loss: 1.0531073033809661
  batch 850 loss: 1.0567328667640685
  batch 900 loss: 1.0437746226787568
LOSS train 1.04377 valid 1.06452, valid PER 34.83%
EPOCH 11:
  batch 50 loss: 1.0121272110939026
  batch 100 loss: 0.9890645098686218
  batch 150 loss: 0.9891815960407258
  batch 200 loss: 1.0498962545394896
  batch 250 loss: 1.0266938591003418
  batch 300 loss: 0.968069669008255
  batch 350 loss: 1.0175299549102783
  batch 400 loss: 1.0266636466979981
  batch 450 loss: 1.0521782970428466
  batch 500 loss: 1.068108766078949
  batch 550 loss: 1.0235582220554351
  batch 600 loss: 0.9980304253101349
  batch 650 loss: 1.0635001420974732
  batch 700 loss: 0.9718638300895691
  batch 750 loss: 1.0007366526126862
  batch 800 loss: 1.0551338744163514
  batch 850 loss: 1.066716614961624
  batch 900 loss: 1.0534961807727814
LOSS train 1.05350 valid 1.02728, valid PER 33.26%
EPOCH 12:
  batch 50 loss: 1.0028150951862336
  batch 100 loss: 0.9745351314544678
  batch 150 loss: 0.9839628124237061
  batch 200 loss: 0.9910831451416016
  batch 250 loss: 1.0214661848545075
  batch 300 loss: 0.9802615702152252
  batch 350 loss: 0.9905431842803956
  batch 400 loss: 1.0231383812427521
  batch 450 loss: 1.0121875596046448
  batch 500 loss: 1.0231121242046357
  batch 550 loss: 0.9666094052791595
  batch 600 loss: 0.9517191326618195
  batch 650 loss: 1.0751285135746003
  batch 700 loss: 1.064628360271454
  batch 750 loss: 1.0118626856803894
  batch 800 loss: 1.007664932012558
  batch 850 loss: 1.048258328437805
  batch 900 loss: 1.0665379190444946
LOSS train 1.06654 valid 1.05651, valid PER 34.18%
EPOCH 13:
  batch 50 loss: 1.0039185750484467
  batch 100 loss: 1.011896651983261
  batch 150 loss: 0.9660653674602508
  batch 200 loss: 0.9915173625946045
  batch 250 loss: 0.9861377012729645
  batch 300 loss: 0.9613006830215454
  batch 350 loss: 0.971452454328537
  batch 400 loss: 0.9914693057537078
  batch 450 loss: 1.0293773460388183
  batch 500 loss: 0.9646437740325928
  batch 550 loss: 0.991763505935669
  batch 600 loss: 1.0129753470420837
  batch 650 loss: 1.0118585741519928
  batch 700 loss: 0.9978562605381012
  batch 750 loss: 0.9510900759696961
  batch 800 loss: 0.9709847462177277
  batch 850 loss: 1.0319258332252503
  batch 900 loss: 1.0096783185005187
LOSS train 1.00968 valid 1.01442, valid PER 33.20%
EPOCH 14:
  batch 50 loss: 0.9895527577400207
  batch 100 loss: 0.9743954730033875
  batch 150 loss: 1.0085993349552154
  batch 200 loss: 1.032871606349945
  batch 250 loss: 0.9817857956886291
  batch 300 loss: 0.984564950466156
  batch 350 loss: 0.9609946095943451
  batch 400 loss: 0.9747004187107087
  batch 450 loss: 0.9746039962768555
  batch 500 loss: 0.9855544996261597
  batch 550 loss: 0.9935514879226685
  batch 600 loss: 0.9744028282165528
  batch 650 loss: 1.0064410638809205
  batch 700 loss: 1.0397500121593475
  batch 750 loss: 0.9554147565364838
  batch 800 loss: 0.9351684701442718
  batch 850 loss: 0.9678785836696625
  batch 900 loss: 0.9894731175899506
LOSS train 0.98947 valid 0.99156, valid PER 32.26%
EPOCH 15:
  batch 50 loss: 0.9515388345718384
  batch 100 loss: 0.9405451822280884
  batch 150 loss: 0.931590861082077
  batch 200 loss: 0.9896016108989716
  batch 250 loss: 0.9524442768096923
  batch 300 loss: 0.9195526754856109
  batch 350 loss: 0.931779693365097
  batch 400 loss: 0.9453874909877777
  batch 450 loss: 0.9664170861244201
  batch 500 loss: 0.9403862905502319
  batch 550 loss: 0.9959532558918
  batch 600 loss: 1.0248354613780974
  batch 650 loss: 0.9904121708869934
  batch 700 loss: 0.9886720836162567
  batch 750 loss: 0.9914813685417175
  batch 800 loss: 0.9597766411304474
  batch 850 loss: 0.9375930178165436
  batch 900 loss: 0.9623601031303406
LOSS train 0.96236 valid 1.01057, valid PER 32.48%
EPOCH 16:
  batch 50 loss: 0.9441499960422516
  batch 100 loss: 0.9072984516620636
  batch 150 loss: 0.9267844080924987
  batch 200 loss: 0.9404320847988129
  batch 250 loss: 0.9445823466777802
  batch 300 loss: 0.9410128092765808
  batch 350 loss: 0.9866516089439392
  batch 400 loss: 0.9515904438495636
  batch 450 loss: 0.9684523510932922
  batch 500 loss: 0.9074248337745666
  batch 550 loss: 0.9543902397155761
  batch 600 loss: 0.9316401469707489
  batch 650 loss: 0.934323742389679
  batch 700 loss: 0.913961375951767
  batch 750 loss: 0.9396678924560546
  batch 800 loss: 0.9438162827491761
  batch 850 loss: 0.9281652438640594
  batch 900 loss: 0.919598070383072
LOSS train 0.91960 valid 0.99289, valid PER 31.66%
EPOCH 17:
  batch 50 loss: 0.9243521451950073
  batch 100 loss: 0.9203374171257019
  batch 150 loss: 0.9208779752254486
  batch 200 loss: 0.9160271954536437
  batch 250 loss: 0.937906424999237
  batch 300 loss: 0.9284171104431153
  batch 350 loss: 0.9461654877662659
  batch 400 loss: 0.9698625314235687
  batch 450 loss: 1.0131610810756684
  batch 500 loss: 0.9540904867649078
  batch 550 loss: 0.9659720242023468
  batch 600 loss: 0.9949452209472657
  batch 650 loss: 0.9238652110099792
  batch 700 loss: 0.9124567139148713
  batch 750 loss: 0.9033591306209564
  batch 800 loss: 0.9229653239250183
  batch 850 loss: 0.9233055222034454
  batch 900 loss: 0.8963711035251617
LOSS train 0.89637 valid 0.95627, valid PER 30.64%
EPOCH 18:
  batch 50 loss: 0.8933380091190338
  batch 100 loss: 0.9150184559822082
  batch 150 loss: 0.9359127068519593
  batch 200 loss: 0.9165584886074066
  batch 250 loss: 0.9155519306659698
  batch 300 loss: 0.9073124837875366
  batch 350 loss: 0.9389488971233368
  batch 400 loss: 0.9111710500717163
  batch 450 loss: 0.9802064025402069
  batch 500 loss: 0.9440443313121796
  batch 550 loss: 0.9256259763240814
  batch 600 loss: 0.9211115407943725
  batch 650 loss: 0.9002289938926696
  batch 700 loss: 0.9484686613082886
  batch 750 loss: 0.9424317276477814
  batch 800 loss: 0.9361960399150848
  batch 850 loss: 0.9364340591430664
  batch 900 loss: 0.9319007849693298
LOSS train 0.93190 valid 0.96304, valid PER 30.93%
EPOCH 19:
  batch 50 loss: 0.8669600021839142
  batch 100 loss: 0.8749783539772034
  batch 150 loss: 0.894484828710556
  batch 200 loss: 0.9134658360481263
  batch 250 loss: 0.9235661411285401
  batch 300 loss: 0.922094202041626
  batch 350 loss: 0.9207670915126801
  batch 400 loss: 0.9210882079601288
  batch 450 loss: 0.9321952712535858
  batch 500 loss: 0.9181876194477081
  batch 550 loss: 0.9033744990825653
  batch 600 loss: 0.9365874934196472
  batch 650 loss: 0.9742732667922973
  batch 700 loss: 0.8840442955493927
  batch 750 loss: 0.9155659556388855
  batch 800 loss: 0.9685800981521606
  batch 850 loss: 0.9619397222995758
  batch 900 loss: 0.9270849609375
LOSS train 0.92708 valid 0.97650, valid PER 31.59%
EPOCH 20:
  batch 50 loss: 0.8876511466503143
  batch 100 loss: 0.8801299810409546
  batch 150 loss: 0.8873746609687805
  batch 200 loss: 0.8936292445659637
  batch 250 loss: 0.8938038969039916
  batch 300 loss: 0.904010671377182
  batch 350 loss: 0.8804064345359802
  batch 400 loss: 0.8860671043395996
  batch 450 loss: 0.906498030424118
  batch 500 loss: 0.8789704930782318
  batch 550 loss: 0.9505076098442078
  batch 600 loss: 0.8728897643089294
  batch 650 loss: 0.9159318423271179
  batch 700 loss: 0.9577741086483001
  batch 750 loss: 0.9520603084564209
  batch 800 loss: 0.9924356150627136
  batch 850 loss: 0.9454554724693298
  batch 900 loss: 0.9182876110076904
LOSS train 0.91829 valid 0.97312, valid PER 30.48%
Training finished in 3.0 minutes.
Model saved to checkpoints/20231206_203710/model_17
Loading model from checkpoints/20231206_203710/model_17
SUB: 15.88%, DEL: 14.60%, INS: 1.93%, COR: 69.53%, PER: 32.41%
