Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.8, optimiser='sgd')
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 19.997957496643068
  batch 100 loss: 19.716879768371584
  batch 150 loss: 19.754753723144532
  batch 200 loss: 19.430854835510253
  batch 250 loss: 19.577195873260496
  batch 300 loss: 19.629845695495604
  batch 350 loss: 19.5707177734375
  batch 400 loss: 19.81062084197998
  batch 450 loss: 19.58472261428833
  batch 500 loss: 20.164171676635743
  batch 550 loss: 19.75581533432007
  batch 600 loss: 19.291475467681884
  batch 650 loss: 19.697758827209473
  batch 700 loss: 19.598213996887207
  batch 750 loss: 19.617128639221193
  batch 800 loss: 19.08176389694214
  batch 850 loss: 19.52951244354248
  batch 900 loss: 19.57687145233154
LOSS train 19.57687 valid 20.00323, valid PER 100.00%
EPOCH 2:
  batch 50 loss: 19.220919876098634
  batch 100 loss: 19.51841468811035
  batch 150 loss: 19.904512271881103
  batch 200 loss: 19.031797580718994
  batch 250 loss: 19.33909210205078
  batch 300 loss: 19.644655094146728
  batch 350 loss: 19.718024826049806
  batch 400 loss: 19.557177085876464
  batch 450 loss: 19.41023220062256
  batch 500 loss: 19.378016624450684
  batch 550 loss: 19.703099784851073
  batch 600 loss: 19.524477005004883
  batch 650 loss: 19.658528594970704
  batch 700 loss: 19.85581111907959
  batch 750 loss: 19.92724967956543
  batch 800 loss: 19.436528282165526
  batch 850 loss: 20.29128952026367
  batch 900 loss: 19.55524417877197
LOSS train 19.55524 valid 20.05652, valid PER 100.00%
EPOCH 3:
  batch 50 loss: 19.772286701202393
  batch 100 loss: 20.082129821777343
  batch 150 loss: 19.6809716796875
  batch 200 loss: 19.521585578918458
  batch 250 loss: 19.459784240722655
  batch 300 loss: 19.482002868652344
  batch 350 loss: 19.586682624816895
  batch 400 loss: 19.67671548843384
  batch 450 loss: 20.09426055908203
  batch 500 loss: 19.390909175872803
  batch 550 loss: 19.628172912597655
  batch 600 loss: 19.548808135986327
  batch 650 loss: 19.4377799987793
  batch 700 loss: 20.023893871307372
  batch 750 loss: 19.654497146606445
  batch 800 loss: 19.586926956176757
  batch 850 loss: 19.39969367980957
  batch 900 loss: 19.232741394042968
LOSS train 19.23274 valid 20.04848, valid PER 100.00%
EPOCH 4:
  batch 50 loss: 19.31170873641968
  batch 100 loss: 20.358873901367186
  batch 150 loss: 19.10325054168701
  batch 200 loss: 19.709715423583983
  batch 250 loss: 19.385192146301268
  batch 300 loss: 19.319879302978517
  batch 350 loss: 19.84264549255371
  batch 400 loss: 19.855696563720702
  batch 450 loss: 20.144006690979005
  batch 500 loss: 19.627293510437013
  batch 550 loss: 19.975828170776367
  batch 600 loss: 19.95444881439209
  batch 650 loss: 19.726077957153322
  batch 700 loss: 19.70330841064453
  batch 750 loss: 19.482599182128908
  batch 800 loss: 19.18213348388672
  batch 850 loss: 19.02401247024536
  batch 900 loss: 19.70000804901123
LOSS train 19.70001 valid 20.02254, valid PER 100.00%
EPOCH 5:
  batch 50 loss: 19.46428508758545
  batch 100 loss: 19.7091898727417
  batch 150 loss: 19.18129135131836
  batch 200 loss: 19.987020378112792
  batch 250 loss: 19.367699127197266
  batch 300 loss: 19.551350708007813
  batch 350 loss: 20.31992256164551
  batch 400 loss: 19.636248512268068
  batch 450 loss: 19.598134136199953
  batch 500 loss: 19.213290004730226
  batch 550 loss: 19.551472549438476
  batch 600 loss: 19.438871574401855
  batch 650 loss: 19.327633323669435
  batch 700 loss: 19.3387434387207
  batch 750 loss: 19.523082275390625
  batch 800 loss: 20.121821403503418
  batch 850 loss: 20.052397842407228
  batch 900 loss: 19.89066457748413
LOSS train 19.89066 valid 20.04952, valid PER 100.00%
EPOCH 6:
  batch 50 loss: 19.858388404846192
  batch 100 loss: 19.607647819519045
  batch 150 loss: 19.508914642333984
  batch 200 loss: 19.681410636901855
  batch 250 loss: 19.82204574584961
  batch 300 loss: 18.986675968170164
  batch 350 loss: 19.19163621902466
  batch 400 loss: 19.955831604003905
  batch 450 loss: 19.564415702819826
  batch 500 loss: 19.375154876708983
  batch 550 loss: 20.067416801452637
  batch 600 loss: 19.3505770111084
  batch 650 loss: 19.758332290649413
  batch 700 loss: 20.093402328491212
  batch 750 loss: 19.524097785949706
  batch 800 loss: 19.835060234069825
  batch 850 loss: 19.75373977661133
  batch 900 loss: 19.779208393096923
LOSS train 19.77921 valid 20.04827, valid PER 100.00%
EPOCH 7:
  batch 50 loss: 19.569312496185304
  batch 100 loss: 19.8522798538208
  batch 150 loss: 19.634477806091308
  batch 200 loss: 19.35597381591797
  batch 250 loss: 19.891513862609862
  batch 300 loss: 19.317925815582274
  batch 350 loss: 19.738454265594484
  batch 400 loss: 19.779874534606932
  batch 450 loss: 19.63928192138672
  batch 500 loss: 19.182606601715086
  batch 550 loss: 19.692253723144532
  batch 600 loss: 20.063828468322754
  batch 650 loss: 20.09921781539917
  batch 700 loss: 19.15734394073486
  batch 750 loss: 19.61044651031494
  batch 800 loss: 19.55806713104248
  batch 850 loss: 19.74388008117676
  batch 900 loss: 19.442894973754882
LOSS train 19.44289 valid 20.02259, valid PER 100.00%
EPOCH 8:
  batch 50 loss: 20.101661720275878
  batch 100 loss: 19.658112907409667
  batch 150 loss: 19.954659996032714
  batch 200 loss: 19.22130491256714
  batch 250 loss: 19.27102699279785
  batch 300 loss: 19.708457145690918
  batch 350 loss: 19.530849380493166
  batch 400 loss: 19.438694496154785
  batch 450 loss: 19.421413097381592
  batch 500 loss: 19.344374961853028
  batch 550 loss: 20.04388294219971
  batch 600 loss: 19.857719821929933
  batch 650 loss: 19.650783462524416
  batch 700 loss: 18.95347457885742
  batch 750 loss: 19.73007387161255
  batch 800 loss: 19.743453788757325
  batch 850 loss: 20.34917739868164
  batch 900 loss: 19.335590229034423
LOSS train 19.33559 valid 20.04323, valid PER 100.00%
EPOCH 9:
  batch 50 loss: 19.524073867797853
  batch 100 loss: 19.944664707183836
  batch 150 loss: 19.68295248031616
  batch 200 loss: 19.771714477539064
  batch 250 loss: 19.293485908508302
  batch 300 loss: 19.798782348632812
  batch 350 loss: 19.768247966766356
  batch 400 loss: 19.25916109085083
  batch 450 loss: 19.8330016708374
  batch 500 loss: 19.28288080215454
  batch 550 loss: 19.471688499450682
  batch 600 loss: 20.18713520050049
  batch 650 loss: 19.529665260314943
  batch 700 loss: 19.749801330566406
  batch 750 loss: 19.443307323455812
  batch 800 loss: 19.385096969604493
  batch 850 loss: 19.715006713867187
  batch 900 loss: 19.62823352813721
LOSS train 19.62823 valid 19.89278, valid PER 100.00%
EPOCH 10:
  batch 50 loss: 19.47287487030029
  batch 100 loss: 19.462842559814455
  batch 150 loss: 19.263714637756348
  batch 200 loss: 19.656584434509277
  batch 250 loss: 19.754748611450196
  batch 300 loss: 19.967257194519043
  batch 350 loss: 19.642386703491212
  batch 400 loss: 20.013862838745116
  batch 450 loss: 19.514646186828614
  batch 500 loss: 19.88961700439453
  batch 550 loss: 19.677605056762694
  batch 600 loss: 19.666424770355224
  batch 650 loss: 19.592459564208983
  batch 700 loss: 19.496640663146973
  batch 750 loss: 19.566564788818358
  batch 800 loss: 19.613486499786376
  batch 850 loss: 19.5738671875
  batch 900 loss: 19.72949676513672
LOSS train 19.72950 valid 20.08776, valid PER 100.00%
EPOCH 11:
  batch 50 loss: 19.997794151306152
  batch 100 loss: 20.028349838256837
  batch 150 loss: 19.348407516479494
  batch 200 loss: 19.658147373199462
  batch 250 loss: 19.597694091796875
  batch 300 loss: 19.711117782592773
  batch 350 loss: 19.331532020568847
  batch 400 loss: 19.556585216522215
  batch 450 loss: 19.663212528228758
  batch 500 loss: 19.797000923156737
  batch 550 loss: 19.68716377258301
  batch 600 loss: 19.65950126647949
  batch 650 loss: 19.877080574035645
  batch 700 loss: 19.23813404083252
  batch 750 loss: 19.862556686401366
  batch 800 loss: 19.521105346679686
  batch 850 loss: 19.40125686645508
  batch 900 loss: 19.40250274658203
LOSS train 19.40250 valid 20.03201, valid PER 100.00%
EPOCH 12:
  batch 50 loss: 19.54953983306885
  batch 100 loss: 19.51878105163574
  batch 150 loss: 19.7291561126709
  batch 200 loss: 20.24795808792114
  batch 250 loss: 19.65666347503662
  batch 300 loss: 19.111623134613037
  batch 350 loss: 20.06768341064453
  batch 400 loss: 20.22880283355713
  batch 450 loss: 19.40733345031738
  batch 500 loss: 19.625596199035645
  batch 550 loss: 19.393692512512207
  batch 600 loss: 19.814645843505858
  batch 650 loss: 19.750450172424316
  batch 700 loss: 19.551764831542968
  batch 750 loss: 19.404877128601075
  batch 800 loss: 19.209280548095702
  batch 850 loss: 19.44034076690674
  batch 900 loss: 19.66290180206299
LOSS train 19.66290 valid 19.99873, valid PER 100.00%
EPOCH 13:
  batch 50 loss: 19.216184768676758
  batch 100 loss: 19.880668869018553
  batch 150 loss: 19.392802505493165
  batch 200 loss: 19.69739959716797
  batch 250 loss: 19.548234882354738
  batch 300 loss: 19.798944091796876
  batch 350 loss: 19.751082725524903
  batch 400 loss: 19.518223056793214
  batch 450 loss: 19.609572715759278
  batch 500 loss: 19.73035530090332
  batch 550 loss: 19.77032199859619
  batch 600 loss: 19.979295959472655
  batch 650 loss: 19.30622856140137
  batch 700 loss: 19.394609451293945
  batch 750 loss: 19.37764831542969
  batch 800 loss: 19.75470859527588
  batch 850 loss: 19.59667028427124
  batch 900 loss: 19.5939066696167
LOSS train 19.59391 valid 20.02991, valid PER 100.00%
EPOCH 14:
  batch 50 loss: 19.59427143096924
  batch 100 loss: 20.13747901916504
  batch 150 loss: 19.496602058410645
  batch 200 loss: 19.467082290649415
  batch 250 loss: 19.302681465148925
  batch 300 loss: 19.596631088256835
  batch 350 loss: 20.123311882019042
  batch 400 loss: 19.09111095428467
  batch 450 loss: 19.940182876586913
  batch 500 loss: 19.880581398010253
  batch 550 loss: 19.2859268951416
  batch 600 loss: 20.025949935913086
  batch 650 loss: 19.599939708709716
  batch 700 loss: 19.14051290512085
  batch 750 loss: 19.46790756225586
  batch 800 loss: 19.87839385986328
  batch 850 loss: 19.57240421295166
  batch 900 loss: 19.59405963897705
LOSS train 19.59406 valid 20.06240, valid PER 100.00%
EPOCH 15:
  batch 50 loss: 19.48264835357666
  batch 100 loss: 19.21321376800537
  batch 150 loss: 19.549155654907228
  batch 200 loss: 19.738317947387696
  batch 250 loss: 19.93580539703369
  batch 300 loss: 19.508312759399413
  batch 350 loss: 19.37670965194702
  batch 400 loss: 19.651429481506348
  batch 450 loss: 19.546832962036135
  batch 500 loss: 19.758911209106444
  batch 550 loss: 19.945975131988526
  batch 600 loss: 19.356458740234373
  batch 650 loss: 19.818139400482178
  batch 700 loss: 19.169524574279784
  batch 750 loss: 19.595843601226807
  batch 800 loss: 19.629831581115724
  batch 850 loss: 19.943830032348632
  batch 900 loss: 19.698279914855956
LOSS train 19.69828 valid 20.02362, valid PER 100.00%
EPOCH 16:
  batch 50 loss: 19.668394737243652
  batch 100 loss: 19.867589149475098
  batch 150 loss: 19.7674666595459
  batch 200 loss: 19.96928867340088
  batch 250 loss: 19.57004451751709
  batch 300 loss: 19.331542472839356
  batch 350 loss: 19.765517959594728
  batch 400 loss: 19.864910774230957
  batch 450 loss: 18.874928970336914
  batch 500 loss: 19.528640975952147
  batch 550 loss: 19.93572591781616
  batch 600 loss: 19.359913368225097
  batch 650 loss: 19.73575336456299
  batch 700 loss: 19.445463562011717
  batch 750 loss: 19.04269943237305
  batch 800 loss: 19.61952491760254
  batch 850 loss: 19.858908767700196
  batch 900 loss: 19.734020652770997
LOSS train 19.73402 valid 20.02594, valid PER 100.00%
EPOCH 17:
  batch 50 loss: 19.38812122344971
  batch 100 loss: 19.436486892700195
  batch 150 loss: 19.49725730895996
  batch 200 loss: 19.597005081176757
  batch 250 loss: 19.213213157653808
  batch 300 loss: 19.746186447143554
  batch 350 loss: 19.552967224121094
  batch 400 loss: 19.743689460754396
  batch 450 loss: 20.021042976379395
  batch 500 loss: 19.71214782714844
  batch 550 loss: 19.21417205810547
  batch 600 loss: 20.188851127624513
  batch 650 loss: 19.465593185424805
  batch 700 loss: 19.802894897460938
  batch 750 loss: 19.834981155395507
  batch 800 loss: 19.768885173797607
  batch 850 loss: 19.13303997039795
  batch 900 loss: 19.760634536743165
LOSS train 19.76063 valid 20.04695, valid PER 100.00%
EPOCH 18:
  batch 50 loss: 19.186047801971437
  batch 100 loss: 19.664012870788575
  batch 150 loss: 19.56324161529541
  batch 200 loss: 20.102038764953612
  batch 250 loss: 19.665358505249024
  batch 300 loss: 19.82707130432129
  batch 350 loss: 19.791509208679198
  batch 400 loss: 19.562273902893068
  batch 450 loss: 19.466920413970946
  batch 500 loss: 19.66837978363037
  batch 550 loss: 19.343114318847658
  batch 600 loss: 19.81309799194336
  batch 650 loss: 19.30246223449707
  batch 700 loss: 19.21434055328369
  batch 750 loss: 19.81813159942627
  batch 800 loss: 20.075290927886964
  batch 850 loss: 19.403204078674317
  batch 900 loss: 19.275562591552735
LOSS train 19.27556 valid 20.04002, valid PER 100.00%
EPOCH 19:
  batch 50 loss: 19.900772399902344
  batch 100 loss: 19.48391525268555
  batch 150 loss: 19.70696990966797
  batch 200 loss: 19.744258060455323
  batch 250 loss: 19.360347595214844
  batch 300 loss: 19.420236282348633
  batch 350 loss: 19.641637268066408
  batch 400 loss: 20.120057945251464
  batch 450 loss: 19.596927394866942
  batch 500 loss: 19.876599731445314
  batch 550 loss: 19.73852828979492
  batch 600 loss: 19.376243267059326
  batch 650 loss: 19.72792881011963
  batch 700 loss: 19.64226127624512
  batch 750 loss: 19.950416526794434
  batch 800 loss: 19.495270767211913
  batch 850 loss: 19.3995916557312
  batch 900 loss: 19.10977165222168
LOSS train 19.10977 valid 20.03501, valid PER 100.00%
EPOCH 20:
  batch 50 loss: 19.54931198120117
  batch 100 loss: 19.706193809509276
  batch 150 loss: 19.65911724090576
  batch 200 loss: 19.5848779296875
  batch 250 loss: 19.70415222167969
  batch 300 loss: 19.46513698577881
  batch 350 loss: 19.810532531738282
  batch 400 loss: 19.675645847320556
  batch 450 loss: 19.687500381469725
  batch 500 loss: 19.326620445251464
  batch 550 loss: 19.34906188964844
  batch 600 loss: 19.744132423400877
  batch 650 loss: 19.962127990722657
  batch 700 loss: 19.807267532348632
  batch 750 loss: 19.69497859954834
  batch 800 loss: 19.289727535247803
  batch 850 loss: 19.56983991622925
  batch 900 loss: 19.563720932006834
LOSS train 19.56372 valid 19.99210, valid PER 100.00%
Training finished in 5.0 minutes.
Model saved to checkpoints/20231204_180749/model_9
Loading model from checkpoints/20231204_180749/model_9
SUB: 0.00%, DEL: 100.00%, INS: 0.00%, COR: 0.00%, PER: 100.00%
