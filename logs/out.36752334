Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.0)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 5.109557447433471
  batch 100 loss: 3.298157820701599
  batch 150 loss: 3.1005423974990847
  batch 200 loss: 2.864187965393066
  batch 250 loss: 2.6755204391479492
  batch 300 loss: 2.493889479637146
  batch 350 loss: 2.398358507156372
  batch 400 loss: 2.3485293054580687
  batch 450 loss: 2.2797482013702393
  batch 500 loss: 2.189026582241058
  batch 550 loss: 2.13443834066391
  batch 600 loss: 2.083241605758667
  batch 650 loss: 1.9998148369789124
  batch 700 loss: 2.0077413654327394
  batch 750 loss: 1.9409379982948303
  batch 800 loss: 1.930073299407959
  batch 850 loss: 1.8806702971458436
  batch 900 loss: 1.8776150512695313
LOSS train 1.87762 valid 1.84849, valid PER 71.06%
EPOCH 2:
  batch 50 loss: 1.8182656645774842
  batch 100 loss: 1.772030041217804
  batch 150 loss: 1.7498980116844178
  batch 200 loss: 1.7655878472328186
  batch 250 loss: 1.7487942504882812
  batch 300 loss: 1.722405662536621
  batch 350 loss: 1.6374476194381713
  batch 400 loss: 1.6654215621948243
  batch 450 loss: 1.6274616432189941
  batch 500 loss: 1.659091682434082
  batch 550 loss: 1.6533115577697755
  batch 600 loss: 1.6013702726364136
  batch 650 loss: 1.6468203377723694
  batch 700 loss: 1.5939060378074645
  batch 750 loss: 1.5893118405342102
  batch 800 loss: 1.5291510152816772
  batch 850 loss: 1.54065945148468
  batch 900 loss: 1.5654213953018188
LOSS train 1.56542 valid 1.46736, valid PER 57.82%
EPOCH 3:
  batch 50 loss: 1.5401982760429382
  batch 100 loss: 1.4923190784454345
  batch 150 loss: 1.4854628729820252
  batch 200 loss: 1.4791767978668213
  batch 250 loss: 1.4677403569221497
  batch 300 loss: 1.4758201479911803
  batch 350 loss: 1.5059868955612183
  batch 400 loss: 1.4811182284355164
  batch 450 loss: 1.4507852721214294
  batch 500 loss: 1.4507970881462098
  batch 550 loss: 1.4508147096633912
  batch 600 loss: 1.415094814300537
  batch 650 loss: 1.4140966653823852
  batch 700 loss: 1.415743691921234
  batch 750 loss: 1.4797910928726197
  batch 800 loss: 1.3904586124420166
  batch 850 loss: 1.4344718766212463
  batch 900 loss: 1.3681439328193665
LOSS train 1.36814 valid 1.32283, valid PER 46.96%
EPOCH 4:
  batch 50 loss: 1.3756830930709838
  batch 100 loss: 1.386249577999115
  batch 150 loss: 1.3388664937019348
  batch 200 loss: 1.3739793586730957
  batch 250 loss: 1.3641995668411255
  batch 300 loss: 1.3808863973617553
  batch 350 loss: 1.3027399182319641
  batch 400 loss: 1.363174467086792
  batch 450 loss: 1.3301800560951234
  batch 500 loss: 1.3085657250881195
  batch 550 loss: 1.3315190744400025
  batch 600 loss: 1.3605811238288879
  batch 650 loss: 1.324011240005493
  batch 700 loss: 1.2969607281684876
  batch 750 loss: 1.2845183444023132
  batch 800 loss: 1.255414855480194
  batch 850 loss: 1.3043019318580626
  batch 900 loss: 1.3287449824810027
LOSS train 1.32874 valid 1.24590, valid PER 43.36%
EPOCH 5:
  batch 50 loss: 1.2685794854164123
  batch 100 loss: 1.2522928547859191
  batch 150 loss: 1.3158169603347778
  batch 200 loss: 1.23311714053154
  batch 250 loss: 1.2353100728988649
  batch 300 loss: 1.2584950935840606
  batch 350 loss: 1.2701175689697266
  batch 400 loss: 1.2583899569511414
  batch 450 loss: 1.2309139811992644
  batch 500 loss: 1.2536315894126893
  batch 550 loss: 1.2003674042224883
  batch 600 loss: 1.2758646535873412
  batch 650 loss: 1.2243261229991913
  batch 700 loss: 1.2769823312759399
  batch 750 loss: 1.198326699733734
  batch 800 loss: 1.2482598793506623
  batch 850 loss: 1.2230964624881744
  batch 900 loss: 1.2456416535377501
LOSS train 1.24564 valid 1.14954, valid PER 38.70%
EPOCH 6:
  batch 50 loss: 1.2363208281993865
  batch 100 loss: 1.1797050285339354
  batch 150 loss: 1.1854815125465392
  batch 200 loss: 1.1833595824241638
  batch 250 loss: 1.21070574760437
  batch 300 loss: 1.2176206851005553
  batch 350 loss: 1.1781333267688752
  batch 400 loss: 1.1760876274108887
  batch 450 loss: 1.2069630062580108
  batch 500 loss: 1.1840683603286744
  batch 550 loss: 1.2036577188968658
  batch 600 loss: 1.1843793392181396
  batch 650 loss: 1.187406302690506
  batch 700 loss: 1.165748163461685
  batch 750 loss: 1.1635807502269744
  batch 800 loss: 1.1387929213047028
  batch 850 loss: 1.1367001736164093
  batch 900 loss: 1.178043841123581
LOSS train 1.17804 valid 1.13855, valid PER 37.94%
EPOCH 7:
  batch 50 loss: 1.1623980116844177
  batch 100 loss: 1.171594638824463
  batch 150 loss: 1.1427515482902526
  batch 200 loss: 1.1107091629505157
  batch 250 loss: 1.1414678120613098
  batch 300 loss: 1.1143681943416595
  batch 350 loss: 1.1307734167575836
  batch 400 loss: 1.1346975755691528
  batch 450 loss: 1.1389749312400819
  batch 500 loss: 1.1166385173797608
  batch 550 loss: 1.1185328483581543
  batch 600 loss: 1.15119549036026
  batch 650 loss: 1.1025707077980043
  batch 700 loss: 1.145737122297287
  batch 750 loss: 1.1032986962795257
  batch 800 loss: 1.117547754049301
  batch 850 loss: 1.1438032376766205
  batch 900 loss: 1.1643379843235016
LOSS train 1.16434 valid 1.07838, valid PER 36.78%
EPOCH 8:
  batch 50 loss: 1.0847827172279358
  batch 100 loss: 1.0859045851230622
  batch 150 loss: 1.0758379554748536
  batch 200 loss: 1.0477656126022339
  batch 250 loss: 1.089909520149231
  batch 300 loss: 1.0269581329822541
  batch 350 loss: 1.1176357543468476
  batch 400 loss: 1.072780886888504
  batch 450 loss: 1.0785371148586274
  batch 500 loss: 1.1118886828422547
  batch 550 loss: 1.0609780156612396
  batch 600 loss: 1.0954737603664397
  batch 650 loss: 1.121398026943207
  batch 700 loss: 1.067885057926178
  batch 750 loss: 1.0839011347293854
  batch 800 loss: 1.0936813354492188
  batch 850 loss: 1.0894219779968262
  batch 900 loss: 1.0962062239646913
LOSS train 1.09621 valid 1.05540, valid PER 34.30%
EPOCH 9:
  batch 50 loss: 1.0256050395965577
  batch 100 loss: 1.05358127951622
  batch 150 loss: 1.046854259967804
  batch 200 loss: 1.0248379731178283
  batch 250 loss: 1.0489556765556336
  batch 300 loss: 1.0670783948898315
  batch 350 loss: 1.0722615337371826
  batch 400 loss: 1.0655477595329286
  batch 450 loss: 1.0450704383850098
  batch 500 loss: 1.0267925989627837
  batch 550 loss: 1.0560270822048188
  batch 600 loss: 1.0487112927436828
  batch 650 loss: 1.037355946302414
  batch 700 loss: 1.023507797718048
  batch 750 loss: 1.0400727128982543
  batch 800 loss: 1.0764764761924743
  batch 850 loss: 1.0586486089229583
  batch 900 loss: 1.0211418604850768
LOSS train 1.02114 valid 1.04560, valid PER 33.04%
EPOCH 10:
  batch 50 loss: 0.9917778015136719
  batch 100 loss: 1.0001024794578552
  batch 150 loss: 1.026805101633072
  batch 200 loss: 1.031267513036728
  batch 250 loss: 1.0292090845108033
  batch 300 loss: 1.0007137656211853
  batch 350 loss: 1.033539400100708
  batch 400 loss: 0.9887072563171386
  batch 450 loss: 0.9786083257198334
  batch 500 loss: 1.0325331997871399
  batch 550 loss: 1.0327660858631134
  batch 600 loss: 1.0073938715457915
  batch 650 loss: 0.995085426568985
  batch 700 loss: 1.0186906445026398
  batch 750 loss: 1.0067759895324706
  batch 800 loss: 1.018495876789093
  batch 850 loss: 1.0132951354980468
  batch 900 loss: 1.025778011083603
LOSS train 1.02578 valid 1.02490, valid PER 33.72%
EPOCH 11:
  batch 50 loss: 0.9554466247558594
  batch 100 loss: 0.9755626893043519
  batch 150 loss: 0.9814309859275818
  batch 200 loss: 1.0102867329120635
  batch 250 loss: 0.9894908368587494
  batch 300 loss: 0.9605901312828063
  batch 350 loss: 0.9921354556083679
  batch 400 loss: 0.9990890765190125
  batch 450 loss: 0.9968454051017761
  batch 500 loss: 0.978582707643509
  batch 550 loss: 0.9833907771110535
  batch 600 loss: 0.9794709062576294
  batch 650 loss: 1.014677836894989
  batch 700 loss: 0.9790237247943878
  batch 750 loss: 0.9656279766559601
  batch 800 loss: 0.9961309087276459
  batch 850 loss: 1.0150827479362488
  batch 900 loss: 0.9901537561416626
LOSS train 0.99015 valid 0.99506, valid PER 32.86%
EPOCH 12:
  batch 50 loss: 0.9726496303081512
  batch 100 loss: 0.9488692629337311
  batch 150 loss: 0.9364749872684479
  batch 200 loss: 0.9392044639587402
  batch 250 loss: 0.951240508556366
  batch 300 loss: 0.9498679876327515
  batch 350 loss: 0.9527552342414856
  batch 400 loss: 0.9764688682556152
  batch 450 loss: 0.9727913236618042
  batch 500 loss: 0.9789857840538025
  batch 550 loss: 0.92010972738266
  batch 600 loss: 0.9288128423690796
  batch 650 loss: 0.9928265285491943
  batch 700 loss: 0.9644391560554504
  batch 750 loss: 0.9649443292617798
  batch 800 loss: 0.9312566673755646
  batch 850 loss: 1.003574857711792
  batch 900 loss: 0.9689154541492462
LOSS train 0.96892 valid 0.97219, valid PER 31.22%
EPOCH 13:
  batch 50 loss: 0.9246489882469178
  batch 100 loss: 0.9463496100902558
  batch 150 loss: 0.926853529214859
  batch 200 loss: 0.9485430717468262
  batch 250 loss: 0.9296143746376038
  batch 300 loss: 0.930866152048111
  batch 350 loss: 0.9156410479545594
  batch 400 loss: 0.9518966543674469
  batch 450 loss: 0.9507093548774719
  batch 500 loss: 0.9077162992954254
  batch 550 loss: 0.9486512136459351
  batch 600 loss: 0.9253306686878204
  batch 650 loss: 0.9496230268478394
  batch 700 loss: 0.9528394615650178
  batch 750 loss: 0.9084007668495179
  batch 800 loss: 0.901671189069748
  batch 850 loss: 0.9769710147380829
  batch 900 loss: 0.950476655960083
LOSS train 0.95048 valid 0.95617, valid PER 30.25%
EPOCH 14:
  batch 50 loss: 0.9120171773433685
  batch 100 loss: 0.9269419455528259
  batch 150 loss: 0.9083725488185883
  batch 200 loss: 0.9107150328159332
  batch 250 loss: 0.9099468958377838
  batch 300 loss: 0.9437944710254669
  batch 350 loss: 0.8959281051158905
  batch 400 loss: 0.919899924993515
  batch 450 loss: 0.9134689235687256
  batch 500 loss: 0.9237248229980469
  batch 550 loss: 0.928858151435852
  batch 600 loss: 0.8929239284992218
  batch 650 loss: 0.9430966877937317
  batch 700 loss: 0.9465151238441467
  batch 750 loss: 0.9114389145374298
  batch 800 loss: 0.8832315897941589
  batch 850 loss: 0.9291884458065033
  batch 900 loss: 0.9177712881565094
LOSS train 0.91777 valid 0.95950, valid PER 30.72%
EPOCH 15:
  batch 50 loss: 0.903912969827652
  batch 100 loss: 0.8969696116447449
  batch 150 loss: 0.9016577267646789
  batch 200 loss: 0.9339220130443573
  batch 250 loss: 0.9000805854797364
  batch 300 loss: 0.8851594698429107
  batch 350 loss: 0.9026554644107818
  batch 400 loss: 0.8892406570911408
  batch 450 loss: 0.8852775263786316
  batch 500 loss: 0.8603277480602265
  batch 550 loss: 0.8873730540275574
  batch 600 loss: 0.9090750467777252
  batch 650 loss: 0.924332424402237
  batch 700 loss: 0.9224998474121093
  batch 750 loss: 0.9190246677398681
  batch 800 loss: 0.9094291055202484
  batch 850 loss: 0.8943608236312867
  batch 900 loss: 0.8989627838134766
LOSS train 0.89896 valid 0.94601, valid PER 30.23%
EPOCH 16:
  batch 50 loss: 0.920074188709259
  batch 100 loss: 0.8515433359146118
  batch 150 loss: 0.8607525503635407
  batch 200 loss: 0.8719438827037811
  batch 250 loss: 0.8801029753684998
  batch 300 loss: 0.8923149943351746
  batch 350 loss: 0.9039321744441986
  batch 400 loss: 0.8965802538394928
  batch 450 loss: 0.9264900314807892
  batch 500 loss: 0.869897335767746
  batch 550 loss: 0.9022993111610412
  batch 600 loss: 0.8865363466739654
  batch 650 loss: 0.891235728263855
  batch 700 loss: 0.8731589090824127
  batch 750 loss: 0.8615413248538971
  batch 800 loss: 0.9091759908199311
  batch 850 loss: 0.881218992471695
  batch 900 loss: 0.8831343698501587
LOSS train 0.88313 valid 0.93135, valid PER 28.70%
EPOCH 17:
  batch 50 loss: 0.8604513329267501
  batch 100 loss: 0.8557660937309265
  batch 150 loss: 0.8478272521495819
  batch 200 loss: 0.84533527135849
  batch 250 loss: 0.8499117374420166
  batch 300 loss: 0.8993055939674377
  batch 350 loss: 0.8486966502666473
  batch 400 loss: 0.8979775130748748
  batch 450 loss: 0.9051359403133392
  batch 500 loss: 0.8545013546943665
  batch 550 loss: 0.8663804030418396
  batch 600 loss: 0.9085922503471374
  batch 650 loss: 0.8688379001617431
  batch 700 loss: 0.8687486124038696
  batch 750 loss: 0.8612965285778046
  batch 800 loss: 0.8833385872840881
  batch 850 loss: 0.8813024759292603
  batch 900 loss: 0.8575834381580353
LOSS train 0.85758 valid 0.92893, valid PER 29.41%
EPOCH 18:
  batch 50 loss: 0.8477058029174804
  batch 100 loss: 0.8485616528987885
  batch 150 loss: 0.8672425889968872
  batch 200 loss: 0.8667062318325043
  batch 250 loss: 0.8778919219970703
  batch 300 loss: 0.8430513906478881
  batch 350 loss: 0.872490005493164
  batch 400 loss: 0.8356154298782349
  batch 450 loss: 0.89986847281456
  batch 500 loss: 0.863008679151535
  batch 550 loss: 0.8910595953464509
  batch 600 loss: 0.8416995942592621
  batch 650 loss: 0.8531216037273407
  batch 700 loss: 0.8790033149719239
  batch 750 loss: 0.8375665032863617
  batch 800 loss: 0.8474856162071228
  batch 850 loss: 0.8370754277706146
  batch 900 loss: 0.8993788397312165
LOSS train 0.89938 valid 0.94749, valid PER 29.78%
EPOCH 19:
  batch 50 loss: 0.7997794425487519
  batch 100 loss: 0.8076532018184662
  batch 150 loss: 0.8278487062454224
  batch 200 loss: 0.8376780676841736
  batch 250 loss: 0.8639305627346039
  batch 300 loss: 0.8349624741077423
  batch 350 loss: 0.8360538792610168
  batch 400 loss: 0.843309463262558
  batch 450 loss: 0.8605514800548554
  batch 500 loss: 0.841278737783432
  batch 550 loss: 0.8376437366008759
  batch 600 loss: 0.8429329335689545
  batch 650 loss: 0.8828765738010407
  batch 700 loss: 0.8293445348739624
  batch 750 loss: 0.8418869554996491
  batch 800 loss: 0.8530276131629944
  batch 850 loss: 0.8559378015995026
  batch 900 loss: 0.8472509717941284
LOSS train 0.84725 valid 0.92646, valid PER 29.08%
EPOCH 20:
  batch 50 loss: 0.7908643782138824
  batch 100 loss: 0.8095099604129792
  batch 150 loss: 0.8260321879386902
  batch 200 loss: 0.8261514329910278
  batch 250 loss: 0.8239663755893707
  batch 300 loss: 0.8345380997657776
  batch 350 loss: 0.8063941395282745
  batch 400 loss: 0.8223080670833588
  batch 450 loss: 0.8307446432113648
  batch 500 loss: 0.8215073108673095
  batch 550 loss: 0.8709363067150115
  batch 600 loss: 0.8090630352497101
  batch 650 loss: 0.8493244445323944
  batch 700 loss: 0.855543200969696
  batch 750 loss: 0.8215504276752472
  batch 800 loss: 0.8478738999366761
  batch 850 loss: 0.8691019690036774
  batch 900 loss: 0.8525163257122039
LOSS train 0.85252 valid 0.90769, valid PER 28.18%
Training finished in 5.0 minutes.
Model saved to checkpoints/20231206_212248/model_20
Loading model from checkpoints/20231206_212248/model_20
SUB: 15.38%, DEL: 13.55%, INS: 1.79%, COR: 71.07%, PER: 30.72%
