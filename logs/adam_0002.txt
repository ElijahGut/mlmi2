Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.0002, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='adam', grad_clip=1.5)
Total number of model parameters is 166952
EPOCH 1, Learning Rate: 0.0002
  batch 50 loss: 22.719241104125977
  batch 100 loss: 7.354003372192383
  batch 150 loss: 3.2795166301727297
  batch 200 loss: 3.1780158376693723
  batch 250 loss: 3.1391544914245606
  batch 300 loss: 3.057094864845276
  batch 350 loss: 2.9882563972473144
  batch 400 loss: 2.9483008003234863
  batch 450 loss: 2.938892970085144
  batch 500 loss: 2.8630211973190307
  batch 550 loss: 2.8476178550720217
  batch 600 loss: 2.8015317153930663
  batch 650 loss: 2.764642367362976
  batch 700 loss: 2.7271441888809203
  batch 750 loss: 2.6876605701446534
  batch 800 loss: 2.6691297674179078
  batch 850 loss: 2.623452696800232
  batch 900 loss: 2.560087704658508
avg val loss: 2.5530269145965576
LOSS train 2.56009 valid 2.55303, valid PER 79.06%
EPOCH 2, Learning Rate: 0.0002
  batch 50 loss: 2.5465831089019777
  batch 100 loss: 2.51049024105072
  batch 150 loss: 2.433891496658325
  batch 200 loss: 2.4271881675720213
  batch 250 loss: 2.4213589525222776
  batch 300 loss: 2.3914772033691407
  batch 350 loss: 2.314574851989746
  batch 400 loss: 2.3256735944747926
  batch 450 loss: 2.2802219820022582
  batch 500 loss: 2.2772748041152955
  batch 550 loss: 2.265405673980713
  batch 600 loss: 2.224910616874695
  batch 650 loss: 2.2078120470047
  batch 700 loss: 2.1990316772460936
  batch 750 loss: 2.176521410942078
  batch 800 loss: 2.1150826454162597
  batch 850 loss: 2.107930579185486
  batch 900 loss: 2.111145613193512
avg val loss: 2.0842185020446777
LOSS train 2.11115 valid 2.08422, valid PER 77.19%
EPOCH 3, Learning Rate: 0.0002
  batch 50 loss: 2.0907202553749085
  batch 100 loss: 2.037952809333801
  batch 150 loss: 2.034596085548401
  batch 200 loss: 2.0366301202774046
  batch 250 loss: 2.002975492477417
  batch 300 loss: 1.9911277890205383
  batch 350 loss: 2.0215351486206057
  batch 400 loss: 1.978154468536377
  batch 450 loss: 1.9456717038154603
  batch 500 loss: 1.953305790424347
  batch 550 loss: 1.9356070804595946
  batch 600 loss: 1.9019986772537232
  batch 650 loss: 1.8854688906669617
  batch 700 loss: 1.9063154983520507
  batch 750 loss: 1.9294199752807617
  batch 800 loss: 1.8671897292137145
  batch 850 loss: 1.8880236554145813
  batch 900 loss: 1.8389040112495423
avg val loss: 1.843876600265503
LOSS train 1.83890 valid 1.84388, valid PER 72.02%
EPOCH 4, Learning Rate: 0.0002
  batch 50 loss: 1.8470149374008178
  batch 100 loss: 1.850472288131714
  batch 150 loss: 1.8042637276649476
  batch 200 loss: 1.845772261619568
  batch 250 loss: 1.8198019433021546
  batch 300 loss: 1.8179177927970886
  batch 350 loss: 1.753832633495331
  batch 400 loss: 1.7925622415542604
  batch 450 loss: 1.7921094059944154
  batch 500 loss: 1.7551411080360413
  batch 550 loss: 1.7779874968528748
  batch 600 loss: 1.7874299693107605
  batch 650 loss: 1.7768724417686463
  batch 700 loss: 1.7399925923347472
  batch 750 loss: 1.7267198514938356
  batch 800 loss: 1.6996960186958312
  batch 850 loss: 1.7297241830825805
  batch 900 loss: 1.7560812282562255
avg val loss: 1.70106840133667
LOSS train 1.75608 valid 1.70107, valid PER 66.58%
EPOCH 5, Learning Rate: 0.0002
  batch 50 loss: 1.7112028813362121
  batch 100 loss: 1.6914095020294189
  batch 150 loss: 1.7142708349227904
  batch 200 loss: 1.669872934818268
  batch 250 loss: 1.6797579908370972
  batch 300 loss: 1.6955154895782472
  batch 350 loss: 1.6852345848083496
  batch 400 loss: 1.6741604566574098
  batch 450 loss: 1.6565689539909363
  batch 500 loss: 1.6816502356529235
  batch 550 loss: 1.627850158214569
  batch 600 loss: 1.6932117319107056
  batch 650 loss: 1.6399186730384827
  batch 700 loss: 1.6701595568656922
  batch 750 loss: 1.6098756527900695
  batch 800 loss: 1.6266320180892944
  batch 850 loss: 1.650082039833069
  batch 900 loss: 1.6523169207572936
avg val loss: 1.6110013723373413
LOSS train 1.65232 valid 1.61100, valid PER 62.10%
EPOCH 6, Learning Rate: 0.0002
  batch 50 loss: 1.6490243649482728
  batch 100 loss: 1.5983490633964539
  batch 150 loss: 1.588700134754181
  batch 200 loss: 1.605684278011322
  batch 250 loss: 1.625746293067932
  batch 300 loss: 1.5788683128356933
  batch 350 loss: 1.5996145009994507
  batch 400 loss: 1.5704042720794678
  batch 450 loss: 1.6076048970222474
  batch 500 loss: 1.57393404006958
  batch 550 loss: 1.5925191783905028
  batch 600 loss: 1.5737671804428102
  batch 650 loss: 1.5829140281677245
  batch 700 loss: 1.578830726146698
  batch 750 loss: 1.5488187193870544
  batch 800 loss: 1.543739321231842
  batch 850 loss: 1.5449454808235168
  batch 900 loss: 1.5731210684776307
avg val loss: 1.541591763496399
LOSS train 1.57312 valid 1.54159, valid PER 60.41%
EPOCH 7, Learning Rate: 0.0002
  batch 50 loss: 1.559736807346344
  batch 100 loss: 1.565523693561554
  batch 150 loss: 1.543414568901062
  batch 200 loss: 1.533764536380768
  batch 250 loss: 1.5294224095344544
  batch 300 loss: 1.5119357657432557
  batch 350 loss: 1.5057759380340576
  batch 400 loss: 1.5241153264045715
  batch 450 loss: 1.5183430552482604
  batch 500 loss: 1.5136418294906617
  batch 550 loss: 1.5179622054100037
  batch 600 loss: 1.5150823616981506
  batch 650 loss: 1.4949433422088623
  batch 700 loss: 1.5178137421607971
  batch 750 loss: 1.489294526576996
  batch 800 loss: 1.4885179471969605
  batch 850 loss: 1.5077326130867004
  batch 900 loss: 1.54214772939682
avg val loss: 1.4839766025543213
LOSS train 1.54215 valid 1.48398, valid PER 56.73%
EPOCH 8, Learning Rate: 0.0002
  batch 50 loss: 1.4910722303390502
  batch 100 loss: 1.4893765044212341
  batch 150 loss: 1.4671981072425841
  batch 200 loss: 1.4525241303443908
  batch 250 loss: 1.4850211477279662
  batch 300 loss: 1.4131678104400636
  batch 350 loss: 1.479887807369232
  batch 400 loss: 1.4613158369064332
  batch 450 loss: 1.4761268639564513
  batch 500 loss: 1.5126172423362731
  batch 550 loss: 1.437452371120453
  batch 600 loss: 1.489092538356781
  batch 650 loss: 1.4992447638511657
  batch 700 loss: 1.4351828908920288
  batch 750 loss: 1.4561953258514404
  batch 800 loss: 1.4582492768764497
  batch 850 loss: 1.4579394102096557
  batch 900 loss: 1.4371010065078735
avg val loss: 1.4237085580825806
LOSS train 1.43710 valid 1.42371, valid PER 52.81%
EPOCH 9, Learning Rate: 0.0002
  batch 50 loss: 1.394925684928894
  batch 100 loss: 1.456468391418457
  batch 150 loss: 1.4446416926383971
  batch 200 loss: 1.391367380619049
  batch 250 loss: 1.4247780346870422
  batch 300 loss: 1.4404813408851624
  batch 350 loss: 1.4453143095970153
  batch 400 loss: 1.4175050711631776
  batch 450 loss: 1.4362084007263183
  batch 500 loss: 1.3986950349807739
  batch 550 loss: 1.4282344818115233
  batch 600 loss: 1.4219959735870362
  batch 650 loss: 1.4015409231185914
  batch 700 loss: 1.414057376384735
  batch 750 loss: 1.3954106044769288
  batch 800 loss: 1.4108148550987243
  batch 850 loss: 1.428113431930542
  batch 900 loss: 1.3948038411140442
avg val loss: 1.3823986053466797
LOSS train 1.39480 valid 1.38240, valid PER 50.85%
EPOCH 10, Learning Rate: 0.0002
  batch 50 loss: 1.3730951261520385
  batch 100 loss: 1.3888402676582337
  batch 150 loss: 1.4144658851623535
  batch 200 loss: 1.4048030483722687
  batch 250 loss: 1.3776946830749512
  batch 300 loss: 1.3611464810371399
  batch 350 loss: 1.3983878779411316
  batch 400 loss: 1.3598232555389405
  batch 450 loss: 1.344012393951416
  batch 500 loss: 1.4068803215026855
  batch 550 loss: 1.3985093212127686
  batch 600 loss: 1.3804841136932373
  batch 650 loss: 1.3574952387809753
  batch 700 loss: 1.3841962623596191
  batch 750 loss: 1.3683121156692506
  batch 800 loss: 1.3766854596138
  batch 850 loss: 1.376535451412201
  batch 900 loss: 1.3891541743278504
avg val loss: 1.3522071838378906
LOSS train 1.38915 valid 1.35221, valid PER 48.51%
EPOCH 11, Learning Rate: 0.0002
  batch 50 loss: 1.3330012440681458
  batch 100 loss: 1.342125539779663
  batch 150 loss: 1.3266527438163758
  batch 200 loss: 1.376221250295639
  batch 250 loss: 1.3659159851074218
  batch 300 loss: 1.3322997641563417
  batch 350 loss: 1.3486683940887452
  batch 400 loss: 1.3586927676200866
  batch 450 loss: 1.345938048362732
  batch 500 loss: 1.3336553800106048
  batch 550 loss: 1.3293166542053223
  batch 600 loss: 1.328002164363861
  batch 650 loss: 1.3886228013038635
  batch 700 loss: 1.311054060459137
  batch 750 loss: 1.3269669365882875
  batch 800 loss: 1.372741137742996
  batch 850 loss: 1.3661866617202758
  batch 900 loss: 1.3543264746665955
avg val loss: 1.3039687871932983
LOSS train 1.35433 valid 1.30397, valid PER 45.23%
EPOCH 12, Learning Rate: 0.0002
  batch 50 loss: 1.332103865146637
  batch 100 loss: 1.3077566242218017
  batch 150 loss: 1.3230475747585297
  batch 200 loss: 1.3050717890262604
  batch 250 loss: 1.3493310070037843
  batch 300 loss: 1.3180972146987915
  batch 350 loss: 1.3220686316490173
  batch 400 loss: 1.3276837587356567
  batch 450 loss: 1.3266206407546997
  batch 500 loss: 1.3472447609901428
  batch 550 loss: 1.2563101291656493
  batch 600 loss: 1.2850978517532348
  batch 650 loss: 1.3283196949958802
  batch 700 loss: 1.3183471250534058
  batch 750 loss: 1.2932237243652345
  batch 800 loss: 1.2728618907928466
  batch 850 loss: 1.306926223039627
  batch 900 loss: 1.3210203874111175
avg val loss: 1.3033949136734009
LOSS train 1.32102 valid 1.30339, valid PER 45.41%
EPOCH 13, Learning Rate: 0.0002
  batch 50 loss: 1.2608845233917236
  batch 100 loss: 1.311631405353546
  batch 150 loss: 1.2719095730781556
  batch 200 loss: 1.3000141406059265
  batch 250 loss: 1.281435887813568
  batch 300 loss: 1.2643217873573303
  batch 350 loss: 1.2859346175193787
  batch 400 loss: 1.2922899198532105
  batch 450 loss: 1.3012598764896393
  batch 500 loss: 1.263908301591873
  batch 550 loss: 1.2637710678577423
  batch 600 loss: 1.2743323040008545
  batch 650 loss: 1.276385703086853
  batch 700 loss: 1.2880717968940736
  batch 750 loss: 1.255206356048584
  batch 800 loss: 1.2637135076522827
  batch 850 loss: 1.3043575382232666
  batch 900 loss: 1.296309609413147
avg val loss: 1.2588391304016113
LOSS train 1.29631 valid 1.25884, valid PER 43.19%
EPOCH 14, Learning Rate: 0.0002
  batch 50 loss: 1.2577789425849915
  batch 100 loss: 1.2852306270599365
  batch 150 loss: 1.2433533263206482
  batch 200 loss: 1.259011640548706
  batch 250 loss: 1.258235250711441
  batch 300 loss: 1.2849430680274962
  batch 350 loss: 1.2301715171337129
  batch 400 loss: 1.243305311203003
  batch 450 loss: 1.242356915473938
  batch 500 loss: 1.2704597008228302
  batch 550 loss: 1.27662082195282
  batch 600 loss: 1.2169768583774567
  batch 650 loss: 1.2562029540538788
  batch 700 loss: 1.2771801781654357
  batch 750 loss: 1.2023491203784942
  batch 800 loss: 1.203073697090149
  batch 850 loss: 1.2573918426036834
  batch 900 loss: 1.2468575549125671
avg val loss: 1.2384711503982544
LOSS train 1.24686 valid 1.23847, valid PER 42.35%
EPOCH 15, Learning Rate: 0.0002
  batch 50 loss: 1.2471429228782653
  batch 100 loss: 1.2323407804965973
  batch 150 loss: 1.2271031367778777
  batch 200 loss: 1.2496464836597443
  batch 250 loss: 1.25570659160614
  batch 300 loss: 1.2016851568222047
  batch 350 loss: 1.2246209943294526
  batch 400 loss: 1.2236582672595977
  batch 450 loss: 1.214983628988266
  batch 500 loss: 1.1862776625156402
  batch 550 loss: 1.2276357519626617
  batch 600 loss: 1.2416011250019074
  batch 650 loss: 1.2515302550792695
  batch 700 loss: 1.2520816349983215
  batch 750 loss: 1.2279786443710328
  batch 800 loss: 1.2044680523872375
  batch 850 loss: 1.204355376958847
  batch 900 loss: 1.2278921914100647
avg val loss: 1.2095385789871216
LOSS train 1.22789 valid 1.20954, valid PER 39.56%
EPOCH 16, Learning Rate: 0.0002
  batch 50 loss: 1.2249289977550506
  batch 100 loss: 1.1752552199363708
  batch 150 loss: 1.200850976705551
  batch 200 loss: 1.1929078674316407
  batch 250 loss: 1.2322155916690827
  batch 300 loss: 1.2026568496227263
  batch 350 loss: 1.2241430866718293
  batch 400 loss: 1.202538890838623
  batch 450 loss: 1.2303782439231872
  batch 500 loss: 1.1793274295330047
  batch 550 loss: 1.2426159477233887
  batch 600 loss: 1.190729126930237
  batch 650 loss: 1.218154524564743
  batch 700 loss: 1.183942401409149
  batch 750 loss: 1.1938223147392273
  batch 800 loss: 1.187597451210022
  batch 850 loss: 1.1851038908958436
  batch 900 loss: 1.169375114440918
avg val loss: 1.200621485710144
LOSS train 1.16938 valid 1.20062, valid PER 39.94%
EPOCH 17, Learning Rate: 0.0002
  batch 50 loss: 1.1944717061519623
  batch 100 loss: 1.1775569796562195
  batch 150 loss: 1.173476641178131
  batch 200 loss: 1.1665720558166504
  batch 250 loss: 1.1831894195079804
  batch 300 loss: 1.1995398807525635
  batch 350 loss: 1.1497573220729829
  batch 400 loss: 1.2229624843597413
  batch 450 loss: 1.2046245777606963
  batch 500 loss: 1.1505772769451141
  batch 550 loss: 1.197567024230957
  batch 600 loss: 1.239963754415512
  batch 650 loss: 1.1773239696025848
  batch 700 loss: 1.17871311545372
  batch 750 loss: 1.132944687604904
  batch 800 loss: 1.1389125669002533
  batch 850 loss: 1.1648381268978119
  batch 900 loss: 1.1523430049419403
avg val loss: 1.1768367290496826
LOSS train 1.15234 valid 1.17684, valid PER 38.87%
EPOCH 18, Learning Rate: 0.0002
  batch 50 loss: 1.1596145963668822
  batch 100 loss: 1.1703165698051452
  batch 150 loss: 1.1809278678894044
  batch 200 loss: 1.16233132481575
  batch 250 loss: 1.1801136112213135
  batch 300 loss: 1.1538696610927581
  batch 350 loss: 1.1602493131160736
  batch 400 loss: 1.1438566184043883
  batch 450 loss: 1.2005747592449187
  batch 500 loss: 1.1594100320339202
  batch 550 loss: 1.1553109359741212
  batch 600 loss: 1.1519619488716126
  batch 650 loss: 1.1368828368186952
  batch 700 loss: 1.1761026859283448
  batch 750 loss: 1.140525619983673
  batch 800 loss: 1.1541205799579621
  batch 850 loss: 1.1381686437129974
  batch 900 loss: 1.1770493936538697
avg val loss: 1.1518826484680176
LOSS train 1.17705 valid 1.15188, valid PER 37.42%
EPOCH 19, Learning Rate: 0.0002
  batch 50 loss: 1.1042664933204651
  batch 100 loss: 1.1183874380588532
  batch 150 loss: 1.1409933745861054
  batch 200 loss: 1.1353798151016234
  batch 250 loss: 1.170308722257614
  batch 300 loss: 1.1615288138389588
  batch 350 loss: 1.1396565997600556
  batch 400 loss: 1.1562678980827332
  batch 450 loss: 1.1486227405071259
  batch 500 loss: 1.1731823420524596
  batch 550 loss: 1.1294231355190276
  batch 600 loss: 1.1384273743629456
  batch 650 loss: 1.1917522478103637
  batch 700 loss: 1.1104951417446136
  batch 750 loss: 1.1129663896560669
  batch 800 loss: 1.1466817140579224
  batch 850 loss: 1.1611434316635132
  batch 900 loss: 1.111126216650009
avg val loss: 1.1316128969192505
LOSS train 1.11113 valid 1.13161, valid PER 36.34%
EPOCH 20, Learning Rate: 0.0002
  batch 50 loss: 1.102983946800232
  batch 100 loss: 1.127809556722641
  batch 150 loss: 1.1055717384815216
  batch 200 loss: 1.1163415598869324
  batch 250 loss: 1.1294915187358856
  batch 300 loss: 1.139709051847458
  batch 350 loss: 1.1004471719264983
  batch 400 loss: 1.1271109664440155
  batch 450 loss: 1.110473096370697
  batch 500 loss: 1.087063559293747
  batch 550 loss: 1.167650170326233
  batch 600 loss: 1.091420397758484
  batch 650 loss: 1.1487845432758332
  batch 700 loss: 1.107379891872406
  batch 750 loss: 1.1304440951347352
  batch 800 loss: 1.1612457096576692
  batch 850 loss: 1.1409240341186524
  batch 900 loss: 1.1408875131607055
avg val loss: 1.1218059062957764
LOSS train 1.14089 valid 1.12181, valid PER 37.05%
Training finished in 4.0 minutes.
Model saved to checkpoints/20231210_172019/model_20
Loading model from checkpoints/20231210_172019/model_20
SUB: 15.63%, DEL: 21.88%, INS: 1.15%, COR: 62.49%, PER: 38.66%
