Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.7, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 166952
EPOCH 1, Learning Rate: 0.7
  batch 50 loss: 4.366060090065003
  batch 100 loss: 3.0926817083358764
  batch 150 loss: 2.9579703998565674
  batch 200 loss: 2.771579384803772
  batch 250 loss: 2.7071131324768065
  batch 300 loss: 2.5482980537414552
  batch 350 loss: 2.4042571449279784
  batch 400 loss: 2.37194935798645
  batch 450 loss: 2.284647033214569
  batch 500 loss: 2.204032425880432
  batch 550 loss: 2.0731361508369446
  batch 600 loss: 2.0031903696060183
  batch 650 loss: 1.9010949373245238
  batch 700 loss: 1.9020817756652832
  batch 750 loss: 1.8436315846443176
  batch 800 loss: 1.8122919487953186
  batch 850 loss: 1.767458074092865
  batch 900 loss: 1.7508319997787476
LOSS train 1.75083 valid 1.68554, valid PER 63.82%
EPOCH 2, Learning Rate: 0.7
  batch 50 loss: 1.7233406138420104
  batch 100 loss: 1.6584126877784728
  batch 150 loss: 1.6425141525268554
  batch 200 loss: 1.657748384475708
  batch 250 loss: 1.6591618704795836
  batch 300 loss: 1.6193084120750427
  batch 350 loss: 1.5396503162384034
  batch 400 loss: 1.5662435030937194
  batch 450 loss: 1.4961274456977844
  batch 500 loss: 1.5394219613075257
  batch 550 loss: 1.547358798980713
  batch 600 loss: 1.4797694444656373
  batch 650 loss: 1.5035779905319213
  batch 700 loss: 1.4969159626960755
  batch 750 loss: 1.4588126826286316
  batch 800 loss: 1.4041279077529907
  batch 850 loss: 1.4169095861911774
  batch 900 loss: 1.4432041501998902
LOSS train 1.44320 valid 1.34334, valid PER 44.72%
EPOCH 3, Learning Rate: 0.7
  batch 50 loss: 1.3763926243782043
  batch 100 loss: 1.3679260063171386
  batch 150 loss: 1.373113408088684
  batch 200 loss: 1.3498822522163392
  batch 250 loss: 1.3586162781715394
  batch 300 loss: 1.3323246479034423
  batch 350 loss: 1.3985749077796936
  batch 400 loss: 1.3518564629554748
  batch 450 loss: 1.3417329227924346
  batch 500 loss: 1.3114403462409974
  batch 550 loss: 1.3321123552322387
  batch 600 loss: 1.293961102962494
  batch 650 loss: 1.2812088859081268
  batch 700 loss: 1.293048026561737
  batch 750 loss: 1.3615439891815186
  batch 800 loss: 1.2694452238082885
  batch 850 loss: 1.2938778567314149
  batch 900 loss: 1.2609637796878814
LOSS train 1.26096 valid 1.22895, valid PER 38.81%
EPOCH 4, Learning Rate: 0.7
  batch 50 loss: 1.2334478044509887
  batch 100 loss: 1.2637426328659058
  batch 150 loss: 1.2065332043170929
  batch 200 loss: 1.2594091200828552
  batch 250 loss: 1.292741219997406
  batch 300 loss: 1.2786907684803008
  batch 350 loss: 1.17980930685997
  batch 400 loss: 1.2394145703315735
  batch 450 loss: 1.2117550683021545
  batch 500 loss: 1.2118878662586212
  batch 550 loss: 1.2401597428321838
  batch 600 loss: 1.2666084694862365
  batch 650 loss: 1.25175351023674
  batch 700 loss: 1.213397730588913
  batch 750 loss: 1.2096113300323486
  batch 800 loss: 1.1685727095603944
  batch 850 loss: 1.211231391429901
  batch 900 loss: 1.244965853691101
LOSS train 1.24497 valid 1.14784, valid PER 36.44%
EPOCH 5, Learning Rate: 0.7
  batch 50 loss: 1.1633129239082336
  batch 100 loss: 1.1704380047321319
  batch 150 loss: 1.2300888633728027
  batch 200 loss: 1.159338221549988
  batch 250 loss: 1.1451202523708344
  batch 300 loss: 1.1811133635044098
  batch 350 loss: 1.171639770269394
  batch 400 loss: 1.1743511509895326
  batch 450 loss: 1.1703423428535462
  batch 500 loss: 1.1898429000377655
  batch 550 loss: 1.1343334925174713
  batch 600 loss: 1.208777778148651
  batch 650 loss: 1.149473123550415
  batch 700 loss: 1.1880057096481322
  batch 750 loss: 1.112139366865158
  batch 800 loss: 1.1647807204723357
  batch 850 loss: 1.1914407587051392
  batch 900 loss: 1.166396028995514
LOSS train 1.16640 valid 1.09582, valid PER 35.51%
EPOCH 6, Learning Rate: 0.7
  batch 50 loss: 1.1694928538799285
  batch 100 loss: 1.1260378456115723
  batch 150 loss: 1.0918979728221894
  batch 200 loss: 1.105003011226654
  batch 250 loss: 1.1292635762691499
  batch 300 loss: 1.1236327004432678
  batch 350 loss: 1.1025885272026061
  batch 400 loss: 1.1465439701080322
  batch 450 loss: 1.153021788597107
  batch 500 loss: 1.1249171149730683
  batch 550 loss: 1.1319665682315827
  batch 600 loss: 1.1072947096824646
  batch 650 loss: 1.1159720516204834
  batch 700 loss: 1.120842000246048
  batch 750 loss: 1.1074729681015014
  batch 800 loss: 1.1124319517612458
  batch 850 loss: 1.0958751916885376
  batch 900 loss: 1.1160224950313569
LOSS train 1.11602 valid 1.10349, valid PER 34.59%
EPOCH 7, Learning Rate: 0.7
  batch 50 loss: 1.1066296231746673
  batch 100 loss: 1.1045498263835907
  batch 150 loss: 1.1032278668880462
  batch 200 loss: 1.0913458418846131
  batch 250 loss: 1.0795808982849122
  batch 300 loss: 1.082981526851654
  batch 350 loss: 1.1062156498432159
  batch 400 loss: 1.0966572427749635
  batch 450 loss: 1.0953878998756408
  batch 500 loss: 1.0652681970596314
  batch 550 loss: 1.06611319065094
  batch 600 loss: 1.0655031287670136
  batch 650 loss: 1.0566243267059325
  batch 700 loss: 1.105242282152176
  batch 750 loss: 1.0690964877605438
  batch 800 loss: 1.070747240781784
  batch 850 loss: 1.0979988312721252
  batch 900 loss: 1.1212155091762543
LOSS train 1.12122 valid 1.05444, valid PER 34.56%
EPOCH 8, Learning Rate: 0.7
  batch 50 loss: 1.0517325627803802
  batch 100 loss: 1.0452559101581573
  batch 150 loss: 1.032722797393799
  batch 200 loss: 1.0374209988117218
  batch 250 loss: 1.0832304966449737
  batch 300 loss: 0.9914657008647919
  batch 350 loss: 1.076712063550949
  batch 400 loss: 1.0612283992767333
  batch 450 loss: 1.0764814972877503
  batch 500 loss: 1.0919305777549744
  batch 550 loss: 1.0215003895759582
  batch 600 loss: 1.0866377234458924
  batch 650 loss: 1.0963289964199066
  batch 700 loss: 1.0422677314281463
  batch 750 loss: 1.043705233335495
  batch 800 loss: 1.0414610087871552
  batch 850 loss: 1.0422298526763916
  batch 900 loss: 1.1000727474689485
LOSS train 1.10007 valid 1.03577, valid PER 33.04%
EPOCH 9, Learning Rate: 0.7
  batch 50 loss: 0.9837412714958191
  batch 100 loss: 1.0343476736545563
  batch 150 loss: 1.0368076395988464
  batch 200 loss: 0.9937215352058411
  batch 250 loss: 1.0149526977539063
  batch 300 loss: 1.0218508195877076
  batch 350 loss: 1.0533328437805176
  batch 400 loss: 1.0295272076129913
  batch 450 loss: 1.034575719833374
  batch 500 loss: 0.9806503832340241
  batch 550 loss: 1.0166590917110443
  batch 600 loss: 1.0526841461658478
  batch 650 loss: 1.0363337540626525
  batch 700 loss: 0.997422366142273
  batch 750 loss: 1.0062248921394348
  batch 800 loss: 1.0239851772785187
  batch 850 loss: 1.0526540672779083
  batch 900 loss: 0.9764162421226501
LOSS train 0.97642 valid 1.01274, valid PER 32.99%
EPOCH 10, Learning Rate: 0.7
  batch 50 loss: 0.9598446857929229
  batch 100 loss: 0.9996572375297547
  batch 150 loss: 1.0096350502967835
  batch 200 loss: 1.0207621359825134
  batch 250 loss: 1.0144025039672853
  batch 300 loss: 0.9979704153537751
  batch 350 loss: 1.0148166024684906
  batch 400 loss: 0.9588020193576813
  batch 450 loss: 0.9790004110336303
  batch 500 loss: 1.007387602329254
  batch 550 loss: 1.0393313324451448
  batch 600 loss: 0.9961003041267396
  batch 650 loss: 0.9859673452377319
  batch 700 loss: 0.9947015213966369
  batch 750 loss: 0.9740793991088867
  batch 800 loss: 1.0066599929332734
  batch 850 loss: 1.0037510752677918
  batch 900 loss: 0.9950560903549195
LOSS train 0.99506 valid 1.00238, valid PER 32.52%
EPOCH 11, Learning Rate: 0.7
  batch 50 loss: 0.9475795531272888
  batch 100 loss: 0.933243191242218
  batch 150 loss: 0.9321825337409974
  batch 200 loss: 0.9899638020992279
  batch 250 loss: 0.975669139623642
  batch 300 loss: 0.9530925798416138
  batch 350 loss: 0.9711809635162354
  batch 400 loss: 0.986916902065277
  batch 450 loss: 0.9789794135093689
  batch 500 loss: 0.9477954971790313
  batch 550 loss: 0.957125893831253
  batch 600 loss: 0.9293659019470215
  batch 650 loss: 0.9971934032440185
  batch 700 loss: 0.9103133141994476
  batch 750 loss: 0.9473074173927307
  batch 800 loss: 1.0041212332248688
  batch 850 loss: 1.0114707803726197
  batch 900 loss: 1.0062926506996155
LOSS train 1.00629 valid 0.99113, valid PER 31.82%
EPOCH 12, Learning Rate: 0.7
  batch 50 loss: 0.9445471501350403
  batch 100 loss: 0.9188746547698975
  batch 150 loss: 0.923347692489624
  batch 200 loss: 0.9332778167724609
  batch 250 loss: 0.9530691969394683
  batch 300 loss: 0.9533264589309692
  batch 350 loss: 0.9663339269161224
  batch 400 loss: 0.9754912889003754
  batch 450 loss: 0.9825823152065277
  batch 500 loss: 0.9871654558181763
  batch 550 loss: 0.9328268086910247
  batch 600 loss: 0.9484789824485779
  batch 650 loss: 0.9656937515735626
  batch 700 loss: 0.9828167903423309
  batch 750 loss: 0.9451306855678558
  batch 800 loss: 0.9296902775764465
  batch 850 loss: 0.9925540137290955
  batch 900 loss: 0.9759737980365754
LOSS train 0.97597 valid 0.97780, valid PER 31.68%
EPOCH 13, Learning Rate: 0.7
  batch 50 loss: 0.8955321025848388
  batch 100 loss: 0.931096498966217
  batch 150 loss: 0.9172119200229645
  batch 200 loss: 0.9271415638923645
  batch 250 loss: 0.9354784715175629
  batch 300 loss: 0.9258717501163483
  batch 350 loss: 0.9295380449295044
  batch 400 loss: 0.9438552689552308
  batch 450 loss: 0.9485021138191223
  batch 500 loss: 0.922248865365982
  batch 550 loss: 0.9522624456882477
  batch 600 loss: 0.9560652351379395
  batch 650 loss: 0.9562296867370605
  batch 700 loss: 0.962747894525528
  batch 750 loss: 0.9183753049373626
  batch 800 loss: 0.9206154155731201
  batch 850 loss: 1.0061051011085511
  batch 900 loss: 0.9772445642948151
LOSS train 0.97724 valid 0.98512, valid PER 31.89%
EPOCH 14, Learning Rate: 0.7
  batch 50 loss: 0.9347674453258514
  batch 100 loss: 0.9142537927627563
  batch 150 loss: 0.9027544569969177
  batch 200 loss: 0.9009127008914948
  batch 250 loss: 0.9187355315685273
  batch 300 loss: 0.9608012962341309
  batch 350 loss: 0.9013080489635468
  batch 400 loss: 0.9176415169239044
  batch 450 loss: 0.9128361988067627
  batch 500 loss: 0.9387813222408294
  batch 550 loss: 0.95416539311409
  batch 600 loss: 0.9053287172317505
  batch 650 loss: 0.9313468849658966
  batch 700 loss: 0.9625758147239685
  batch 750 loss: 0.9156015455722809
  batch 800 loss: 0.9019915914535522
  batch 850 loss: 0.946396312713623
  batch 900 loss: 0.9435938012599945
LOSS train 0.94359 valid 0.96264, valid PER 30.80%
EPOCH 15, Learning Rate: 0.7
  batch 50 loss: 0.900128357410431
  batch 100 loss: 0.8976928436756134
  batch 150 loss: 0.8993078196048736
  batch 200 loss: 0.9430575489997863
  batch 250 loss: 0.9044045543670655
  batch 300 loss: 0.8987547504901886
  batch 350 loss: 0.8882357263565064
  batch 400 loss: 0.8780535280704498
  batch 450 loss: 0.9077667570114136
  batch 500 loss: 0.8678612053394318
  batch 550 loss: 0.9288675332069397
  batch 600 loss: 0.9506136751174927
  batch 650 loss: 0.9456759536266327
  batch 700 loss: 0.9279343032836914
  batch 750 loss: 0.9368556952476501
  batch 800 loss: 0.8996616911888122
  batch 850 loss: 0.9017208337783813
  batch 900 loss: 0.913790146112442
LOSS train 0.91379 valid 0.96677, valid PER 31.14%
EPOCH 16, Learning Rate: 0.7
  batch 50 loss: 0.9100208497047424
  batch 100 loss: 0.8560226762294769
  batch 150 loss: 0.8742013216018677
  batch 200 loss: 0.874075380563736
  batch 250 loss: 0.9227938914299011
  batch 300 loss: 0.8732212901115417
  batch 350 loss: 0.9352764141559601
  batch 400 loss: 0.9425855028629303
  batch 450 loss: 0.9136688435077667
  batch 500 loss: 0.8485167002677918
  batch 550 loss: 0.8972242927551269
  batch 600 loss: 0.8992233419418335
  batch 650 loss: 0.9028320050239563
  batch 700 loss: 0.8812157797813416
  batch 750 loss: 0.9320965361595154
  batch 800 loss: 0.9476763439178467
  batch 850 loss: 0.9277538859844208
  batch 900 loss: 0.9347879636287689
LOSS train 0.93479 valid 0.96474, valid PER 30.79%
EPOCH 17, Learning Rate: 0.7
  batch 50 loss: 0.9041737079620361
  batch 100 loss: 0.9143468081951142
  batch 150 loss: 0.8744463181495666
  batch 200 loss: 0.8665074861049652
  batch 250 loss: 0.8948218548297882
  batch 300 loss: 0.8745650148391724
  batch 350 loss: 0.8646495139598847
  batch 400 loss: 0.924397827386856
  batch 450 loss: 0.9358097219467163
  batch 500 loss: 0.8973860049247742
  batch 550 loss: 0.8960053861141205
  batch 600 loss: 0.939828690290451
  batch 650 loss: 0.889109628200531
  batch 700 loss: 0.887726104259491
  batch 750 loss: 0.873227777481079
  batch 800 loss: 0.8706594455242157
  batch 850 loss: 0.8883747506141663
  batch 900 loss: 0.8708879709243774
LOSS train 0.87089 valid 0.95600, valid PER 30.04%
EPOCH 18, Learning Rate: 0.7
  batch 50 loss: 0.8526264905929566
  batch 100 loss: 0.8767233335971832
  batch 150 loss: 0.8841820180416107
  batch 200 loss: 0.8868517541885376
  batch 250 loss: 0.8734137606620789
  batch 300 loss: 0.8298941683769226
  batch 350 loss: 0.8808180463314056
  batch 400 loss: 0.8480124497413635
  batch 450 loss: 0.9370818984508514
  batch 500 loss: 0.8898431718349457
  batch 550 loss: 0.8881640267372132
  batch 600 loss: 0.8562290751934052
  batch 650 loss: 0.8688896572589875
  batch 700 loss: 0.9268967175483703
  batch 750 loss: 0.8677859616279602
  batch 800 loss: 0.8557625198364258
  batch 850 loss: 0.8729460978507996
  batch 900 loss: 0.8971232306957245
LOSS train 0.89712 valid 0.97905, valid PER 31.78%
EPOCH 19, Learning Rate: 0.7
  batch 50 loss: 0.809681327342987
  batch 100 loss: 0.8192680096626281
  batch 150 loss: 0.8234210252761841
  batch 200 loss: 0.8533868598937988
  batch 250 loss: 0.8739222478866577
  batch 300 loss: 0.877079256772995
  batch 350 loss: 0.8474277245998383
  batch 400 loss: 0.8770315670967102
  batch 450 loss: 0.8704068899154663
  batch 500 loss: 0.8637988770008087
  batch 550 loss: 0.8475463044643402
  batch 600 loss: 0.8412933790683746
  batch 650 loss: 0.952191481590271
  batch 700 loss: 0.8499735116958618
  batch 750 loss: 0.8297215926647187
  batch 800 loss: 0.8515100920200348
  batch 850 loss: 0.8723594629764557
  batch 900 loss: 0.8510882234573365
LOSS train 0.85109 valid 0.96979, valid PER 31.57%
EPOCH 20, Learning Rate: 0.7
  batch 50 loss: 0.8167761135101318
  batch 100 loss: 0.8191240954399109
  batch 150 loss: 0.8013035315275192
  batch 200 loss: 0.8342794638872146
  batch 250 loss: 0.833250880241394
  batch 300 loss: 0.8473227977752685
  batch 350 loss: 0.8266761887073517
  batch 400 loss: 0.8448472166061402
  batch 450 loss: 0.8557659804821014
  batch 500 loss: 0.8281088721752167
  batch 550 loss: 0.8942360472679138
  batch 600 loss: 0.8258433175086975
  batch 650 loss: 0.8634339821338654
  batch 700 loss: 0.8651150369644165
  batch 750 loss: 0.8654380130767823
  batch 800 loss: 0.8733814620971679
  batch 850 loss: 0.8890774786472321
  batch 900 loss: 0.8799914002418519
LOSS train 0.87999 valid 0.94908, valid PER 29.84%
Training finished in 3.0 minutes.
Model saved to checkpoints/20231210_005016/model_20
Loading model from checkpoints/20231210_005016/model_20
SUB: 16.71%, DEL: 14.31%, INS: 1.67%, COR: 68.98%, PER: 32.68%
