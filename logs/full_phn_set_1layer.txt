Namespace(seed=123, train_json='train_fbank61.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_61.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=0.5)
Total number of model parameters is 172863
EPOCH 1, Learning Rate: 0.5
  batch 50 loss: 7.267055397033691
  batch 100 loss: 3.894131426811218
  batch 150 loss: 3.8243035459518433
  batch 200 loss: 3.7864379930496215
  batch 250 loss: 3.7179252433776857
  batch 300 loss: 3.641203670501709
  batch 350 loss: 3.5054691219329834
  batch 400 loss: 3.4001072359085085
  batch 450 loss: 3.2578142642974854
  batch 500 loss: 3.123220181465149
  batch 550 loss: 3.0089443349838256
  batch 600 loss: 2.8935992240905763
  batch 650 loss: 2.7774475812911987
  batch 700 loss: 2.714784345626831
  batch 750 loss: 2.67141685962677
  batch 800 loss: 2.6073467206954954
  batch 850 loss: 2.551091651916504
  batch 900 loss: 2.521472773551941
LOSS train 2.52147 valid 3.45931, valid PER 79.62%
EPOCH 2, Learning Rate: 0.5
  batch 50 loss: 2.4444697809219362
  batch 100 loss: 2.3700648760795593
  batch 150 loss: 2.340805079936981
  batch 200 loss: 2.320297956466675
  batch 250 loss: 2.3118720960617067
  batch 300 loss: 2.2694416856765747
  batch 350 loss: 2.212872178554535
  batch 400 loss: 2.1884018683433535
  batch 450 loss: 2.1586222314834593
  batch 500 loss: 2.185023264884949
  batch 550 loss: 2.1694748520851137
  batch 600 loss: 2.1394334483146666
  batch 650 loss: 2.1493344140052795
  batch 700 loss: 2.075063750743866
  batch 750 loss: 2.06737291097641
  batch 800 loss: 2.0354263019561767
  batch 850 loss: 2.049071056842804
  batch 900 loss: 2.0537453961372374
LOSS train 2.05375 valid 3.45841, valid PER 63.28%
EPOCH 3, Learning Rate: 0.5
  batch 50 loss: 1.9998453760147095
  batch 100 loss: 1.9824700951576233
  batch 150 loss: 1.9717352843284608
  batch 200 loss: 1.941921467781067
  batch 250 loss: 1.917290906906128
  batch 300 loss: 1.9228712725639343
  batch 350 loss: 1.9501083827018737
  batch 400 loss: 1.9132405352592468
  batch 450 loss: 1.909336290359497
  batch 500 loss: 1.881335792541504
  batch 550 loss: 1.8927848482131957
  batch 600 loss: 1.8458661341667175
  batch 650 loss: 1.8256036686897277
  batch 700 loss: 1.8233317065238952
  batch 750 loss: 1.8875914907455444
  batch 800 loss: 1.8184174585342407
  batch 850 loss: 1.8571962356567382
  batch 900 loss: 1.8014468383789062
LOSS train 1.80145 valid 3.53446, valid PER 54.22%
EPOCH 4, Learning Rate: 0.5
  batch 50 loss: 1.7876788306236266
  batch 100 loss: 1.798129642009735
  batch 150 loss: 1.773073616027832
  batch 200 loss: 1.7808052206039429
  batch 250 loss: 1.7760347676277162
  batch 300 loss: 1.7913671398162843
  batch 350 loss: 1.7164200162887573
  batch 400 loss: 1.73693674325943
  batch 450 loss: 1.738246202468872
  batch 500 loss: 1.7200590395927429
  batch 550 loss: 1.756055886745453
  batch 600 loss: 1.725766932964325
  batch 650 loss: 1.7147392201423646
  batch 700 loss: 1.6946197509765626
  batch 750 loss: 1.683362865447998
  batch 800 loss: 1.6527667689323424
  batch 850 loss: 1.701598298549652
  batch 900 loss: 1.7025744485855103
LOSS train 1.70257 valid 3.49635, valid PER 47.83%
EPOCH 5, Learning Rate: 0.5
  batch 50 loss: 1.680257043838501
  batch 100 loss: 1.6569928455352783
  batch 150 loss: 1.6840009021759033
  batch 200 loss: 1.6107691526412964
  batch 250 loss: 1.632260479927063
  batch 300 loss: 1.6349424481391908
  batch 350 loss: 1.6395256185531617
  batch 400 loss: 1.6071227145195008
  batch 450 loss: 1.6377635216712951
  batch 500 loss: 1.6358332180976867
  batch 550 loss: 1.5591846561431886
  batch 600 loss: 1.6530134224891662
  batch 650 loss: 1.602351040840149
  batch 700 loss: 1.6340616941452026
  batch 750 loss: 1.586321439743042
  batch 800 loss: 1.6002129912376404
  batch 850 loss: 1.6189024949073791
  batch 900 loss: 1.601444137096405
LOSS train 1.60144 valid 3.47837, valid PER 45.75%
EPOCH 6, Learning Rate: 0.5
  batch 50 loss: 1.598685119152069
  batch 100 loss: 1.540614070892334
  batch 150 loss: 1.5579033184051514
  batch 200 loss: 1.5316087889671326
  batch 250 loss: 1.581182494163513
  batch 300 loss: 1.5355435061454772
  batch 350 loss: 1.5537498021125793
  batch 400 loss: 1.5424544954299926
  batch 450 loss: 1.566371772289276
  batch 500 loss: 1.5258530855178833
  batch 550 loss: 1.5744161868095399
  batch 600 loss: 1.52210275888443
  batch 650 loss: 1.5276628422737122
  batch 700 loss: 1.5090892720222473
  batch 750 loss: 1.485893132686615
  batch 800 loss: 1.512615804672241
  batch 850 loss: 1.5065958786010742
  batch 900 loss: 1.5260721826553345
LOSS train 1.52607 valid 3.49515, valid PER 41.92%
EPOCH 7, Learning Rate: 0.5
  batch 50 loss: 1.4970952701568603
  batch 100 loss: 1.508658242225647
  batch 150 loss: 1.491585087776184
  batch 200 loss: 1.4893604731559753
  batch 250 loss: 1.4827844262123109
  batch 300 loss: 1.4778542709350586
  batch 350 loss: 1.468957872390747
  batch 400 loss: 1.4712209177017213
  batch 450 loss: 1.4858359026908874
  batch 500 loss: 1.4730330753326415
  batch 550 loss: 1.4683251929283143
  batch 600 loss: 1.4790272521972656
  batch 650 loss: 1.4559747362136841
  batch 700 loss: 1.458999457359314
  batch 750 loss: 1.4549175190925598
  batch 800 loss: 1.438093991279602
  batch 850 loss: 1.4594494771957398
  batch 900 loss: 1.5101117157936097
LOSS train 1.51011 valid 3.49795, valid PER 40.79%
EPOCH 8, Learning Rate: 0.5
  batch 50 loss: 1.4306194305419921
  batch 100 loss: 1.4449547791481019
  batch 150 loss: 1.4374389696121215
  batch 200 loss: 1.3965486454963685
  batch 250 loss: 1.4361918699741363
  batch 300 loss: 1.374903380870819
  batch 350 loss: 1.4561747407913208
  batch 400 loss: 1.4074272632598877
  batch 450 loss: 1.43413982629776
  batch 500 loss: 1.4177961707115174
  batch 550 loss: 1.3705131340026855
  batch 600 loss: 1.44324214220047
  batch 650 loss: 1.4282849383354188
  batch 700 loss: 1.3809511017799379
  batch 750 loss: 1.4142118620872497
  batch 800 loss: 1.4193830633163451
  batch 850 loss: 1.4103573060035706
  batch 900 loss: 1.400969545841217
LOSS train 1.40097 valid 3.38741, valid PER 38.01%
EPOCH 9, Learning Rate: 0.5
  batch 50 loss: 1.3410812830924987
  batch 100 loss: 1.395948314666748
  batch 150 loss: 1.4115867304801941
  batch 200 loss: 1.3437013840675354
  batch 250 loss: 1.3883760046958924
  batch 300 loss: 1.391973180770874
  batch 350 loss: 1.409862208366394
  batch 400 loss: 1.3639263963699342
  batch 450 loss: 1.3776878690719605
  batch 500 loss: 1.3595141792297363
  batch 550 loss: 1.390188705921173
  batch 600 loss: 1.3967109942436218
  batch 650 loss: 1.342848038673401
  batch 700 loss: 1.3406240844726562
  batch 750 loss: 1.3635720705986023
  batch 800 loss: 1.3553162264823913
  batch 850 loss: 1.3836805653572082
  batch 900 loss: 1.3550105619430541
LOSS train 1.35501 valid 3.42552, valid PER 36.59%
EPOCH 10, Learning Rate: 0.5
  batch 50 loss: 1.303136212825775
  batch 100 loss: 1.3447666299343108
  batch 150 loss: 1.3443385815620423
  batch 200 loss: 1.3512983512878418
  batch 250 loss: 1.349405267238617
  batch 300 loss: 1.3122665548324586
  batch 350 loss: 1.3390828943252564
  batch 400 loss: 1.312901428937912
  batch 450 loss: 1.3032546138763428
  batch 500 loss: 1.3452744030952453
  batch 550 loss: 1.3488780736923218
  batch 600 loss: 1.350418484210968
  batch 650 loss: 1.303960781097412
  batch 700 loss: 1.3162065970897674
  batch 750 loss: 1.3051648926734924
  batch 800 loss: 1.317288589477539
  batch 850 loss: 1.3382429528236388
  batch 900 loss: 1.3453799271583557
LOSS train 1.34538 valid 3.54601, valid PER 37.19%
EPOCH 11, Learning Rate: 0.5
  batch 50 loss: 1.2856972408294678
  batch 100 loss: 1.2903463649749756
  batch 150 loss: 1.2749880385398864
  batch 200 loss: 1.314243849515915
  batch 250 loss: 1.302897688150406
  batch 300 loss: 1.272288409471512
  batch 350 loss: 1.294566617012024
  batch 400 loss: 1.3258726584911347
  batch 450 loss: 1.3102307653427123
  batch 500 loss: 1.2748406195640565
  batch 550 loss: 1.308668212890625
  batch 600 loss: 1.2639535403251647
  batch 650 loss: 1.33831636428833
  batch 700 loss: 1.2348661875724793
  batch 750 loss: 1.257891275882721
  batch 800 loss: 1.320196306705475
  batch 850 loss: 1.3101472926139832
  batch 900 loss: 1.330047504901886
LOSS train 1.33005 valid 3.45940, valid PER 35.90%
EPOCH 12, Learning Rate: 0.5
  batch 50 loss: 1.284726731777191
  batch 100 loss: 1.2544985115528107
  batch 150 loss: 1.2278996574878693
  batch 200 loss: 1.263735008239746
  batch 250 loss: 1.2830755698680878
  batch 300 loss: 1.257048350572586
  batch 350 loss: 1.2605589413642884
  batch 400 loss: 1.2593690478801727
  batch 450 loss: 1.2912584853172302
  batch 500 loss: 1.2913184142112732
  batch 550 loss: 1.1975511944293975
  batch 600 loss: 1.2235783350467682
  batch 650 loss: 1.2959298980236054
  batch 700 loss: 1.2775756192207337
  batch 750 loss: 1.2330721008777619
  batch 800 loss: 1.236196882724762
  batch 850 loss: 1.2844061589241027
  batch 900 loss: 1.3020127272605897
LOSS train 1.30201 valid 3.56917, valid PER 35.45%
EPOCH 13, Learning Rate: 0.5
  batch 50 loss: 1.200753573179245
  batch 100 loss: 1.2437919282913208
  batch 150 loss: 1.2036192154884338
  batch 200 loss: 1.2490036129951476
  batch 250 loss: 1.2446264922618866
  batch 300 loss: 1.2228626143932342
  batch 350 loss: 1.2276680743694306
  batch 400 loss: 1.2484004819393157
  batch 450 loss: 1.2424518537521363
  batch 500 loss: 1.2347694909572602
  batch 550 loss: 1.2449794280529023
  batch 600 loss: 1.2074816727638245
  batch 650 loss: 1.2565686702728271
  batch 700 loss: 1.252669197320938
  batch 750 loss: 1.2075945901870728
  batch 800 loss: 1.2266856360435485
  batch 850 loss: 1.2701438057422638
  batch 900 loss: 1.2444835853576661
LOSS train 1.24448 valid 3.56307, valid PER 34.98%
EPOCH 14, Learning Rate: 0.5
  batch 50 loss: 1.1887628722190857
  batch 100 loss: 1.20240415930748
  batch 150 loss: 1.1973813772201538
  batch 200 loss: 1.2025189995765686
  batch 250 loss: 1.2044691622257233
  batch 300 loss: 1.251029896736145
  batch 350 loss: 1.182951147556305
  batch 400 loss: 1.1887144696712495
  batch 450 loss: 1.1849708139896393
  batch 500 loss: 1.2200918412208557
  batch 550 loss: 1.231567780971527
  batch 600 loss: 1.2088966751098633
  batch 650 loss: 1.222938095331192
  batch 700 loss: 1.2368187999725342
  batch 750 loss: 1.1971159625053405
  batch 800 loss: 1.1844581699371337
  batch 850 loss: 1.2234102869033814
  batch 900 loss: 1.2220721971988677
LOSS train 1.22207 valid 3.59551, valid PER 34.01%
EPOCH 15, Learning Rate: 0.5
  batch 50 loss: 1.1967631387710571
  batch 100 loss: 1.1694082081317902
  batch 150 loss: 1.1558494555950165
  batch 200 loss: 1.205096061229706
  batch 250 loss: 1.2262530493736268
  batch 300 loss: 1.1719426357746123
  batch 350 loss: 1.179083789587021
  batch 400 loss: 1.1712976002693176
  batch 450 loss: 1.1796823954582214
  batch 500 loss: 1.1608814334869384
  batch 550 loss: 1.1972953045368195
  batch 600 loss: 1.206086859703064
  batch 650 loss: 1.2091978514194488
  batch 700 loss: 1.205034258365631
  batch 750 loss: 1.1856802570819855
  batch 800 loss: 1.170563154220581
  batch 850 loss: 1.1442336165904998
  batch 900 loss: 1.217933804988861
LOSS train 1.21793 valid 3.52523, valid PER 32.54%
EPOCH 16, Learning Rate: 0.5
  batch 50 loss: 1.1694122493267058
  batch 100 loss: 1.133269921541214
  batch 150 loss: 1.16723659992218
  batch 200 loss: 1.1707457447052
  batch 250 loss: 1.197968257665634
  batch 300 loss: 1.1740725028514862
  batch 350 loss: 1.2011843359470367
  batch 400 loss: 1.1834683763980864
  batch 450 loss: 1.1809695553779602
  batch 500 loss: 1.1377314639091491
  batch 550 loss: 1.1532808148860931
  batch 600 loss: 1.1591104614734649
  batch 650 loss: 1.1442800569534302
  batch 700 loss: 1.1541662883758546
  batch 750 loss: 1.172361581325531
  batch 800 loss: 1.1737677919864655
  batch 850 loss: 1.1774370896816253
  batch 900 loss: 1.166136064529419
LOSS train 1.16614 valid 3.57197, valid PER 32.72%
EPOCH 17, Learning Rate: 0.5
  batch 50 loss: 1.1334439992904664
  batch 100 loss: 1.1506043589115142
  batch 150 loss: 1.114542521238327
  batch 200 loss: 1.1298067307472228
  batch 250 loss: 1.161502879858017
  batch 300 loss: 1.1463111114501954
  batch 350 loss: 1.130569794178009
  batch 400 loss: 1.1872057569026948
  batch 450 loss: 1.1747826206684113
  batch 500 loss: 1.1254697334766388
  batch 550 loss: 1.1266334915161134
  batch 600 loss: 1.172477205991745
  batch 650 loss: 1.1233730006217957
  batch 700 loss: 1.1347754859924317
  batch 750 loss: 1.1327573466300964
  batch 800 loss: 1.1340115547180176
  batch 850 loss: 1.1412111449241638
  batch 900 loss: 1.143702919483185
LOSS train 1.14370 valid 3.65671, valid PER 32.38%
EPOCH 18, Learning Rate: 0.5
  batch 50 loss: 1.102473566532135
  batch 100 loss: 1.1195537757873535
  batch 150 loss: 1.1395581078529358
  batch 200 loss: 1.1401473844051362
  batch 250 loss: 1.1446676671504974
  batch 300 loss: 1.1167377173900603
  batch 350 loss: 1.1157656371593476
  batch 400 loss: 1.1026824152469634
  batch 450 loss: 1.1558816874027251
  batch 500 loss: 1.1469943010807038
  batch 550 loss: 1.129668436050415
  batch 600 loss: 1.1171978211402893
  batch 650 loss: 1.0950914692878724
  batch 700 loss: 1.1354988074302674
  batch 750 loss: 1.1402839028835297
  batch 800 loss: 1.1223888885974884
  batch 850 loss: 1.1131082105636596
  batch 900 loss: 1.1624028551578522
LOSS train 1.16240 valid 3.62790, valid PER 32.77%
EPOCH 19, Learning Rate: 0.5
  batch 50 loss: 1.0529723644256592
  batch 100 loss: 1.0890436351299286
  batch 150 loss: 1.121827208995819
  batch 200 loss: 1.1198149704933167
  batch 250 loss: 1.1121046566963195
  batch 300 loss: 1.1225011014938355
  batch 350 loss: 1.1101511633396148
  batch 400 loss: 1.0998815858364106
  batch 450 loss: 1.1352431333065034
  batch 500 loss: 1.1032650971412659
  batch 550 loss: 1.0745198237895965
  batch 600 loss: 1.1159349954128266
  batch 650 loss: 1.157413400411606
  batch 700 loss: 1.1040513801574707
  batch 750 loss: 1.0924522733688355
  batch 800 loss: 1.1356536841392517
  batch 850 loss: 1.13467254281044
  batch 900 loss: 1.1226818263530731
LOSS train 1.12268 valid 3.66155, valid PER 32.03%
EPOCH 20, Learning Rate: 0.5
  batch 50 loss: 1.0833514404296876
  batch 100 loss: 1.087813264131546
  batch 150 loss: 1.0732375729084014
  batch 200 loss: 1.0810592210292815
  batch 250 loss: 1.0873997223377228
  batch 300 loss: 1.1010258042812346
  batch 350 loss: 1.0326021885871888
  batch 400 loss: 1.0875306737422943
  batch 450 loss: 1.0889376938343047
  batch 500 loss: 1.0762312257289885
  batch 550 loss: 1.1549807751178742
  batch 600 loss: 1.0497927510738372
  batch 650 loss: 1.1135828185081482
  batch 700 loss: 1.1165178942680358
  batch 750 loss: 1.0796983182430266
  batch 800 loss: 1.1457832098007201
  batch 850 loss: 1.0903806173801422
  batch 900 loss: 1.1108986926078797
LOSS train 1.11090 valid 3.67614, valid PER 30.95%
Training finished in 5.0 minutes.
Model saved to checkpoints/20231209_164653/model_8
Loading model from checkpoints/20231209_164653/model_8
SUB: 16.09%, DEL: 22.27%, INS: 1.14%, COR: 61.64%, PER: 39.50%
