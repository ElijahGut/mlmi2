Namespace(seed=123, train_json='train_fbank61.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_61.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=0.5)
Total number of model parameters is 172863
EPOCH 1, Learning Rate: 0.5
  batch 50 loss: 7.267058281898499
  batch 100 loss: 3.8941318082809446
  batch 150 loss: 3.8243034410476686
  batch 200 loss: 3.7864382219314576
  batch 250 loss: 3.7179250383377074
  batch 300 loss: 3.641204347610474
  batch 350 loss: 3.5054701948165894
  batch 400 loss: 3.4001109647750853
  batch 450 loss: 3.2578159236907958
  batch 500 loss: 3.1232211112976076
  batch 550 loss: 3.008943448066711
  batch 600 loss: 2.893596749305725
  batch 650 loss: 2.7774438190460207
  batch 700 loss: 2.7148136615753176
  batch 750 loss: 2.6712911796569823
  batch 800 loss: 2.6057888317108153
  batch 850 loss: 2.5507985830307005
  batch 900 loss: 2.52134042263031
LOSS train 2.52134 valid 3.45770, valid PER 89.44%
EPOCH 2, Learning Rate: 0.5
  batch 50 loss: 2.445112156867981
  batch 100 loss: 2.369609484672546
  batch 150 loss: 2.343422546386719
  batch 200 loss: 2.320570387840271
  batch 250 loss: 2.312268877029419
  batch 300 loss: 2.2733265924453736
  batch 350 loss: 2.2157123136520385
  batch 400 loss: 2.1895569491386415
  batch 450 loss: 2.157751672267914
  batch 500 loss: 2.1852708292007446
  batch 550 loss: 2.168282058238983
  batch 600 loss: 2.138689522743225
  batch 650 loss: 2.1510852932929994
  batch 700 loss: 2.075070605278015
  batch 750 loss: 2.0666552877426145
  batch 800 loss: 2.0306745076179507
  batch 850 loss: 2.049519383907318
  batch 900 loss: 2.051963975429535
LOSS train 2.05196 valid 3.44793, valid PER 76.02%
EPOCH 3, Learning Rate: 0.5
  batch 50 loss: 2.0009002256393433
  batch 100 loss: 1.984168703556061
  batch 150 loss: 1.9713694095611571
  batch 200 loss: 1.942145230770111
  batch 250 loss: 1.9183450388908385
  batch 300 loss: 1.925667097568512
  batch 350 loss: 1.949424252510071
  batch 400 loss: 1.9146420669555664
  batch 450 loss: 1.9107800579071046
  batch 500 loss: 1.8811717200279237
  batch 550 loss: 1.895996367931366
  batch 600 loss: 1.8479216074943543
  batch 650 loss: 1.827107288837433
  batch 700 loss: 1.823246042728424
  batch 750 loss: 1.8873621106147767
  batch 800 loss: 1.817087082862854
  batch 850 loss: 1.8574904108047485
  batch 900 loss: 1.8045673012733459
LOSS train 1.80457 valid 3.54187, valid PER 69.78%
EPOCH 4, Learning Rate: 0.5
  batch 50 loss: 1.789054412841797
  batch 100 loss: 1.7992782497406006
  batch 150 loss: 1.7731956577301025
  batch 200 loss: 1.7845114707946776
  batch 250 loss: 1.7773780465126037
  batch 300 loss: 1.7903830194473267
  batch 350 loss: 1.7191087055206298
  batch 400 loss: 1.7407771801948548
  batch 450 loss: 1.7392134284973144
  batch 500 loss: 1.7191712498664855
  batch 550 loss: 1.7532203030586242
  batch 600 loss: 1.7283866786956787
  batch 650 loss: 1.7141909670829774
  batch 700 loss: 1.6959428644180299
  batch 750 loss: 1.6851822233200073
  batch 800 loss: 1.6561950182914733
  batch 850 loss: 1.7027813506126404
  batch 900 loss: 1.6980901789665221
LOSS train 1.69809 valid 3.54142, valid PER 65.20%
EPOCH 5, Learning Rate: 0.5
  batch 50 loss: 1.6821241807937621
  batch 100 loss: 1.660762836933136
  batch 150 loss: 1.68631094455719
  batch 200 loss: 1.6116332602500916
  batch 250 loss: 1.634444236755371
  batch 300 loss: 1.633057255744934
  batch 350 loss: 1.648248963356018
  batch 400 loss: 1.6074501538276673
  batch 450 loss: 1.6398834586143494
  batch 500 loss: 1.6400813722610474
  batch 550 loss: 1.563064706325531
  batch 600 loss: 1.6538137078285218
  batch 650 loss: 1.6057951903343202
  batch 700 loss: 1.638121109008789
  batch 750 loss: 1.5901289296150207
  batch 800 loss: 1.5985958886146545
  batch 850 loss: 1.62102130651474
  batch 900 loss: 1.6015662932395935
LOSS train 1.60157 valid 3.47822, valid PER 62.92%
EPOCH 6, Learning Rate: 0.5
  batch 50 loss: 1.5981228804588319
  batch 100 loss: 1.5409274315834045
  batch 150 loss: 1.5638743019104004
  batch 200 loss: 1.5327850198745727
  batch 250 loss: 1.5849384617805482
  batch 300 loss: 1.5335906076431274
  batch 350 loss: 1.5538431739807128
  batch 400 loss: 1.5429932403564453
  batch 450 loss: 1.5688745427131652
  batch 500 loss: 1.5258379435539247
  batch 550 loss: 1.572399423122406
  batch 600 loss: 1.526958384513855
  batch 650 loss: 1.5269967555999755
  batch 700 loss: 1.510311005115509
  batch 750 loss: 1.4884923219680786
  batch 800 loss: 1.513438470363617
  batch 850 loss: 1.510290446281433
  batch 900 loss: 1.5319606637954712
LOSS train 1.53196 valid 3.53612, valid PER 60.64%
EPOCH 7, Learning Rate: 0.5
  batch 50 loss: 1.4974614453315735
  batch 100 loss: 1.5151710200309754
  batch 150 loss: 1.494675223827362
  batch 200 loss: 1.4926415991783142
  batch 250 loss: 1.4848541903495789
  batch 300 loss: 1.4807190108299255
  batch 350 loss: 1.468294985294342
  batch 400 loss: 1.472056076526642
  batch 450 loss: 1.4872463583946227
  batch 500 loss: 1.4769878101348877
  batch 550 loss: 1.4705902886390687
  batch 600 loss: 1.4768914914131164
  batch 650 loss: 1.4541604447364807
  batch 700 loss: 1.4585815358161927
  batch 750 loss: 1.4576329064369202
  batch 800 loss: 1.4402613401412965
  batch 850 loss: 1.4606077361106873
  batch 900 loss: 1.5148964715003967
LOSS train 1.51490 valid 3.50667, valid PER 59.32%
EPOCH 8, Learning Rate: 0.5
  batch 50 loss: 1.432506549358368
  batch 100 loss: 1.4466772603988647
  batch 150 loss: 1.4402504205703734
  batch 200 loss: 1.3968993258476257
  batch 250 loss: 1.4357787036895753
  batch 300 loss: 1.37816330909729
  batch 350 loss: 1.4593368768692017
  batch 400 loss: 1.4083941054344178
  batch 450 loss: 1.434611291885376
  batch 500 loss: 1.4186179542541504
  batch 550 loss: 1.3728834390640259
  batch 600 loss: 1.4421732783317567
  batch 650 loss: 1.434308145046234
  batch 700 loss: 1.3829538106918335
  batch 750 loss: 1.4114209771156312
  batch 800 loss: 1.4175157260894775
  batch 850 loss: 1.409118231534958
  batch 900 loss: 1.3986656641960145
LOSS train 1.39867 valid 3.49787, valid PER 57.49%
EPOCH 9, Learning Rate: 0.5
  batch 50 loss: 1.3391883158683777
  batch 100 loss: 1.3959119749069213
  batch 150 loss: 1.411928391456604
  batch 200 loss: 1.3462279796600343
  batch 250 loss: 1.3880706238746643
  batch 300 loss: 1.3942947125434875
  batch 350 loss: 1.4139674735069274
  batch 400 loss: 1.3640765166282653
  batch 450 loss: 1.3826226472854615
  batch 500 loss: 1.3581098008155823
  batch 550 loss: 1.3947705245018005
  batch 600 loss: 1.3940625023841857
  batch 650 loss: 1.3440758621692657
  batch 700 loss: 1.3421683275699616
  batch 750 loss: 1.360989384651184
  batch 800 loss: 1.3548787665367126
  batch 850 loss: 1.3831678986549378
  batch 900 loss: 1.3541248965263366
LOSS train 1.35412 valid 3.45499, valid PER 56.41%
EPOCH 10, Learning Rate: 0.5
  batch 50 loss: 1.3074017190933227
  batch 100 loss: 1.3463340020179748
  batch 150 loss: 1.3466762042045592
  batch 200 loss: 1.348332278728485
  batch 250 loss: 1.3490592527389527
  batch 300 loss: 1.3117373037338256
  batch 350 loss: 1.3437306761741639
  batch 400 loss: 1.3110174608230591
  batch 450 loss: 1.3003655779361725
  batch 500 loss: 1.3500992512702943
  batch 550 loss: 1.351386580467224
  batch 600 loss: 1.3450292503833772
  batch 650 loss: 1.301536146402359
  batch 700 loss: 1.317083351612091
  batch 750 loss: 1.3074777591228486
  batch 800 loss: 1.3240072321891785
  batch 850 loss: 1.340857093334198
  batch 900 loss: 1.354978096485138
LOSS train 1.35498 valid 3.61542, valid PER 57.86%
EPOCH 11, Learning Rate: 0.5
  batch 50 loss: 1.287183552980423
  batch 100 loss: 1.2891823709011079
  batch 150 loss: 1.2790647268295288
  batch 200 loss: 1.3138242530822755
  batch 250 loss: 1.3024448919296265
  batch 300 loss: 1.2719599735736846
  batch 350 loss: 1.2998535442352295
  batch 400 loss: 1.3298224139213561
  batch 450 loss: 1.3092873454093934
  batch 500 loss: 1.2791013371944429
  batch 550 loss: 1.3056502723693848
  batch 600 loss: 1.264913308620453
  batch 650 loss: 1.3378279733657836
  batch 700 loss: 1.2381728780269623
  batch 750 loss: 1.2615451097488404
  batch 800 loss: 1.3251289272308349
  batch 850 loss: 1.3108168315887452
  batch 900 loss: 1.32816082239151
LOSS train 1.32816 valid 3.46837, valid PER 55.72%
EPOCH 12, Learning Rate: 0.5
  batch 50 loss: 1.2862098467350007
  batch 100 loss: 1.2544666564464568
  batch 150 loss: 1.229401935338974
  batch 200 loss: 1.2640755653381348
  batch 250 loss: 1.2841312766075135
  batch 300 loss: 1.2632594084739686
  batch 350 loss: 1.2596527409553528
  batch 400 loss: 1.264697207212448
  batch 450 loss: 1.2904824638366699
  batch 500 loss: 1.2886195254325867
  batch 550 loss: 1.1996771323680877
  batch 600 loss: 1.2158195281028747
  batch 650 loss: 1.2992858612537384
  batch 700 loss: 1.2783882558345794
  batch 750 loss: 1.2344614744186402
  batch 800 loss: 1.2400423872470856
  batch 850 loss: 1.2918314242362976
  batch 900 loss: 1.3002692151069641
LOSS train 1.30027 valid 3.58753, valid PER 56.20%
EPOCH 13, Learning Rate: 0.5
  batch 50 loss: 1.2063877832889558
  batch 100 loss: 1.2400716924667359
  batch 150 loss: 1.1992609810829162
  batch 200 loss: 1.2535850322246551
  batch 250 loss: 1.2366340565681457
  batch 300 loss: 1.22551682472229
  batch 350 loss: 1.2292418456077576
  batch 400 loss: 1.2515046417713165
  batch 450 loss: 1.239411199092865
  batch 500 loss: 1.2317239463329315
  batch 550 loss: 1.2486376476287842
  batch 600 loss: 1.2027807319164276
  batch 650 loss: 1.2511868095397949
  batch 700 loss: 1.251472430229187
  batch 750 loss: 1.209396265745163
  batch 800 loss: 1.224201685190201
  batch 850 loss: 1.2710624969005584
  batch 900 loss: 1.2423453664779662
LOSS train 1.24235 valid 3.57885, valid PER 55.99%
EPOCH 14, Learning Rate: 0.5
  batch 50 loss: 1.1901657104492187
  batch 100 loss: 1.2067710530757905
  batch 150 loss: 1.1954963552951812
  batch 200 loss: 1.2021868228912354
  batch 250 loss: 1.2037716794013977
  batch 300 loss: 1.25399498462677
  batch 350 loss: 1.184989913702011
  batch 400 loss: 1.1922036457061767
  batch 450 loss: 1.1852825045585633
  batch 500 loss: 1.225282872915268
  batch 550 loss: 1.2261411786079406
  batch 600 loss: 1.2090207517147065
  batch 650 loss: 1.2223657369613647
  batch 700 loss: 1.243952808380127
  batch 750 loss: 1.1973551142215728
  batch 800 loss: 1.1922539126873017
  batch 850 loss: 1.2283327078819275
  batch 900 loss: 1.217724906206131
LOSS train 1.21772 valid 3.54921, valid PER 54.21%
EPOCH 15, Learning Rate: 0.5
  batch 50 loss: 1.198025132417679
  batch 100 loss: 1.1655012166500092
  batch 150 loss: 1.1597223365306855
  batch 200 loss: 1.2085605561733246
  batch 250 loss: 1.2151493656635284
  batch 300 loss: 1.175045943260193
  batch 350 loss: 1.1838120901584626
  batch 400 loss: 1.1651828539371492
  batch 450 loss: 1.171404137611389
  batch 500 loss: 1.1622819125652313
  batch 550 loss: 1.1947039532661439
  batch 600 loss: 1.21191850066185
  batch 650 loss: 1.2086870396137237
  batch 700 loss: 1.2046284663677216
  batch 750 loss: 1.1827659916877746
  batch 800 loss: 1.1725837302207947
  batch 850 loss: 1.145347045660019
  batch 900 loss: 1.219633741378784
LOSS train 1.21963 valid 3.59074, valid PER 54.05%
EPOCH 16, Learning Rate: 0.5
  batch 50 loss: 1.175053483247757
  batch 100 loss: 1.1229580867290496
  batch 150 loss: 1.1705234944820404
  batch 200 loss: 1.1618879520893097
  batch 250 loss: 1.2040061461925506
  batch 300 loss: 1.1741547429561614
  batch 350 loss: 1.199042397737503
  batch 400 loss: 1.1784734654426574
  batch 450 loss: 1.1780442714691162
  batch 500 loss: 1.139971798658371
  batch 550 loss: 1.1477933609485627
  batch 600 loss: 1.162938379049301
  batch 650 loss: 1.1455712378025056
  batch 700 loss: 1.1538304018974304
  batch 750 loss: 1.1728357541561127
  batch 800 loss: 1.1718941712379456
  batch 850 loss: 1.1824586749076844
  batch 900 loss: 1.1716916429996491
LOSS train 1.17169 valid 3.55982, valid PER 53.38%
EPOCH 17, Learning Rate: 0.5
  batch 50 loss: 1.138296014070511
  batch 100 loss: 1.1462364709377288
  batch 150 loss: 1.108466249704361
  batch 200 loss: 1.1341135895252228
  batch 250 loss: 1.1551740622520448
  batch 300 loss: 1.1441685080528259
  batch 350 loss: 1.134958267211914
  batch 400 loss: 1.1862045788764954
  batch 450 loss: 1.1753257524967193
  batch 500 loss: 1.123525778055191
  batch 550 loss: 1.131618629693985
  batch 600 loss: 1.174842998981476
  batch 650 loss: 1.1253145575523376
  batch 700 loss: 1.1378592360019684
  batch 750 loss: 1.1300450527668
  batch 800 loss: 1.1287336456775665
  batch 850 loss: 1.1548355793952942
  batch 900 loss: 1.144168986082077
LOSS train 1.14417 valid 3.60787, valid PER 53.38%
EPOCH 18, Learning Rate: 0.5
  batch 50 loss: 1.1006185603141785
  batch 100 loss: 1.1228968930244445
  batch 150 loss: 1.1378279531002045
  batch 200 loss: 1.1300947594642639
  batch 250 loss: 1.1390630030632019
  batch 300 loss: 1.111622611284256
  batch 350 loss: 1.108552907705307
  batch 400 loss: 1.10053085565567
  batch 450 loss: 1.1596207118034363
  batch 500 loss: 1.1361160957813263
  batch 550 loss: 1.1308009159564971
  batch 600 loss: 1.1157823097705841
  batch 650 loss: 1.101568077802658
  batch 700 loss: 1.1401776790618896
  batch 750 loss: 1.1445845425128938
  batch 800 loss: 1.1185334146022796
  batch 850 loss: 1.116143034696579
  batch 900 loss: 1.1646331000328063
LOSS train 1.16463 valid 3.62549, valid PER 53.61%
EPOCH 19, Learning Rate: 0.5
  batch 50 loss: 1.0570697736740113
  batch 100 loss: 1.0876750373840331
  batch 150 loss: 1.1164500963687898
  batch 200 loss: 1.1114127326011658
  batch 250 loss: 1.1077508008480073
  batch 300 loss: 1.125327079296112
  batch 350 loss: 1.1093687164783477
  batch 400 loss: 1.1055020678043366
  batch 450 loss: 1.1377240252494811
  batch 500 loss: 1.1116335153579713
  batch 550 loss: 1.0732082176208495
  batch 600 loss: 1.12192480802536
  batch 650 loss: 1.1560184919834138
  batch 700 loss: 1.0911043250560761
  batch 750 loss: 1.0884824550151826
  batch 800 loss: 1.139563773870468
  batch 850 loss: 1.1358732235431672
  batch 900 loss: 1.1231227779388429
LOSS train 1.12312 valid 3.66875, valid PER 53.44%
EPOCH 20, Learning Rate: 0.5
  batch 50 loss: 1.081626513004303
  batch 100 loss: 1.091157966852188
  batch 150 loss: 1.0693671786785126
  batch 200 loss: 1.088589676618576
  batch 250 loss: 1.0754586493968963
  batch 300 loss: 1.097428513765335
  batch 350 loss: 1.0309148240089416
  batch 400 loss: 1.091827849149704
  batch 450 loss: 1.08366423368454
  batch 500 loss: 1.0788055312633515
  batch 550 loss: 1.159443612098694
  batch 600 loss: 1.0478065383434296
  batch 650 loss: 1.1179821729660033
  batch 700 loss: 1.121972186565399
  batch 750 loss: 1.0824271762371063
  batch 800 loss: 1.1432177138328552
  batch 850 loss: 1.087632761001587
  batch 900 loss: 1.1078766417503356
LOSS train 1.10788 valid 3.64318, valid PER 53.80%
Training finished in 4.0 minutes.
Model saved to checkpoints/20231209_131342/model_2
Loading model from checkpoints/20231209_131342/model_2
SUB: 24.40%, DEL: 52.73%, INS: 0.19%, COR: 22.87%, PER: 77.33%
