Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.4, optimiser='sgd')
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.162575092315674
  batch 100 loss: 3.1735270404815674
  batch 150 loss: 3.0330257654190063
  batch 200 loss: 2.928381152153015
  batch 250 loss: 2.857780032157898
  batch 300 loss: 2.7497922229766845
  batch 350 loss: 2.608125
  batch 400 loss: 2.4635716247558594
  batch 450 loss: 2.360832271575928
  batch 500 loss: 2.247537407875061
  batch 550 loss: 2.185529532432556
  batch 600 loss: 2.1429852271080017
  batch 650 loss: 2.039723024368286
  batch 700 loss: 2.05781476020813
  batch 750 loss: 1.9901768398284911
  batch 800 loss: 1.9700162458419799
  batch 850 loss: 1.911291892528534
  batch 900 loss: 1.9071009850502014
LOSS train 1.90710 valid 1.87132, valid PER 72.17%
EPOCH 2:
  batch 50 loss: 1.8602898192405701
  batch 100 loss: 1.8149356961250305
  batch 150 loss: 1.781048674583435
  batch 200 loss: 1.774974262714386
  batch 250 loss: 1.794163920879364
  batch 300 loss: 1.7451192450523376
  batch 350 loss: 1.6620176863670348
  batch 400 loss: 1.688331789970398
  batch 450 loss: 1.641235466003418
  batch 500 loss: 1.6586627340316773
  batch 550 loss: 1.6772222113609314
  batch 600 loss: 1.6124331498146056
  batch 650 loss: 1.6482476806640625
  batch 700 loss: 1.6350274395942688
  batch 750 loss: 1.6130564379692078
  batch 800 loss: 1.5632581400871277
  batch 850 loss: 1.556030855178833
  batch 900 loss: 1.5669466948509216
LOSS train 1.56695 valid 1.44844, valid PER 52.58%
EPOCH 3:
  batch 50 loss: 1.5276005172729492
  batch 100 loss: 1.5060677695274354
  batch 150 loss: 1.5045554065704345
  batch 200 loss: 1.4843430566787719
  batch 250 loss: 1.4736529064178467
  batch 300 loss: 1.4746090412139892
  batch 350 loss: 1.5100979042053222
  batch 400 loss: 1.484165177345276
  batch 450 loss: 1.4475218224525452
  batch 500 loss: 1.4230646300315857
  batch 550 loss: 1.4472760915756226
  batch 600 loss: 1.4408890867233277
  batch 650 loss: 1.3823843836784362
  batch 700 loss: 1.4631700563430785
  batch 750 loss: 1.48110089302063
  batch 800 loss: 1.383381152153015
  batch 850 loss: 1.4438547682762146
  batch 900 loss: 1.347663164138794
LOSS train 1.34766 valid 1.32950, valid PER 43.11%
EPOCH 4:
  batch 50 loss: 1.3650592827796937
  batch 100 loss: 1.3965528106689453
  batch 150 loss: 1.3215743064880372
  batch 200 loss: 1.3438807678222657
  batch 250 loss: 1.3554321479797364
  batch 300 loss: 1.354274915456772
  batch 350 loss: 1.2841675758361817
  batch 400 loss: 1.3345606815814972
  batch 450 loss: 1.3355140328407287
  batch 500 loss: 1.32330757856369
  batch 550 loss: 1.3303641366958618
  batch 600 loss: 1.355324285030365
  batch 650 loss: 1.3481439769268035
  batch 700 loss: 1.3094257497787476
  batch 750 loss: 1.3022142267227172
  batch 800 loss: 1.2410666036605835
  batch 850 loss: 1.2963537001609802
  batch 900 loss: 1.3447166311740875
LOSS train 1.34472 valid 1.21715, valid PER 39.61%
EPOCH 5:
  batch 50 loss: 1.236560391187668
  batch 100 loss: 1.2513235354423522
  batch 150 loss: 1.3103468585014344
  batch 200 loss: 1.2802422058582306
  batch 250 loss: 1.2581083536148072
  batch 300 loss: 1.280701093673706
  batch 350 loss: 1.2558885169029237
  batch 400 loss: 1.2692161631584167
  batch 450 loss: 1.2558133614063263
  batch 500 loss: 1.273653862476349
  batch 550 loss: 1.2318625855445862
  batch 600 loss: 1.2891810870170592
  batch 650 loss: 1.2433613562583923
  batch 700 loss: 1.2803605544567107
  batch 750 loss: 1.2482968294620513
  batch 800 loss: 1.253742117881775
  batch 850 loss: 1.2683663332462312
  batch 900 loss: 1.2434016525745393
LOSS train 1.24340 valid 1.15236, valid PER 37.61%
EPOCH 6:
  batch 50 loss: 1.2359689998626708
  batch 100 loss: 1.1736839830875396
  batch 150 loss: 1.1672516214847564
  batch 200 loss: 1.1954806315898896
  batch 250 loss: 1.2140036737918853
  batch 300 loss: 1.183006751537323
  batch 350 loss: 1.1913972210884094
  batch 400 loss: 1.1862456095218659
  batch 450 loss: 1.2289105534553528
  batch 500 loss: 1.2216952002048493
  batch 550 loss: 1.2609407198429108
  batch 600 loss: 1.248440726995468
  batch 650 loss: 1.259127175807953
  batch 700 loss: 1.238290296792984
  batch 750 loss: 1.2140573871135711
  batch 800 loss: 1.2117636477947236
  batch 850 loss: 1.1828682243824005
  batch 900 loss: 1.222807184457779
LOSS train 1.22281 valid 1.15691, valid PER 37.44%
EPOCH 7:
  batch 50 loss: 1.1886569154262543
  batch 100 loss: 1.1887904167175294
  batch 150 loss: 1.1822032368183135
  batch 200 loss: 1.1559135222434997
  batch 250 loss: 1.2001909816265106
  batch 300 loss: 1.1650187242031098
  batch 350 loss: 1.141434222459793
  batch 400 loss: 1.1738320624828338
  batch 450 loss: 1.1766193103790283
  batch 500 loss: 1.1497828948497772
  batch 550 loss: 1.164579142332077
  batch 600 loss: 1.158965483903885
  batch 650 loss: 1.1719301700592042
  batch 700 loss: 1.164027658700943
  batch 750 loss: 1.147273359298706
  batch 800 loss: 1.1405844438076018
  batch 850 loss: 1.1885780203342438
  batch 900 loss: 1.2250150012969971
LOSS train 1.22502 valid 1.12203, valid PER 36.93%
EPOCH 8:
  batch 50 loss: 1.1615697467327117
  batch 100 loss: 1.1424470746517181
  batch 150 loss: 1.1384037351608276
  batch 200 loss: 1.1286748111248017
  batch 250 loss: 1.1664918291568755
  batch 300 loss: 1.0959465670585633
  batch 350 loss: 1.1743191754817963
  batch 400 loss: 1.1157941329479217
  batch 450 loss: 1.1375717771053315
  batch 500 loss: 1.1740401756763459
  batch 550 loss: 1.099723653793335
  batch 600 loss: 1.1740378713607789
  batch 650 loss: 1.1765749645233154
  batch 700 loss: 1.1066376793384551
  batch 750 loss: 1.12000506401062
  batch 800 loss: 1.1507915925979615
  batch 850 loss: 1.1146779620647431
  batch 900 loss: 1.1050523793697358
LOSS train 1.10505 valid 1.06235, valid PER 34.80%
EPOCH 9:
  batch 50 loss: 1.0612046933174133
  batch 100 loss: 1.1107526206970215
  batch 150 loss: 1.1252661120891572
  batch 200 loss: 1.065912457704544
  batch 250 loss: 1.1121500945091247
  batch 300 loss: 1.1060798239707947
  batch 350 loss: 1.1108178734779357
  batch 400 loss: 1.0939890658855438
  batch 450 loss: 1.1069206988811493
  batch 500 loss: 1.079537740945816
  batch 550 loss: 1.0989248740673065
  batch 600 loss: 1.1242137897014617
  batch 650 loss: 1.0913177561759948
  batch 700 loss: 1.10235822558403
  batch 750 loss: 1.0996184062957763
  batch 800 loss: 1.1105472469329833
  batch 850 loss: 1.1223870646953582
  batch 900 loss: 1.0887957918643951
LOSS train 1.08880 valid 1.04955, valid PER 34.40%
EPOCH 10:
  batch 50 loss: 1.038481159210205
  batch 100 loss: 1.0579840743541717
  batch 150 loss: 1.0718494355678558
  batch 200 loss: 1.1164600932598114
  batch 250 loss: 1.0946347880363465
  batch 300 loss: 1.0661552214622498
  batch 350 loss: 1.0990425741672516
  batch 400 loss: 1.0788923871517182
  batch 450 loss: 1.05027805685997
  batch 500 loss: 1.106344232559204
  batch 550 loss: 1.118311038017273
  batch 600 loss: 1.0823462080955506
  batch 650 loss: 1.044515471458435
  batch 700 loss: 1.088877546787262
  batch 750 loss: 1.0630656862258911
  batch 800 loss: 1.07395250916481
  batch 850 loss: 1.0971933162212373
  batch 900 loss: 1.2622410094738006
LOSS train 1.26224 valid 1.08263, valid PER 37.30%
EPOCH 11:
  batch 50 loss: 1.081361438035965
  batch 100 loss: 1.0519203901290894
  batch 150 loss: 1.045511543750763
  batch 200 loss: 1.0844329524040222
  batch 250 loss: 1.0745203471183777
  batch 300 loss: 1.0165286290645599
  batch 350 loss: 1.0520936942100525
  batch 400 loss: 1.061595641374588
  batch 450 loss: 1.0774668395519256
  batch 500 loss: 1.1142384696006775
  batch 550 loss: 1.1292821049690247
  batch 600 loss: 1.0937817251682282
  batch 650 loss: 1.1288645720481874
  batch 700 loss: 1.0714005398750306
  batch 750 loss: 1.0857684540748596
  batch 800 loss: 1.1667679893970488
  batch 850 loss: 1.147096129655838
  batch 900 loss: 1.0874405491352082
LOSS train 1.08744 valid 1.04120, valid PER 34.01%
EPOCH 12:
  batch 50 loss: 1.0650323557853698
  batch 100 loss: 1.0413188076019286
  batch 150 loss: 1.063177089691162
  batch 200 loss: 1.0274826061725617
  batch 250 loss: 1.0619538342952728
  batch 300 loss: 1.031842122077942
  batch 350 loss: 1.0768841183185578
  batch 400 loss: 1.0847828888893127
  batch 450 loss: 1.0811196076869964
  batch 500 loss: 1.0972080397605897
  batch 550 loss: 1.0223080623149872
  batch 600 loss: 1.0262493634223937
  batch 650 loss: 1.103885691165924
  batch 700 loss: 1.0924563896656037
  batch 750 loss: 1.0425399720668793
  batch 800 loss: 1.1118996477127074
  batch 850 loss: 1.1173075723648072
  batch 900 loss: 1.1017982149124146
LOSS train 1.10180 valid 1.03185, valid PER 33.95%
EPOCH 13:
  batch 50 loss: 1.0218166625499725
  batch 100 loss: 1.0488849472999573
  batch 150 loss: 1.0202398335933685
  batch 200 loss: 1.0260955905914306
  batch 250 loss: 1.033562867641449
  batch 300 loss: 1.0117318797111512
  batch 350 loss: 1.0292661368846894
  batch 400 loss: 1.0659907698631286
  batch 450 loss: 1.0454851520061492
  batch 500 loss: 1.0348032891750336
  batch 550 loss: 1.0576106560230256
  batch 600 loss: 1.0748539566993713
  batch 650 loss: 1.0528720593452454
  batch 700 loss: 1.0526800489425658
  batch 750 loss: 1.0213004326820374
  batch 800 loss: 1.0316501665115356
  batch 850 loss: 1.080831288099289
  batch 900 loss: 1.0616565716266633
LOSS train 1.06166 valid 1.01577, valid PER 33.08%
EPOCH 14:
  batch 50 loss: 1.0079396665096283
  batch 100 loss: 0.9890505182743072
  batch 150 loss: 0.9961652886867524
  batch 200 loss: 1.010430442094803
  batch 250 loss: 1.0353548109531403
  batch 300 loss: 1.0322182619571685
  batch 350 loss: 0.9982872247695923
  batch 400 loss: 0.9836948609352112
  batch 450 loss: 0.9925859308242798
  batch 500 loss: 1.0232156538963317
  batch 550 loss: 1.0161024725437164
  batch 600 loss: 0.9846277713775635
  batch 650 loss: 1.0618999099731445
  batch 700 loss: 1.08411802649498
  batch 750 loss: 1.0845146083831787
  batch 800 loss: 1.02002179145813
  batch 850 loss: 1.0722277927398682
  batch 900 loss: 1.0703532230854034
LOSS train 1.07035 valid 1.05450, valid PER 34.69%
EPOCH 15:
  batch 50 loss: 1.0629051566123962
  batch 100 loss: 1.0115235149860382
  batch 150 loss: 1.0082897889614104
  batch 200 loss: 1.0832011306285858
  batch 250 loss: 1.0690794777870178
  batch 300 loss: 1.0344137251377106
  batch 350 loss: 1.0126548612117767
  batch 400 loss: 1.0100930106639863
  batch 450 loss: 1.0524980342388153
  batch 500 loss: 0.9872937595844269
  batch 550 loss: 1.0508633601665496
  batch 600 loss: 1.0836639499664307
  batch 650 loss: 1.0652076637744903
  batch 700 loss: 1.0400530290603638
  batch 750 loss: 1.0311930191516876
  batch 800 loss: 1.0260034811496734
  batch 850 loss: 1.0009053754806518
  batch 900 loss: 1.0147545647621155
LOSS train 1.01475 valid 1.02959, valid PER 33.74%
EPOCH 16:
  batch 50 loss: 1.0074346411228179
  batch 100 loss: 0.9505284333229065
  batch 150 loss: 0.9869200623035431
  batch 200 loss: 0.9951821792125702
  batch 250 loss: 1.0296073067188263
  batch 300 loss: 0.9930707406997681
  batch 350 loss: 1.0566723072528839
  batch 400 loss: 1.0509701704978942
  batch 450 loss: 1.0412473738193513
  batch 500 loss: 0.9646142184734344
  batch 550 loss: 1.0107032406330108
  batch 600 loss: 1.0077413415908814
  batch 650 loss: 1.0253635263442993
  batch 700 loss: 0.9798886823654175
  batch 750 loss: 1.0178930926322938
  batch 800 loss: 1.0088019061088562
  batch 850 loss: 0.9926467406749725
  batch 900 loss: 0.9833326113224029
LOSS train 0.98333 valid 1.00825, valid PER 32.38%
EPOCH 17:
  batch 50 loss: 0.9921904361248016
  batch 100 loss: 0.9862762129306794
  batch 150 loss: 0.9623243248462677
  batch 200 loss: 0.9782649040222168
  batch 250 loss: 1.025874104499817
  batch 300 loss: 1.0452969014644622
  batch 350 loss: 1.0315451729297638
  batch 400 loss: 1.0658902299404145
  batch 450 loss: 1.0366603779792785
  batch 500 loss: 0.9767956113815308
  batch 550 loss: 0.9901784420013428
  batch 600 loss: 1.0357542717456818
  batch 650 loss: 0.9887898004055024
  batch 700 loss: 0.9741214847564698
  batch 750 loss: 0.9537304365634918
  batch 800 loss: 0.9852628493309021
  batch 850 loss: 0.9919372510910034
  batch 900 loss: 0.9732326495647431
LOSS train 0.97323 valid 1.02248, valid PER 33.14%
EPOCH 18:
  batch 50 loss: 0.9778614890575409
  batch 100 loss: 1.0397030401229859
  batch 150 loss: 1.0169179677963256
  batch 200 loss: 1.0149693739414216
  batch 250 loss: 1.0050533461570739
  batch 300 loss: 0.9970992279052734
  batch 350 loss: 0.9997675931453704
  batch 400 loss: 0.9762095463275909
  batch 450 loss: 1.0274172103405
  batch 500 loss: 1.0187820041179656
  batch 550 loss: 1.0113169658184051
  batch 600 loss: 0.9841501986980439
  batch 650 loss: 0.9696377027034759
  batch 700 loss: 1.0333508026599885
  batch 750 loss: 1.0218730497360229
  batch 800 loss: 1.0717809653282167
  batch 850 loss: 1.0557679867744445
  batch 900 loss: 1.076280996799469
LOSS train 1.07628 valid 1.03002, valid PER 33.97%
EPOCH 19:
  batch 50 loss: 0.9372800517082215
  batch 100 loss: 0.97194695353508
  batch 150 loss: 0.9968496966361999
  batch 200 loss: 0.9795965707302093
  batch 250 loss: 0.9974258315563201
  batch 300 loss: 0.9922128689289093
  batch 350 loss: 0.964764301776886
  batch 400 loss: 0.9731889200210572
  batch 450 loss: 0.996791650056839
  batch 500 loss: 0.9991182100772857
  batch 550 loss: 0.96783895611763
  batch 600 loss: 0.995641485452652
  batch 650 loss: 1.0252654540538788
  batch 700 loss: 0.9538227260112763
  batch 750 loss: 0.9649477672576904
  batch 800 loss: 0.999169044494629
  batch 850 loss: 0.9972416913509369
  batch 900 loss: 0.9619736254215241
LOSS train 0.96197 valid 0.99114, valid PER 32.09%
EPOCH 20:
  batch 50 loss: 0.9140432870388031
  batch 100 loss: 0.9380574309825898
  batch 150 loss: 0.9505672860145569
  batch 200 loss: 0.951146878004074
  batch 250 loss: 0.974087017774582
  batch 300 loss: 1.0171436154842377
  batch 350 loss: 0.9383364152908326
  batch 400 loss: 1.010723704099655
  batch 450 loss: 0.9939727139472961
  batch 500 loss: 0.9714895081520081
  batch 550 loss: 1.0915248155593873
  batch 600 loss: 0.9776444733142853
  batch 650 loss: 1.0376892232894896
  batch 700 loss: 1.0722833716869353
  batch 750 loss: 1.0156494998931884
  batch 800 loss: 1.0490877437591553
  batch 850 loss: 1.037535022497177
  batch 900 loss: 1.0328396570682525
LOSS train 1.03284 valid 1.02020, valid PER 33.55%
Training finished in 5.0 minutes.
Model saved to checkpoints/20231206_172852/model_19
Loading model from checkpoints/20231206_172852/model_19
SUB: 16.54%, DEL: 15.47%, INS: 1.67%, COR: 67.98%, PER: 33.69%
