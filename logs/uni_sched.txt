Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 83496
EPOCH 1, Learning Rate: 0.5
  batch 50 loss: 4.73568371295929
  batch 100 loss: 3.2343927001953126
  batch 150 loss: 3.1133881187438965
  batch 200 loss: 2.8876737785339355
  batch 250 loss: 2.6791415929794313
  batch 300 loss: 2.5197915744781496
  batch 350 loss: 2.4452397632598877
  batch 400 loss: 2.3964713191986085
  batch 450 loss: 2.3155382251739502
  batch 500 loss: 2.225941915512085
  batch 550 loss: 2.1702848148345946
  batch 600 loss: 2.1270799446105957
  batch 650 loss: 2.076685252189636
  batch 700 loss: 2.0780813550949095
  batch 750 loss: 2.029203851222992
  batch 800 loss: 2.0198551344871523
  batch 850 loss: 1.9623309826850892
  batch 900 loss: 1.9380837488174438
running loss: 46.162713289260864
LOSS train 1.93808 valid 1.88339, valid PER 73.57%
EPOCH 2, Learning Rate: 0.5
  batch 50 loss: 1.9071334719657898
  batch 100 loss: 1.8676722717285157
  batch 150 loss: 1.8354074883460998
  batch 200 loss: 1.8606409621238709
  batch 250 loss: 1.8436194777488708
  batch 300 loss: 1.8269893288612367
  batch 350 loss: 1.738950161933899
  batch 400 loss: 1.7568498516082764
  batch 450 loss: 1.7081425786018372
  batch 500 loss: 1.7345739603042603
  batch 550 loss: 1.74883234500885
  batch 600 loss: 1.682799985408783
  batch 650 loss: 1.7228296995162964
  batch 700 loss: 1.6784407377243042
  batch 750 loss: 1.6638355565071106
  batch 800 loss: 1.6157595992088318
  batch 850 loss: 1.6235779237747192
  batch 900 loss: 1.6418061804771424
running loss: 38.02423548698425
LOSS train 1.64181 valid 1.55245, valid PER 57.46%
EPOCH 3, Learning Rate: 0.5
  batch 50 loss: 1.5895258212089538
  batch 100 loss: 1.5882938885688782
  batch 150 loss: 1.5647162652015687
  batch 200 loss: 1.5639716863632203
  batch 250 loss: 1.5512384295463562
  batch 300 loss: 1.5425056886672974
  batch 350 loss: 1.581943142414093
  batch 400 loss: 1.5724044346809387
  batch 450 loss: 1.544346489906311
  batch 500 loss: 1.5277042508125305
  batch 550 loss: 1.51108904838562
  batch 600 loss: 1.4986088061332703
  batch 650 loss: 1.4704258275032043
  batch 700 loss: 1.4848531913757324
  batch 750 loss: 1.5540353775024414
  batch 800 loss: 1.4734634160995483
  batch 850 loss: 1.5139765119552613
  batch 900 loss: 1.4480591702461243
running loss: 35.88036262989044
LOSS train 1.44806 valid 1.46136, valid PER 48.49%
EPOCH 4, Learning Rate: 0.5
  batch 50 loss: 1.4774623370170594
  batch 100 loss: 1.486632869243622
  batch 150 loss: 1.4502602458000182
  batch 200 loss: 1.4676619958877564
  batch 250 loss: 1.4756507253646851
  batch 300 loss: 1.498132336139679
  batch 350 loss: 1.3955777978897095
  batch 400 loss: 1.4376835441589355
  batch 450 loss: 1.4617122197151184
  batch 500 loss: 1.4286622738838195
  batch 550 loss: 1.4526555705070496
  batch 600 loss: 1.4523466539382934
  batch 650 loss: 1.4650809717178346
  batch 700 loss: 1.4553758478164673
  batch 750 loss: 1.4044800043106078
  batch 800 loss: 1.4094000625610352
  batch 850 loss: 1.4177798414230347
  batch 900 loss: 1.4568381106853485
running loss: 34.123706579208374
LOSS train 1.45684 valid 1.32634, valid PER 42.81%
EPOCH 5, Learning Rate: 0.5
  batch 50 loss: 1.376945276260376
  batch 100 loss: 1.3986284732818604
  batch 150 loss: 1.4302941536903382
  batch 200 loss: 1.3780693578720093
  batch 250 loss: 1.3506699895858765
  batch 300 loss: 1.386134476661682
  batch 350 loss: 1.3972189569473266
  batch 400 loss: 1.3842859506607055
  batch 450 loss: 1.4033152914047242
  batch 500 loss: 1.433550124168396
  batch 550 loss: 1.396001410484314
  batch 600 loss: 1.430981652736664
  batch 650 loss: 1.3745629119873046
  batch 700 loss: 1.4377757048606872
  batch 750 loss: 1.3831555891036986
  batch 800 loss: 1.3793814551830292
  batch 850 loss: 1.3739695262908935
  batch 900 loss: 1.3983286643028259
running loss: 32.18873751163483
LOSS train 1.39833 valid 1.36351, valid PER 43.77%
EPOCH 6, Learning Rate: 0.5
  batch 50 loss: 1.4171945238113404
  batch 100 loss: 1.353225736618042
  batch 150 loss: 1.3321875357627868
  batch 200 loss: 1.3392671203613282
  batch 250 loss: 1.3622350978851319
  batch 300 loss: 1.3612947654724121
  batch 350 loss: 1.3354413688182831
  batch 400 loss: 1.3246080422401427
  batch 450 loss: 1.3831482672691344
  batch 500 loss: 1.369363329410553
  batch 550 loss: 1.366665370464325
  batch 600 loss: 1.3406065917015075
  batch 650 loss: 1.4044787669181824
  batch 700 loss: 1.354370641708374
  batch 750 loss: 1.33973646402359
  batch 800 loss: 1.3475930452346803
  batch 850 loss: 1.3526732778549195
  batch 900 loss: 1.3595847296714783
running loss: 31.930777549743652
LOSS train 1.35958 valid 1.28845, valid PER 42.34%
EPOCH 7, Learning Rate: 0.5
  batch 50 loss: 1.3319487524032594
  batch 100 loss: 1.3395120048522948
  batch 150 loss: 1.3572786045074463
  batch 200 loss: 1.3063412809371948
  batch 250 loss: 1.3333816814422608
  batch 300 loss: 1.2960984599590302
  batch 350 loss: 1.2904797029495239
  batch 400 loss: 1.2865685081481935
  batch 450 loss: 1.3268311834335327
  batch 500 loss: 1.2992473554611206
  batch 550 loss: 1.2848498702049256
  batch 600 loss: 1.2961322259902954
  batch 650 loss: 1.3036554765701294
  batch 700 loss: 1.332600589990616
  batch 750 loss: 1.3103034496307373
  batch 800 loss: 1.263273103237152
  batch 850 loss: 1.3079462432861328
  batch 900 loss: 1.3523086023330688
running loss: 30.71878170967102
LOSS train 1.35231 valid 1.26595, valid PER 41.62%
EPOCH 8, Learning Rate: 0.5
  batch 50 loss: 1.3020107626914978
  batch 100 loss: 1.2762497091293334
  batch 150 loss: 1.2599996531009674
  batch 200 loss: 1.239121277332306
  batch 250 loss: 1.3114851665496827
  batch 300 loss: 1.2500282669067382
  batch 350 loss: 1.3092872095108032
  batch 400 loss: 1.2604984283447265
  batch 450 loss: 1.3153882932662964
  batch 500 loss: 1.3229133629798888
  batch 550 loss: 1.291164481639862
  batch 600 loss: 1.3418668413162231
  batch 650 loss: 1.3701621866226197
  batch 700 loss: 1.3208794033527373
  batch 750 loss: 1.3379500699043274
  batch 800 loss: 1.3365621089935302
  batch 850 loss: 1.3243243050575257
  batch 900 loss: 1.2965934431552888
running loss: 31.39369297027588
LOSS train 1.29659 valid 1.26020, valid PER 39.89%
EPOCH 9, Learning Rate: 0.25
  batch 50 loss: 1.218736103773117
  batch 100 loss: 1.2556217300891876
  batch 150 loss: 1.2362474071979523
  batch 200 loss: 1.1673231852054595
  batch 250 loss: 1.200721000432968
  batch 300 loss: 1.21260733127594
  batch 350 loss: 1.2292239999771117
  batch 400 loss: 1.2024229061603546
  batch 450 loss: 1.2140912461280822
  batch 500 loss: 1.1621539795398712
  batch 550 loss: 1.2176554346084594
  batch 600 loss: 1.1886007702350616
  batch 650 loss: 1.1919326996803283
  batch 700 loss: 1.1665812969207763
  batch 750 loss: 1.185145925283432
  batch 800 loss: 1.2072145736217499
  batch 850 loss: 1.2399212217330933
  batch 900 loss: 1.165806782245636
running loss: 28.62069547176361
LOSS train 1.16581 valid 1.15301, valid PER 36.92%
EPOCH 10, Learning Rate: 0.25
  batch 50 loss: 1.155619500875473
  batch 100 loss: 1.1733103477954865
  batch 150 loss: 1.2009275615215302
  batch 200 loss: 1.2082856941223143
  batch 250 loss: 1.1863094890117645
  batch 300 loss: 1.129415876865387
  batch 350 loss: 1.1647646605968476
  batch 400 loss: 1.1742892646789551
  batch 450 loss: 1.1247028422355652
  batch 500 loss: 1.195302803516388
  batch 550 loss: 1.1794849491119386
  batch 600 loss: 1.1674481201171876
  batch 650 loss: 1.1571778523921967
  batch 700 loss: 1.1981032276153565
  batch 750 loss: 1.1313473284244537
  batch 800 loss: 1.1729836857318878
  batch 850 loss: 1.1919178318977357
  batch 900 loss: 1.206427048444748
running loss: 28.465303421020508
LOSS train 1.20643 valid 1.17421, valid PER 37.87%
EPOCH 11, Learning Rate: 0.25
  batch 50 loss: 1.1708616209030152
  batch 100 loss: 1.1275176477432252
  batch 150 loss: 1.1125415432453156
  batch 200 loss: 1.1930032134056092
  batch 250 loss: 1.1705832839012147
  batch 300 loss: 1.1074423396587372
  batch 350 loss: 1.1471723532676696
  batch 400 loss: 1.1592823910713195
  batch 450 loss: 1.1839497303962707
  batch 500 loss: 1.1290029454231263
  batch 550 loss: 1.1518794429302215
  batch 600 loss: 1.132894595861435
  batch 650 loss: 1.1886251223087312
  batch 700 loss: 1.1136902129650117
  batch 750 loss: 1.130997338294983
  batch 800 loss: 1.1813033676147462
  batch 850 loss: 1.1823648250102996
  batch 900 loss: 1.1723133945465087
running loss: 28.11203783750534
LOSS train 1.17231 valid 1.17991, valid PER 37.87%
EPOCH 12, Learning Rate: 0.25
  batch 50 loss: 1.1682501029968262
  batch 100 loss: 1.1337004220485687
  batch 150 loss: 1.1118689239025117
  batch 200 loss: 1.1294687616825103
  batch 250 loss: 1.1373338961601258
  batch 300 loss: 1.11840083360672
  batch 350 loss: 1.1440959346294404
  batch 400 loss: 1.1709138703346254
  batch 450 loss: 1.1486601901054383
  batch 500 loss: 1.1652259457111358
  batch 550 loss: 1.0958054065704346
  batch 600 loss: 1.1449970650672912
  batch 650 loss: 1.1868218827247619
  batch 700 loss: 1.1549832665920257
  batch 750 loss: 1.1383172988891601
  batch 800 loss: 1.1131846809387207
  batch 850 loss: 1.191729303598404
  batch 900 loss: 1.1779939877986907
running loss: 27.18454247713089
LOSS train 1.17799 valid 1.14051, valid PER 36.81%
EPOCH 13, Learning Rate: 0.25
  batch 50 loss: 1.116695271730423
  batch 100 loss: 1.1372358226776123
  batch 150 loss: 1.116751172542572
  batch 200 loss: 1.1482428729534149
  batch 250 loss: 1.143082045316696
  batch 300 loss: 1.1065940499305724
  batch 350 loss: 1.1307000315189362
  batch 400 loss: 1.1489867866039276
  batch 450 loss: 1.16397101521492
  batch 500 loss: 1.1037306106090545
  batch 550 loss: 1.1442873418331145
  batch 600 loss: 1.1250464689731599
  batch 650 loss: 1.1236793601512909
  batch 700 loss: 1.1258535659313202
  batch 750 loss: 1.1108660078048707
  batch 800 loss: 1.095660367012024
  batch 850 loss: 1.1773650074005126
  batch 900 loss: 1.1477352499961853
running loss: 27.245424926280975
LOSS train 1.14774 valid 1.11087, valid PER 35.47%
EPOCH 14, Learning Rate: 0.125
  batch 50 loss: 1.0955928874015808
  batch 100 loss: 1.1016693377494813
  batch 150 loss: 1.0778088402748107
  batch 200 loss: 1.1018772745132446
  batch 250 loss: 1.0752275347709657
  batch 300 loss: 1.1045427978038789
  batch 350 loss: 1.0503776931762696
  batch 400 loss: 1.066464933156967
  batch 450 loss: 1.0589359605312347
  batch 500 loss: 1.0859545588493347
  batch 550 loss: 1.074936739206314
  batch 600 loss: 1.0498093819618226
  batch 650 loss: 1.0880545103549957
  batch 700 loss: 1.1003922617435455
  batch 750 loss: 1.051504819393158
  batch 800 loss: 1.0340808939933777
  batch 850 loss: 1.0730144822597503
  batch 900 loss: 1.0843212580680848
running loss: 26.546545922756195
LOSS train 1.08432 valid 1.09606, valid PER 35.56%
EPOCH 15, Learning Rate: 0.125
  batch 50 loss: 1.0845558845996857
  batch 100 loss: 1.0655229818820953
  batch 150 loss: 1.0621740341186523
  batch 200 loss: 1.084751296043396
  batch 250 loss: 1.060753493309021
  batch 300 loss: 1.0505190420150756
  batch 350 loss: 1.049795801639557
  batch 400 loss: 1.0468068838119506
  batch 450 loss: 1.053093092441559
  batch 500 loss: 1.0295163762569428
  batch 550 loss: 1.0680188739299774
  batch 600 loss: 1.0888336873054505
  batch 650 loss: 1.0820734524726867
  batch 700 loss: 1.067944449186325
  batch 750 loss: 1.0507847630977631
  batch 800 loss: 1.059123499393463
  batch 850 loss: 1.0342015302181244
  batch 900 loss: 1.0632863712310792
running loss: 24.94793963432312
LOSS train 1.06329 valid 1.07667, valid PER 34.94%
EPOCH 16, Learning Rate: 0.125
  batch 50 loss: 1.0882092142105102
  batch 100 loss: 1.0323042035102845
  batch 150 loss: 1.0352210521697998
  batch 200 loss: 1.0412499761581422
  batch 250 loss: 1.0675684380531312
  batch 300 loss: 1.056358675956726
  batch 350 loss: 1.0715997469425202
  batch 400 loss: 1.046873722076416
  batch 450 loss: 1.0734382498264312
  batch 500 loss: 1.009104586839676
  batch 550 loss: 1.0715692675113677
  batch 600 loss: 1.0596724700927735
  batch 650 loss: 1.055466103553772
  batch 700 loss: 1.0232787764072417
  batch 750 loss: 1.0421208941936493
  batch 800 loss: 1.0360315644741058
  batch 850 loss: 1.0225850391387938
  batch 900 loss: 1.035793536901474
running loss: 25.456490635871887
LOSS train 1.03579 valid 1.12496, valid PER 35.36%
EPOCH 17, Learning Rate: 0.0625
  batch 50 loss: 1.0754802739620208
  batch 100 loss: 1.049319543838501
  batch 150 loss: 1.0240966475009918
  batch 200 loss: 1.0204035830497742
  batch 250 loss: 1.0412809038162232
  batch 300 loss: 1.0290974962711334
  batch 350 loss: 1.0278048455715179
  batch 400 loss: 1.068846504688263
  batch 450 loss: 1.0411027777194977
  batch 500 loss: 1.0146925175189971
  batch 550 loss: 1.021199107170105
  batch 600 loss: 1.0589774537086487
  batch 650 loss: 1.0146754920482635
  batch 700 loss: 1.0206954288482666
  batch 750 loss: 1.0112485027313232
  batch 800 loss: 0.9991187381744385
  batch 850 loss: 1.0152679097652435
  batch 900 loss: 0.9923920142650604
running loss: 25.62131541967392
LOSS train 0.99239 valid 1.05934, valid PER 33.81%
EPOCH 18, Learning Rate: 0.03125
  batch 50 loss: 1.0058732283115388
  batch 100 loss: 1.029419059753418
  batch 150 loss: 1.0279043638706207
  batch 200 loss: 1.0126821315288543
  batch 250 loss: 1.0180703222751617
  batch 300 loss: 0.9983177125453949
  batch 350 loss: 1.0333981227874756
  batch 400 loss: 0.9850458860397339
  batch 450 loss: 1.021428133249283
  batch 500 loss: 0.9995760798454285
  batch 550 loss: 0.9845485067367554
  batch 600 loss: 0.9704421412944794
  batch 650 loss: 0.9710107111930847
  batch 700 loss: 1.0120398962497712
  batch 750 loss: 0.9973086333274841
  batch 800 loss: 0.9894989132881165
  batch 850 loss: 0.9625202870368957
  batch 900 loss: 0.9944749426841736
running loss: 22.807388484477997
LOSS train 0.99447 valid 1.03928, valid PER 33.16%
EPOCH 19, Learning Rate: 0.03125
  batch 50 loss: 0.9474185979366303
  batch 100 loss: 0.9634900104999542
  batch 150 loss: 0.9878470170497894
  batch 200 loss: 0.999510178565979
  batch 250 loss: 1.0244383311271668
  batch 300 loss: 0.997863906621933
  batch 350 loss: 0.978810328245163
  batch 400 loss: 1.000268428325653
  batch 450 loss: 0.9991334271430969
  batch 500 loss: 0.9990403628349305
  batch 550 loss: 0.9760449695587158
  batch 600 loss: 1.000517830848694
  batch 650 loss: 1.0245144331455232
  batch 700 loss: 0.9677990651130677
  batch 750 loss: 0.9721135950088501
  batch 800 loss: 1.0117860984802247
  batch 850 loss: 1.005339140892029
  batch 900 loss: 1.0008271944522857
running loss: 23.96888566017151
LOSS train 1.00083 valid 1.04288, valid PER 32.88%
EPOCH 20, Learning Rate: 0.015625
  batch 50 loss: 0.9758170211315155
  batch 100 loss: 0.999798504114151
  batch 150 loss: 0.9850380909442902
  batch 200 loss: 0.9990806305408477
  batch 250 loss: 0.969154109954834
  batch 300 loss: 1.0004148042201997
  batch 350 loss: 0.9479894602298736
  batch 400 loss: 0.9788929939270019
  batch 450 loss: 0.9830504298210144
  batch 500 loss: 0.9687429988384246
  batch 550 loss: 1.0246382558345795
  batch 600 loss: 0.9536169385910034
  batch 650 loss: 0.9949539375305175
  batch 700 loss: 0.9915199446678161
  batch 750 loss: 0.9725241911411285
  batch 800 loss: 1.0104702019691467
  batch 850 loss: 0.9957534146308898
  batch 900 loss: 0.99106840133667
running loss: 23.519181549549103
LOSS train 0.99107 valid 1.03507, valid PER 32.81%
Training finished in 2.0 minutes.
Model saved to checkpoints/20231210_040429/model_20
Loading model from checkpoints/20231210_040429/model_20
SUB: 19.13%, DEL: 13.71%, INS: 1.53%, COR: 67.16%, PER: 34.37%
