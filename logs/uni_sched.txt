Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 83496
EPOCH 1, Learning Rate: 0.5
  batch 50 loss: 4.73568398475647
  batch 100 loss: 3.234394087791443
  batch 150 loss: 3.113393158912659
  batch 200 loss: 2.887688889503479
  batch 250 loss: 2.6791427373886108
  batch 300 loss: 2.5197948694229124
  batch 350 loss: 2.445241494178772
  batch 400 loss: 2.396472215652466
  batch 450 loss: 2.3155421209335327
  batch 500 loss: 2.2260017251968383
  batch 550 loss: 2.17023366689682
  batch 600 loss: 2.1269663858413694
  batch 650 loss: 2.0767900443077085
  batch 700 loss: 2.078170473575592
  batch 750 loss: 2.0288707184791566
  batch 800 loss: 2.0193769264221193
  batch 850 loss: 1.9645949053764342
  batch 900 loss: 1.9397183632850648
avg val loss: 1.8810980319976807
LOSS train 1.93972 valid 1.88110, valid PER 73.73%
EPOCH 2, Learning Rate: 0.5
  batch 50 loss: 1.9065805768966675
  batch 100 loss: 1.8671216893196105
  batch 150 loss: 1.8319580626487733
  batch 200 loss: 1.858357276916504
  batch 250 loss: 1.8385425758361817
  batch 300 loss: 1.8293084692955017
  batch 350 loss: 1.73698397397995
  batch 400 loss: 1.7599339818954467
  batch 450 loss: 1.706008596420288
  batch 500 loss: 1.7311302065849303
  batch 550 loss: 1.7519736671447754
  batch 600 loss: 1.682439408302307
  batch 650 loss: 1.7256108546257019
  batch 700 loss: 1.6731374621391297
  batch 750 loss: 1.6677086353302002
  batch 800 loss: 1.6144084572792052
  batch 850 loss: 1.6274868583679198
  batch 900 loss: 1.6426818346977234
avg val loss: 1.561874508857727
LOSS train 1.64268 valid 1.56187, valid PER 57.97%
EPOCH 3, Learning Rate: 0.5
  batch 50 loss: 1.586965730190277
  batch 100 loss: 1.5838002347946167
  batch 150 loss: 1.5696130251884461
  batch 200 loss: 1.5536118030548096
  batch 250 loss: 1.558566951751709
  batch 300 loss: 1.5404856944084167
  batch 350 loss: 1.5729655456542968
  batch 400 loss: 1.5725395584106445
  batch 450 loss: 1.5489706802368164
  batch 500 loss: 1.5355744791030883
  batch 550 loss: 1.5144712185859681
  batch 600 loss: 1.505291781425476
  batch 650 loss: 1.46304860830307
  batch 700 loss: 1.478779718875885
  batch 750 loss: 1.5247852325439453
  batch 800 loss: 1.4890797710418702
  batch 850 loss: 1.4948764586448668
  batch 900 loss: 1.4524165058135987
avg val loss: 1.4085115194320679
LOSS train 1.45242 valid 1.40851, valid PER 46.15%
EPOCH 4, Learning Rate: 0.5
  batch 50 loss: 1.4525220727920531
  batch 100 loss: 1.4859197330474854
  batch 150 loss: 1.4156273460388185
  batch 200 loss: 1.4599131369590759
  batch 250 loss: 1.4689878416061402
  batch 300 loss: 1.5027874898910523
  batch 350 loss: 1.4068246626853942
  batch 400 loss: 1.4721086812019348
  batch 450 loss: 1.4525287580490112
  batch 500 loss: 1.4228372383117676
  batch 550 loss: 1.4511605405807495
  batch 600 loss: 1.4536915874481202
  batch 650 loss: 1.4516772603988648
  batch 700 loss: 1.4259438729286193
  batch 750 loss: 1.3978387427330017
  batch 800 loss: 1.3989668440818788
  batch 850 loss: 1.4289641237258912
  batch 900 loss: 1.4711084866523743
avg val loss: 1.3434360027313232
LOSS train 1.47111 valid 1.34344, valid PER 43.99%
EPOCH 5, Learning Rate: 0.5
  batch 50 loss: 1.373664643764496
  batch 100 loss: 1.3700337290763855
  batch 150 loss: 1.4535009813308717
  batch 200 loss: 1.3655174493789672
  batch 250 loss: 1.3773637437820434
  batch 300 loss: 1.412470552921295
  batch 350 loss: 1.401827528476715
  batch 400 loss: 1.420361773967743
  batch 450 loss: 1.391585967540741
  batch 500 loss: 1.4204021644592286
  batch 550 loss: 1.3610081672668457
  batch 600 loss: 1.4395954489707947
  batch 650 loss: 1.3879202151298522
  batch 700 loss: 1.4913629817962646
  batch 750 loss: 1.374882836341858
  batch 800 loss: 1.3754268050193788
  batch 850 loss: 1.4244082975387573
  batch 900 loss: 1.3939517092704774
avg val loss: 1.3589264154434204
LOSS train 1.39395 valid 1.35893, valid PER 43.58%
EPOCH 6, Learning Rate: 0.25
  batch 50 loss: 1.3557686686515809
  batch 100 loss: 1.276805944442749
  batch 150 loss: 1.2629033935070038
  batch 200 loss: 1.2670962989330292
  batch 250 loss: 1.295836293697357
  batch 300 loss: 1.2577294051647185
  batch 350 loss: 1.2538034963607787
  batch 400 loss: 1.2488289713859557
  batch 450 loss: 1.2769278502464294
  batch 500 loss: 1.2763898622989656
  batch 550 loss: 1.307550082206726
  batch 600 loss: 1.304131200313568
  batch 650 loss: 1.2881667387485505
  batch 700 loss: 1.2573315191268921
  batch 750 loss: 1.2431075048446656
  batch 800 loss: 1.2411768114566804
  batch 850 loss: 1.2277800130844116
  batch 900 loss: 1.2400214326381684
avg val loss: 1.2307387590408325
LOSS train 1.24002 valid 1.23074, valid PER 40.71%
EPOCH 7, Learning Rate: 0.25
  batch 50 loss: 1.2518362414836883
  batch 100 loss: 1.2575207233428956
  batch 150 loss: 1.2541094541549682
  batch 200 loss: 1.2366817116737365
  batch 250 loss: 1.259499979019165
  batch 300 loss: 1.1958616435527802
  batch 350 loss: 1.2272060012817383
  batch 400 loss: 1.2330120706558227
  batch 450 loss: 1.2273813891410827
  batch 500 loss: 1.242357782125473
  batch 550 loss: 1.216036058664322
  batch 600 loss: 1.2520255541801453
  batch 650 loss: 1.2658028519153595
  batch 700 loss: 1.2663548040390014
  batch 750 loss: 1.218238571882248
  batch 800 loss: 1.2259643542766572
  batch 850 loss: 1.2357557022571564
  batch 900 loss: 1.2835824847221375
avg val loss: 1.1890554428100586
LOSS train 1.28358 valid 1.18906, valid PER 38.90%
EPOCH 8, Learning Rate: 0.25
  batch 50 loss: 1.244386352300644
  batch 100 loss: 1.2044725799560547
  batch 150 loss: 1.2271853768825531
  batch 200 loss: 1.210730221271515
  batch 250 loss: 1.1997714960575103
  batch 300 loss: 1.1410992801189423
  batch 350 loss: 1.2275813210010529
  batch 400 loss: 1.1839852905273438
  batch 450 loss: 1.2408698856830598
  batch 500 loss: 1.2704302620887757
  batch 550 loss: 1.2104846715927124
  batch 600 loss: 1.2422170484066009
  batch 650 loss: 1.2740274262428284
  batch 700 loss: 1.1915283071994782
  batch 750 loss: 1.2347248315811157
  batch 800 loss: 1.2132991063594818
  batch 850 loss: 1.1966536295413972
  batch 900 loss: 1.1574926114082336
avg val loss: 1.1951972246170044
LOSS train 1.15749 valid 1.19520, valid PER 38.63%
EPOCH 9, Learning Rate: 0.125
  batch 50 loss: 1.1440340173244476
  batch 100 loss: 1.187996723651886
  batch 150 loss: 1.1541638123989104
  batch 200 loss: 1.101323266029358
  batch 250 loss: 1.1496298837661743
  batch 300 loss: 1.1874893128871917
  batch 350 loss: 1.163235456943512
  batch 400 loss: 1.153846926689148
  batch 450 loss: 1.1557923531532288
  batch 500 loss: 1.1193968212604524
  batch 550 loss: 1.1543640625476836
  batch 600 loss: 1.1414339828491211
  batch 650 loss: 1.1319449901580811
  batch 700 loss: 1.1141609120368958
  batch 750 loss: 1.1410146403312682
  batch 800 loss: 1.1507173848152161
  batch 850 loss: 1.1756297731399536
  batch 900 loss: 1.1208728754520416
avg val loss: 1.1261404752731323
LOSS train 1.12087 valid 1.12614, valid PER 35.74%
EPOCH 10, Learning Rate: 0.125
  batch 50 loss: 1.1072692203521728
  batch 100 loss: 1.1223223829269409
  batch 150 loss: 1.1520056068897246
  batch 200 loss: 1.140639169216156
  batch 250 loss: 1.1523982656002045
  batch 300 loss: 1.1089361786842347
  batch 350 loss: 1.1232672119140625
  batch 400 loss: 1.1006435716152192
  batch 450 loss: 1.079102370738983
  batch 500 loss: 1.1523328912258148
  batch 550 loss: 1.1451492249965667
  batch 600 loss: 1.134595561027527
  batch 650 loss: 1.1253298759460448
  batch 700 loss: 1.1400046920776368
  batch 750 loss: 1.1116449892520905
  batch 800 loss: 1.1549750101566314
  batch 850 loss: 1.1615253353118897
  batch 900 loss: 1.1513574695587159
avg val loss: 1.1286975145339966
LOSS train 1.15136 valid 1.12870, valid PER 36.39%
EPOCH 11, Learning Rate: 0.0625
  batch 50 loss: 1.094464123249054
  batch 100 loss: 1.0816349923610686
  batch 150 loss: 1.0730166661739349
  batch 200 loss: 1.1233023750782012
  batch 250 loss: 1.138692399263382
  batch 300 loss: 1.063791779279709
  batch 350 loss: 1.098444412946701
  batch 400 loss: 1.0988398158550263
  batch 450 loss: 1.1137755906581879
  batch 500 loss: 1.0647516894340514
  batch 550 loss: 1.0860957646369933
  batch 600 loss: 1.0698523759841918
  batch 650 loss: 1.136312266588211
  batch 700 loss: 1.0555388569831847
  batch 750 loss: 1.0754543507099152
  batch 800 loss: 1.0943775081634521
  batch 850 loss: 1.1085424041748047
  batch 900 loss: 1.1006509017944337
avg val loss: 1.0900357961654663
LOSS train 1.10065 valid 1.09004, valid PER 35.48%
EPOCH 12, Learning Rate: 0.0625
  batch 50 loss: 1.0949267280101775
  batch 100 loss: 1.0710197186470032
  batch 150 loss: 1.059766914844513
  batch 200 loss: 1.0627598249912262
  batch 250 loss: 1.0957776427268981
  batch 300 loss: 1.0848571002483367
  batch 350 loss: 1.093650506734848
  batch 400 loss: 1.1112591886520387
  batch 450 loss: 1.0855235946178436
  batch 500 loss: 1.1000185644626617
  batch 550 loss: 1.0195246887207032
  batch 600 loss: 1.0644488334655762
  batch 650 loss: 1.1182033610343933
  batch 700 loss: 1.085381693840027
  batch 750 loss: 1.0636542356014251
  batch 800 loss: 1.0407715547084808
  batch 850 loss: 1.0927128410339355
  batch 900 loss: 1.1016553568840026
avg val loss: 1.0856106281280518
LOSS train 1.10166 valid 1.08561, valid PER 35.30%
EPOCH 13, Learning Rate: 0.0625
  batch 50 loss: 1.0592053151130676
  batch 100 loss: 1.0841837203502656
  batch 150 loss: 1.0588539600372315
  batch 200 loss: 1.094046117067337
  batch 250 loss: 1.0675971162319184
  batch 300 loss: 1.0423918735980988
  batch 350 loss: 1.0648056483268737
  batch 400 loss: 1.061655639410019
  batch 450 loss: 1.114024157524109
  batch 500 loss: 1.040937750339508
  batch 550 loss: 1.0611275935173035
  batch 600 loss: 1.0758022964000702
  batch 650 loss: 1.0589827239513396
  batch 700 loss: 1.0618707048892975
  batch 750 loss: 1.0502069985866547
  batch 800 loss: 1.0605385208129883
  batch 850 loss: 1.112075183391571
  batch 900 loss: 1.0926903593540191
avg val loss: 1.081951379776001
LOSS train 1.09269 valid 1.08195, valid PER 35.52%
EPOCH 14, Learning Rate: 0.0625
  batch 50 loss: 1.072702625989914
  batch 100 loss: 1.1011206781864167
  batch 150 loss: 1.0543132984638215
  batch 200 loss: 1.073708108663559
  batch 250 loss: 1.0711591386795043
  batch 300 loss: 1.093753398656845
  batch 350 loss: 1.029397885799408
  batch 400 loss: 1.064717447757721
  batch 450 loss: 1.0519165480136872
  batch 500 loss: 1.089835821390152
  batch 550 loss: 1.0770953476428986
  batch 600 loss: 1.041384800672531
  batch 650 loss: 1.0766925120353699
  batch 700 loss: 1.084781928062439
  batch 750 loss: 1.0310519909858704
  batch 800 loss: 1.0285170698165893
  batch 850 loss: 1.0800150215625763
  batch 900 loss: 1.0844968497753142
avg val loss: 1.0873639583587646
LOSS train 1.08450 valid 1.08736, valid PER 35.29%
EPOCH 15, Learning Rate: 0.03125
  batch 50 loss: 1.0794610869884491
  batch 100 loss: 1.0582597625255585
  batch 150 loss: 1.0428913998603822
  batch 200 loss: 1.0710286569595338
  batch 250 loss: 1.0574139750003815
  batch 300 loss: 1.0416535604000092
  batch 350 loss: 1.0540391266345979
  batch 400 loss: 1.0310146367549897
  batch 450 loss: 1.0479367780685425
  batch 500 loss: 1.0040697038173676
  batch 550 loss: 1.0484147763252258
  batch 600 loss: 1.0628097343444824
  batch 650 loss: 1.055705610513687
  batch 700 loss: 1.072200187444687
  batch 750 loss: 1.0509920775890351
  batch 800 loss: 1.0443712437152863
  batch 850 loss: 1.030920490026474
  batch 900 loss: 1.0592616164684296
avg val loss: 1.07200026512146
LOSS train 1.05926 valid 1.07200, valid PER 34.67%
EPOCH 16, Learning Rate: 0.03125
  batch 50 loss: 1.0802568900585174
  batch 100 loss: 1.0188199603557586
  batch 150 loss: 1.0383970510959626
  batch 200 loss: 1.0383045303821563
  batch 250 loss: 1.067768199443817
  batch 300 loss: 1.0604273438453675
  batch 350 loss: 1.0533114516735076
  batch 400 loss: 1.0279558396339417
  batch 450 loss: 1.056450148820877
  batch 500 loss: 1.01398681640625
  batch 550 loss: 1.0446058344841003
  batch 600 loss: 1.0606729817390441
  batch 650 loss: 1.0480788195133208
  batch 700 loss: 1.0260547375679017
  batch 750 loss: 1.0307777678966523
  batch 800 loss: 1.0266517615318298
  batch 850 loss: 1.0132917535305024
  batch 900 loss: 1.0403513658046721
avg val loss: 1.070088267326355
LOSS train 1.04035 valid 1.07009, valid PER 34.53%
EPOCH 17, Learning Rate: 0.03125
  batch 50 loss: 1.0486188411712647
  batch 100 loss: 1.0341637432575226
  batch 150 loss: 1.0035503137111663
  batch 200 loss: 1.0200748598575593
  batch 250 loss: 1.0364836025238038
  batch 300 loss: 1.0237334501743316
  batch 350 loss: 1.0110062170028686
  batch 400 loss: 1.0762894093990325
  batch 450 loss: 1.0503710842132568
  batch 500 loss: 1.0266494023799897
  batch 550 loss: 1.0315651834011077
  batch 600 loss: 1.0654708778858184
  batch 650 loss: 1.0341482985019683
  batch 700 loss: 1.0381491720676421
  batch 750 loss: 1.027590081691742
  batch 800 loss: 1.0088850629329682
  batch 850 loss: 1.0353702640533446
  batch 900 loss: 1.0260048043727874
avg val loss: 1.0640584230422974
LOSS train 1.02600 valid 1.06406, valid PER 33.86%
EPOCH 18, Learning Rate: 0.03125
  batch 50 loss: 1.0300145196914672
  batch 100 loss: 1.0505580163002015
  batch 150 loss: 1.0507750523090362
  batch 200 loss: 1.0370639097690582
  batch 250 loss: 1.0400849044322968
  batch 300 loss: 1.0287041115760802
  batch 350 loss: 1.0586989831924438
  batch 400 loss: 1.02003897190094
  batch 450 loss: 1.0625093054771424
  batch 500 loss: 1.0357995879650117
  batch 550 loss: 1.0182515358924866
  batch 600 loss: 0.9982855379581451
  batch 650 loss: 0.9960702931880951
  batch 700 loss: 1.0533562171459199
  batch 750 loss: 1.0296648120880127
  batch 800 loss: 1.0223127162456513
  batch 850 loss: 0.9995862317085266
  batch 900 loss: 1.0395281183719636
avg val loss: 1.064637541770935
LOSS train 1.03953 valid 1.06464, valid PER 33.93%
EPOCH 19, Learning Rate: 0.015625
  batch 50 loss: 0.9905961453914642
  batch 100 loss: 0.9914848935604096
  batch 150 loss: 1.0163167345523834
  batch 200 loss: 1.0388303732872008
  batch 250 loss: 1.0555173218250276
  batch 300 loss: 1.0178721952438354
  batch 350 loss: 1.009320479631424
  batch 400 loss: 1.0333038353919983
  batch 450 loss: 1.0179082179069519
  batch 500 loss: 1.0432108020782471
  batch 550 loss: 1.0051713800430297
  batch 600 loss: 1.0223270618915559
  batch 650 loss: 1.0595848977565765
  batch 700 loss: 0.9864592242240906
  batch 750 loss: 0.9901703572273255
  batch 800 loss: 1.035384806394577
  batch 850 loss: 1.0454719591140746
  batch 900 loss: 1.0163709938526153
avg val loss: 1.0598784685134888
LOSS train 1.01637 valid 1.05988, valid PER 34.08%
EPOCH 20, Learning Rate: 0.015625
  batch 50 loss: 1.0023518991470337
  batch 100 loss: 1.034958132505417
  batch 150 loss: 1.0123279631137847
  batch 200 loss: 1.0157278728485108
  batch 250 loss: 1.013596968650818
  batch 300 loss: 1.0423310148715972
  batch 350 loss: 0.9795721530914306
  batch 400 loss: 1.008008748292923
  batch 450 loss: 1.0142994022369385
  batch 500 loss: 0.9969177436828613
  batch 550 loss: 1.0502025127410888
  batch 600 loss: 0.9958886802196503
  batch 650 loss: 1.0209133780002595
  batch 700 loss: 1.0262909984588624
  batch 750 loss: 0.9955320358276367
  batch 800 loss: 1.0293450832366944
  batch 850 loss: 1.0354882216453551
  batch 900 loss: 1.0202922821044922
avg val loss: 1.0564018487930298
LOSS train 1.02029 valid 1.05640, valid PER 34.13%
Training finished in 2.0 minutes.
Model saved to checkpoints/20231210_140745/model_20
Loading model from checkpoints/20231210_140745/model_20
SUB: 19.44%, DEL: 14.86%, INS: 1.31%, COR: 65.70%, PER: 35.60%
