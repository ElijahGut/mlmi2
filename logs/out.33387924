Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=10.0, optimiser='sgd')
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.164660348892212
  batch 100 loss: 3.1752347326278687
  batch 150 loss: 3.028201823234558
  batch 200 loss: 2.9083097553253174
  batch 250 loss: 2.816081066131592
  batch 300 loss: 2.7081328582763673
  batch 350 loss: 2.4710346698760985
  batch 400 loss: 2.3868767642974853
  batch 450 loss: 2.3105116057395936
  batch 500 loss: 2.179046022891998
  batch 550 loss: 2.102365732192993
  batch 600 loss: 2.0765784335136415
  batch 650 loss: 1.9713055801391601
  batch 700 loss: 1.9694572544097901
  batch 750 loss: 1.9118782329559325
  batch 800 loss: 1.8842027592658996
  batch 850 loss: 1.8299158716201782
  batch 900 loss: 1.8213597130775452
LOSS train 1.82136 valid 1.79862, valid PER 69.02%
EPOCH 2:
  batch 50 loss: 1.7442981219291687
  batch 100 loss: 1.6924047183990478
  batch 150 loss: 1.67979469537735
  batch 200 loss: 1.6655294394493103
  batch 250 loss: 1.6804363799095154
  batch 300 loss: 1.6193206548690795
  batch 350 loss: 1.535021297931671
  batch 400 loss: 1.5368035316467286
  batch 450 loss: 1.4728832530975342
  batch 500 loss: 1.5185324883460998
  batch 550 loss: 1.5160650706291199
  batch 600 loss: 1.4565682530403137
  batch 650 loss: 1.4591434860229493
  batch 700 loss: 1.4466628289222718
  batch 750 loss: 1.4147398281097412
  batch 800 loss: 1.3620616865158082
  batch 850 loss: 1.3736698818206787
  batch 900 loss: 1.3779414105415344
LOSS train 1.37794 valid 1.37679, valid PER 42.30%
EPOCH 3:
  batch 50 loss: 1.3286215543746949
  batch 100 loss: 1.3303732228279115
  batch 150 loss: 1.3179478359222412
  batch 200 loss: 1.3020903873443603
  batch 250 loss: 1.2781024050712586
  batch 300 loss: 1.2675936973094941
  batch 350 loss: 1.327183074951172
  batch 400 loss: 1.291341084241867
  batch 450 loss: 1.2653670704364777
  batch 500 loss: 1.238475911617279
  batch 550 loss: 1.2877273488044738
  batch 600 loss: 1.2250351810455322
  batch 650 loss: 1.201782476902008
  batch 700 loss: 1.214384913444519
  batch 750 loss: 1.3014234519004821
  batch 800 loss: 1.204034037590027
  batch 850 loss: 1.2321140599250793
  batch 900 loss: 1.1794428360462188
LOSS train 1.17944 valid 1.24092, valid PER 37.73%
EPOCH 4:
  batch 50 loss: 1.1596369326114655
  batch 100 loss: 1.1794123423099518
  batch 150 loss: 1.1348446357250213
  batch 200 loss: 1.1699963295459748
  batch 250 loss: 1.1648574411869048
  batch 300 loss: 1.1764768624305726
  batch 350 loss: 1.0984671461582183
  batch 400 loss: 1.1549078333377838
  batch 450 loss: 1.133701937198639
  batch 500 loss: 1.1310371601581573
  batch 550 loss: 1.1570955431461334
  batch 600 loss: 1.1854987740516663
  batch 650 loss: 1.151988240480423
  batch 700 loss: 1.108296868801117
  batch 750 loss: 1.1030797851085663
  batch 800 loss: 1.064564940929413
  batch 850 loss: 1.109633595943451
  batch 900 loss: 1.161934459209442
LOSS train 1.16193 valid 1.13067, valid PER 35.34%
EPOCH 5:
  batch 50 loss: 1.0560178446769715
  batch 100 loss: 1.06300883769989
  batch 150 loss: 1.1094709098339082
  batch 200 loss: 1.0470949292182923
  batch 250 loss: 1.1209939301013947
  batch 300 loss: 1.1410894763469697
  batch 350 loss: 1.0765036022663117
  batch 400 loss: 1.0889153134822847
  batch 450 loss: 1.1114725053310395
  batch 500 loss: 1.0744614505767822
  batch 550 loss: 1.0247271800041198
  batch 600 loss: 1.1414058911800384
  batch 650 loss: 1.0666368436813354
  batch 700 loss: 1.0875576293468476
  batch 750 loss: 1.020936497449875
  batch 800 loss: 1.0600388145446777
  batch 850 loss: 1.057524199485779
  batch 900 loss: 1.0686935997009277
LOSS train 1.06869 valid 1.11350, valid PER 35.16%
EPOCH 6:
  batch 50 loss: 1.0660038495063782
  batch 100 loss: 0.9992885553836822
  batch 150 loss: 0.9948569345474243
  batch 200 loss: 1.03065460562706
  batch 250 loss: 1.036356154680252
  batch 300 loss: 1.021596599817276
  batch 350 loss: 1.008678959608078
  batch 400 loss: 1.0079928994178773
  batch 450 loss: 1.0703518450260163
  batch 500 loss: 1.024397873878479
  batch 550 loss: 1.0418687534332276
  batch 600 loss: 0.9999877476692199
  batch 650 loss: 1.0146003675460815
  batch 700 loss: 1.0044185400009156
  batch 750 loss: 0.9833349251747131
  batch 800 loss: 0.9912261569499969
  batch 850 loss: 0.9678362965583801
  batch 900 loss: 1.0054968309402466
LOSS train 1.00550 valid 1.07963, valid PER 33.80%
EPOCH 7:
  batch 50 loss: 0.9949367690086365
  batch 100 loss: 0.9909299206733704
  batch 150 loss: 0.9775796687602997
  batch 200 loss: 0.9790587413311005
  batch 250 loss: 0.9494267272949218
  batch 300 loss: 0.9658255696296691
  batch 350 loss: 0.9579790377616882
  batch 400 loss: 0.9751608288288116
  batch 450 loss: 0.9833720934391021
  batch 500 loss: 0.9491186702251434
  batch 550 loss: 0.9856922817230225
  batch 600 loss: 0.9667969226837159
  batch 650 loss: 0.9537132906913758
  batch 700 loss: 0.9923013472557067
  batch 750 loss: 1.0025175273418427
  batch 800 loss: 0.9715780484676361
  batch 850 loss: 1.0088992857933043
  batch 900 loss: 1.0618423211574555
LOSS train 1.06184 valid 1.04716, valid PER 33.36%
EPOCH 8:
  batch 50 loss: 0.9382534527778625
  batch 100 loss: 0.9499748659133911
  batch 150 loss: 0.9742333292961121
  batch 200 loss: 0.9346647095680237
  batch 250 loss: 0.9601389765739441
  batch 300 loss: 0.8815288674831391
  batch 350 loss: 0.963571583032608
  batch 400 loss: 0.9342519783973694
  batch 450 loss: 0.9648759078979492
  batch 500 loss: 0.9758882486820221
  batch 550 loss: 0.9189151382446289
  batch 600 loss: 0.9362706887722015
  batch 650 loss: 0.9949409198760987
  batch 700 loss: 0.9492127621173858
  batch 750 loss: 0.9397992181777954
  batch 800 loss: 0.9389745974540711
  batch 850 loss: 0.9420356297492981
  batch 900 loss: 0.9534523749351501
LOSS train 0.95345 valid 1.00167, valid PER 31.46%
EPOCH 9:
  batch 50 loss: 0.8744748461246491
  batch 100 loss: 0.9092064237594605
  batch 150 loss: 0.9528173804283142
  batch 200 loss: 0.8898959958553314
  batch 250 loss: 0.91360072016716
  batch 300 loss: 0.9256143140792846
  batch 350 loss: 0.9539585137367248
  batch 400 loss: 0.9080188262462616
  batch 450 loss: 0.9337419950962067
  batch 500 loss: 0.9135917019844055
  batch 550 loss: 0.9537128829956054
  batch 600 loss: 0.9576125693321228
  batch 650 loss: 0.9474102592468262
  batch 700 loss: 1.0332229363918304
  batch 750 loss: 0.9755429971218109
  batch 800 loss: 0.9749442601203918
  batch 850 loss: 0.9607341909408569
  batch 900 loss: 0.9208882927894593
LOSS train 0.92089 valid 1.01435, valid PER 31.78%
EPOCH 10:
  batch 50 loss: 0.8707412147521972
  batch 100 loss: 0.8887418687343598
  batch 150 loss: 0.9002471697330475
  batch 200 loss: 0.925325665473938
  batch 250 loss: 0.9319167256355285
  batch 300 loss: 0.883260201215744
  batch 350 loss: 0.9218281137943268
  batch 400 loss: 0.8795849442481994
  batch 450 loss: 0.8831270265579224
  batch 500 loss: 0.9622782707214356
  batch 550 loss: 0.9533746695518494
  batch 600 loss: 0.9276553821563721
  batch 650 loss: 0.8881782603263855
  batch 700 loss: 0.8992741310596466
  batch 750 loss: 0.8754924345016479
  batch 800 loss: 0.8995957040786743
  batch 850 loss: 0.9113935041427612
  batch 900 loss: 0.8860712695121765
LOSS train 0.88607 valid 1.00792, valid PER 32.14%
EPOCH 11:
  batch 50 loss: 0.8319043207168579
  batch 100 loss: 0.8124285197257995
  batch 150 loss: 0.8185182881355285
  batch 200 loss: 0.8782459199428558
  batch 250 loss: 0.8733291292190551
  batch 300 loss: 0.8470226764678955
  batch 350 loss: 0.8703418564796448
  batch 400 loss: 0.8778981053829193
  batch 450 loss: 0.8503225123882294
  batch 500 loss: 0.8441349923610687
  batch 550 loss: 0.8477985239028931
  batch 600 loss: 0.8346127927303314
  batch 650 loss: 0.9004080033302307
  batch 700 loss: 0.8108544850349426
  batch 750 loss: 0.8445906484127045
  batch 800 loss: 0.8768628060817718
  batch 850 loss: 0.8948557484149933
  batch 900 loss: 1.0421798205375672
LOSS train 1.04218 valid 1.03696, valid PER 31.87%
EPOCH 12:
  batch 50 loss: 0.9117806565761566
  batch 100 loss: 0.876644093990326
  batch 150 loss: 0.9138524270057679
  batch 200 loss: 0.8875200259685516
  batch 250 loss: 0.8961922872066498
  batch 300 loss: 0.851309918165207
  batch 350 loss: 0.8775662136077881
  batch 400 loss: 0.915727527141571
  batch 450 loss: 0.9370048367977142
  batch 500 loss: 0.951275954246521
  batch 550 loss: 0.8486258852481842
  batch 600 loss: 0.8650144279003144
  batch 650 loss: 0.9142343914508819
  batch 700 loss: 0.8969292581081391
  batch 750 loss: 0.8726885080337524
  batch 800 loss: 0.8675622832775116
  batch 850 loss: 0.8887362265586853
  batch 900 loss: 0.9049384355545044
LOSS train 0.90494 valid 0.99457, valid PER 30.76%
EPOCH 13:
  batch 50 loss: 0.812621431350708
  batch 100 loss: 0.8281141185760498
  batch 150 loss: 0.8191632950305938
  batch 200 loss: 0.8375999796390533
  batch 250 loss: 0.8434399783611297
  batch 300 loss: 0.8299495613574982
  batch 350 loss: 0.8609054243564606
  batch 400 loss: 0.9084378862380982
  batch 450 loss: 0.8739255511760712
  batch 500 loss: 0.8360257053375244
  batch 550 loss: 0.8562425935268402
  batch 600 loss: 0.8480195653438568
  batch 650 loss: 0.8390990179777146
  batch 700 loss: 0.8550128722190857
  batch 750 loss: 0.8077605783939361
  batch 800 loss: 0.8200463426113128
  batch 850 loss: 0.8636029386520385
  batch 900 loss: 0.849118812084198
LOSS train 0.84912 valid 0.99827, valid PER 29.98%
EPOCH 14:
  batch 50 loss: 0.852469310760498
  batch 100 loss: 0.8178557467460632
  batch 150 loss: 0.8147550749778748
  batch 200 loss: 0.8111116325855255
  batch 250 loss: 0.8041822528839111
  batch 300 loss: 0.8544910657405853
  batch 350 loss: 0.801091593503952
  batch 400 loss: 0.8511743497848511
  batch 450 loss: 0.8338114964962006
  batch 500 loss: 0.8310820579528808
  batch 550 loss: 0.8522461700439453
  batch 600 loss: 0.8233737075328826
  batch 650 loss: 0.8510744726657867
  batch 700 loss: 0.864997169971466
  batch 750 loss: 0.8107841181755066
  batch 800 loss: 0.8002870154380798
  batch 850 loss: 0.8665178692340851
  batch 900 loss: 0.8437961995601654
LOSS train 0.84380 valid 0.98097, valid PER 30.24%
EPOCH 15:
  batch 50 loss: 0.7958619570732117
  batch 100 loss: 0.7687611401081085
  batch 150 loss: 0.8050016856193543
  batch 200 loss: 0.8350450420379638
  batch 250 loss: 0.821783356666565
  batch 300 loss: 0.8139047944545745
  batch 350 loss: 0.815560759305954
  batch 400 loss: 0.8103246355056762
  batch 450 loss: 0.8025400006771087
  batch 500 loss: 0.7850008261203766
  batch 550 loss: 0.8417142808437348
  batch 600 loss: 0.8911581611633301
  batch 650 loss: 0.8652299427986145
  batch 700 loss: 0.8661135232448578
  batch 750 loss: 0.8614001417160034
  batch 800 loss: 0.816889077425003
  batch 850 loss: 0.8123632669448853
  batch 900 loss: 0.8276119923591614
LOSS train 0.82761 valid 0.99544, valid PER 31.48%
EPOCH 16:
  batch 50 loss: 0.8161895978450775
  batch 100 loss: 0.7676940381526947
  batch 150 loss: 0.7667895781993866
  batch 200 loss: 0.7602685475349427
  batch 250 loss: 0.8057610893249512
  batch 300 loss: 0.793267296552658
  batch 350 loss: 0.824160749912262
  batch 400 loss: 0.7981951820850373
  batch 450 loss: 0.8185353529453278
  batch 500 loss: 0.7591670179367065
  batch 550 loss: 0.7878332555294036
  batch 600 loss: 0.786725070476532
  batch 650 loss: 0.8159586477279663
  batch 700 loss: 0.7918000519275665
  batch 750 loss: 0.8147853541374207
  batch 800 loss: 0.8266510093212127
  batch 850 loss: 0.8258008599281311
  batch 900 loss: 0.7978481996059418
LOSS train 0.79785 valid 0.97564, valid PER 29.62%
EPOCH 17:
  batch 50 loss: 0.7766643875837326
  batch 100 loss: 0.823060377240181
  batch 150 loss: 0.7828800618648529
  batch 200 loss: 0.7634333777427673
  batch 250 loss: 0.8106571316719056
  batch 300 loss: 0.7968131268024444
  batch 350 loss: 0.7791890728473664
  batch 400 loss: 0.8277712488174438
  batch 450 loss: 0.8207117569446564
  batch 500 loss: 0.7758869802951813
  batch 550 loss: 0.805077394247055
  batch 600 loss: 0.8390981674194335
  batch 650 loss: 0.8139866435527802
  batch 700 loss: 0.7912370026111603
  batch 750 loss: 0.7911043000221253
  batch 800 loss: 0.8125077414512635
  batch 850 loss: 0.8234526705741883
  batch 900 loss: 0.7754443001747131
LOSS train 0.77544 valid 0.99872, valid PER 29.98%
EPOCH 18:
  batch 50 loss: 0.7581306755542755
  batch 100 loss: 0.7759172928333282
  batch 150 loss: 0.8002807426452637
  batch 200 loss: 0.7752008527517319
  batch 250 loss: 0.7964616012573242
  batch 300 loss: 0.7784729123115539
  batch 350 loss: 0.8017114841938019
  batch 400 loss: 0.7524711382389069
  batch 450 loss: 0.8240299105644227
  batch 500 loss: 0.7908921563625335
  batch 550 loss: 0.7995626890659332
  batch 600 loss: 0.7735806322097778
  batch 650 loss: 0.8238540148735046
  batch 700 loss: 0.8277014422416688
  batch 750 loss: 0.7881827425956726
  batch 800 loss: 0.7838464087247848
  batch 850 loss: 0.7957360363006591
  batch 900 loss: 0.8139123070240021
LOSS train 0.81391 valid 0.99683, valid PER 30.63%
EPOCH 19:
  batch 50 loss: 0.7154136943817139
  batch 100 loss: 0.7334789860248566
  batch 150 loss: 0.7531756377220153
  batch 200 loss: 0.7527369600534439
  batch 250 loss: 0.7651123511791229
  batch 300 loss: 0.7641962039470672
  batch 350 loss: 0.7567405188083649
  batch 400 loss: 0.8285251629352569
  batch 450 loss: 0.814500560760498
  batch 500 loss: 0.7952457082271576
  batch 550 loss: 0.7958414745330811
  batch 600 loss: 0.7807770025730133
  batch 650 loss: 0.8538612806797028
  batch 700 loss: 0.775375452041626
  batch 750 loss: 0.7653355884552002
  batch 800 loss: 0.8145083689689636
  batch 850 loss: 0.7937305420637131
  batch 900 loss: 0.7802252113819123
LOSS train 0.78023 valid 0.99132, valid PER 30.96%
EPOCH 20:
  batch 50 loss: 0.7340820848941803
  batch 100 loss: 0.7781201303005219
  batch 150 loss: 0.7908347702026367
  batch 200 loss: 0.7780183863639831
  batch 250 loss: 0.7808480769395828
  batch 300 loss: 0.8216315138339997
  batch 350 loss: 0.7732307660579681
  batch 400 loss: 0.7933847606182098
  batch 450 loss: 0.801149018406868
  batch 500 loss: 0.764100661277771
  batch 550 loss: 0.8406799209117889
  batch 600 loss: 0.8014684677124023
  batch 650 loss: 0.8852923560142517
  batch 700 loss: 0.9323651826381684
  batch 750 loss: 0.8850720429420471
  batch 800 loss: 0.8887295794486999
  batch 850 loss: 0.8603031170368195
  batch 900 loss: 0.9052153718471527
LOSS train 0.90522 valid 1.05992, valid PER 31.75%
Training finished in 5.0 minutes.
Model saved to checkpoints/20231122_173506/model_16
Loading model from checkpoints/20231122_173506/model_16
SUB: 16.60%, DEL: 11.86%, INS: 2.53%, COR: 71.54%, PER: 30.99%
