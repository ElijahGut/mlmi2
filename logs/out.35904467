Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=2, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.8, optimiser='sgd')
Total number of model parameters is 562216
EPOCH 1:
  batch 50 loss: 4.157488441467285
  batch 100 loss: 3.2987375354766844
  batch 150 loss: 3.2460777664184572
  batch 200 loss: 3.1558447790145876
  batch 250 loss: 3.037202672958374
  batch 300 loss: 2.828453640937805
  batch 350 loss: 2.704123821258545
  batch 400 loss: 2.5914657306671143
  batch 450 loss: 2.5105706596374513
  batch 500 loss: 2.3816928339004515
  batch 550 loss: 2.319248242378235
  batch 600 loss: 2.264422574043274
  batch 650 loss: 2.1762858271598815
  batch 700 loss: 2.158102123737335
  batch 750 loss: 2.0768398237228394
  batch 800 loss: 2.0376439833641053
  batch 850 loss: 2.002095465660095
  batch 900 loss: 1.972162594795227
LOSS train 1.97216 valid 1.82568, valid PER 68.89%
EPOCH 2:
  batch 50 loss: 1.9248864841461182
  batch 100 loss: 1.8759756898880005
  batch 150 loss: 1.749533669948578
  batch 200 loss: 1.7804387664794923
  batch 250 loss: 1.7698394012451173
  batch 300 loss: 1.7332656908035278
  batch 350 loss: 1.7071263313293457
  batch 400 loss: 1.654141435623169
  batch 450 loss: 1.6534352684020996
  batch 500 loss: 1.6618506622314453
  batch 550 loss: 1.6186266446113586
  batch 600 loss: 1.6100156331062316
  batch 650 loss: 1.5574520111083985
  batch 700 loss: 1.5624853563308716
  batch 750 loss: 1.5299413371086121
  batch 800 loss: 1.469800159931183
  batch 850 loss: 1.4970487642288208
  batch 900 loss: 1.4586248016357422
LOSS train 1.45862 valid 1.36769, valid PER 43.22%
EPOCH 3:
  batch 50 loss: 1.373555281162262
  batch 100 loss: 1.454974627494812
  batch 150 loss: 1.46139621257782
  batch 200 loss: 1.3786019897460937
  batch 250 loss: 1.3976954555511474
  batch 300 loss: 1.3762929677963256
  batch 350 loss: 1.3824765515327453
  batch 400 loss: 1.3810510277748107
  batch 450 loss: 1.334378080368042
  batch 500 loss: 1.3334822690486907
  batch 550 loss: 1.327447509765625
  batch 600 loss: 1.255936462879181
  batch 650 loss: 1.2976149332523346
  batch 700 loss: 1.2997757637500762
  batch 750 loss: 1.3080100631713867
  batch 800 loss: 1.3268759751319885
  batch 850 loss: 1.265146255493164
  batch 900 loss: 1.2903949809074402
LOSS train 1.29039 valid 1.17723, valid PER 35.12%
EPOCH 4:
  batch 50 loss: 1.2958772897720336
  batch 100 loss: 1.2065654921531677
  batch 150 loss: 1.253539822101593
  batch 200 loss: 1.2265125799179077
  batch 250 loss: 1.24292533159256
  batch 300 loss: 1.24615159034729
  batch 350 loss: 1.2151300954818725
  batch 400 loss: 1.1856768262386321
  batch 450 loss: 1.1707742190361023
  batch 500 loss: 1.2534809172153474
  batch 550 loss: 1.164796028137207
  batch 600 loss: 1.1806674337387084
  batch 650 loss: 1.2647200846672058
  batch 700 loss: 1.2413785767555237
  batch 750 loss: 1.1795959198474884
  batch 800 loss: 1.1571335196495056
  batch 850 loss: 1.1559257471561433
  batch 900 loss: 1.163569425344467
LOSS train 1.16357 valid 1.08290, valid PER 32.74%
EPOCH 5:
  batch 50 loss: 1.1567786884307862
  batch 100 loss: 1.1492500162124635
  batch 150 loss: 1.1792869138717652
  batch 200 loss: 1.1850517094135284
  batch 250 loss: 1.1100128638744353
  batch 300 loss: 1.1566683912277222
  batch 350 loss: 1.1183863496780395
  batch 400 loss: 1.097628848552704
  batch 450 loss: 1.0989597129821778
  batch 500 loss: 1.0691852915287017
  batch 550 loss: 1.144507508277893
  batch 600 loss: 1.1363181853294373
  batch 650 loss: 1.1414294242858887
  batch 700 loss: 1.1026435375213623
  batch 750 loss: 1.1217672324180603
  batch 800 loss: 1.1350935471057892
  batch 850 loss: 1.1199983584880828
  batch 900 loss: 1.0927638232707977
LOSS train 1.09276 valid 1.03996, valid PER 31.58%
EPOCH 6:
  batch 50 loss: 1.0922983634471892
  batch 100 loss: 1.0959698474407196
  batch 150 loss: 1.0882385218143462
  batch 200 loss: 1.0512331056594848
  batch 250 loss: 1.0730294811725616
  batch 300 loss: 1.0907130014896393
  batch 350 loss: 1.0993796765804291
  batch 400 loss: 1.0679645669460296
  batch 450 loss: 1.0991995418071747
  batch 500 loss: 1.0583548820018769
  batch 550 loss: 1.0700581288337707
  batch 600 loss: 1.050794632434845
  batch 650 loss: 1.0327709698677063
  batch 700 loss: 1.0427970147132875
  batch 750 loss: 1.0510865330696106
  batch 800 loss: 1.060219156742096
  batch 850 loss: 1.0978433299064636
  batch 900 loss: 1.1190705072879792
LOSS train 1.11907 valid 0.96995, valid PER 30.20%
EPOCH 7:
  batch 50 loss: 1.0107924354076385
  batch 100 loss: 1.0821752119064332
  batch 150 loss: 1.0215080153942109
  batch 200 loss: 1.0142325520515443
  batch 250 loss: 1.0666684019565582
  batch 300 loss: 1.0436042988300323
  batch 350 loss: 1.0535535800457
  batch 400 loss: 1.0140147471427918
  batch 450 loss: 1.0217712008953095
  batch 500 loss: 1.0233232188224792
  batch 550 loss: 1.01284437417984
  batch 600 loss: 1.018736959695816
  batch 650 loss: 0.9940029382705688
  batch 700 loss: 1.041121917963028
  batch 750 loss: 1.0214578926563262
  batch 800 loss: 1.0102243983745576
  batch 850 loss: 1.0094281911849976
  batch 900 loss: 1.014247579574585
LOSS train 1.01425 valid 0.98447, valid PER 30.12%
EPOCH 8:
  batch 50 loss: 1.0286966335773469
  batch 100 loss: 0.9938826835155488
  batch 150 loss: 1.0092378461360931
  batch 200 loss: 0.9985985279083252
  batch 250 loss: 0.9868634986877441
  batch 300 loss: 0.9674509608745575
  batch 350 loss: 0.9643446397781372
  batch 400 loss: 0.9677134490013123
  batch 450 loss: 1.0295097589492799
  batch 500 loss: 0.9777439951896667
  batch 550 loss: 0.9811329448223114
  batch 600 loss: 0.964176378250122
  batch 650 loss: 0.9868475878238678
  batch 700 loss: 1.0087116980552673
  batch 750 loss: 1.0055283117294311
  batch 800 loss: 1.0010546123981476
  batch 850 loss: 0.9824393284320831
  batch 900 loss: 0.9958861911296845
LOSS train 0.99589 valid 0.95837, valid PER 29.10%
EPOCH 9:
  batch 50 loss: 0.9587130582332611
  batch 100 loss: 0.9385425996780395
  batch 150 loss: 0.9587150979042053
  batch 200 loss: 0.9563412010669708
  batch 250 loss: 0.9358446097373962
  batch 300 loss: 0.9563909614086151
  batch 350 loss: 0.923625658750534
  batch 400 loss: 0.9642983269691467
  batch 450 loss: 0.9971861922740937
  batch 500 loss: 0.9485611295700074
  batch 550 loss: 0.9559917950630188
  batch 600 loss: 1.0103372192382813
  batch 650 loss: 0.9825545299053192
  batch 700 loss: 0.9463154435157776
  batch 750 loss: 0.9604923403263093
  batch 800 loss: 0.9763029718399048
  batch 850 loss: 0.9663441741466522
  batch 900 loss: 0.9266408288478851
LOSS train 0.92664 valid 0.92649, valid PER 28.26%
EPOCH 10:
  batch 50 loss: 0.928602603673935
  batch 100 loss: 0.954715234041214
  batch 150 loss: 0.9749893355369568
  batch 200 loss: 1.0052659332752227
  batch 250 loss: 0.9510992050170899
  batch 300 loss: 0.9394599056243896
  batch 350 loss: 0.9324488031864167
  batch 400 loss: 0.9176249444484711
  batch 450 loss: 0.918679872751236
  batch 500 loss: 0.9389881491661072
  batch 550 loss: 0.9207011008262634
  batch 600 loss: 0.92663733959198
  batch 650 loss: 0.9498783671855926
  batch 700 loss: 0.9362410283088685
  batch 750 loss: 0.9632524526119233
  batch 800 loss: 0.955440742969513
  batch 850 loss: 0.9234139633178711
  batch 900 loss: 0.9151703190803527
LOSS train 0.91517 valid 0.93750, valid PER 28.68%
EPOCH 11:
  batch 50 loss: 0.9198406791687012
  batch 100 loss: 0.9052321565151215
  batch 150 loss: 0.900819548368454
  batch 200 loss: 0.8733987057209015
  batch 250 loss: 0.8936968004703522
  batch 300 loss: 0.8898136579990387
  batch 350 loss: 0.938861300945282
  batch 400 loss: 0.9073136758804321
  batch 450 loss: 0.9255102682113647
  batch 500 loss: 0.8946139872074127
  batch 550 loss: 0.9413662254810333
  batch 600 loss: 0.9282710516452789
  batch 650 loss: 0.9263718020915985
  batch 700 loss: 0.9849747049808503
  batch 750 loss: 0.9197865307331086
  batch 800 loss: 0.9438576781749726
  batch 850 loss: 0.9240667116641998
  batch 900 loss: 0.9350642538070679
LOSS train 0.93506 valid 0.89736, valid PER 27.55%
EPOCH 12:
  batch 50 loss: 0.8838064444065094
  batch 100 loss: 0.8518949222564697
  batch 150 loss: 0.8797015523910523
  batch 200 loss: 0.8931935691833496
  batch 250 loss: 0.8906911671161651
  batch 300 loss: 0.9159549129009247
  batch 350 loss: 0.8968037629127502
  batch 400 loss: 0.9035664415359497
  batch 450 loss: 0.883541499376297
  batch 500 loss: 0.8971200549602508
  batch 550 loss: 0.9193081867694854
  batch 600 loss: 0.9029275822639465
  batch 650 loss: 0.8999739301204681
  batch 700 loss: 0.9254682683944702
  batch 750 loss: 0.9987713515758514
  batch 800 loss: 0.8978671383857727
  batch 850 loss: 0.9153696703910827
  batch 900 loss: 0.9052305006980896
LOSS train 0.90523 valid 0.90652, valid PER 27.20%
EPOCH 13:
  batch 50 loss: 0.8685735654830933
  batch 100 loss: 0.8744549453258514
  batch 150 loss: 0.8956464755535126
  batch 200 loss: 0.860508862733841
  batch 250 loss: 0.8732925295829773
  batch 300 loss: 0.9356651192903519
  batch 350 loss: 0.8786278784275054
  batch 400 loss: 0.8920130217075348
  batch 450 loss: 0.8979290175437927
  batch 500 loss: 0.8934491384029388
  batch 550 loss: 0.9466626155376434
  batch 600 loss: 0.9170560348033905
  batch 650 loss: 0.8929482519626617
  batch 700 loss: 0.9273959994316101
  batch 750 loss: 0.8697989010810852
  batch 800 loss: 0.8890766751766205
  batch 850 loss: 0.8616578686237335
  batch 900 loss: 0.8615265548229217
LOSS train 0.86153 valid 0.87079, valid PER 26.16%
EPOCH 14:
  batch 50 loss: 0.8478902328014374
  batch 100 loss: 0.8440047919750213
  batch 150 loss: 0.8568450033664703
  batch 200 loss: 0.8792534387111663
  batch 250 loss: 0.8555965197086334
  batch 300 loss: 0.8399030029773712
  batch 350 loss: 0.8534261512756348
  batch 400 loss: 0.8736090791225434
  batch 450 loss: 0.8556430041790009
  batch 500 loss: 0.8778501904010773
  batch 550 loss: 0.8864220213890076
  batch 600 loss: 0.8628165829181671
  batch 650 loss: 0.8858679926395416
  batch 700 loss: 0.8825304722785949
  batch 750 loss: 0.8665000855922699
  batch 800 loss: 0.8406519764661788
  batch 850 loss: 0.9209230935573578
  batch 900 loss: 0.872005945444107
LOSS train 0.87201 valid 0.84355, valid PER 25.72%
EPOCH 15:
  batch 50 loss: 0.7888122868537902
  batch 100 loss: 0.8261215090751648
  batch 150 loss: 0.846632639169693
  batch 200 loss: 0.8549051368236542
  batch 250 loss: 0.8455674016475677
  batch 300 loss: 0.8359360468387603
  batch 350 loss: 0.8268349182605743
  batch 400 loss: 0.8700525200366974
  batch 450 loss: 0.8371452903747558
  batch 500 loss: 0.8368584549427033
  batch 550 loss: 0.8795809638500214
  batch 600 loss: 0.9010090565681458
  batch 650 loss: 0.8598853385448456
  batch 700 loss: 0.8527711832523346
  batch 750 loss: 0.8729596352577209
  batch 800 loss: 0.8333398032188416
  batch 850 loss: 0.8391997051239014
  batch 900 loss: 0.8029478514194488
LOSS train 0.80295 valid 0.84033, valid PER 26.04%
EPOCH 16:
  batch 50 loss: 0.8064881324768066
  batch 100 loss: 0.8001541924476624
  batch 150 loss: 0.8397584736347199
  batch 200 loss: 0.8540032505989075
  batch 250 loss: 0.8259669172763825
  batch 300 loss: 0.867412531375885
  batch 350 loss: 0.8481616365909577
  batch 400 loss: 0.8249030721187591
  batch 450 loss: 0.827185012102127
  batch 500 loss: 0.8025456559658051
  batch 550 loss: 0.7888855874538422
  batch 600 loss: 0.8549346113204956
  batch 650 loss: 0.854243723154068
  batch 700 loss: 0.803340402841568
  batch 750 loss: 0.8605106794834136
  batch 800 loss: 0.8245011532306671
  batch 850 loss: 0.8136321628093719
  batch 900 loss: 0.8417169749736786
LOSS train 0.84172 valid 0.84395, valid PER 25.58%
EPOCH 17:
  batch 50 loss: 0.8289165937900543
  batch 100 loss: 0.7910810792446137
  batch 150 loss: 0.8612506198883056
  batch 200 loss: 0.8201151597499847
  batch 250 loss: 0.8584281587600708
  batch 300 loss: 0.8156077015399933
  batch 350 loss: 0.8308848959207534
  batch 400 loss: 0.8377563619613647
  batch 450 loss: 0.8320004284381867
  batch 500 loss: 0.8451574397087097
  batch 550 loss: 0.8383112895488739
  batch 600 loss: 0.8651026618480683
  batch 650 loss: 0.8275237166881562
  batch 700 loss: 0.8520618557929993
  batch 750 loss: 0.7873362505435944
  batch 800 loss: 0.8497368419170379
  batch 850 loss: 0.8292428559064865
  batch 900 loss: 0.850583838224411
LOSS train 0.85058 valid 0.84629, valid PER 25.41%
EPOCH 18:
  batch 50 loss: 0.8119396018981934
  batch 100 loss: 0.814627720117569
  batch 150 loss: 0.8262335729598999
  batch 200 loss: 0.8101743853092194
  batch 250 loss: 0.8082090497016907
  batch 300 loss: 0.8201640856266021
  batch 350 loss: 0.7913970941305161
  batch 400 loss: 0.8179477620124816
  batch 450 loss: 0.836611864566803
  batch 500 loss: 0.8098089838027954
  batch 550 loss: 0.8426944935321807
  batch 600 loss: 0.8170763862133026
  batch 650 loss: 0.7866389894485474
  batch 700 loss: 0.7950597095489502
  batch 750 loss: 0.8340501630306244
  batch 800 loss: 0.8512303602695465
  batch 850 loss: 0.8277905237674713
  batch 900 loss: 0.8232752484083176
LOSS train 0.82328 valid 0.82473, valid PER 25.00%
EPOCH 19:
  batch 50 loss: 0.8132910716533661
  batch 100 loss: 0.8308814418315887
  batch 150 loss: 0.8227399480342865
  batch 200 loss: 0.7655928003787994
  batch 250 loss: 0.8347664129734039
  batch 300 loss: 0.8397878873348236
  batch 350 loss: 0.8194413304328918
  batch 400 loss: 0.7981538724899292
  batch 450 loss: 0.7650516080856323
  batch 500 loss: 0.7904139137268067
  batch 550 loss: 0.8049559271335602
  batch 600 loss: 0.8122895157337189
  batch 650 loss: 0.8032706141471863
  batch 700 loss: 0.8198071324825287
  batch 750 loss: 0.7886049723625184
  batch 800 loss: 0.7818172430992126
  batch 850 loss: 0.7951413130760193
  batch 900 loss: 0.7871427178382874
LOSS train 0.78714 valid 0.82617, valid PER 24.90%
EPOCH 20:
  batch 50 loss: 0.7215575730800629
  batch 100 loss: 0.7659891963005065
  batch 150 loss: 0.7826380801200866
  batch 200 loss: 0.7750371104478836
  batch 250 loss: 0.8037192690372467
  batch 300 loss: 0.7965568435192109
  batch 350 loss: 0.7595830410718918
  batch 400 loss: 0.7895608592033386
  batch 450 loss: 0.7734954404830933
  batch 500 loss: 0.8041107070446014
  batch 550 loss: 0.787050668001175
  batch 600 loss: 0.7873752725124359
  batch 650 loss: 0.8127127635478973
  batch 700 loss: 0.7643638277053832
  batch 750 loss: 0.7716841530799866
  batch 800 loss: 0.8065191185474396
  batch 850 loss: 0.8450528991222381
  batch 900 loss: 0.7935799872875213
LOSS train 0.79358 valid 0.81814, valid PER 24.65%
Training finished in 6.0 minutes.
Model saved to checkpoints/20231205_071037/model_20
Loading model from checkpoints/20231205_071037/model_20
SUB: 16.38%, DEL: 7.13%, INS: 2.86%, COR: 76.49%, PER: 26.37%
