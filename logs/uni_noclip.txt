Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=None, is_bidir=False)
Total number of model parameters is 166952
EPOCH 1, Learning Rate: 0.5
  batch 50 loss: 4.168340821266174
  batch 100 loss: 3.171695952415466
  batch 150 loss: 3.033504810333252
  batch 200 loss: 2.920682039260864
  batch 250 loss: 2.845752730369568
  batch 300 loss: 2.6683820724487304
  batch 350 loss: 2.533417057991028
  batch 400 loss: 2.495355153083801
  batch 450 loss: 2.410981845855713
  batch 500 loss: 2.3986785197257996
  batch 550 loss: 2.272304515838623
  batch 600 loss: 2.224797213077545
  batch 650 loss: 2.1353495645523073
  batch 700 loss: 2.0763548278808592
  batch 750 loss: 2.011169934272766
  batch 800 loss: 1.9783602118492127
  batch 850 loss: 1.9251766324043273
  batch 900 loss: 1.8969633483886719
LOSS train 1.89696 valid 1.83546, valid PER 70.90%
EPOCH 2, Learning Rate: 0.5
  batch 50 loss: 1.8437202668190003
  batch 100 loss: 1.7944526433944703
  batch 150 loss: 1.776555004119873
  batch 200 loss: 1.7527212142944335
  batch 250 loss: 1.7846567153930664
  batch 300 loss: 1.7247943568229676
  batch 350 loss: 1.635962383747101
  batch 400 loss: 1.663340289592743
  batch 450 loss: 1.6084868097305298
  batch 500 loss: 1.6259918022155762
  batch 550 loss: 1.648258113861084
  batch 600 loss: 1.5811403846740724
  batch 650 loss: 1.6132504773139953
  batch 700 loss: 1.6012984251976012
  batch 750 loss: 1.5776989078521728
  batch 800 loss: 1.5070845484733582
  batch 850 loss: 1.5165298795700073
  batch 900 loss: 1.5359996819496156
LOSS train 1.53600 valid 1.41759, valid PER 48.47%
EPOCH 3, Learning Rate: 0.5
  batch 50 loss: 1.4886762738227843
  batch 100 loss: 1.4695774865150453
  batch 150 loss: 1.4587280178070068
  batch 200 loss: 1.431565990447998
  batch 250 loss: 1.4181770586967468
  batch 300 loss: 1.417219533920288
  batch 350 loss: 1.4481121516227722
  batch 400 loss: 1.4206221532821655
  batch 450 loss: 1.3983392906188965
  batch 500 loss: 1.3962044882774354
  batch 550 loss: 1.3955356860160828
  batch 600 loss: 1.4008254623413086
  batch 650 loss: 1.3367960357666016
  batch 700 loss: 1.3691643738746644
  batch 750 loss: 1.4147448801994325
  batch 800 loss: 1.3351848888397218
  batch 850 loss: 1.3827638721466065
  batch 900 loss: 1.3001748991012574
LOSS train 1.30017 valid 1.29632, valid PER 40.76%
EPOCH 4, Learning Rate: 0.5
  batch 50 loss: 1.3083590388298034
  batch 100 loss: 1.3344999432563782
  batch 150 loss: 1.2770789384841919
  batch 200 loss: 1.3039983797073365
  batch 250 loss: 1.3313045811653137
  batch 300 loss: 1.3282283782958983
  batch 350 loss: 1.24804647564888
  batch 400 loss: 1.299469939470291
  batch 450 loss: 1.28451043009758
  batch 500 loss: 1.27403905749321
  batch 550 loss: 1.2678477001190185
  batch 600 loss: 1.30893217086792
  batch 650 loss: 1.3092487227916718
  batch 700 loss: 1.2508285915851594
  batch 750 loss: 1.2656041944026948
  batch 800 loss: 1.1969372487068177
  batch 850 loss: 1.2469810080528259
  batch 900 loss: 1.2885995090007782
LOSS train 1.28860 valid 1.17212, valid PER 37.39%
EPOCH 5, Learning Rate: 0.5
  batch 50 loss: 1.1952193748950959
  batch 100 loss: 1.2001496624946595
  batch 150 loss: 1.254360158443451
  batch 200 loss: 1.1683785104751587
  batch 250 loss: 1.1946170485019685
  batch 300 loss: 1.204683827161789
  batch 350 loss: 1.221792905330658
  batch 400 loss: 1.2256967520713806
  batch 450 loss: 1.1962458443641664
  batch 500 loss: 1.2146748423576355
  batch 550 loss: 1.167741245031357
  batch 600 loss: 1.2454801940917968
  batch 650 loss: 1.197264269590378
  batch 700 loss: 1.2288350939750672
  batch 750 loss: 1.1585459661483766
  batch 800 loss: 1.2026701939105988
  batch 850 loss: 1.2061970365047454
  batch 900 loss: 1.236382199525833
LOSS train 1.23638 valid 1.16131, valid PER 37.26%
EPOCH 6, Learning Rate: 0.5
  batch 50 loss: 1.1951883244514465
  batch 100 loss: 1.1387603497505188
  batch 150 loss: 1.113595542907715
  batch 200 loss: 1.1442558515071868
  batch 250 loss: 1.129897459745407
  batch 300 loss: 1.1453350341320039
  batch 350 loss: 1.14802352309227
  batch 400 loss: 1.1415772140026093
  batch 450 loss: 1.1569134390354157
  batch 500 loss: 1.1549927473068238
  batch 550 loss: 1.161957632303238
  batch 600 loss: 1.132459112405777
  batch 650 loss: 1.1679542434215546
  batch 700 loss: 1.158785572052002
  batch 750 loss: 1.1524038779735566
  batch 800 loss: 1.11421884059906
  batch 850 loss: 1.126348340511322
  batch 900 loss: 1.16246542096138
LOSS train 1.16247 valid 1.13133, valid PER 36.26%
EPOCH 7, Learning Rate: 0.5
  batch 50 loss: 1.147113264799118
  batch 100 loss: 1.164413537979126
  batch 150 loss: 1.1503876900672914
  batch 200 loss: 1.1003181660175323
  batch 250 loss: 1.1191970789432526
  batch 300 loss: 1.2181235647201538
  batch 350 loss: 1.1977762913703918
  batch 400 loss: 1.1551875913143157
  batch 450 loss: 1.1571350073814393
  batch 500 loss: 1.1498150527477264
  batch 550 loss: 1.1378831839561463
  batch 600 loss: 1.137164729833603
  batch 650 loss: 1.146563836336136
  batch 700 loss: 1.146967761516571
  batch 750 loss: 1.1200819408893585
  batch 800 loss: 1.1305043339729308
  batch 850 loss: 1.1432196187973023
  batch 900 loss: 1.1824693524837493
LOSS train 1.18247 valid 1.11945, valid PER 36.72%
EPOCH 8, Learning Rate: 0.5
  batch 50 loss: 1.1024514210224152
  batch 100 loss: 1.124676877260208
  batch 150 loss: 1.0811168837547303
  batch 200 loss: 1.073425101041794
  batch 250 loss: 1.0760489559173585
  batch 300 loss: 1.0364356088638305
  batch 350 loss: 1.1200313770771027
  batch 400 loss: 1.0843748426437378
  batch 450 loss: 1.1115629374980927
  batch 500 loss: 1.1396108376979828
  batch 550 loss: 1.0852055323123933
  batch 600 loss: 1.1080574464797974
  batch 650 loss: 1.1292369091510772
  batch 700 loss: 1.0680953776836395
  batch 750 loss: 1.0669517874717713
  batch 800 loss: 1.0916703939437866
  batch 850 loss: 1.1019372963905334
  batch 900 loss: 1.0839796876907348
LOSS train 1.08398 valid 1.06042, valid PER 34.52%
EPOCH 9, Learning Rate: 0.5
  batch 50 loss: 1.0539010214805602
  batch 100 loss: 1.0816639864444733
  batch 150 loss: 1.0633315896987916
  batch 200 loss: 1.0157233834266663
  batch 250 loss: 1.0439182579517365
  batch 300 loss: 1.0505822587013245
  batch 350 loss: 1.0877751207351685
  batch 400 loss: 1.0643912148475647
  batch 450 loss: 1.0681918263435364
  batch 500 loss: 1.0016786313056947
  batch 550 loss: 1.0666003453731536
  batch 600 loss: 1.0845090091228484
  batch 650 loss: 1.0289146387577057
  batch 700 loss: 1.0347595763206483
  batch 750 loss: 1.0261609828472138
  batch 800 loss: 1.0462541961669922
  batch 850 loss: 1.0785748422145844
  batch 900 loss: 1.0347481167316437
LOSS train 1.03475 valid 1.01395, valid PER 33.43%
EPOCH 10, Learning Rate: 0.5
  batch 50 loss: 0.9748388743400573
  batch 100 loss: 1.022757042646408
  batch 150 loss: 1.0517347800731658
  batch 200 loss: 1.0425511479377747
  batch 250 loss: 1.0275832104682923
  batch 300 loss: 0.9755569303035736
  batch 350 loss: 1.0436759090423584
  batch 400 loss: 1.0206183660030366
  batch 450 loss: 0.9925830674171447
  batch 500 loss: 1.0457164919376374
  batch 550 loss: 1.0495954513549806
  batch 600 loss: 1.030189403295517
  batch 650 loss: 0.9942424821853638
  batch 700 loss: 1.0174236845970155
  batch 750 loss: 1.0226506805419922
  batch 800 loss: 1.0293250942230225
  batch 850 loss: 1.0471981191635131
  batch 900 loss: 1.0653876996040343
LOSS train 1.06539 valid 1.08799, valid PER 35.61%
EPOCH 11, Learning Rate: 0.5
  batch 50 loss: 1.004833642244339
  batch 100 loss: 0.9623831796646118
  batch 150 loss: 0.9658981370925903
  batch 200 loss: 1.0343179750442504
  batch 250 loss: 1.014985113143921
  batch 300 loss: 0.9780794775485993
  batch 350 loss: 1.013138701915741
  batch 400 loss: 1.030632039308548
  batch 450 loss: 1.0124875366687776
  batch 500 loss: 1.0039769840240478
  batch 550 loss: 1.015397585630417
  batch 600 loss: 0.9935008084774017
  batch 650 loss: 1.0472020637989043
  batch 700 loss: 0.9533386957645417
  batch 750 loss: 0.9902437281608581
  batch 800 loss: 1.0535253489017486
  batch 850 loss: 1.046220339536667
  batch 900 loss: 1.038487387895584
LOSS train 1.03849 valid 1.01707, valid PER 32.76%
EPOCH 12, Learning Rate: 0.5
  batch 50 loss: 0.995049523115158
  batch 100 loss: 0.9959521067142486
  batch 150 loss: 0.9766169810295104
  batch 200 loss: 0.9702355229854583
  batch 250 loss: 1.0090323793888092
  batch 300 loss: 0.9791676986217499
  batch 350 loss: 0.98109290599823
  batch 400 loss: 1.0045932018756867
  batch 450 loss: 1.0042869055271149
  batch 500 loss: 1.0228275954723358
  batch 550 loss: 0.9438813042640686
  batch 600 loss: 0.9512146997451782
  batch 650 loss: 1.0049955916404725
  batch 700 loss: 1.0061781084537507
  batch 750 loss: 0.9571094620227814
  batch 800 loss: 0.9793713498115539
  batch 850 loss: 1.000518354177475
  batch 900 loss: 0.998101737499237
LOSS train 0.99810 valid 0.98717, valid PER 32.41%
EPOCH 13, Learning Rate: 0.5
  batch 50 loss: 0.9441429018974304
  batch 100 loss: 0.996588534116745
  batch 150 loss: 0.9537781882286072
  batch 200 loss: 0.978419908285141
  batch 250 loss: 0.9645817494392395
  batch 300 loss: 0.9554626381397248
  batch 350 loss: 0.9731597650051117
  batch 400 loss: 0.9951661944389343
  batch 450 loss: 0.9884234666824341
  batch 500 loss: 0.9381605386734009
  batch 550 loss: 0.9544014549255371
  batch 600 loss: 0.9793084120750427
  batch 650 loss: 0.9963266682624817
  batch 700 loss: 0.9780195415019989
  batch 750 loss: 0.9486051571369171
  batch 800 loss: 0.9897855198383332
  batch 850 loss: 1.031525799036026
  batch 900 loss: 1.0086479711532592
LOSS train 1.00865 valid 1.00297, valid PER 32.35%
EPOCH 14, Learning Rate: 0.5
  batch 50 loss: 0.970193384885788
  batch 100 loss: 0.9517837202548981
  batch 150 loss: 0.9546885955333709
  batch 200 loss: 0.9838877296447754
  batch 250 loss: 1.004820828437805
  batch 300 loss: 0.9963148295879364
  batch 350 loss: 0.948525447845459
  batch 400 loss: 0.9759896194934845
  batch 450 loss: 0.9538257014751435
  batch 500 loss: 0.9788943469524384
  batch 550 loss: 0.985001095533371
  batch 600 loss: 0.953626766204834
  batch 650 loss: 0.9796192169189453
  batch 700 loss: 0.9963331592082977
  batch 750 loss: 0.9582718753814697
  batch 800 loss: 0.9289030122756958
  batch 850 loss: 0.968006751537323
  batch 900 loss: 0.9710761654376984
LOSS train 0.97108 valid 1.01495, valid PER 32.86%
EPOCH 15, Learning Rate: 0.5
  batch 50 loss: 0.9865535497665405
  batch 100 loss: 0.9460596537590027
  batch 150 loss: 0.9746523952484131
  batch 200 loss: 0.9892137360572815
  batch 250 loss: 0.9679688251018524
  batch 300 loss: 0.9361704409122467
  batch 350 loss: 0.9706051409244537
  batch 400 loss: 0.9261715626716613
  batch 450 loss: 0.961956821680069
  batch 500 loss: 0.907680116891861
  batch 550 loss: 0.9620672750473023
  batch 600 loss: 1.0162357187271118
  batch 650 loss: 0.9902462875843048
  batch 700 loss: 0.9902055478096008
  batch 750 loss: 0.9916719603538513
  batch 800 loss: 0.9491614580154419
  batch 850 loss: 0.954232405424118
  batch 900 loss: 0.964090803861618
LOSS train 0.96409 valid 1.00785, valid PER 32.98%
EPOCH 16, Learning Rate: 0.5
  batch 50 loss: 0.9551373827457428
  batch 100 loss: 0.9357267785072326
  batch 150 loss: 0.9619146740436554
  batch 200 loss: 0.9276452624797821
  batch 250 loss: 0.9824089133739471
  batch 300 loss: 0.9291500616073608
  batch 350 loss: 0.9712317955493927
  batch 400 loss: 0.9590291476249695
  batch 450 loss: 0.9944200491905213
  batch 500 loss: 0.9198830664157868
  batch 550 loss: 0.9719969928264618
  batch 600 loss: 0.9483412218093872
  batch 650 loss: 0.9600638246536255
  batch 700 loss: 0.9304237735271453
  batch 750 loss: 0.9515833640098572
  batch 800 loss: 1.0402147305011749
  batch 850 loss: 1.0251324963569641
  batch 900 loss: 0.9980841565132141
LOSS train 0.99808 valid 1.01124, valid PER 32.96%
EPOCH 17, Learning Rate: 0.5
  batch 50 loss: 0.9600272309780121
  batch 100 loss: 0.9613801610469818
  batch 150 loss: 0.9349436295032502
  batch 200 loss: 0.927815419435501
  batch 250 loss: 0.9373321461677552
  batch 300 loss: 0.940831253528595
  batch 350 loss: 0.915112053155899
  batch 400 loss: 0.9689823663234711
  batch 450 loss: 0.9795085275173188
  batch 500 loss: 0.9205926132202148
  batch 550 loss: 0.9520497298240662
  batch 600 loss: 1.0090008652210236
  batch 650 loss: 0.9268685269355774
  batch 700 loss: 0.9302913963794708
  batch 750 loss: 0.9036333501338959
  batch 800 loss: 0.9501578009128571
  batch 850 loss: 0.9815765047073364
  batch 900 loss: 0.9615567982196808
LOSS train 0.96156 valid 1.02243, valid PER 32.60%
EPOCH 18, Learning Rate: 0.5
  batch 50 loss: 0.9410276317596435
  batch 100 loss: 0.949856870174408
  batch 150 loss: 0.9529172790050506
  batch 200 loss: 0.9386378681659698
  batch 250 loss: 1.0015016174316407
  batch 300 loss: 0.9384049689769745
  batch 350 loss: 0.9809953546524048
  batch 400 loss: 0.9194085431098938
  batch 450 loss: 0.9815938901901246
  batch 500 loss: 0.9796571850776672
  batch 550 loss: 0.9309095656871795
  batch 600 loss: 0.91796861410141
  batch 650 loss: 0.9508954155445098
  batch 700 loss: 1.0409834933280946
  batch 750 loss: 0.9561470663547516
  batch 800 loss: 0.9555024516582489
  batch 850 loss: 0.9222450482845307
  batch 900 loss: 0.9588261449337006
LOSS train 0.95883 valid 0.99088, valid PER 32.24%
EPOCH 19, Learning Rate: 0.5
  batch 50 loss: 0.8883333623409271
  batch 100 loss: 0.9270433938503265
  batch 150 loss: 0.9553584909439087
  batch 200 loss: 0.9499446952342987
  batch 250 loss: 0.9579582035541534
  batch 300 loss: 0.9528250122070312
  batch 350 loss: 0.9126000666618347
  batch 400 loss: 0.9233546662330627
  batch 450 loss: 0.9339540684223175
  batch 500 loss: 0.957419365644455
  batch 550 loss: 0.9398932576179504
  batch 600 loss: 0.9563466799259186
  batch 650 loss: 1.0004983782768249
  batch 700 loss: 0.9483772540092468
  batch 750 loss: 0.9349361228942871
  batch 800 loss: 0.9627393209934234
  batch 850 loss: 0.9654981505870819
  batch 900 loss: 0.9379139029979706
LOSS train 0.93791 valid 0.98855, valid PER 31.71%
EPOCH 20, Learning Rate: 0.5
  batch 50 loss: 0.9324989593029023
  batch 100 loss: 0.934650536775589
  batch 150 loss: 0.9270503664016724
  batch 200 loss: 0.9066593229770661
  batch 250 loss: 0.9041770017147064
  batch 300 loss: 0.9211779570579529
  batch 350 loss: 0.899897712469101
  batch 400 loss: 0.9145857477188111
  batch 450 loss: 0.9135727572441101
  batch 500 loss: 0.8734871697425842
  batch 550 loss: 0.9575067257881165
  batch 600 loss: 0.9024045872688293
  batch 650 loss: 0.9389465808868408
  batch 700 loss: 0.9433935415744782
  batch 750 loss: 0.9379799664020538
  batch 800 loss: 0.9794576954841614
  batch 850 loss: 0.9582848978042603
  batch 900 loss: 0.9460008358955383
LOSS train 0.94600 valid 0.97248, valid PER 31.28%
Training finished in 3.0 minutes.
Model saved to checkpoints/20231207_204440/model_20
Loading model from checkpoints/20231207_204440/model_20
SUB: 16.13%, DEL: 14.60%, INS: 1.86%, COR: 69.28%, PER: 32.59%
