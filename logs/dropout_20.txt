Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.2, optimiser='sgd')
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.171411662101746
  batch 100 loss: 3.177429885864258
  batch 150 loss: 3.035505385398865
  batch 200 loss: 2.926720767021179
  batch 250 loss: 2.8472041654586793
  batch 300 loss: 2.696587619781494
  batch 350 loss: 2.5473979711532593
  batch 400 loss: 2.4820313119888304
  batch 450 loss: 2.4385925769805907
  batch 500 loss: 2.325553231239319
  batch 550 loss: 2.2054784297943115
  batch 600 loss: 2.114395549297333
  batch 650 loss: 2.0369242811203003
  batch 700 loss: 2.0118224525451662
  batch 750 loss: 1.9627639317512513
  batch 800 loss: 1.9398734664916992
  batch 850 loss: 1.8814493799209595
  batch 900 loss: 1.868051655292511
LOSS train 1.86805 valid 1.80462, valid PER 70.24%
EPOCH 2:
  batch 50 loss: 1.80620502948761
  batch 100 loss: 1.7618476486206054
  batch 150 loss: 1.7242202877998352
  batch 200 loss: 1.7167767930030822
  batch 250 loss: 1.748679656982422
  batch 300 loss: 1.6932258415222168
  batch 350 loss: 1.6019525957107543
  batch 400 loss: 1.6199003124237061
  batch 450 loss: 1.5669917845726014
  batch 500 loss: 1.5898641109466554
  batch 550 loss: 1.602964174747467
  batch 600 loss: 1.5320839405059814
  batch 650 loss: 1.5795458912849427
  batch 700 loss: 1.5565031313896178
  batch 750 loss: 1.5272765898704528
  batch 800 loss: 1.4749243712425233
  batch 850 loss: 1.4502668571472168
  batch 900 loss: 1.4752715635299682
LOSS train 1.47527 valid 1.38076, valid PER 46.19%
EPOCH 3:
  batch 50 loss: 1.4176178693771362
  batch 100 loss: 1.4121478056907655
  batch 150 loss: 1.418545081615448
  batch 200 loss: 1.3841086721420288
  batch 250 loss: 1.3576397681236267
  batch 300 loss: 1.3540515995025635
  batch 350 loss: 1.403472468852997
  batch 400 loss: 1.3939823341369628
  batch 450 loss: 1.3683781099319459
  batch 500 loss: 1.3493054509162903
  batch 550 loss: 1.3503688549995423
  batch 600 loss: 1.3359925031661988
  batch 650 loss: 1.2967328643798828
  batch 700 loss: 1.3072555994987487
  batch 750 loss: 1.3646710050106048
  batch 800 loss: 1.2957246124744415
  batch 850 loss: 1.3348559713363648
  batch 900 loss: 1.2667511534690856
LOSS train 1.26675 valid 1.27571, valid PER 40.00%
EPOCH 4:
  batch 50 loss: 1.2630212366580964
  batch 100 loss: 1.2730391550064086
  batch 150 loss: 1.2189756989479066
  batch 200 loss: 1.2587066519260406
  batch 250 loss: 1.2623554241657258
  batch 300 loss: 1.2461207377910615
  batch 350 loss: 1.19972913146019
  batch 400 loss: 1.2393446779251098
  batch 450 loss: 1.2276637887954711
  batch 500 loss: 1.2291034853458405
  batch 550 loss: 1.2348679840564727
  batch 600 loss: 1.2497783076763154
  batch 650 loss: 1.2289839804172515
  batch 700 loss: 1.205935617685318
  batch 750 loss: 1.1863619184494019
  batch 800 loss: 1.1549306797981262
  batch 850 loss: 1.2084811913967133
  batch 900 loss: 1.2431869745254516
LOSS train 1.24319 valid 1.16804, valid PER 36.95%
EPOCH 5:
  batch 50 loss: 1.1609398221969605
  batch 100 loss: 1.1742220664024352
  batch 150 loss: 1.2174832236766815
  batch 200 loss: 1.160641452074051
  batch 250 loss: 1.171106904745102
  batch 300 loss: 1.1825648772716522
  batch 350 loss: 1.1792740988731385
  batch 400 loss: 1.1700857782363892
  batch 450 loss: 1.163879964351654
  batch 500 loss: 1.1790576541423798
  batch 550 loss: 1.1293979072570801
  batch 600 loss: 1.221500004529953
  batch 650 loss: 1.1539105999469756
  batch 700 loss: 1.1932442915439605
  batch 750 loss: 1.1286522626876831
  batch 800 loss: 1.1632961225509644
  batch 850 loss: 1.1719366896152497
  batch 900 loss: 1.1714568042755127
LOSS train 1.17146 valid 1.11448, valid PER 35.61%
EPOCH 6:
  batch 50 loss: 1.1550442576408386
  batch 100 loss: 1.1126448392868042
  batch 150 loss: 1.089761871099472
  batch 200 loss: 1.1288609099388123
  batch 250 loss: 1.1433603262901306
  batch 300 loss: 1.1577563285827637
  batch 350 loss: 1.1131170284748078
  batch 400 loss: 1.0998720407485962
  batch 450 loss: 1.132853581905365
  batch 500 loss: 1.1210756719112396
  batch 550 loss: 1.1521268856525422
  batch 600 loss: 1.1224898386001587
  batch 650 loss: 1.1303477203845977
  batch 700 loss: 1.1179427623748779
  batch 750 loss: 1.1117124736309052
  batch 800 loss: 1.0937145972251892
  batch 850 loss: 1.0790386855602265
  batch 900 loss: 1.100042567253113
LOSS train 1.10004 valid 1.10466, valid PER 35.26%
EPOCH 7:
  batch 50 loss: 1.0973908078670502
  batch 100 loss: 1.1003085148334504
  batch 150 loss: 1.0993384802341462
  batch 200 loss: 1.0976150631904602
  batch 250 loss: 1.1143309926986695
  batch 300 loss: 1.0999589884281158
  batch 350 loss: 1.129825223684311
  batch 400 loss: 1.0689853298664094
  batch 450 loss: 1.127058149576187
  batch 500 loss: 1.095883151292801
  batch 550 loss: 1.139633845090866
  batch 600 loss: 1.1032259380817413
  batch 650 loss: 1.0975265884399414
  batch 700 loss: 1.1180469000339508
  batch 750 loss: 1.0999432277679444
  batch 800 loss: 1.0744069910049439
  batch 850 loss: 1.0921775245666503
  batch 900 loss: 1.1322907781600953
LOSS train 1.13229 valid 1.08286, valid PER 35.03%
EPOCH 8:
  batch 50 loss: 1.0925593101978301
  batch 100 loss: 1.0752089989185334
  batch 150 loss: 1.04773007273674
  batch 200 loss: 1.029627126455307
  batch 250 loss: 1.0469721007347106
  batch 300 loss: 1.0193042194843291
  batch 350 loss: 1.0732181024551393
  batch 400 loss: 1.0460327577590942
  batch 450 loss: 1.0613622534275056
  batch 500 loss: 1.1139649283885955
  batch 550 loss: 1.023913769721985
  batch 600 loss: 1.0545516443252563
  batch 650 loss: 1.0947297823429107
  batch 700 loss: 1.0557453382015227
  batch 750 loss: 1.0512002170085908
  batch 800 loss: 1.0441195821762086
  batch 850 loss: 1.047069125175476
  batch 900 loss: 1.0267635023593902
LOSS train 1.02676 valid 1.03901, valid PER 33.82%
EPOCH 9:
  batch 50 loss: 0.9843018126487731
  batch 100 loss: 1.010280133485794
  batch 150 loss: 1.0179350793361663
  batch 200 loss: 0.9790429091453552
  batch 250 loss: 1.0113316237926484
  batch 300 loss: 1.0185300433635711
  batch 350 loss: 1.0352536249160766
  batch 400 loss: 1.039550198316574
  batch 450 loss: 1.026848109960556
  batch 500 loss: 0.9643328642845154
  batch 550 loss: 1.0459496569633484
  batch 600 loss: 1.1748230385780334
  batch 650 loss: 1.0758971071243286
  batch 700 loss: 1.0372559857368469
  batch 750 loss: 1.0266550827026366
  batch 800 loss: 1.0764914166927337
  batch 850 loss: 1.0944721138477325
  batch 900 loss: 1.0739743947982787
LOSS train 1.07397 valid 1.06836, valid PER 33.86%
EPOCH 10:
  batch 50 loss: 1.0103822982311248
  batch 100 loss: 1.0349355578422545
  batch 150 loss: 1.0437409663200379
  batch 200 loss: 1.047388732433319
  batch 250 loss: 1.046835948228836
  batch 300 loss: 1.0149889397621155
  batch 350 loss: 1.0519085371494292
  batch 400 loss: 1.0147082138061523
  batch 450 loss: 1.0564606153964997
  batch 500 loss: 1.0652925395965576
  batch 550 loss: 1.0753969275951385
  batch 600 loss: 1.0510175478458406
  batch 650 loss: 1.0267153704166412
  batch 700 loss: 1.0183949339389802
  batch 750 loss: 1.0064906203746795
  batch 800 loss: 1.0351486098766327
  batch 850 loss: 1.029039021730423
  batch 900 loss: 1.0106023943424225
LOSS train 1.01060 valid 1.04229, valid PER 34.67%
EPOCH 11:
  batch 50 loss: 0.9644654405117035
  batch 100 loss: 0.9489666140079498
  batch 150 loss: 0.971678638458252
  batch 200 loss: 1.0086308491230012
  batch 250 loss: 0.9925259411334991
  batch 300 loss: 0.9721602058410644
  batch 350 loss: 0.9960225141048431
  batch 400 loss: 1.0121612024307252
  batch 450 loss: 0.977775776386261
  batch 500 loss: 0.9740944004058838
  batch 550 loss: 0.9914206278324127
  batch 600 loss: 0.9705757069587707
  batch 650 loss: 1.0490644407272338
  batch 700 loss: 0.9599962997436523
  batch 750 loss: 0.9931279003620148
  batch 800 loss: 1.0113540732860564
  batch 850 loss: 1.0367142260074615
  batch 900 loss: 1.0138157427310943
LOSS train 1.01382 valid 1.03012, valid PER 33.09%
EPOCH 12:
  batch 50 loss: 0.9826051914691925
  batch 100 loss: 0.9461393892765045
  batch 150 loss: 0.9569067275524139
  batch 200 loss: 0.9506414294242859
  batch 250 loss: 0.9760402953624725
  batch 300 loss: 0.9830478394031524
  batch 350 loss: 0.9585926699638366
  batch 400 loss: 0.9952614271640777
  batch 450 loss: 0.978377159833908
  batch 500 loss: 0.9903881251811981
  batch 550 loss: 0.9301194763183593
  batch 600 loss: 0.9464775002002717
  batch 650 loss: 0.9907430446147919
  batch 700 loss: 0.97461905002594
  batch 750 loss: 0.9421363234519958
  batch 800 loss: 0.9370925164222718
  batch 850 loss: 0.9799357962608337
  batch 900 loss: 0.9840126705169677
LOSS train 0.98401 valid 0.97719, valid PER 31.70%
EPOCH 13:
  batch 50 loss: 0.8877938854694366
  batch 100 loss: 0.9300579345226287
  batch 150 loss: 0.9052551186084747
  batch 200 loss: 0.9255648934841156
  batch 250 loss: 0.9151813924312592
  batch 300 loss: 0.9176834225654602
  batch 350 loss: 0.9349433410167695
  batch 400 loss: 0.9450618994235992
  batch 450 loss: 0.952026230096817
  batch 500 loss: 0.9182889711856842
  batch 550 loss: 0.9288379955291748
  batch 600 loss: 0.9232442450523376
  batch 650 loss: 0.955459189414978
  batch 700 loss: 0.9646814894676209
  batch 750 loss: 0.9104030454158782
  batch 800 loss: 0.9255002152919769
  batch 850 loss: 0.966059250831604
  batch 900 loss: 0.9959340393543243
LOSS train 0.99593 valid 1.00453, valid PER 31.78%
EPOCH 14:
  batch 50 loss: 0.9475492548942566
  batch 100 loss: 0.914552206993103
  batch 150 loss: 0.9168348491191864
  batch 200 loss: 0.9327315771579743
  batch 250 loss: 0.9306525599956512
  batch 300 loss: 0.9471295857429505
  batch 350 loss: 0.8817205405235291
  batch 400 loss: 0.9117270505428314
  batch 450 loss: 0.9131631278991699
  batch 500 loss: 0.9507704293727874
  batch 550 loss: 0.9254907584190368
  batch 600 loss: 0.900038058757782
  batch 650 loss: 1.0364476644992828
  batch 700 loss: 0.9982263147830963
  batch 750 loss: 0.9694687998294831
  batch 800 loss: 0.9267412030696869
  batch 850 loss: 0.9805130910873413
  batch 900 loss: 0.9457805347442627
LOSS train 0.94578 valid 1.05816, valid PER 33.18%
EPOCH 15:
  batch 50 loss: 0.9754877781867981
  batch 100 loss: 0.9442974233627319
  batch 150 loss: 0.9071295654773712
  batch 200 loss: 0.9505055141448975
  batch 250 loss: 0.9249573624134064
  batch 300 loss: 0.9162333798408508
  batch 350 loss: 0.9144007897377014
  batch 400 loss: 0.8963850378990174
  batch 450 loss: 0.9029617261886597
  batch 500 loss: 0.8678980255126953
  batch 550 loss: 0.9763585877418518
  batch 600 loss: 0.9938587760925293
  batch 650 loss: 0.9493560183048249
  batch 700 loss: 0.9556086599826813
  batch 750 loss: 0.9411695301532745
  batch 800 loss: 0.9191276037693024
  batch 850 loss: 0.9083579504489898
  batch 900 loss: 0.9227298998832703
LOSS train 0.92273 valid 0.98343, valid PER 31.68%
EPOCH 16:
  batch 50 loss: 0.9086849915981293
  batch 100 loss: 0.8873788189888
  batch 150 loss: 0.8763305997848511
  batch 200 loss: 0.8780650544166565
  batch 250 loss: 0.9036990118026733
  batch 300 loss: 0.8883930337429047
  batch 350 loss: 0.9188916707038879
  batch 400 loss: 0.908689900636673
  batch 450 loss: 0.9010294854640961
  batch 500 loss: 0.8722374165058135
  batch 550 loss: 0.8993017828464508
  batch 600 loss: 0.8790880870819092
  batch 650 loss: 0.8907216477394104
  batch 700 loss: 0.8688616442680359
  batch 750 loss: 0.8966304326057434
  batch 800 loss: 0.9014849722385406
  batch 850 loss: 0.9060443651676178
  batch 900 loss: 0.8872828531265259
LOSS train 0.88728 valid 0.96508, valid PER 30.48%
EPOCH 17:
  batch 50 loss: 0.8652461856603623
  batch 100 loss: 0.8622352159023285
  batch 150 loss: 0.8829641568660737
  batch 200 loss: 0.8682337260246277
  batch 250 loss: 0.8570197427272797
  batch 300 loss: 0.8642057836055755
  batch 350 loss: 0.8607427096366882
  batch 400 loss: 0.9168800210952759
  batch 450 loss: 0.9144669246673583
  batch 500 loss: 0.8914417243003845
  batch 550 loss: 0.9243521630764008
  batch 600 loss: 0.9620494616031646
  batch 650 loss: 0.9318592524528504
  batch 700 loss: 0.8991475772857666
  batch 750 loss: 0.8888986921310424
  batch 800 loss: 0.9038113212585449
  batch 850 loss: 0.8914784348011017
  batch 900 loss: 0.865136774778366
LOSS train 0.86514 valid 0.99153, valid PER 31.42%
EPOCH 18:
  batch 50 loss: 0.885700763463974
  batch 100 loss: 0.9001961362361908
  batch 150 loss: 0.8726477813720703
  batch 200 loss: 0.8748483312129974
  batch 250 loss: 0.8795500731468201
  batch 300 loss: 0.8526042592525482
  batch 350 loss: 0.8845984613895417
  batch 400 loss: 0.8584606766700744
  batch 450 loss: 0.9053375267982483
  batch 500 loss: 0.8896039724349976
  batch 550 loss: 0.8647478330135345
  batch 600 loss: 0.8436783468723297
  batch 650 loss: 0.8490026903152466
  batch 700 loss: 0.9094907414913177
  batch 750 loss: 0.8500407481193543
  batch 800 loss: 0.8488383030891419
  batch 850 loss: 0.8599737501144409
  batch 900 loss: 0.8800448977947235
LOSS train 0.88004 valid 0.98056, valid PER 31.55%
EPOCH 19:
  batch 50 loss: 0.8159153175354004
  batch 100 loss: 0.8034677219390869
  batch 150 loss: 0.87769322514534
  batch 200 loss: 0.8653764116764069
  batch 250 loss: 0.9000372850894928
  batch 300 loss: 0.8938886332511902
  batch 350 loss: 0.8420435309410095
  batch 400 loss: 0.8826792192459106
  batch 450 loss: 0.9055123066902161
  batch 500 loss: 0.8946814560890197
  batch 550 loss: 0.8684713101387024
  batch 600 loss: 0.8589162874221802
  batch 650 loss: 0.9387620484828949
  batch 700 loss: 0.8501153409481048
  batch 750 loss: 0.846853723526001
  batch 800 loss: 0.8893837773799896
  batch 850 loss: 0.891978212594986
  batch 900 loss: 0.864518871307373
LOSS train 0.86452 valid 0.97907, valid PER 31.33%
EPOCH 20:
  batch 50 loss: 0.8255390465259552
  batch 100 loss: 0.8362306523323059
  batch 150 loss: 0.8298084115982056
  batch 200 loss: 0.8494718742370605
  batch 250 loss: 0.8400301718711853
  batch 300 loss: 0.8634293389320373
  batch 350 loss: 0.8466170942783355
  batch 400 loss: 0.8671083402633667
  batch 450 loss: 0.8620234084129333
  batch 500 loss: 0.8237962198257446
  batch 550 loss: 0.9016463351249695
  batch 600 loss: 0.8388516569137573
  batch 650 loss: 0.8833114635944367
  batch 700 loss: 0.9012407171726227
  batch 750 loss: 0.8651601326465607
  batch 800 loss: 0.9230313587188721
  batch 850 loss: 0.8904082083702087
  batch 900 loss: 0.875685168504715
LOSS train 0.87569 valid 0.97579, valid PER 30.92%
Training finished in 6.0 minutes.
Model saved to checkpoints/20231206_172852/model_16
Loading model from checkpoints/20231206_172852/model_16
SUB: 16.11%, DEL: 14.12%, INS: 1.79%, COR: 69.76%, PER: 32.03%
