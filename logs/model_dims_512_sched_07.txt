Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=512, concat=1, lr=0.7, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 2240552
EPOCH 1, Learning Rate: 0.7
  batch 50 loss: 4.601791791915893
  batch 100 loss: 3.1225393056869506
  batch 150 loss: 2.960361623764038
  batch 200 loss: 2.779753813743591
  batch 250 loss: 2.7699027109146117
  batch 300 loss: 2.710642919540405
  batch 350 loss: 2.4418573665618895
  batch 400 loss: 2.311616795063019
  batch 450 loss: 2.209216797351837
  batch 500 loss: 2.133899531364441
  batch 550 loss: 2.079888446331024
  batch 600 loss: 1.991781964302063
  batch 650 loss: 1.9011519289016723
  batch 700 loss: 1.8826585507392883
  batch 750 loss: 1.8586260843276978
  batch 800 loss: 1.8206266927719117
  batch 850 loss: 1.782277524471283
  batch 900 loss: 1.7250169658660888
avg val loss: 1.742311954498291
LOSS train 1.72502 valid 1.74231, valid PER 66.13%
EPOCH 2, Learning Rate: 0.7
  batch 50 loss: 1.7275621104240417
  batch 100 loss: 1.6506846785545348
  batch 150 loss: 1.6421677994728088
  batch 200 loss: 1.6488103485107422
  batch 250 loss: 1.6598468899726868
  batch 300 loss: 1.6040329432487488
  batch 350 loss: 1.5191722989082337
  batch 400 loss: 1.5447545051574707
  batch 450 loss: 1.504716305732727
  batch 500 loss: 1.5281768465042114
  batch 550 loss: 1.5077496814727782
  batch 600 loss: 1.4632442212104797
  batch 650 loss: 1.490248637199402
  batch 700 loss: 1.461487247943878
  batch 750 loss: 1.4215312385559082
  batch 800 loss: 1.373682870864868
  batch 850 loss: 1.3690991401672363
  batch 900 loss: 1.3937255454063415
avg val loss: 1.308817982673645
LOSS train 1.39373 valid 1.30882, valid PER 41.45%
EPOCH 3, Learning Rate: 0.7
  batch 50 loss: 1.3218106496334077
  batch 100 loss: 1.3054406428337098
  batch 150 loss: 1.3033273386955262
  batch 200 loss: 1.2726398134231567
  batch 250 loss: 1.258006820678711
  batch 300 loss: 1.2548674368858337
  batch 350 loss: 1.3147104334831239
  batch 400 loss: 1.280177127122879
  batch 450 loss: 1.2710165917873382
  batch 500 loss: 1.236384299993515
  batch 550 loss: 1.2421766757965087
  batch 600 loss: 1.2175322353839875
  batch 650 loss: 1.1745234262943267
  batch 700 loss: 1.2057721674442292
  batch 750 loss: 1.263030607700348
  batch 800 loss: 1.1990023279190063
  batch 850 loss: 1.2120334100723267
  batch 900 loss: 1.1570369863510133
avg val loss: 1.1911892890930176
LOSS train 1.15704 valid 1.19119, valid PER 36.30%
EPOCH 4, Learning Rate: 0.7
  batch 50 loss: 1.136067509651184
  batch 100 loss: 1.1718132817745208
  batch 150 loss: 1.1180571687221528
  batch 200 loss: 1.1581153190135955
  batch 250 loss: 1.15902606010437
  batch 300 loss: 1.1651799523830413
  batch 350 loss: 1.0885825729370118
  batch 400 loss: 1.1211996579170227
  batch 450 loss: 1.1131439077854157
  batch 500 loss: 1.0950676250457763
  batch 550 loss: 1.1397696459293365
  batch 600 loss: 1.1642621326446534
  batch 650 loss: 1.1162107741832734
  batch 700 loss: 1.1001026940345764
  batch 750 loss: 1.0969372153282166
  batch 800 loss: 1.0442975127696992
  batch 850 loss: 1.0814048945903778
  batch 900 loss: 1.1421147751808167
avg val loss: 1.0555946826934814
LOSS train 1.14211 valid 1.05559, valid PER 32.86%
EPOCH 5, Learning Rate: 0.7
  batch 50 loss: 1.0278128588199615
  batch 100 loss: 1.0415858149528503
  batch 150 loss: 1.0719816279411316
  batch 200 loss: 1.0165322649478912
  batch 250 loss: 1.0132365787029267
  batch 300 loss: 1.0605146181583405
  batch 350 loss: 1.0454957151412965
  batch 400 loss: 1.032383176088333
  batch 450 loss: 1.0165368938446044
  batch 500 loss: 1.0201675009727478
  batch 550 loss: 1.013365112543106
  batch 600 loss: 1.0713120293617249
  batch 650 loss: 1.0342834866046906
  batch 700 loss: 1.0660232615470886
  batch 750 loss: 0.9940578591823578
  batch 800 loss: 1.066680430173874
  batch 850 loss: 1.049638910293579
  batch 900 loss: 1.0455376946926116
avg val loss: 1.009806513786316
LOSS train 1.04554 valid 1.00981, valid PER 32.30%
EPOCH 6, Learning Rate: 0.7
  batch 50 loss: 0.9918816602230072
  batch 100 loss: 0.9290899157524108
  batch 150 loss: 0.9495066905021667
  batch 200 loss: 0.9747665989398956
  batch 250 loss: 1.0066617631912231
  batch 300 loss: 0.9762880718708038
  batch 350 loss: 0.9482037973403931
  batch 400 loss: 0.9464364421367645
  batch 450 loss: 0.9932931184768676
  batch 500 loss: 0.9948984289169311
  batch 550 loss: 0.9878389430046082
  batch 600 loss: 0.9436931383609771
  batch 650 loss: 0.9795050239562988
  batch 700 loss: 0.9616725540161133
  batch 750 loss: 0.9394655692577362
  batch 800 loss: 0.9361333250999451
  batch 850 loss: 0.9249532449245453
  batch 900 loss: 0.9478026282787323
avg val loss: 0.9887133836746216
LOSS train 0.94780 valid 0.98871, valid PER 31.21%
EPOCH 7, Learning Rate: 0.7
  batch 50 loss: 0.9183886229991913
  batch 100 loss: 0.9181507062911988
  batch 150 loss: 0.9185520148277283
  batch 200 loss: 0.9023612058162689
  batch 250 loss: 0.8778554773330689
  batch 300 loss: 0.8850257062911987
  batch 350 loss: 0.9191219437122345
  batch 400 loss: 0.9235043132305145
  batch 450 loss: 0.9101868653297425
  batch 500 loss: 0.8938238680362701
  batch 550 loss: 0.8820968794822693
  batch 600 loss: 0.9153285336494446
  batch 650 loss: 0.8932302129268647
  batch 700 loss: 0.9236241269111634
  batch 750 loss: 0.9147412753105164
  batch 800 loss: 0.8992371428012848
  batch 850 loss: 0.9388130295276642
  batch 900 loss: 0.9554793679714203
avg val loss: 0.956104576587677
LOSS train 0.95548 valid 0.95610, valid PER 30.51%
EPOCH 8, Learning Rate: 0.7
  batch 50 loss: 0.844449371099472
  batch 100 loss: 0.8284247446060181
  batch 150 loss: 0.8506876134872436
  batch 200 loss: 0.850394024848938
  batch 250 loss: 0.8393571102619171
  batch 300 loss: 0.7930757451057434
  batch 350 loss: 0.8759719812870026
  batch 400 loss: 0.8439116382598877
  batch 450 loss: 0.8714028668403625
  batch 500 loss: 0.9066215384006501
  batch 550 loss: 0.8287473011016846
  batch 600 loss: 0.8863968539237976
  batch 650 loss: 0.9017725777626038
  batch 700 loss: 0.854448150396347
  batch 750 loss: 0.8520145010948181
  batch 800 loss: 0.8852147901058197
  batch 850 loss: 0.864507622718811
  batch 900 loss: 0.8715594363212585
avg val loss: 0.9341952800750732
LOSS train 0.87156 valid 0.93420, valid PER 28.61%
EPOCH 9, Learning Rate: 0.7
  batch 50 loss: 0.7864541280269622
  batch 100 loss: 0.8141440367698669
  batch 150 loss: 0.8120629227161408
  batch 200 loss: 0.7741350781917572
  batch 250 loss: 0.8230256128311157
  batch 300 loss: 0.8312318301200867
  batch 350 loss: 0.848044091463089
  batch 400 loss: 0.8209591472148895
  batch 450 loss: 0.810863156914711
  batch 500 loss: 0.7860104012489318
  batch 550 loss: 0.8257304871082306
  batch 600 loss: 0.8521882236003876
  batch 650 loss: 0.8225462710857392
  batch 700 loss: 0.8190458738803863
  batch 750 loss: 0.8086916518211364
  batch 800 loss: 0.8550777232646942
  batch 850 loss: 0.8519790780544281
  batch 900 loss: 0.8267540192604065
avg val loss: 0.8989320993423462
LOSS train 0.82675 valid 0.89893, valid PER 28.78%
EPOCH 10, Learning Rate: 0.7
  batch 50 loss: 0.7163703328371048
  batch 100 loss: 0.7475782918930054
  batch 150 loss: 0.7674651038646698
  batch 200 loss: 0.7948398494720459
  batch 250 loss: 0.8051532137393952
  batch 300 loss: 0.7401759958267212
  batch 350 loss: 0.7835687756538391
  batch 400 loss: 0.7548449796438217
  batch 450 loss: 0.762658896446228
  batch 500 loss: 0.7924361205101014
  batch 550 loss: 0.8108803343772888
  batch 600 loss: 0.8030466043949127
  batch 650 loss: 0.7654046320915222
  batch 700 loss: 0.8035963356494904
  batch 750 loss: 0.7798727965354919
  batch 800 loss: 0.7996263062953949
  batch 850 loss: 0.8168537080287933
  batch 900 loss: 0.8086128854751586
avg val loss: 0.9410063028335571
LOSS train 0.80861 valid 0.94101, valid PER 29.38%
EPOCH 11, Learning Rate: 0.35
  batch 50 loss: 0.6744393146038056
  batch 100 loss: 0.6197868990898132
  batch 150 loss: 0.6356287163496017
  batch 200 loss: 0.6787625795602799
  batch 250 loss: 0.6644320750236511
  batch 300 loss: 0.6248123329877854
  batch 350 loss: 0.6494677472114563
  batch 400 loss: 0.6595536702871323
  batch 450 loss: 0.6458929485082626
  batch 500 loss: 0.6351506233215332
  batch 550 loss: 0.6372524011135101
  batch 600 loss: 0.6215827924013138
  batch 650 loss: 0.6804704719781876
  batch 700 loss: 0.6141362023353577
  batch 750 loss: 0.6321802079677582
  batch 800 loss: 0.6556916224956513
  batch 850 loss: 0.6888504022359848
  batch 900 loss: 0.6740168130397797
avg val loss: 0.855942964553833
LOSS train 0.67402 valid 0.85594, valid PER 25.67%
EPOCH 12, Learning Rate: 0.35
  batch 50 loss: 0.615809291601181
  batch 100 loss: 0.6076260459423065
  batch 150 loss: 0.5658438640832901
  batch 200 loss: 0.5817201954126358
  batch 250 loss: 0.5963969242572784
  batch 300 loss: 0.5868861538171768
  batch 350 loss: 0.5931034284830093
  batch 400 loss: 0.6159710502624511
  batch 450 loss: 0.6112595790624619
  batch 500 loss: 0.6339645260572433
  batch 550 loss: 0.5702612870931625
  batch 600 loss: 0.6183929562568664
  batch 650 loss: 0.6391029077768325
  batch 700 loss: 0.6185457265377045
  batch 750 loss: 0.596794359087944
  batch 800 loss: 0.6176228654384613
  batch 850 loss: 0.6537992465496063
  batch 900 loss: 0.6461651277542114
avg val loss: 0.8416470885276794
LOSS train 0.64617 valid 0.84165, valid PER 26.07%
EPOCH 13, Learning Rate: 0.35
  batch 50 loss: 0.5590778243541717
  batch 100 loss: 0.5755325120687484
  batch 150 loss: 0.5356997972726822
  batch 200 loss: 0.5590131145715713
  batch 250 loss: 0.5629020953178405
  batch 300 loss: 0.5511711603403091
  batch 350 loss: 0.5516928279399872
  batch 400 loss: 0.5877837854623794
  batch 450 loss: 0.5764439076185226
  batch 500 loss: 0.5573545074462891
  batch 550 loss: 0.5820297068357467
  batch 600 loss: 0.576450936794281
  batch 650 loss: 0.5951841461658478
  batch 700 loss: 0.5982563507556915
  batch 750 loss: 0.5559454250335694
  batch 800 loss: 0.5619464826583862
  batch 850 loss: 0.5880206447839736
  batch 900 loss: 0.5876771932840348
avg val loss: 0.8446012139320374
LOSS train 0.58768 valid 0.84460, valid PER 25.72%
EPOCH 14, Learning Rate: 0.175
  batch 50 loss: 0.5031435215473175
  batch 100 loss: 0.4998582899570465
  batch 150 loss: 0.4912215083837509
  batch 200 loss: 0.4752169558405876
  batch 250 loss: 0.4910848021507263
  batch 300 loss: 0.5276811063289643
  batch 350 loss: 0.4678602385520935
  batch 400 loss: 0.496102522611618
  batch 450 loss: 0.5015193504095078
  batch 500 loss: 0.4926013833284378
  batch 550 loss: 0.5143215179443359
  batch 600 loss: 0.46394228100776674
  batch 650 loss: 0.49926026046276095
  batch 700 loss: 0.5311175984144211
  batch 750 loss: 0.4868220365047455
  batch 800 loss: 0.4730130189657211
  batch 850 loss: 0.5013180142641067
  batch 900 loss: 0.5020342999696732
avg val loss: 0.8329447507858276
LOSS train 0.50203 valid 0.83294, valid PER 24.98%
EPOCH 15, Learning Rate: 0.175
  batch 50 loss: 0.4615630328655243
  batch 100 loss: 0.44924528181552886
  batch 150 loss: 0.4580236804485321
  batch 200 loss: 0.46935818403959273
  batch 250 loss: 0.48339257419109344
  batch 300 loss: 0.4612525117397308
  batch 350 loss: 0.4739739632606506
  batch 400 loss: 0.4614901405572891
  batch 450 loss: 0.45339065372943876
  batch 500 loss: 0.4378147691488266
  batch 550 loss: 0.4582042419910431
  batch 600 loss: 0.490815612077713
  batch 650 loss: 0.48494019091129303
  batch 700 loss: 0.4934486299753189
  batch 750 loss: 0.4829789620637894
  batch 800 loss: 0.4703431302309036
  batch 850 loss: 0.44646362006664275
  batch 900 loss: 0.4804507461190224
avg val loss: 0.8477661609649658
LOSS train 0.48045 valid 0.84777, valid PER 24.82%
EPOCH 16, Learning Rate: 0.0875
  batch 50 loss: 0.4462193405628204
  batch 100 loss: 0.42030456870794297
  batch 150 loss: 0.4218427836894989
  batch 200 loss: 0.4169665431976318
  batch 250 loss: 0.42496195673942566
  batch 300 loss: 0.42153894186019897
  batch 350 loss: 0.4229531955718994
  batch 400 loss: 0.43156279504299166
  batch 450 loss: 0.43612231731414797
  batch 500 loss: 0.40821070462465286
  batch 550 loss: 0.4080176842212677
  batch 600 loss: 0.4145383197069168
  batch 650 loss: 0.4251982182264328
  batch 700 loss: 0.40906901746988295
  batch 750 loss: 0.433658213019371
  batch 800 loss: 0.429454425573349
  batch 850 loss: 0.4204783955216408
  batch 900 loss: 0.4263908785581589
avg val loss: 0.8462192416191101
LOSS train 0.42639 valid 0.84622, valid PER 24.40%
EPOCH 17, Learning Rate: 0.04375
  batch 50 loss: 0.3991496914625168
  batch 100 loss: 0.40911277890205383
  batch 150 loss: 0.3887571665644646
  batch 200 loss: 0.3829773274064064
  batch 250 loss: 0.4180175852775574
  batch 300 loss: 0.3957591241598129
  batch 350 loss: 0.37803096443414685
  batch 400 loss: 0.42070468962192537
  batch 450 loss: 0.3950063580274582
  batch 500 loss: 0.3802426195144653
  batch 550 loss: 0.39793306291103364
  batch 600 loss: 0.4091166174411774
  batch 650 loss: 0.39821565747261045
  batch 700 loss: 0.39032339453697207
  batch 750 loss: 0.3785510188341141
  batch 800 loss: 0.3781440055370331
  batch 850 loss: 0.4058449527621269
  batch 900 loss: 0.38048217833042147
avg val loss: 0.8537076115608215
LOSS train 0.38048 valid 0.85371, valid PER 24.20%
EPOCH 18, Learning Rate: 0.021875
  batch 50 loss: 0.3801661151647568
  batch 100 loss: 0.3808336326479912
  batch 150 loss: 0.39604693740606306
  batch 200 loss: 0.3860442346334457
  batch 250 loss: 0.4036176347732544
  batch 300 loss: 0.36666984885931014
  batch 350 loss: 0.3768991887569427
  batch 400 loss: 0.3698125520348549
  batch 450 loss: 0.387242391705513
  batch 500 loss: 0.3822493389248848
  batch 550 loss: 0.3995911702513695
  batch 600 loss: 0.3693834286928177
  batch 650 loss: 0.35758036583662034
  batch 700 loss: 0.4031690561771393
  batch 750 loss: 0.3660556399822235
  batch 800 loss: 0.38004217356443404
  batch 850 loss: 0.3878116926550865
  batch 900 loss: 0.39214012801647186
avg val loss: 0.8564453721046448
LOSS train 0.39214 valid 0.85645, valid PER 24.33%
EPOCH 19, Learning Rate: 0.0109375
  batch 50 loss: 0.37348475933074954
  batch 100 loss: 0.3587143683433533
  batch 150 loss: 0.3774882790446281
  batch 200 loss: 0.3789652022719383
  batch 250 loss: 0.3821460989117622
  batch 300 loss: 0.38784605234861375
  batch 350 loss: 0.3538009575009346
  batch 400 loss: 0.36239692091941833
  batch 450 loss: 0.3811235162615776
  batch 500 loss: 0.3716144481301308
  batch 550 loss: 0.3714847919344902
  batch 600 loss: 0.3560093647241592
  batch 650 loss: 0.3983318826556206
  batch 700 loss: 0.373538970053196
  batch 750 loss: 0.358516056239605
  batch 800 loss: 0.38025782495737076
  batch 850 loss: 0.3732034105062485
  batch 900 loss: 0.3747320520877838
avg val loss: 0.8577213883399963
LOSS train 0.37473 valid 0.85772, valid PER 24.41%
EPOCH 20, Learning Rate: 0.00546875
  batch 50 loss: 0.38794092476367953
  batch 100 loss: 0.36846997290849687
  batch 150 loss: 0.35346239238977434
  batch 200 loss: 0.36043221682310106
  batch 250 loss: 0.3764763754606247
  batch 300 loss: 0.3698501694202423
  batch 350 loss: 0.3554438951611519
  batch 400 loss: 0.37751538038253785
  batch 450 loss: 0.38567770808935165
  batch 500 loss: 0.3500544962286949
  batch 550 loss: 0.40178918033838273
  batch 600 loss: 0.35022097408771513
  batch 650 loss: 0.3692708578705788
  batch 700 loss: 0.36599098980426786
  batch 750 loss: 0.3573044949769974
  batch 800 loss: 0.3838945198059082
  batch 850 loss: 0.37339029550552366
  batch 900 loss: 0.36583141058683394
avg val loss: 0.8591909408569336
LOSS train 0.36583 valid 0.85919, valid PER 24.44%
Training finished in 8.0 minutes.
Model saved to checkpoints/20231210_135512/model_14
Loading model from checkpoints/20231210_135512/model_14
SUB: 15.29%, DEL: 17.81%, INS: 1.40%, COR: 66.90%, PER: 34.51%
