Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=512, concat=1, lr=0.7, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 2240552
EPOCH 1, Learning Rate: 0.7
  batch 50 loss: 4.656060767173767
  batch 100 loss: 3.24462037563324
  batch 150 loss: 3.03351065158844
  batch 200 loss: 2.764227952957153
  batch 250 loss: 2.624539408683777
  batch 300 loss: 2.449502024650574
  batch 350 loss: 2.3695532941818236
  batch 400 loss: 2.306422848701477
  batch 450 loss: 2.2126345825195313
  batch 500 loss: 2.1237405371665954
  batch 550 loss: 2.0830162286758425
  batch 600 loss: 2.0003173255920412
  batch 650 loss: 1.9178040409088135
  batch 700 loss: 1.9123270750045775
  batch 750 loss: 1.860134482383728
  batch 800 loss: 1.838042016029358
  batch 850 loss: 1.8069607424736023
  batch 900 loss: 1.7488070464134216
running loss: 40.78654897212982
LOSS train 1.74881 valid 1.67101, valid PER 64.80%
EPOCH 2, Learning Rate: 0.7
  batch 50 loss: 1.7189420366287231
  batch 100 loss: 1.656712999343872
  batch 150 loss: 1.6557370948791503
  batch 200 loss: 1.6590020751953125
  batch 250 loss: 1.6956717562675476
  batch 300 loss: 1.6036439895629884
  batch 350 loss: 1.54666565656662
  batch 400 loss: 1.5618778133392335
  batch 450 loss: 1.5133841681480407
  batch 500 loss: 1.5776897883415222
  batch 550 loss: 1.5226954317092896
  batch 600 loss: 1.493381814956665
  batch 650 loss: 1.5319243788719177
  batch 700 loss: 1.5079616045951842
  batch 750 loss: 1.4943278861045837
  batch 800 loss: 1.425399694442749
  batch 850 loss: 1.4345053339004517
  batch 900 loss: 1.4634387850761414
running loss: 34.25506520271301
LOSS train 1.46344 valid 1.40017, valid PER 50.53%
EPOCH 3, Learning Rate: 0.7
  batch 50 loss: 1.410271098613739
  batch 100 loss: 1.3826074838638305
  batch 150 loss: 1.3836747241020202
  batch 200 loss: 1.3497261428833007
  batch 250 loss: 1.3646835279464722
  batch 300 loss: 1.3432346606254577
  batch 350 loss: 1.4027453899383544
  batch 400 loss: 1.3564689993858337
  batch 450 loss: 1.3534021627902986
  batch 500 loss: 1.3293801975250243
  batch 550 loss: 1.3291186928749084
  batch 600 loss: 1.3207264196872712
  batch 650 loss: 1.281754938364029
  batch 700 loss: 1.3071506011486054
  batch 750 loss: 1.3668039429187775
  batch 800 loss: 1.27954261302948
  batch 850 loss: 1.299026710987091
  batch 900 loss: 1.218136224746704
running loss: 29.22859799861908
LOSS train 1.21814 valid 1.28717, valid PER 42.09%
EPOCH 4, Learning Rate: 0.7
  batch 50 loss: 1.2361626982688905
  batch 100 loss: 1.2512011635303497
  batch 150 loss: 1.174646726846695
  batch 200 loss: 1.2189352464675904
  batch 250 loss: 1.220696827173233
  batch 300 loss: 1.2193945920467377
  batch 350 loss: 1.136826149225235
  batch 400 loss: 1.1845643723011017
  batch 450 loss: 1.1851924133300782
  batch 500 loss: 1.1625700569152833
  batch 550 loss: 1.1777779424190522
  batch 600 loss: 1.1830691516399383
  batch 650 loss: 1.1550235676765441
  batch 700 loss: 1.1506425476074218
  batch 750 loss: 1.124919100999832
  batch 800 loss: 1.0928629052639007
  batch 850 loss: 1.1205393397808074
  batch 900 loss: 1.1722442662715913
running loss: 26.831695914268494
LOSS train 1.17224 valid 1.10693, valid PER 34.80%
EPOCH 5, Learning Rate: 0.7
  batch 50 loss: 1.0670944583415984
  batch 100 loss: 1.065423321723938
  batch 150 loss: 1.1252331781387328
  batch 200 loss: 1.0600315976142882
  batch 250 loss: 1.0684179162979126
  batch 300 loss: 1.0756174659729003
  batch 350 loss: 1.0742032194137574
  batch 400 loss: 1.0753492820262909
  batch 450 loss: 1.0458900678157805
  batch 500 loss: 1.043808375597
  batch 550 loss: 1.043205807209015
  batch 600 loss: 1.0979618752002716
  batch 650 loss: 1.0601420164108277
  batch 700 loss: 1.0821259796619416
  batch 750 loss: 0.9940771305561066
  batch 800 loss: 1.054154896736145
  batch 850 loss: 1.068025417327881
  batch 900 loss: 1.0591900253295898
running loss: 24.253114819526672
LOSS train 1.05919 valid 1.01432, valid PER 32.65%
EPOCH 6, Learning Rate: 0.7
  batch 50 loss: 1.018499151468277
  batch 100 loss: 0.9749430191516876
  batch 150 loss: 0.9602922546863556
  batch 200 loss: 0.992040843963623
  batch 250 loss: 1.0197745597362518
  batch 300 loss: 0.9823008596897125
  batch 350 loss: 0.9961747193336487
  batch 400 loss: 0.9912387835979461
  batch 450 loss: 0.9958245003223419
  batch 500 loss: 0.9907610011100769
  batch 550 loss: 1.0097232675552368
  batch 600 loss: 0.9473788869380951
  batch 650 loss: 0.9876089358329773
  batch 700 loss: 0.9808326649665833
  batch 750 loss: 0.9893144595623017
  batch 800 loss: 0.9852977931499481
  batch 850 loss: 0.9556688523292541
  batch 900 loss: 0.9656051206588745
running loss: 23.44968956708908
LOSS train 0.96561 valid 1.00700, valid PER 31.89%
EPOCH 7, Learning Rate: 0.7
  batch 50 loss: 0.9446260058879852
  batch 100 loss: 0.9253901755809784
  batch 150 loss: 0.9410545444488525
  batch 200 loss: 0.9159506499767304
  batch 250 loss: 0.8878787434101105
  batch 300 loss: 0.9144539368152619
  batch 350 loss: 0.922337064743042
  batch 400 loss: 0.9208507657051086
  batch 450 loss: 0.9427341485023498
  batch 500 loss: 0.9440319991111755
  batch 550 loss: 0.9324293887615204
  batch 600 loss: 0.9273574602603912
  batch 650 loss: 0.9306429898738862
  batch 700 loss: 0.9576123154163361
  batch 750 loss: 0.9094055330753327
  batch 800 loss: 0.9243414318561554
  batch 850 loss: 0.9338673722743988
  batch 900 loss: 0.9614840996265411
running loss: 22.07408231496811
LOSS train 0.96148 valid 0.96252, valid PER 29.87%
EPOCH 8, Learning Rate: 0.7
  batch 50 loss: 0.8547986125946045
  batch 100 loss: 0.8521575045585632
  batch 150 loss: 0.8552747595310212
  batch 200 loss: 0.8530231499671936
  batch 250 loss: 0.8782976925373077
  batch 300 loss: 0.8317578077316284
  batch 350 loss: 0.9206891965866089
  batch 400 loss: 0.8331423461437225
  batch 450 loss: 0.8975601160526275
  batch 500 loss: 0.9225934278964997
  batch 550 loss: 0.8501541709899902
  batch 600 loss: 0.891341518163681
  batch 650 loss: 0.9163795685768128
  batch 700 loss: 0.8660517501831054
  batch 750 loss: 0.8680051112174988
  batch 800 loss: 0.8924853813648224
  batch 850 loss: 0.8699393963813782
  batch 900 loss: 0.8796333944797516
running loss: 19.93869984149933
LOSS train 0.87963 valid 0.93763, valid PER 29.04%
EPOCH 9, Learning Rate: 0.7
  batch 50 loss: 0.7563840025663375
  batch 100 loss: 0.8001280152797698
  batch 150 loss: 0.8353587174415589
  batch 200 loss: 0.7847972452640534
  batch 250 loss: 0.8346282291412354
  batch 300 loss: 0.8371338188648224
  batch 350 loss: 0.8353863310813904
  batch 400 loss: 0.8373824048042298
  batch 450 loss: 0.817510513663292
  batch 500 loss: 0.8303853046894073
  batch 550 loss: 0.8445952212810517
  batch 600 loss: 0.8693674623966217
  batch 650 loss: 0.848996514081955
  batch 700 loss: 0.8236418044567109
  batch 750 loss: 0.8118632012605667
  batch 800 loss: 0.8511150181293488
  batch 850 loss: 0.8700279688835144
  batch 900 loss: 0.8090893983840942
running loss: 20.241124749183655
LOSS train 0.80909 valid 0.93384, valid PER 28.82%
EPOCH 10, Learning Rate: 0.35
  batch 50 loss: 0.7070769369602203
  batch 100 loss: 0.7091404765844345
  batch 150 loss: 0.7099086654186249
  batch 200 loss: 0.706705447435379
  batch 250 loss: 0.7090602362155914
  batch 300 loss: 0.6735106182098388
  batch 350 loss: 0.6971419477462768
  batch 400 loss: 0.638497342467308
  batch 450 loss: 0.6569850724935532
  batch 500 loss: 0.6818502253293991
  batch 550 loss: 0.705205705165863
  batch 600 loss: 0.6829724812507629
  batch 650 loss: 0.6842188900709152
  batch 700 loss: 0.704964856505394
  batch 750 loss: 0.6797215569019318
  batch 800 loss: 0.6988571798801422
  batch 850 loss: 0.7122025442123413
  batch 900 loss: 0.6814426922798157
running loss: 16.591409027576447
LOSS train 0.68144 valid 0.85737, valid PER 26.57%
EPOCH 11, Learning Rate: 0.35
  batch 50 loss: 0.6208374172449111
  batch 100 loss: 0.6016148620843887
  batch 150 loss: 0.6120789813995361
  batch 200 loss: 0.651372155547142
  batch 250 loss: 0.6606936007738113
  batch 300 loss: 0.6044418853521347
  batch 350 loss: 0.6335043621063232
  batch 400 loss: 0.6608356565237046
  batch 450 loss: 0.6637358784675598
  batch 500 loss: 0.6381762093305587
  batch 550 loss: 0.6451107478141784
  batch 600 loss: 0.6287469756603241
  batch 650 loss: 0.6735695540904999
  batch 700 loss: 0.6269800937175751
  batch 750 loss: 0.6392657512426376
  batch 800 loss: 0.671003440618515
  batch 850 loss: 0.6827560555934906
  batch 900 loss: 0.6699003154039382
running loss: 16.01116392016411
LOSS train 0.66990 valid 0.84991, valid PER 26.13%
EPOCH 12, Learning Rate: 0.35
  batch 50 loss: 0.6128362506628037
  batch 100 loss: 0.586523381471634
  batch 150 loss: 0.5522271662950515
  batch 200 loss: 0.5837292921543121
  batch 250 loss: 0.5943353068828583
  batch 300 loss: 0.6045821344852448
  batch 350 loss: 0.5974428564310074
  batch 400 loss: 0.619108943939209
  batch 450 loss: 0.6240765178203582
  batch 500 loss: 0.6256610715389251
  batch 550 loss: 0.5780620175600052
  batch 600 loss: 0.6168682563304901
  batch 650 loss: 0.6324779665470124
  batch 700 loss: 0.6311343276500702
  batch 750 loss: 0.5935790503025055
  batch 800 loss: 0.6269793671369552
  batch 850 loss: 0.658244469165802
  batch 900 loss: 0.6344486999511719
running loss: 15.306961178779602
LOSS train 0.63445 valid 0.84624, valid PER 26.34%
EPOCH 13, Learning Rate: 0.35
  batch 50 loss: 0.5508189707994461
  batch 100 loss: 0.5716408288478851
  batch 150 loss: 0.5394320505857467
  batch 200 loss: 0.5956367564201355
  batch 250 loss: 0.5675385493040085
  batch 300 loss: 0.567117853462696
  batch 350 loss: 0.5578370493650436
  batch 400 loss: 0.5894666802883148
  batch 450 loss: 0.593518139719963
  batch 500 loss: 0.571690468788147
  batch 550 loss: 0.6016034054756164
  batch 600 loss: 0.5826006633043289
  batch 650 loss: 0.5965038919448853
  batch 700 loss: 0.6135747092962265
  batch 750 loss: 0.5740321266651154
  batch 800 loss: 0.5789721387624741
  batch 850 loss: 0.6001338654756546
  batch 900 loss: 0.6231639611721039
running loss: 14.786162763834
LOSS train 0.62316 valid 0.86996, valid PER 25.47%
EPOCH 14, Learning Rate: 0.35
  batch 50 loss: 0.5341088509559632
  batch 100 loss: 0.5201674181222916
  batch 150 loss: 0.5318124485015869
  batch 200 loss: 0.5302490681409836
  batch 250 loss: 0.538439090847969
  batch 300 loss: 0.550993548631668
  batch 350 loss: 0.5342237269878387
  batch 400 loss: 0.5331461769342423
  batch 450 loss: 0.5569981068372727
  batch 500 loss: 0.544351070523262
  batch 550 loss: 0.5676804798841476
  batch 600 loss: 0.5396598225831986
  batch 650 loss: 0.5494052344560623
  batch 700 loss: 0.5936097460985184
  batch 750 loss: 0.5585057538747787
  batch 800 loss: 0.5584066951274872
  batch 850 loss: 0.5830309969186783
  batch 900 loss: 0.5752736687660217
running loss: 14.544144600629807
LOSS train 0.57527 valid 0.87195, valid PER 26.17%
EPOCH 15, Learning Rate: 0.35
  batch 50 loss: 0.48426330149173735
  batch 100 loss: 0.4948474949598312
  batch 150 loss: 0.5166330474615097
  batch 200 loss: 0.5225608223676681
  batch 250 loss: 0.5365838938951493
  batch 300 loss: 0.5096400338411331
  batch 350 loss: 0.5373997122049332
  batch 400 loss: 0.5254128384590149
  batch 450 loss: 0.5134899091720581
  batch 500 loss: 0.49504821598529813
  batch 550 loss: 0.5348235619068146
  batch 600 loss: 0.5660845988988876
  batch 650 loss: 0.5496402764320374
  batch 700 loss: 0.5445279914140702
  batch 750 loss: 0.526744841337204
  batch 800 loss: 0.5330873411893845
  batch 850 loss: 0.5066933327913284
  batch 900 loss: 0.5361356312036514
running loss: 13.117018073797226
LOSS train 0.53614 valid 0.90011, valid PER 26.27%
EPOCH 16, Learning Rate: 0.35
  batch 50 loss: 0.501769779920578
  batch 100 loss: 0.47845728278160093
  batch 150 loss: 0.4797292411327362
  batch 200 loss: 0.46404428124427793
  batch 250 loss: 0.48515306413173676
  batch 300 loss: 0.4890805208683014
  batch 350 loss: 0.49819247126579286
  batch 400 loss: 0.4951802158355713
  batch 450 loss: 0.5066072630882263
  batch 500 loss: 0.4989392393827438
  batch 550 loss: 0.4972343826293945
  batch 600 loss: 0.5002466088533402
  batch 650 loss: 0.5293492060899735
  batch 700 loss: 0.5163811582326889
  batch 750 loss: 0.515830151438713
  batch 800 loss: 0.520159729719162
  batch 850 loss: 0.5108699399232864
  batch 900 loss: 0.5300901263952256
running loss: 12.381188362836838
LOSS train 0.53009 valid 0.89498, valid PER 25.76%
EPOCH 17, Learning Rate: 0.35
  batch 50 loss: 0.4555200201272964
  batch 100 loss: 0.458298864364624
  batch 150 loss: 0.4505742532014847
  batch 200 loss: 0.456051185131073
  batch 250 loss: 0.48308732330799103
  batch 300 loss: 0.4685034382343292
  batch 350 loss: 0.44091947495937345
  batch 400 loss: 0.4914195340871811
  batch 450 loss: 0.4698236703872681
  batch 500 loss: 0.4478959000110626
  batch 550 loss: 0.4819390362501144
  batch 600 loss: 0.4937360244989395
  batch 650 loss: 0.4836007696390152
  batch 700 loss: 0.46351953089237213
  batch 750 loss: 0.4743244171142578
  batch 800 loss: 0.4852820831537247
  batch 850 loss: 0.4882615041732788
  batch 900 loss: 0.4771492844820023
running loss: 11.993665039539337
LOSS train 0.47715 valid 0.90790, valid PER 25.42%
EPOCH 18, Learning Rate: 0.35
  batch 50 loss: 0.40892703920602796
  batch 100 loss: 0.438192725777626
  batch 150 loss: 0.4463595175743103
  batch 200 loss: 0.43517453253269195
  batch 250 loss: 0.44725940108299256
  batch 300 loss: 0.41682921469211576
  batch 350 loss: 0.4337106305360794
  batch 400 loss: 0.4408847290277481
  batch 450 loss: 0.45621142387390134
  batch 500 loss: 0.4398679527640343
  batch 550 loss: 0.4556916409730911
  batch 600 loss: 0.44698929399251935
  batch 650 loss: 0.4411567774415016
  batch 700 loss: 0.4754246425628662
  batch 750 loss: 0.44708067178726196
  batch 800 loss: 0.4560132467746735
  batch 850 loss: 0.47139703333377836
  batch 900 loss: 0.47062786102294923
running loss: 11.307010650634766
LOSS train 0.47063 valid 0.92282, valid PER 25.24%
EPOCH 19, Learning Rate: 0.35
  batch 50 loss: 0.40316278368234637
  batch 100 loss: 0.3933142814040184
  batch 150 loss: 0.4040514463186264
  batch 200 loss: 0.4074708414077759
  batch 250 loss: 0.4229392963647842
  batch 300 loss: 0.4095150798559189
  batch 350 loss: 0.4009015840291977
  batch 400 loss: 0.41658823311328885
  batch 450 loss: 0.4398916101455688
  batch 500 loss: 0.4263701122999191
  batch 550 loss: 0.4107358258962631
  batch 600 loss: 0.4080361759662628
  batch 650 loss: 0.4525340169668198
  batch 700 loss: 0.4267725133895874
  batch 750 loss: 0.42736624598503115
  batch 800 loss: 0.44476433873176574
  batch 850 loss: 0.44578367233276367
  batch 900 loss: 0.4534913319349289
running loss: 10.753611505031586
LOSS train 0.45349 valid 0.93980, valid PER 25.70%
EPOCH 20, Learning Rate: 0.35
  batch 50 loss: 0.3943081194162369
  batch 100 loss: 0.3634272637963295
  batch 150 loss: 0.37204063951969146
  batch 200 loss: 0.3815338996052742
  batch 250 loss: 0.3860371309518814
  batch 300 loss: 0.39198285073041916
  batch 350 loss: 0.37110920310020445
  batch 400 loss: 0.40462301969528197
  batch 450 loss: 0.4143570774793625
  batch 500 loss: 0.38888776302337646
  batch 550 loss: 0.4258259844779968
  batch 600 loss: 0.39516148120164873
  batch 650 loss: 0.4106897956132889
  batch 700 loss: 0.4097650945186615
  batch 750 loss: 0.3942232164740562
  batch 800 loss: 0.43931201577186585
  batch 850 loss: 0.44115768134593963
  batch 900 loss: 0.42477412611246107
running loss: 10.224100843071938
LOSS train 0.42477 valid 0.97128, valid PER 25.58%
Training finished in 8.0 minutes.
Model saved to checkpoints/20231210_043329/model_12
Loading model from checkpoints/20231210_043329/model_12
SUB: 14.60%, DEL: 11.94%, INS: 1.71%, COR: 73.46%, PER: 28.25%
