Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.3, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 83496
EPOCH 1, Learning Rate: 0.3
  batch 50 loss: 5.2366985511779784
  batch 100 loss: 3.276135482788086
  batch 150 loss: 3.2115975189208985
  batch 200 loss: 3.114946002960205
  batch 250 loss: 2.9748262929916383
  batch 300 loss: 2.765314302444458
  batch 350 loss: 2.669601492881775
  batch 400 loss: 2.5901646518707278
  batch 450 loss: 2.5197444438934324
  batch 500 loss: 2.4219211626052854
  batch 550 loss: 2.360851125717163
  batch 600 loss: 2.3233680152893066
  batch 650 loss: 2.2562528467178344
  batch 700 loss: 2.2568886423110963
  batch 750 loss: 2.2254508900642396
  batch 800 loss: 2.1838918423652647
  batch 850 loss: 2.1456446957588198
  batch 900 loss: 2.1071075010299682
running loss: 50.08555459976196
LOSS train 2.10711 valid 2.09106, valid PER 78.88%
EPOCH 2, Learning Rate: 0.3
  batch 50 loss: 2.078532750606537
  batch 100 loss: 2.0280225324630736
  batch 150 loss: 1.988780791759491
  batch 200 loss: 2.016519525051117
  batch 250 loss: 1.9991826605796814
  batch 300 loss: 1.9726079487800598
  batch 350 loss: 1.8887451314926147
  batch 400 loss: 1.8995126175880432
  batch 450 loss: 1.8494080090522766
  batch 500 loss: 1.8750570917129517
  batch 550 loss: 1.8752529096603394
  batch 600 loss: 1.8201983547210694
  batch 650 loss: 1.84915935754776
  batch 700 loss: 1.8108161497116089
  batch 750 loss: 1.8084401988983154
  batch 800 loss: 1.7539514923095703
  batch 850 loss: 1.7494711017608642
  batch 900 loss: 1.7524527025222778
running loss: 41.32462430000305
LOSS train 1.75245 valid 1.67737, valid PER 66.66%
EPOCH 3, Learning Rate: 0.3
  batch 50 loss: 1.720750057697296
  batch 100 loss: 1.7041478991508483
  batch 150 loss: 1.6955197477340698
  batch 200 loss: 1.676596441268921
  batch 250 loss: 1.664050166606903
  batch 300 loss: 1.665115008354187
  batch 350 loss: 1.69505348443985
  batch 400 loss: 1.6782225465774536
  batch 450 loss: 1.6423080992698669
  batch 500 loss: 1.623562490940094
  batch 550 loss: 1.608059411048889
  batch 600 loss: 1.5980834007263183
  batch 650 loss: 1.5606532764434815
  batch 700 loss: 1.581979465484619
  batch 750 loss: 1.620781764984131
  batch 800 loss: 1.5588475966453552
  batch 850 loss: 1.6009990715980529
  batch 900 loss: 1.532707302570343
running loss: 37.22229862213135
LOSS train 1.53271 valid 1.49054, valid PER 51.83%
EPOCH 4, Learning Rate: 0.3
  batch 50 loss: 1.5409908032417297
  batch 100 loss: 1.5694722771644591
  batch 150 loss: 1.4896974754333496
  batch 200 loss: 1.5379948616027832
  batch 250 loss: 1.5353456163406372
  batch 300 loss: 1.5641237592697144
  batch 350 loss: 1.4597789525985718
  batch 400 loss: 1.5192710995674132
  batch 450 loss: 1.5003884983062745
  batch 500 loss: 1.4668550515174865
  batch 550 loss: 1.508729875087738
  batch 600 loss: 1.5123540663719177
  batch 650 loss: 1.5059524869918823
  batch 700 loss: 1.4569159865379333
  batch 750 loss: 1.4449122714996339
  batch 800 loss: 1.4607616782188415
  batch 850 loss: 1.4519728851318359
  batch 900 loss: 1.4847870993614196
running loss: 34.89463198184967
LOSS train 1.48479 valid 1.38229, valid PER 45.69%
EPOCH 5, Learning Rate: 0.3
  batch 50 loss: 1.4371525979042052
  batch 100 loss: 1.4205133128166199
  batch 150 loss: 1.4761545753479004
  batch 200 loss: 1.4090893828868867
  batch 250 loss: 1.3935548973083496
  batch 300 loss: 1.4230520844459533
  batch 350 loss: 1.4403175616264343
  batch 400 loss: 1.4583523440361024
  batch 450 loss: 1.4232369923591615
  batch 500 loss: 1.4440602850914002
  batch 550 loss: 1.3955507969856262
  batch 600 loss: 1.4485510897636413
  batch 650 loss: 1.4002533435821534
  batch 700 loss: 1.4408956313133239
  batch 750 loss: 1.3752025747299195
  batch 800 loss: 1.4004005622863769
  batch 850 loss: 1.411807119846344
  batch 900 loss: 1.4167809677124024
running loss: 32.90737581253052
LOSS train 1.41678 valid 1.36020, valid PER 44.86%
EPOCH 6, Learning Rate: 0.3
  batch 50 loss: 1.4436710929870606
  batch 100 loss: 1.372130126953125
  batch 150 loss: 1.4138140273094177
  batch 200 loss: 1.3976138019561768
  batch 250 loss: 1.39050311088562
  batch 300 loss: 1.3639486408233643
  batch 350 loss: 1.3873075127601624
  batch 400 loss: 1.3657661747932435
  batch 450 loss: 1.3909075140953064
  batch 500 loss: 1.3638038444519043
  batch 550 loss: 1.3913083148002625
  batch 600 loss: 1.3757756495475768
  batch 650 loss: 1.4083079051971437
  batch 700 loss: 1.361524600982666
  batch 750 loss: 1.347381160259247
  batch 800 loss: 1.3799865698814393
  batch 850 loss: 1.3455949306488038
  batch 900 loss: 1.3777894520759582
running loss: 33.117313861846924
LOSS train 1.37779 valid 1.37032, valid PER 43.49%
EPOCH 7, Learning Rate: 0.15
  batch 50 loss: 1.3436137175559997
  batch 100 loss: 1.3184880590438843
  batch 150 loss: 1.2954853987693786
  batch 200 loss: 1.280906343460083
  batch 250 loss: 1.2834907603263854
  batch 300 loss: 1.262635679244995
  batch 350 loss: 1.269101402759552
  batch 400 loss: 1.2826050567626952
  batch 450 loss: 1.2532358658313751
  batch 500 loss: 1.2452325439453125
  batch 550 loss: 1.2526261866092683
  batch 600 loss: 1.2591296195983888
  batch 650 loss: 1.2588008975982665
  batch 700 loss: 1.2646466314792633
  batch 750 loss: 1.2546746611595154
  batch 800 loss: 1.2264318287372589
  batch 850 loss: 1.2690160810947417
  batch 900 loss: 1.3298471879959106
running loss: 29.67014616727829
LOSS train 1.32985 valid 1.24075, valid PER 41.03%
EPOCH 8, Learning Rate: 0.15
  batch 50 loss: 1.2616306245326996
  batch 100 loss: 1.2369211685657502
  batch 150 loss: 1.2553792154788972
  batch 200 loss: 1.210873258113861
  batch 250 loss: 1.2476520133018494
  batch 300 loss: 1.1895433461666107
  batch 350 loss: 1.2715916848182678
  batch 400 loss: 1.2232878005504608
  batch 450 loss: 1.2763903725147248
  batch 500 loss: 1.261711506843567
  batch 550 loss: 1.2353840208053588
  batch 600 loss: 1.283698649406433
  batch 650 loss: 1.3063382232189178
  batch 700 loss: 1.2208194410800934
  batch 750 loss: 1.24575652718544
  batch 800 loss: 1.2426647412776948
  batch 850 loss: 1.245454570055008
  batch 900 loss: 1.2109540057182313
running loss: 29.663946628570557
LOSS train 1.21095 valid 1.21661, valid PER 39.09%
EPOCH 9, Learning Rate: 0.15
  batch 50 loss: 1.195527013540268
  batch 100 loss: 1.2436529862880707
  batch 150 loss: 1.2288432288169862
  batch 200 loss: 1.1850630509853364
  batch 250 loss: 1.22315962433815
  batch 300 loss: 1.2264507913589477
  batch 350 loss: 1.2375683760643006
  batch 400 loss: 1.2236107468605042
  batch 450 loss: 1.2360006749629975
  batch 500 loss: 1.1895192873477936
  batch 550 loss: 1.2525212168693542
  batch 600 loss: 1.224636549949646
  batch 650 loss: 1.221089631319046
  batch 700 loss: 1.206398457288742
  batch 750 loss: 1.2221589040756227
  batch 800 loss: 1.22542662858963
  batch 850 loss: 1.2823547756671905
  batch 900 loss: 1.225525369644165
running loss: 29.233949303627014
LOSS train 1.22553 valid 1.18141, valid PER 38.72%
EPOCH 10, Learning Rate: 0.15
  batch 50 loss: 1.2020573568344117
  batch 100 loss: 1.1861629331111907
  batch 150 loss: 1.222214869260788
  batch 200 loss: 1.2154465353488921
  batch 250 loss: 1.2132964479923247
  batch 300 loss: 1.1595760357379914
  batch 350 loss: 1.1942687785625459
  batch 400 loss: 1.1901950287818908
  batch 450 loss: 1.157177324295044
  batch 500 loss: 1.205487583875656
  batch 550 loss: 1.1969153678417206
  batch 600 loss: 1.182534520626068
  batch 650 loss: 1.1851474797725678
  batch 700 loss: 1.203173645734787
  batch 750 loss: 1.1600724852085114
  batch 800 loss: 1.216102590560913
  batch 850 loss: 1.2165863072872163
  batch 900 loss: 1.236011998653412
running loss: 28.871116280555725
LOSS train 1.23601 valid 1.18863, valid PER 39.33%
EPOCH 11, Learning Rate: 0.15
  batch 50 loss: 1.1669904112815856
  batch 100 loss: 1.1874571979045867
  batch 150 loss: 1.1830046677589416
  batch 200 loss: 1.2074027395248412
  batch 250 loss: 1.2107179796695708
  batch 300 loss: 1.159582438468933
  batch 350 loss: 1.1944029355049133
  batch 400 loss: 1.1960144519805909
  batch 450 loss: 1.1941708052158355
  batch 500 loss: 1.1768126904964447
  batch 550 loss: 1.1877625226974486
  batch 600 loss: 1.1764785850048065
  batch 650 loss: 1.2247424459457397
  batch 700 loss: 1.152977658510208
  batch 750 loss: 1.1676814448833466
  batch 800 loss: 1.1991711962223053
  batch 850 loss: 1.2030025959014892
  batch 900 loss: 1.1986478805541991
running loss: 28.76840990781784
LOSS train 1.19865 valid 1.14965, valid PER 37.95%
EPOCH 12, Learning Rate: 0.15
  batch 50 loss: 1.1581578302383422
  batch 100 loss: 1.1623848402500152
  batch 150 loss: 1.1574130308628083
  batch 200 loss: 1.1750001013278961
  batch 250 loss: 1.1876045775413513
  batch 300 loss: 1.1737932658195496
  batch 350 loss: 1.1729863858222962
  batch 400 loss: 1.2180551493167877
  batch 450 loss: 1.1944836831092835
  batch 500 loss: 1.1876132452487946
  batch 550 loss: 1.119793313741684
  batch 600 loss: 1.1346845769882201
  batch 650 loss: 1.209951913356781
  batch 700 loss: 1.1723564612865447
  batch 750 loss: 1.1476511585712432
  batch 800 loss: 1.1289409458637238
  batch 850 loss: 1.1809850645065307
  batch 900 loss: 1.2048526346683501
running loss: 27.70849460363388
LOSS train 1.20485 valid 1.14322, valid PER 37.32%
EPOCH 13, Learning Rate: 0.15
  batch 50 loss: 1.1251430213451385
  batch 100 loss: 1.1788221991062164
  batch 150 loss: 1.1231728971004487
  batch 200 loss: 1.1738196563720704
  batch 250 loss: 1.1544997775554657
  batch 300 loss: 1.13772478222847
  batch 350 loss: 1.1671229648590087
  batch 400 loss: 1.1820035314559936
  batch 450 loss: 1.206815938949585
  batch 500 loss: 1.1561813592910766
  batch 550 loss: 1.1567037057876588
  batch 600 loss: 1.1651074707508087
  batch 650 loss: 1.1480100989341735
  batch 700 loss: 1.135434159040451
  batch 750 loss: 1.1340170478820801
  batch 800 loss: 1.132217675447464
  batch 850 loss: 1.1821137177944183
  batch 900 loss: 1.1823031389713288
running loss: 30.142324686050415
LOSS train 1.18230 valid 1.19342, valid PER 37.38%
EPOCH 14, Learning Rate: 0.075
  batch 50 loss: 1.1871237719058991
  batch 100 loss: 1.1586847591400147
  batch 150 loss: 1.1039016783237456
  batch 200 loss: 1.12517031788826
  batch 250 loss: 1.1075491297245026
  batch 300 loss: 1.1348011004924774
  batch 350 loss: 1.0838806784152986
  batch 400 loss: 1.0944036960601806
  batch 450 loss: 1.0970522010326385
  batch 500 loss: 1.1180815672874451
  batch 550 loss: 1.1372406363487244
  batch 600 loss: 1.0713979876041413
  batch 650 loss: 1.138360401391983
  batch 700 loss: 1.1187507355213164
  batch 750 loss: 1.0725561046600343
  batch 800 loss: 1.0406312489509582
  batch 850 loss: 1.098775442838669
  batch 900 loss: 1.0993268418312072
running loss: 27.613928139209747
LOSS train 1.09933 valid 1.10777, valid PER 35.69%
EPOCH 15, Learning Rate: 0.075
  batch 50 loss: 1.0966366803646088
  batch 100 loss: 1.0871087431907653
  batch 150 loss: 1.0759052014350892
  batch 200 loss: 1.1329947018623352
  batch 250 loss: 1.0922094678878784
  batch 300 loss: 1.074391063451767
  batch 350 loss: 1.088144335746765
  batch 400 loss: 1.0678873598575591
  batch 450 loss: 1.0804192531108856
  batch 500 loss: 1.0619619512557983
  batch 550 loss: 1.1221424114704133
  batch 600 loss: 1.0854208374023437
  batch 650 loss: 1.1005083775520326
  batch 700 loss: 1.1323395299911498
  batch 750 loss: 1.105316023826599
  batch 800 loss: 1.0838637018203736
  batch 850 loss: 1.0688239741325378
  batch 900 loss: 1.1065231692790984
running loss: 25.890319347381592
LOSS train 1.10652 valid 1.10172, valid PER 35.82%
EPOCH 16, Learning Rate: 0.075
  batch 50 loss: 1.1182438731193542
  batch 100 loss: 1.0527499580383302
  batch 150 loss: 1.0673818504810333
  batch 200 loss: 1.0765389502048492
  batch 250 loss: 1.1058588647842407
  batch 300 loss: 1.0913287353515626
  batch 350 loss: 1.0777304697036743
  batch 400 loss: 1.06726500749588
  batch 450 loss: 1.102368084192276
  batch 500 loss: 1.057615339756012
  batch 550 loss: 1.0841053307056427
  batch 600 loss: 1.10128941655159
  batch 650 loss: 1.135957635641098
  batch 700 loss: 1.0863367891311646
  batch 750 loss: 1.0910119378566743
  batch 800 loss: 1.0728503227233888
  batch 850 loss: 1.0758420217037201
  batch 900 loss: 1.0786830282211304
running loss: 25.701979160308838
LOSS train 1.07868 valid 1.10220, valid PER 35.60%
EPOCH 17, Learning Rate: 0.075
  batch 50 loss: 1.1048972225189209
  batch 100 loss: 1.0704059839248656
  batch 150 loss: 1.0521005845069886
  batch 200 loss: 1.0570058512687683
  batch 250 loss: 1.0845588874816894
  batch 300 loss: 1.0695586037635803
  batch 350 loss: 1.0529269540309907
  batch 400 loss: 1.1507567262649536
  batch 450 loss: 1.0935745275020599
  batch 500 loss: 1.1030315482616424
  batch 550 loss: 1.0849818682670593
  batch 600 loss: 1.1155059731006622
  batch 650 loss: 1.0737916266918182
  batch 700 loss: 1.0766789662837981
  batch 750 loss: 1.0525563049316407
  batch 800 loss: 1.0424207544326782
  batch 850 loss: 1.067706995010376
  batch 900 loss: 1.0431533002853393
running loss: 26.95083475112915
LOSS train 1.04315 valid 1.08282, valid PER 34.70%
EPOCH 18, Learning Rate: 0.0375
  batch 50 loss: 1.0468110942840576
  batch 100 loss: 1.0489379525184632
  batch 150 loss: 1.0736838757991791
  batch 200 loss: 1.0564276337623597
  batch 250 loss: 1.053500691652298
  batch 300 loss: 1.0414692378044128
  batch 350 loss: 1.065968165397644
  batch 400 loss: 1.029365621805191
  batch 450 loss: 1.0867621731758117
  batch 500 loss: 1.0620401072502137
  batch 550 loss: 1.0487768602371217
  batch 600 loss: 1.0218564033508302
  batch 650 loss: 1.0287304878234864
  batch 700 loss: 1.077382164001465
  batch 750 loss: 1.041979295015335
  batch 800 loss: 1.0417368614673614
  batch 850 loss: 1.0259945821762084
  batch 900 loss: 1.060507835149765
running loss: 24.3330060839653
LOSS train 1.06051 valid 1.07613, valid PER 34.53%
EPOCH 19, Learning Rate: 0.0375
  batch 50 loss: 1.0007860815525056
  batch 100 loss: 0.9979154300689698
  batch 150 loss: 1.02296901345253
  batch 200 loss: 1.0400052225589753
  batch 250 loss: 1.0648959398269653
  batch 300 loss: 1.032186714410782
  batch 350 loss: 1.0242335271835328
  batch 400 loss: 1.0536065900325775
  batch 450 loss: 1.046028240919113
  batch 500 loss: 1.0560933649539948
  batch 550 loss: 1.0276972317695618
  batch 600 loss: 1.0474096536636353
  batch 650 loss: 1.0707427644729615
  batch 700 loss: 1.0042912888526916
  batch 750 loss: 1.020313596725464
  batch 800 loss: 1.0596257722377778
  batch 850 loss: 1.0659004831314087
  batch 900 loss: 1.0387132263183594
running loss: 25.218235969543457
LOSS train 1.03871 valid 1.07020, valid PER 34.28%
EPOCH 20, Learning Rate: 0.01875
  batch 50 loss: 1.0194203174114227
  batch 100 loss: 1.0451165115833283
  batch 150 loss: 1.0272941946983338
  batch 200 loss: 1.0180452907085418
  batch 250 loss: 1.031494767665863
  batch 300 loss: 1.0520803582668306
  batch 350 loss: 0.9892286324501037
  batch 400 loss: 1.0086167454719543
  batch 450 loss: 1.034440234899521
  batch 500 loss: 1.003676438331604
  batch 550 loss: 1.050445283651352
  batch 600 loss: 0.990007336139679
  batch 650 loss: 1.0294678270816804
  batch 700 loss: 1.0352434945106506
  batch 750 loss: 1.0186044871807098
  batch 800 loss: 1.0338866198062897
  batch 850 loss: 1.0289404606819152
  batch 900 loss: 1.028561463356018
running loss: 24.13526278734207
LOSS train 1.02856 valid 1.06198, valid PER 34.21%
Training finished in 2.0 minutes.
Model saved to checkpoints/20231210_041725/model_20
Loading model from checkpoints/20231210_041725/model_20
SUB: 19.45%, DEL: 15.21%, INS: 1.11%, COR: 65.34%, PER: 35.77%
