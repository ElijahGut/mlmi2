Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.3, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 83496
EPOCH 1, Learning Rate: 0.3
  batch 50 loss: 5.236701683998108
  batch 100 loss: 3.2761332893371584
  batch 150 loss: 3.21159637928009
  batch 200 loss: 3.11494487285614
  batch 250 loss: 2.974823694229126
  batch 300 loss: 2.765314064025879
  batch 350 loss: 2.6696023464202883
  batch 400 loss: 2.5901650762557984
  batch 450 loss: 2.519744143486023
  batch 500 loss: 2.4219209861755373
  batch 550 loss: 2.360851454734802
  batch 600 loss: 2.323366618156433
  batch 650 loss: 2.2562532234191894
  batch 700 loss: 2.2568886709213256
  batch 750 loss: 2.2254517388343813
  batch 800 loss: 2.183943190574646
  batch 850 loss: 2.1463675618171694
  batch 900 loss: 2.1071193408966065
avg val loss: 2.09114670753479
LOSS train 2.10712 valid 2.09115, valid PER 78.91%
EPOCH 2, Learning Rate: 0.3
  batch 50 loss: 2.078504993915558
  batch 100 loss: 2.029879138469696
  batch 150 loss: 1.988292157649994
  batch 200 loss: 2.0169157671928404
  batch 250 loss: 1.999237802028656
  batch 300 loss: 1.9724420046806335
  batch 350 loss: 1.8885961413383483
  batch 400 loss: 1.900177400112152
  batch 450 loss: 1.849457302093506
  batch 500 loss: 1.8735824632644653
  batch 550 loss: 1.876486759185791
  batch 600 loss: 1.8202837204933167
  batch 650 loss: 1.8493786144256592
  batch 700 loss: 1.8100215792655945
  batch 750 loss: 1.8082603931427002
  batch 800 loss: 1.7537515997886657
  batch 850 loss: 1.7519293308258057
  batch 900 loss: 1.7539401078224182
avg val loss: 1.6788990497589111
LOSS train 1.75394 valid 1.67890, valid PER 66.50%
EPOCH 3, Learning Rate: 0.3
  batch 50 loss: 1.7224916481971742
  batch 100 loss: 1.70434157371521
  batch 150 loss: 1.6942043018341064
  batch 200 loss: 1.677021324634552
  batch 250 loss: 1.66361332654953
  batch 300 loss: 1.6656473398208618
  batch 350 loss: 1.6955773258209228
  batch 400 loss: 1.6793260407447814
  batch 450 loss: 1.639075825214386
  batch 500 loss: 1.6252354598045349
  batch 550 loss: 1.6093144011497498
  batch 600 loss: 1.5944412493705749
  batch 650 loss: 1.5605164742469788
  batch 700 loss: 1.5840401577949523
  batch 750 loss: 1.6233940458297729
  batch 800 loss: 1.5622759819030763
  batch 850 loss: 1.6039034938812256
  batch 900 loss: 1.5336264443397523
avg val loss: 1.502200722694397
LOSS train 1.53363 valid 1.50220, valid PER 51.99%
EPOCH 4, Learning Rate: 0.3
  batch 50 loss: 1.5448500466346742
  batch 100 loss: 1.5592713093757629
  batch 150 loss: 1.4891855645179748
  batch 200 loss: 1.5342569971084594
  batch 250 loss: 1.5333384156227112
  batch 300 loss: 1.558932864665985
  batch 350 loss: 1.4619094038009643
  batch 400 loss: 1.5302568078041077
  batch 450 loss: 1.518192789554596
  batch 500 loss: 1.4708421802520752
  batch 550 loss: 1.5101616787910461
  batch 600 loss: 1.5155823945999145
  batch 650 loss: 1.506693754196167
  batch 700 loss: 1.4570891284942626
  batch 750 loss: 1.4487618613243103
  batch 800 loss: 1.4493849468231201
  batch 850 loss: 1.454196503162384
  batch 900 loss: 1.4796825766563415
avg val loss: 1.3912208080291748
LOSS train 1.47968 valid 1.39122, valid PER 45.30%
EPOCH 5, Learning Rate: 0.3
  batch 50 loss: 1.4411536574363708
  batch 100 loss: 1.4336574578285217
  batch 150 loss: 1.4633699917793275
  batch 200 loss: 1.4057325387001038
  batch 250 loss: 1.3889410233497619
  batch 300 loss: 1.4326448583602904
  batch 350 loss: 1.439177372455597
  batch 400 loss: 1.44983913898468
  batch 450 loss: 1.432971360683441
  batch 500 loss: 1.4439621829986573
  batch 550 loss: 1.377260365486145
  batch 600 loss: 1.4461609935760498
  batch 650 loss: 1.3951279044151306
  batch 700 loss: 1.4581808567047119
  batch 750 loss: 1.3896606421470643
  batch 800 loss: 1.4438665890693665
  batch 850 loss: 1.4288455605506898
  batch 900 loss: 1.4045447826385498
avg val loss: 1.3310775756835938
LOSS train 1.40454 valid 1.33108, valid PER 43.65%
EPOCH 6, Learning Rate: 0.3
  batch 50 loss: 1.4040538501739501
  batch 100 loss: 1.3708274102210998
  batch 150 loss: 1.3499701523780823
  batch 200 loss: 1.3782242345809936
  batch 250 loss: 1.419847366809845
  batch 300 loss: 1.3687779879570008
  batch 350 loss: 1.3985494112968444
  batch 400 loss: 1.3721295809745788
  batch 450 loss: 1.4146664237976074
  batch 500 loss: 1.3616640782356262
  batch 550 loss: 1.3851390075683594
  batch 600 loss: 1.3866979336738587
  batch 650 loss: 1.405202043056488
  batch 700 loss: 1.3710340631008149
  batch 750 loss: 1.3321909761428834
  batch 800 loss: 1.342827055454254
  batch 850 loss: 1.3278955459594726
  batch 900 loss: 1.3553988122940064
avg val loss: 1.3023594617843628
LOSS train 1.35540 valid 1.30236, valid PER 42.05%
EPOCH 7, Learning Rate: 0.3
  batch 50 loss: 1.3748634362220764
  batch 100 loss: 1.37338618516922
  batch 150 loss: 1.353489580154419
  batch 200 loss: 1.3573290419578552
  batch 250 loss: 1.3536892604827881
  batch 300 loss: 1.337245602607727
  batch 350 loss: 1.344959125518799
  batch 400 loss: 1.3373735046386719
  batch 450 loss: 1.3300481450557708
  batch 500 loss: 1.3177865314483643
  batch 550 loss: 1.3205685198307038
  batch 600 loss: 1.3601342129707337
  batch 650 loss: 1.326745810508728
  batch 700 loss: 1.3346421921253204
  batch 750 loss: 1.2912583374977111
  batch 800 loss: 1.2905639696121216
  batch 850 loss: 1.337606086730957
  batch 900 loss: 1.3741635966300965
avg val loss: 1.2693805694580078
LOSS train 1.37416 valid 1.26938, valid PER 42.01%
EPOCH 8, Learning Rate: 0.3
  batch 50 loss: 1.3163834643363952
  batch 100 loss: 1.2910330867767335
  batch 150 loss: 1.3138893938064575
  batch 200 loss: 1.2747367858886718
  batch 250 loss: 1.317448878288269
  batch 300 loss: 1.264970862865448
  batch 350 loss: 1.336670048236847
  batch 400 loss: 1.2661028957366944
  batch 450 loss: 1.3202047967910766
  batch 500 loss: 1.3413025259971618
  batch 550 loss: 1.2773683595657348
  batch 600 loss: 1.3239503479003907
  batch 650 loss: 1.3565509176254273
  batch 700 loss: 1.3108809530735015
  batch 750 loss: 1.3223335814476014
  batch 800 loss: 1.3079421770572663
  batch 850 loss: 1.3252356433868409
  batch 900 loss: 1.2724441337585448
avg val loss: 1.2192479372024536
LOSS train 1.27244 valid 1.21925, valid PER 39.77%
EPOCH 9, Learning Rate: 0.3
  batch 50 loss: 1.2438901495933532
  batch 100 loss: 1.2839846968650819
  batch 150 loss: 1.2924692058563232
  batch 200 loss: 1.2287828409671784
  batch 250 loss: 1.2729687488079071
  batch 300 loss: 1.2949208402633667
  batch 350 loss: 1.2958173894882201
  batch 400 loss: 1.2635320007801056
  batch 450 loss: 1.2628181374073029
  batch 500 loss: 1.2435086405277251
  batch 550 loss: 1.2948080587387085
  batch 600 loss: 1.2731723535060882
  batch 650 loss: 1.2790492391586303
  batch 700 loss: 1.3156372714042663
  batch 750 loss: 1.2716296315193176
  batch 800 loss: 1.2764272022247314
  batch 850 loss: 1.31088183760643
  batch 900 loss: 1.2492585265636444
avg val loss: 1.2161800861358643
LOSS train 1.24926 valid 1.21618, valid PER 39.31%
EPOCH 10, Learning Rate: 0.3
  batch 50 loss: 1.2442293167114258
  batch 100 loss: 1.26616623878479
  batch 150 loss: 1.2649944448471069
  batch 200 loss: 1.2931477284431458
  batch 250 loss: 1.2690946316719056
  batch 300 loss: 1.2068136787414552
  batch 350 loss: 1.2513470470905304
  batch 400 loss: 1.2374025356769562
  batch 450 loss: 1.211526712179184
  batch 500 loss: 1.265927221775055
  batch 550 loss: 1.266074287891388
  batch 600 loss: 1.2571631896495818
  batch 650 loss: 1.2497496843338012
  batch 700 loss: 1.2497816228866576
  batch 750 loss: 1.2496384489536285
  batch 800 loss: 1.2798526072502137
  batch 850 loss: 1.2849071502685547
  batch 900 loss: 1.3058320832252504
avg val loss: 1.2470684051513672
LOSS train 1.30583 valid 1.24707, valid PER 40.29%
EPOCH 11, Learning Rate: 0.15
  batch 50 loss: 1.2056781661510467
  batch 100 loss: 1.1644180381298066
  batch 150 loss: 1.171492532491684
  batch 200 loss: 1.2091904079914093
  batch 250 loss: 1.2019332289695739
  batch 300 loss: 1.1678913187980653
  batch 350 loss: 1.1769669246673584
  batch 400 loss: 1.1859498035907745
  batch 450 loss: 1.1897980213165282
  batch 500 loss: 1.1662115526199341
  batch 550 loss: 1.164836175441742
  batch 600 loss: 1.1539053905010224
  batch 650 loss: 1.2093211603164673
  batch 700 loss: 1.127524344921112
  batch 750 loss: 1.145271476507187
  batch 800 loss: 1.1620087957382201
  batch 850 loss: 1.1883820652961732
  batch 900 loss: 1.1745549762248992
avg val loss: 1.1590862274169922
LOSS train 1.17455 valid 1.15909, valid PER 37.96%
EPOCH 12, Learning Rate: 0.15
  batch 50 loss: 1.1511347877979279
  batch 100 loss: 1.1209742045402527
  batch 150 loss: 1.128617594242096
  batch 200 loss: 1.1489527583122254
  batch 250 loss: 1.1725801312923432
  batch 300 loss: 1.1410908269882203
  batch 350 loss: 1.1582744932174682
  batch 400 loss: 1.1745404636859893
  batch 450 loss: 1.1568786478042603
  batch 500 loss: 1.155261939764023
  batch 550 loss: 1.1055007112026214
  batch 600 loss: 1.1020617234706878
  batch 650 loss: 1.1782659089565277
  batch 700 loss: 1.1297440242767334
  batch 750 loss: 1.111850656270981
  batch 800 loss: 1.128627496957779
  batch 850 loss: 1.1572013044357299
  batch 900 loss: 1.1662449526786804
avg val loss: 1.1391339302062988
LOSS train 1.16624 valid 1.13913, valid PER 36.47%
EPOCH 13, Learning Rate: 0.15
  batch 50 loss: 1.1185666060447692
  batch 100 loss: 1.128448292016983
  batch 150 loss: 1.1109332942962646
  batch 200 loss: 1.1455361580848693
  batch 250 loss: 1.1162056589126588
  batch 300 loss: 1.096859484910965
  batch 350 loss: 1.1165263307094575
  batch 400 loss: 1.154060513973236
  batch 450 loss: 1.198513501882553
  batch 500 loss: 1.087199958562851
  batch 550 loss: 1.1159689450263977
  batch 600 loss: 1.129159655570984
  batch 650 loss: 1.1178086686134339
  batch 700 loss: 1.1100578200817108
  batch 750 loss: 1.1269940686225892
  batch 800 loss: 1.120610966682434
  batch 850 loss: 1.145886402130127
  batch 900 loss: 1.1585074508190154
avg val loss: 1.1141406297683716
LOSS train 1.15851 valid 1.11414, valid PER 36.03%
EPOCH 14, Learning Rate: 0.15
  batch 50 loss: 1.125826699733734
  batch 100 loss: 1.13166109085083
  batch 150 loss: 1.105310422182083
  batch 200 loss: 1.118660649061203
  batch 250 loss: 1.1113641583919525
  batch 300 loss: 1.1367532396316529
  batch 350 loss: 1.0867312562465667
  batch 400 loss: 1.1068394255638123
  batch 450 loss: 1.1207728898525238
  batch 500 loss: 1.1681186735630036
  batch 550 loss: 1.167277753353119
  batch 600 loss: 1.110135978460312
  batch 650 loss: 1.1421920919418336
  batch 700 loss: 1.1401703417301179
  batch 750 loss: 1.0914487516880036
  batch 800 loss: 1.0539221668243408
  batch 850 loss: 1.1173622620105743
  batch 900 loss: 1.1094772565364837
avg val loss: 1.11027991771698
LOSS train 1.10948 valid 1.11028, valid PER 36.10%
EPOCH 15, Learning Rate: 0.15
  batch 50 loss: 1.1232736372947694
  batch 100 loss: 1.1141255855560304
  batch 150 loss: 1.098660181760788
  batch 200 loss: 1.1616664278507232
  batch 250 loss: 1.10459890127182
  batch 300 loss: 1.0834815204143524
  batch 350 loss: 1.1115605437755585
  batch 400 loss: 1.0725330638885497
  batch 450 loss: 1.094337991476059
  batch 500 loss: 1.073491427898407
  batch 550 loss: 1.10792325258255
  batch 600 loss: 1.122381111383438
  batch 650 loss: 1.115527445077896
  batch 700 loss: 1.143147621154785
  batch 750 loss: 1.112095550298691
  batch 800 loss: 1.0976917934417725
  batch 850 loss: 1.0916101157665252
  batch 900 loss: 1.1328441941738128
avg val loss: 1.105240821838379
LOSS train 1.13284 valid 1.10524, valid PER 36.32%
EPOCH 16, Learning Rate: 0.15
  batch 50 loss: 1.1359006357192993
  batch 100 loss: 1.0657554805278777
  batch 150 loss: 1.0898778760433196
  batch 200 loss: 1.0915004348754882
  batch 250 loss: 1.1213074707984925
  batch 300 loss: 1.1096818435192108
  batch 350 loss: 1.1001268994808198
  batch 400 loss: 1.0878264212608337
  batch 450 loss: 1.108777983188629
  batch 500 loss: 1.0715879142284392
  batch 550 loss: 1.0931996715068817
  batch 600 loss: 1.110636922121048
  batch 650 loss: 1.1082792425155639
  batch 700 loss: 1.0681910991668702
  batch 750 loss: 1.087749274969101
  batch 800 loss: 1.0887591409683228
  batch 850 loss: 1.0960559391975402
  batch 900 loss: 1.093739892244339
avg val loss: 1.0905894041061401
LOSS train 1.09374 valid 1.09059, valid PER 34.74%
EPOCH 17, Learning Rate: 0.15
  batch 50 loss: 1.0947912013530732
  batch 100 loss: 1.0798922169208527
  batch 150 loss: 1.0585654497146606
  batch 200 loss: 1.0707795894145966
  batch 250 loss: 1.0945612263679505
  batch 300 loss: 1.070519242286682
  batch 350 loss: 1.0625052845478058
  batch 400 loss: 1.1512326085567475
  batch 450 loss: 1.0973006999492645
  batch 500 loss: 1.0991366755962373
  batch 550 loss: 1.1053278720378876
  batch 600 loss: 1.1089096105098724
  batch 650 loss: 1.0660464835166932
  batch 700 loss: 1.0862374687194825
  batch 750 loss: 1.0724322700500488
  batch 800 loss: 1.076393072605133
  batch 850 loss: 1.0704326272010802
  batch 900 loss: 1.0587473273277284
avg val loss: 1.1024682521820068
LOSS train 1.05875 valid 1.10247, valid PER 35.25%
EPOCH 18, Learning Rate: 0.075
  batch 50 loss: 1.0594283998012544
  batch 100 loss: 1.0610728335380555
  batch 150 loss: 1.0640196287631989
  batch 200 loss: 1.0482738363742827
  batch 250 loss: 1.049298470020294
  batch 300 loss: 1.0385563004016876
  batch 350 loss: 1.0574217987060548
  batch 400 loss: 1.0255196166038514
  batch 450 loss: 1.0779335165023805
  batch 500 loss: 1.039681795835495
  batch 550 loss: 1.0266414296627044
  batch 600 loss: 1.014296144247055
  batch 650 loss: 1.0294924247264863
  batch 700 loss: 1.0568147957324983
  batch 750 loss: 1.0256531941890716
  batch 800 loss: 1.0287126684188843
  batch 850 loss: 1.0038120365142822
  batch 900 loss: 1.0540288603305816
avg val loss: 1.0642831325531006
LOSS train 1.05403 valid 1.06428, valid PER 34.00%
EPOCH 19, Learning Rate: 0.075
  batch 50 loss: 0.976764942407608
  batch 100 loss: 0.9870112371444703
  batch 150 loss: 1.0143503177165984
  batch 200 loss: 1.0203000819683075
  batch 250 loss: 1.0511564218997955
  batch 300 loss: 1.0279246354103089
  batch 350 loss: 1.0226639819145202
  batch 400 loss: 1.0543814504146576
  batch 450 loss: 1.053235136270523
  batch 500 loss: 1.0479729080200195
  batch 550 loss: 1.0285368478298187
  batch 600 loss: 1.0440019607543944
  batch 650 loss: 1.0618326807022094
  batch 700 loss: 1.0014259040355682
  batch 750 loss: 1.001169639825821
  batch 800 loss: 1.0360607624053955
  batch 850 loss: 1.052896330356598
  batch 900 loss: 1.021735863685608
avg val loss: 1.0509735345840454
LOSS train 1.02174 valid 1.05097, valid PER 33.71%
EPOCH 20, Learning Rate: 0.075
  batch 50 loss: 1.0182600557804107
  batch 100 loss: 1.0330093228816986
  batch 150 loss: 1.0377324068546294
  batch 200 loss: 1.0289954209327699
  batch 250 loss: 1.0391676926612854
  batch 300 loss: 1.0430436909198761
  batch 350 loss: 1.001034264564514
  batch 400 loss: 1.0019962251186372
  batch 450 loss: 1.0342786467075349
  batch 500 loss: 1.014288433790207
  batch 550 loss: 1.0510268986225129
  batch 600 loss: 0.9756272578239441
  batch 650 loss: 1.029283698797226
  batch 700 loss: 1.0368479216098785
  batch 750 loss: 1.0197916889190675
  batch 800 loss: 1.0290084648132325
  batch 850 loss: 1.0251437497138978
  batch 900 loss: 1.021662038564682
avg val loss: 1.0567487478256226
LOSS train 1.02166 valid 1.05675, valid PER 33.66%
Training finished in 2.0 minutes.
Model saved to checkpoints/20231210_140745/model_19
Loading model from checkpoints/20231210_140745/model_19
SUB: 19.63%, DEL: 14.61%, INS: 1.35%, COR: 65.76%, PER: 35.59%
