Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=2, fbank_dims=23, model_dims=128, concat=1, lr=0.9, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 562216
EPOCH 1, Learning Rate: 0.9
  batch 50 loss: 4.480220975875855
  batch 100 loss: 3.265308723449707
  batch 150 loss: 3.1610193729400633
  batch 200 loss: 2.8559177684783936
  batch 250 loss: 2.6739231729507447
  batch 300 loss: 2.471067147254944
  batch 350 loss: 2.3823262548446653
  batch 400 loss: 2.291850073337555
  batch 450 loss: 2.207979097366333
  batch 500 loss: 2.0842612481117246
  batch 550 loss: 2.008148076534271
  batch 600 loss: 1.92729558467865
  batch 650 loss: 1.8537285256385803
  batch 700 loss: 1.8502852749824523
  batch 750 loss: 1.7663871574401855
  batch 800 loss: 1.7416296935081481
  batch 850 loss: 1.7075052213668824
  batch 900 loss: 1.6532038688659667
avg val loss: 1.528929352760315
LOSS train 1.65320 valid 1.52893, valid PER 56.03%
EPOCH 2, Learning Rate: 0.9
  batch 50 loss: 1.6251914858818055
  batch 100 loss: 1.6190473771095275
  batch 150 loss: 1.4857576036453246
  batch 200 loss: 1.5167062282562256
  batch 250 loss: 1.505357632637024
  batch 300 loss: 1.4845971965789795
  batch 350 loss: 1.4683389163017273
  batch 400 loss: 1.4110142421722411
  batch 450 loss: 1.3996337413787843
  batch 500 loss: 1.398211832046509
  batch 550 loss: 1.382392874956131
  batch 600 loss: 1.334138948917389
  batch 650 loss: 1.3063181924819947
  batch 700 loss: 1.3253558611869811
  batch 750 loss: 1.3124258351325988
  batch 800 loss: 1.2566495656967163
  batch 850 loss: 1.2743178260326387
  batch 900 loss: 1.2296353352069855
avg val loss: 1.160341501235962
LOSS train 1.22964 valid 1.16034, valid PER 36.07%
EPOCH 3, Learning Rate: 0.9
  batch 50 loss: 1.165377744436264
  batch 100 loss: 1.2480899560451508
  batch 150 loss: 1.2676047587394714
  batch 200 loss: 1.185034373998642
  batch 250 loss: 1.1965731847286225
  batch 300 loss: 1.1964639830589294
  batch 350 loss: 1.2122011494636535
  batch 400 loss: 1.1847849941253663
  batch 450 loss: 1.157687771320343
  batch 500 loss: 1.1454913139343261
  batch 550 loss: 1.152152738571167
  batch 600 loss: 1.1053436851501466
  batch 650 loss: 1.152492196559906
  batch 700 loss: 1.1409274053573608
  batch 750 loss: 1.1564481675624847
  batch 800 loss: 1.1572252130508422
  batch 850 loss: 1.129580866098404
  batch 900 loss: 1.1298124504089355
avg val loss: 1.0507925748825073
LOSS train 1.12981 valid 1.05079, valid PER 31.72%
EPOCH 4, Learning Rate: 0.9
  batch 50 loss: 1.1280738186836243
  batch 100 loss: 1.0628310692310334
  batch 150 loss: 1.0798031830787658
  batch 200 loss: 1.0748139309883118
  batch 250 loss: 1.0661116027832032
  batch 300 loss: 1.0752900373935699
  batch 350 loss: 1.040666890144348
  batch 400 loss: 1.020658929347992
  batch 450 loss: 1.034257823228836
  batch 500 loss: 1.104382357597351
  batch 550 loss: 1.0218080139160157
  batch 600 loss: 1.004275280237198
  batch 650 loss: 1.0905316412448882
  batch 700 loss: 1.0799038803577423
  batch 750 loss: 1.032727472782135
  batch 800 loss: 1.021843935251236
  batch 850 loss: 1.0166975259780884
  batch 900 loss: 1.0579625236988068
avg val loss: 0.9903249144554138
LOSS train 1.05796 valid 0.99032, valid PER 31.00%
EPOCH 5, Learning Rate: 0.9
  batch 50 loss: 0.9859086191654205
  batch 100 loss: 0.9743755829334259
  batch 150 loss: 1.012570357322693
  batch 200 loss: 1.0237857866287232
  batch 250 loss: 0.9598048031330109
  batch 300 loss: 1.0287821221351623
  batch 350 loss: 0.9451401770114899
  batch 400 loss: 0.9605452370643616
  batch 450 loss: 0.9663043153285981
  batch 500 loss: 0.9583410429954529
  batch 550 loss: 1.0128433060646058
  batch 600 loss: 1.0000343060493468
  batch 650 loss: 1.0055700325965882
  batch 700 loss: 0.9664591670036315
  batch 750 loss: 0.9763793730735779
  batch 800 loss: 1.0127456998825073
  batch 850 loss: 0.9819913363456726
  batch 900 loss: 0.9512614774703979
avg val loss: 0.910573422908783
LOSS train 0.95126 valid 0.91057, valid PER 28.05%
EPOCH 6, Learning Rate: 0.9
  batch 50 loss: 0.9185790431499481
  batch 100 loss: 0.9789160907268524
  batch 150 loss: 0.9459000563621521
  batch 200 loss: 0.8955672073364258
  batch 250 loss: 0.9201205372810364
  batch 300 loss: 0.9407486355304718
  batch 350 loss: 0.9687481689453125
  batch 400 loss: 0.9349283218383789
  batch 450 loss: 0.9614550697803498
  batch 500 loss: 0.9138245499134063
  batch 550 loss: 0.9460548758506775
  batch 600 loss: 0.9236290049552918
  batch 650 loss: 0.9052765762805939
  batch 700 loss: 0.9063396179676055
  batch 750 loss: 0.9284086418151856
  batch 800 loss: 0.904564973115921
  batch 850 loss: 0.9616489255428314
  batch 900 loss: 0.9700805211067199
avg val loss: 0.8815995454788208
LOSS train 0.97008 valid 0.88160, valid PER 28.09%
EPOCH 7, Learning Rate: 0.9
  batch 50 loss: 0.8747048616409302
  batch 100 loss: 0.939346524477005
  batch 150 loss: 0.8580296099185943
  batch 200 loss: 0.8607581913471222
  batch 250 loss: 0.9231458365917206
  batch 300 loss: 0.879827036857605
  batch 350 loss: 0.9234584975242615
  batch 400 loss: 0.8616358411312103
  batch 450 loss: 0.8875926434993744
  batch 500 loss: 0.8725531756877899
  batch 550 loss: 0.8713452517986298
  batch 600 loss: 0.8841335642337799
  batch 650 loss: 0.8548558187484742
  batch 700 loss: 0.9030287098884583
  batch 750 loss: 0.8741247200965881
  batch 800 loss: 0.8652747654914856
  batch 850 loss: 0.8795846843719483
  batch 900 loss: 0.8706100690364837
avg val loss: 0.8940393924713135
LOSS train 0.87061 valid 0.89404, valid PER 28.09%
EPOCH 8, Learning Rate: 0.45
  batch 50 loss: 0.8189325165748597
  batch 100 loss: 0.7592604780197143
  batch 150 loss: 0.7785787391662597
  batch 200 loss: 0.7742565232515335
  batch 250 loss: 0.7641906118392945
  batch 300 loss: 0.7373005354404449
  batch 350 loss: 0.7692030596733094
  batch 400 loss: 0.7424583220481873
  batch 450 loss: 0.7954191029071808
  batch 500 loss: 0.7715507888793945
  batch 550 loss: 0.7702574473619461
  batch 600 loss: 0.7381317627429962
  batch 650 loss: 0.7549689650535584
  batch 700 loss: 0.7744825994968414
  batch 750 loss: 0.7512061136960984
  batch 800 loss: 0.77971794962883
  batch 850 loss: 0.7370842278003693
  batch 900 loss: 0.7483097660541534
avg val loss: 0.7914717793464661
LOSS train 0.74831 valid 0.79147, valid PER 24.77%
EPOCH 9, Learning Rate: 0.45
  batch 50 loss: 0.7148965811729431
  batch 100 loss: 0.6923076772689819
  batch 150 loss: 0.7254230642318725
  batch 200 loss: 0.7175583082437516
  batch 250 loss: 0.6925585180521011
  batch 300 loss: 0.7293521451950074
  batch 350 loss: 0.6837212723493576
  batch 400 loss: 0.7293586874008179
  batch 450 loss: 0.7312457001209259
  batch 500 loss: 0.7314274215698242
  batch 550 loss: 0.7025138682126999
  batch 600 loss: 0.7465063321590424
  batch 650 loss: 0.7400680339336395
  batch 700 loss: 0.7177393788099289
  batch 750 loss: 0.7257381117343903
  batch 800 loss: 0.7494219678640366
  batch 850 loss: 0.7381686520576477
  batch 900 loss: 0.7022241777181626
avg val loss: 0.7563275694847107
LOSS train 0.70222 valid 0.75633, valid PER 23.48%
EPOCH 10, Learning Rate: 0.45
  batch 50 loss: 0.6880227690935135
  batch 100 loss: 0.7009108382463455
  batch 150 loss: 0.723777626156807
  batch 200 loss: 0.6717112499475479
  batch 250 loss: 0.6823175585269928
  batch 300 loss: 0.7068873345851898
  batch 350 loss: 0.6817242854833603
  batch 400 loss: 0.659766954779625
  batch 450 loss: 0.6729017078876496
  batch 500 loss: 0.6757601630687714
  batch 550 loss: 0.7071345537900925
  batch 600 loss: 0.7052771317958831
  batch 650 loss: 0.7120860409736633
  batch 700 loss: 0.705338886976242
  batch 750 loss: 0.7251295077800751
  batch 800 loss: 0.7166139733791351
  batch 850 loss: 0.6891297024488449
  batch 900 loss: 0.7107733702659607
avg val loss: 0.7785853147506714
LOSS train 0.71077 valid 0.77859, valid PER 24.08%
EPOCH 11, Learning Rate: 0.225
  batch 50 loss: 0.6599869042634964
  batch 100 loss: 0.6433454191684723
  batch 150 loss: 0.6260154664516449
  batch 200 loss: 0.5990166622400284
  batch 250 loss: 0.6179413241147995
  batch 300 loss: 0.608162225484848
  batch 350 loss: 0.6422883599996567
  batch 400 loss: 0.6232936066389084
  batch 450 loss: 0.6353655332326889
  batch 500 loss: 0.6241845440864563
  batch 550 loss: 0.6371047031879425
  batch 600 loss: 0.6431814426183701
  batch 650 loss: 0.6592589384317398
  batch 700 loss: 0.6826536947488785
  batch 750 loss: 0.6211926144361496
  batch 800 loss: 0.6451438903808594
  batch 850 loss: 0.6387486892938614
  batch 900 loss: 0.6588147646188736
avg val loss: 0.7406018376350403
LOSS train 0.65881 valid 0.74060, valid PER 23.00%
EPOCH 12, Learning Rate: 0.225
  batch 50 loss: 0.5879260736703873
  batch 100 loss: 0.5906813579797745
  batch 150 loss: 0.6203526437282563
  batch 200 loss: 0.6059079676866531
  batch 250 loss: 0.6108879518508911
  batch 300 loss: 0.6271371686458588
  batch 350 loss: 0.6042723339796067
  batch 400 loss: 0.6342669671773911
  batch 450 loss: 0.6196238857507705
  batch 500 loss: 0.619692120552063
  batch 550 loss: 0.6219628030061721
  batch 600 loss: 0.6292900377511979
  batch 650 loss: 0.6190164107084274
  batch 700 loss: 0.6315934765338898
  batch 750 loss: 0.6131026786565781
  batch 800 loss: 0.5830173319578171
  batch 850 loss: 0.6244238752126694
  batch 900 loss: 0.6365138918161393
avg val loss: 0.7367584705352783
LOSS train 0.63651 valid 0.73676, valid PER 22.45%
EPOCH 13, Learning Rate: 0.225
  batch 50 loss: 0.575908220410347
  batch 100 loss: 0.5832404363155365
  batch 150 loss: 0.6114520871639252
  batch 200 loss: 0.5530257380008697
  batch 250 loss: 0.5741412967443467
  batch 300 loss: 0.6265588289499283
  batch 350 loss: 0.5820033776760102
  batch 400 loss: 0.5974543511867523
  batch 450 loss: 0.6082974427938461
  batch 500 loss: 0.5876320081949234
  batch 550 loss: 0.6267016392946243
  batch 600 loss: 0.6180135351419449
  batch 650 loss: 0.6151505571603775
  batch 700 loss: 0.5988382411003113
  batch 750 loss: 0.5777363067865372
  batch 800 loss: 0.615405113697052
  batch 850 loss: 0.5922815072536468
  batch 900 loss: 0.6133422005176544
avg val loss: 0.7322962284088135
LOSS train 0.61334 valid 0.73230, valid PER 22.46%
EPOCH 14, Learning Rate: 0.225
  batch 50 loss: 0.5749762505292892
  batch 100 loss: 0.5458531498908996
  batch 150 loss: 0.5744893324375152
  batch 200 loss: 0.5889088767766952
  batch 250 loss: 0.5749592208862304
  batch 300 loss: 0.5664091855287552
  batch 350 loss: 0.5817999762296676
  batch 400 loss: 0.6015433406829834
  batch 450 loss: 0.5644493561983108
  batch 500 loss: 0.5902375316619873
  batch 550 loss: 0.5829486173391342
  batch 600 loss: 0.5746493315696717
  batch 650 loss: 0.5952097314596176
  batch 700 loss: 0.5899315106868744
  batch 750 loss: 0.5875432342290878
  batch 800 loss: 0.5846891629695893
  batch 850 loss: 0.6078639817237854
  batch 900 loss: 0.6013435024023056
avg val loss: 0.7356186509132385
LOSS train 0.60134 valid 0.73562, valid PER 22.39%
EPOCH 15, Learning Rate: 0.1125
  batch 50 loss: 0.5367848944664001
  batch 100 loss: 0.5369263088703156
  batch 150 loss: 0.562258939743042
  batch 200 loss: 0.5735105311870575
  batch 250 loss: 0.5674234443902969
  batch 300 loss: 0.5605735355615615
  batch 350 loss: 0.5408785301446914
  batch 400 loss: 0.539072322845459
  batch 450 loss: 0.537959696650505
  batch 500 loss: 0.540427931547165
  batch 550 loss: 0.5816029584407807
  batch 600 loss: 0.5873646515607834
  batch 650 loss: 0.5518156307935714
  batch 700 loss: 0.5349404913187027
  batch 750 loss: 0.5536567562818527
  batch 800 loss: 0.5361254054307938
  batch 850 loss: 0.532157998085022
  batch 900 loss: 0.529682844877243
avg val loss: 0.7243734002113342
LOSS train 0.52968 valid 0.72437, valid PER 22.02%
EPOCH 16, Learning Rate: 0.1125
  batch 50 loss: 0.5404316461086274
  batch 100 loss: 0.5223053795099258
  batch 150 loss: 0.5399499768018723
  batch 200 loss: 0.5557669699192047
  batch 250 loss: 0.5320485788583755
  batch 300 loss: 0.5304102563858032
  batch 350 loss: 0.5295940518379212
  batch 400 loss: 0.5531092715263367
  batch 450 loss: 0.5586823058128357
  batch 500 loss: 0.5224333715438843
  batch 550 loss: 0.5126408040523529
  batch 600 loss: 0.5730535036325455
  batch 650 loss: 0.5629812461137772
  batch 700 loss: 0.4949829399585724
  batch 750 loss: 0.5422757363319397
  batch 800 loss: 0.5312284690141678
  batch 850 loss: 0.5313838440179824
  batch 900 loss: 0.5387071549892426
avg val loss: 0.7271897792816162
LOSS train 0.53871 valid 0.72719, valid PER 21.80%
EPOCH 17, Learning Rate: 0.05625
  batch 50 loss: 0.5347639143466949
  batch 100 loss: 0.49333831191062927
  batch 150 loss: 0.5439395719766617
  batch 200 loss: 0.4948328822851181
  batch 250 loss: 0.5245795917510986
  batch 300 loss: 0.5122181296348571
  batch 350 loss: 0.5309594929218292
  batch 400 loss: 0.5217018896341323
  batch 450 loss: 0.4968202495574951
  batch 500 loss: 0.5262722796201706
  batch 550 loss: 0.5236787271499633
  batch 600 loss: 0.5484584057331086
  batch 650 loss: 0.5009430849552154
  batch 700 loss: 0.5323504340648652
  batch 750 loss: 0.4996693068742752
  batch 800 loss: 0.52210562646389
  batch 850 loss: 0.5399292320013046
  batch 900 loss: 0.5034209018945695
avg val loss: 0.7250263690948486
LOSS train 0.50342 valid 0.72503, valid PER 21.57%
EPOCH 18, Learning Rate: 0.028125
  batch 50 loss: 0.489678920507431
  batch 100 loss: 0.4967347902059555
  batch 150 loss: 0.5372780334949493
  batch 200 loss: 0.512871795296669
  batch 250 loss: 0.5018944293260574
  batch 300 loss: 0.4880411773920059
  batch 350 loss: 0.4864440143108368
  batch 400 loss: 0.49432732939720153
  batch 450 loss: 0.5090751510858536
  batch 500 loss: 0.5009498018026352
  batch 550 loss: 0.5404466384649277
  batch 600 loss: 0.5117581313848496
  batch 650 loss: 0.5057925134897232
  batch 700 loss: 0.497110835313797
  batch 750 loss: 0.5116149634122849
  batch 800 loss: 0.5290495008230209
  batch 850 loss: 0.519467082619667
  batch 900 loss: 0.5124507123231887
avg val loss: 0.7219299077987671
LOSS train 0.51245 valid 0.72193, valid PER 21.56%
EPOCH 19, Learning Rate: 0.028125
  batch 50 loss: 0.5014872419834137
  batch 100 loss: 0.5087687766551972
  batch 150 loss: 0.4976797384023666
  batch 200 loss: 0.48644151866436003
  batch 250 loss: 0.5184872275590897
  batch 300 loss: 0.5229788196086883
  batch 350 loss: 0.5243858581781388
  batch 400 loss: 0.4998831421136856
  batch 450 loss: 0.4693266451358795
  batch 500 loss: 0.48314022898674014
  batch 550 loss: 0.5154215341806412
  batch 600 loss: 0.5158898746967315
  batch 650 loss: 0.5293133777379989
  batch 700 loss: 0.5137050062417984
  batch 750 loss: 0.4870229732990265
  batch 800 loss: 0.49164586305618285
  batch 850 loss: 0.5034567099809647
  batch 900 loss: 0.5114882653951645
avg val loss: 0.7247171401977539
LOSS train 0.51149 valid 0.72472, valid PER 21.49%
EPOCH 20, Learning Rate: 0.0140625
  batch 50 loss: 0.4823275202512741
  batch 100 loss: 0.49900765359401705
  batch 150 loss: 0.5025800162553787
  batch 200 loss: 0.4992941510677338
  batch 250 loss: 0.5196148431301117
  batch 300 loss: 0.5092193472385407
  batch 350 loss: 0.475401771068573
  batch 400 loss: 0.5008755362033844
  batch 450 loss: 0.47834030985832215
  batch 500 loss: 0.5201313930749893
  batch 550 loss: 0.502812038064003
  batch 600 loss: 0.48879634141921996
  batch 650 loss: 0.5124721527099609
  batch 700 loss: 0.4837557256221771
  batch 750 loss: 0.49402109563350677
  batch 800 loss: 0.50481703042984
  batch 850 loss: 0.5022049033641816
  batch 900 loss: 0.4804172968864441
avg val loss: 0.7224196195602417
LOSS train 0.48042 valid 0.72242, valid PER 21.63%
Training finished in 5.0 minutes.
Model saved to checkpoints/20231210_135158/model_18
Loading model from checkpoints/20231210_135158/model_18
SUB: 13.81%, DEL: 7.37%, INS: 1.96%, COR: 78.83%, PER: 23.13%
