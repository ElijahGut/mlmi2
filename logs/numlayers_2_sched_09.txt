Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=2, fbank_dims=23, model_dims=128, concat=1, lr=0.9, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 562216
EPOCH 1, Learning Rate: 0.9
  batch 50 loss: 4.480218954086304
  batch 100 loss: 3.2653083753585816
  batch 150 loss: 3.160858645439148
  batch 200 loss: 2.855923991203308
  batch 250 loss: 2.670361866950989
  batch 300 loss: 2.4696466398239134
  batch 350 loss: 2.383411655426025
  batch 400 loss: 2.2923837518692016
  batch 450 loss: 2.208303542137146
  batch 500 loss: 2.0842126488685606
  batch 550 loss: 2.009385013580322
  batch 600 loss: 1.9279238700866699
  batch 650 loss: 1.8526920175552368
  batch 700 loss: 1.8492523002624512
  batch 750 loss: 1.7741133546829224
  batch 800 loss: 1.7454775285720825
  batch 850 loss: 1.7119940853118896
  batch 900 loss: 1.6550224614143372
running loss: 38.30817949771881
LOSS train 1.65502 valid 1.52951, valid PER 55.73%
EPOCH 2, Learning Rate: 0.9
  batch 50 loss: 1.6257197403907775
  batch 100 loss: 1.6165264797210694
  batch 150 loss: 1.4865846300125123
  batch 200 loss: 1.5078707122802735
  batch 250 loss: 1.5032056999206542
  batch 300 loss: 1.4813143396377564
  batch 350 loss: 1.4599639081954956
  batch 400 loss: 1.4197121143341065
  batch 450 loss: 1.3949802088737489
  batch 500 loss: 1.3951768350601197
  batch 550 loss: 1.3829185366630554
  batch 600 loss: 1.343432195186615
  batch 650 loss: 1.2969088459014892
  batch 700 loss: 1.3171994519233703
  batch 750 loss: 1.308063976764679
  batch 800 loss: 1.2524978017807007
  batch 850 loss: 1.2821816909313202
  batch 900 loss: 1.2227285599708557
running loss: 29.333337128162384
LOSS train 1.22273 valid 1.15544, valid PER 35.94%
EPOCH 3, Learning Rate: 0.9
  batch 50 loss: 1.1712327802181244
  batch 100 loss: 1.2319486224651337
  batch 150 loss: 1.2621603763103486
  batch 200 loss: 1.1851485693454742
  batch 250 loss: 1.1905998742580415
  batch 300 loss: 1.1909731125831604
  batch 350 loss: 1.2123628234863282
  batch 400 loss: 1.1845671772956847
  batch 450 loss: 1.1669074451923371
  batch 500 loss: 1.1455622053146362
  batch 550 loss: 1.1512877392768859
  batch 600 loss: 1.0979996061325072
  batch 650 loss: 1.1435984659194947
  batch 700 loss: 1.1262434506416321
  batch 750 loss: 1.1539510118961334
  batch 800 loss: 1.1534871697425841
  batch 850 loss: 1.1409225726127625
  batch 900 loss: 1.1378449392318726
running loss: 28.060820877552032
LOSS train 1.13784 valid 1.03949, valid PER 31.79%
EPOCH 4, Learning Rate: 0.9
  batch 50 loss: 1.139335948228836
  batch 100 loss: 1.052790756225586
  batch 150 loss: 1.066903908252716
  batch 200 loss: 1.0685687017440797
  batch 250 loss: 1.0640302336215972
  batch 300 loss: 1.076518678665161
  batch 350 loss: 1.0575491631031035
  batch 400 loss: 1.0217339193820953
  batch 450 loss: 1.0361719179153441
  batch 500 loss: 1.090233985185623
  batch 550 loss: 1.0217031335830689
  batch 600 loss: 0.9948412775993347
  batch 650 loss: 1.083787190914154
  batch 700 loss: 1.0876253545284271
  batch 750 loss: 1.0347417283058167
  batch 800 loss: 1.018246066570282
  batch 850 loss: 1.0330939149856568
  batch 900 loss: 1.0520631098747253
running loss: 26.031371891498566
LOSS train 1.05206 valid 0.98418, valid PER 30.30%
EPOCH 5, Learning Rate: 0.9
  batch 50 loss: 0.9943217062950134
  batch 100 loss: 0.9732781946659088
  batch 150 loss: 0.9873300611972808
  batch 200 loss: 1.0193757796287537
  batch 250 loss: 0.956620398759842
  batch 300 loss: 1.0092807197570801
  batch 350 loss: 0.9589030933380127
  batch 400 loss: 0.964103707075119
  batch 450 loss: 0.9679359459877014
  batch 500 loss: 0.9529694283008575
  batch 550 loss: 1.014253672361374
  batch 600 loss: 1.0001308631896972
  batch 650 loss: 1.004695656299591
  batch 700 loss: 0.9833065640926361
  batch 750 loss: 0.9675063109397888
  batch 800 loss: 1.0273480343818664
  batch 850 loss: 0.9878552091121674
  batch 900 loss: 0.9826560258865357
running loss: 23.535061180591583
LOSS train 0.98266 valid 0.92453, valid PER 28.78%
EPOCH 6, Learning Rate: 0.9
  batch 50 loss: 0.9423599219322205
  batch 100 loss: 0.9449490940570832
  batch 150 loss: 0.944931755065918
  batch 200 loss: 0.9132573008537292
  batch 250 loss: 0.9197647964954376
  batch 300 loss: 0.962188560962677
  batch 350 loss: 0.9705560386180878
  batch 400 loss: 0.9207789099216461
  batch 450 loss: 0.9574847543239593
  batch 500 loss: 0.8945683884620667
  batch 550 loss: 0.9474585247039795
  batch 600 loss: 0.9225667941570282
  batch 650 loss: 0.887431765794754
  batch 700 loss: 0.9089735615253448
  batch 750 loss: 0.9550020146369934
  batch 800 loss: 0.9175910103321075
  batch 850 loss: 0.9658026754856109
  batch 900 loss: 0.9516906273365021
running loss: 21.108892738819122
LOSS train 0.95169 valid 0.89765, valid PER 28.17%
EPOCH 7, Learning Rate: 0.9
  batch 50 loss: 0.872935631275177
  batch 100 loss: 0.9408589565753936
  batch 150 loss: 0.8541362631320953
  batch 200 loss: 0.8535123717784882
  batch 250 loss: 0.9149405884742737
  batch 300 loss: 0.8894927000999451
  batch 350 loss: 0.9225091862678528
  batch 400 loss: 0.8591700565814971
  batch 450 loss: 0.874241577386856
  batch 500 loss: 0.8622618114948273
  batch 550 loss: 0.8636721670627594
  batch 600 loss: 0.896370359659195
  batch 650 loss: 0.8809006190299988
  batch 700 loss: 0.9244090139865875
  batch 750 loss: 0.8797456634044647
  batch 800 loss: 0.8585271561145782
  batch 850 loss: 0.8520804178714753
  batch 900 loss: 0.8720871484279633
running loss: 20.417739748954773
LOSS train 0.87209 valid 0.87761, valid PER 27.08%
EPOCH 8, Learning Rate: 0.9
  batch 50 loss: 0.8502339541912078
  batch 100 loss: 0.8258802711963653
  batch 150 loss: 0.8636654168367386
  batch 200 loss: 0.8454329347610474
  batch 250 loss: 0.837711306810379
  batch 300 loss: 0.8168578183650971
  batch 350 loss: 0.832857825756073
  batch 400 loss: 0.8441035115718841
  batch 450 loss: 0.8601447856426239
  batch 500 loss: 0.8714534306526184
  batch 550 loss: 0.84649871468544
  batch 600 loss: 0.8236330199241638
  batch 650 loss: 0.8571807765960693
  batch 700 loss: 0.8673399996757507
  batch 750 loss: 0.8437241518497467
  batch 800 loss: 0.8549066317081452
  batch 850 loss: 0.8144705510139465
  batch 900 loss: 0.8377607381343841
running loss: 19.99902606010437
LOSS train 0.83776 valid 0.88315, valid PER 27.15%
EPOCH 9, Learning Rate: 0.9
  batch 50 loss: 0.786747407913208
  batch 100 loss: 0.788139374256134
  batch 150 loss: 0.7821893751621246
  batch 200 loss: 0.7811194437742234
  batch 250 loss: 0.7752344286441804
  batch 300 loss: 0.8387841951847076
  batch 350 loss: 0.7715782940387725
  batch 400 loss: 0.8278980660438537
  batch 450 loss: 0.8472789061069489
  batch 500 loss: 0.8145579361915588
  batch 550 loss: 0.811863020658493
  batch 600 loss: 0.8368660354614258
  batch 650 loss: 0.8258104145526886
  batch 700 loss: 0.7949029076099395
  batch 750 loss: 0.8240846073627472
  batch 800 loss: 0.8267650771141052
  batch 850 loss: 0.854394963979721
  batch 900 loss: 0.7828414356708526
running loss: 20.04050314426422
LOSS train 0.78284 valid 0.83203, valid PER 25.63%
EPOCH 10, Learning Rate: 0.45
  batch 50 loss: 0.7468082094192505
  batch 100 loss: 0.7389697271585465
  batch 150 loss: 0.726362054347992
  batch 200 loss: 0.6819359129667282
  batch 250 loss: 0.6960891544818878
  batch 300 loss: 0.7049203312397003
  batch 350 loss: 0.693675907254219
  batch 400 loss: 0.6681870949268341
  batch 450 loss: 0.6797545355558395
  batch 500 loss: 0.6764111614227295
  batch 550 loss: 0.698378415107727
  batch 600 loss: 0.6907973086833954
  batch 650 loss: 0.7082775616645813
  batch 700 loss: 0.7011297792196274
  batch 750 loss: 0.7083053052425384
  batch 800 loss: 0.721009315252304
  batch 850 loss: 0.6972874701023102
  batch 900 loss: 0.6901629054546357
running loss: 16.849401891231537
LOSS train 0.69016 valid 0.74699, valid PER 23.34%
EPOCH 11, Learning Rate: 0.45
  batch 50 loss: 0.6682342201471329
  batch 100 loss: 0.6489492899179459
  batch 150 loss: 0.6581591206789017
  batch 200 loss: 0.6188542687892914
  batch 250 loss: 0.6455322033166886
  batch 300 loss: 0.6449985831975937
  batch 350 loss: 0.673672661781311
  batch 400 loss: 0.6561001855134964
  batch 450 loss: 0.6542603397369384
  batch 500 loss: 0.656597146987915
  batch 550 loss: 0.677895896434784
  batch 600 loss: 0.6816415476799011
  batch 650 loss: 0.6711409693956375
  batch 700 loss: 0.7260778760910034
  batch 750 loss: 0.6538226079940795
  batch 800 loss: 0.6870186680555344
  batch 850 loss: 0.6657211172580719
  batch 900 loss: 0.6820524120330811
running loss: 14.871838480234146
LOSS train 0.68205 valid 0.73686, valid PER 22.55%
EPOCH 12, Learning Rate: 0.45
  batch 50 loss: 0.5907157808542252
  batch 100 loss: 0.6045303094387054
  batch 150 loss: 0.6520079886913299
  batch 200 loss: 0.6521605956554413
  batch 250 loss: 0.6322727262973785
  batch 300 loss: 0.656919538974762
  batch 350 loss: 0.6327179574966431
  batch 400 loss: 0.667181356549263
  batch 450 loss: 0.6358384162187576
  batch 500 loss: 0.6599630272388458
  batch 550 loss: 0.6586490595340728
  batch 600 loss: 0.6586567437648774
  batch 650 loss: 0.6427575069665908
  batch 700 loss: 0.6538220649957657
  batch 750 loss: 0.6485487306118012
  batch 800 loss: 0.6384579342603683
  batch 850 loss: 0.6567183715105057
  batch 900 loss: 0.6820396262407303
running loss: 15.628115028142929
LOSS train 0.68204 valid 0.75506, valid PER 22.96%
EPOCH 13, Learning Rate: 0.225
  batch 50 loss: 0.6048661905527115
  batch 100 loss: 0.5997404670715332
  batch 150 loss: 0.6097494965791702
  batch 200 loss: 0.5571989113092423
  batch 250 loss: 0.5632664203643799
  batch 300 loss: 0.6083786350488662
  batch 350 loss: 0.5545904016494752
  batch 400 loss: 0.576659683585167
  batch 450 loss: 0.5824713510274887
  batch 500 loss: 0.5836279380321503
  batch 550 loss: 0.6127306932210922
  batch 600 loss: 0.6005586272478104
  batch 650 loss: 0.5821480369567871
  batch 700 loss: 0.5892487829923629
  batch 750 loss: 0.5675899547338485
  batch 800 loss: 0.5823461896181107
  batch 850 loss: 0.56746228992939
  batch 900 loss: 0.5918402820825577
running loss: 14.21292182803154
LOSS train 0.59184 valid 0.71209, valid PER 21.79%
EPOCH 14, Learning Rate: 0.225
  batch 50 loss: 0.5495157849788666
  batch 100 loss: 0.5421254569292069
  batch 150 loss: 0.5590582627058029
  batch 200 loss: 0.5674672973155975
  batch 250 loss: 0.554389266371727
  batch 300 loss: 0.5574857223033906
  batch 350 loss: 0.5574360704421997
  batch 400 loss: 0.5940401995182037
  batch 450 loss: 0.5590492051839828
  batch 500 loss: 0.5793429988622666
  batch 550 loss: 0.5752452063560486
  batch 600 loss: 0.548071328997612
  batch 650 loss: 0.5800774639844894
  batch 700 loss: 0.5728864425420761
  batch 750 loss: 0.5700474053621292
  batch 800 loss: 0.5566507416963578
  batch 850 loss: 0.5839384818077087
  batch 900 loss: 0.5819241428375244
running loss: 13.371960192918777
LOSS train 0.58192 valid 0.71722, valid PER 21.94%
EPOCH 15, Learning Rate: 0.225
  batch 50 loss: 0.5249278575181962
  batch 100 loss: 0.5416743350028992
  batch 150 loss: 0.5503516685962677
  batch 200 loss: 0.5782517009973526
  batch 250 loss: 0.5570850616693497
  batch 300 loss: 0.5516753214597702
  batch 350 loss: 0.5471418488025666
  batch 400 loss: 0.5561585772037506
  batch 450 loss: 0.5483736062049865
  batch 500 loss: 0.5405020296573639
  batch 550 loss: 0.5864624357223511
  batch 600 loss: 0.5842128002643585
  batch 650 loss: 0.5481589579582214
  batch 700 loss: 0.5436123609542847
  batch 750 loss: 0.5650859659910202
  batch 800 loss: 0.5392138546705246
  batch 850 loss: 0.5305291026830673
  batch 900 loss: 0.5294761818647384
running loss: 13.39912435412407
LOSS train 0.52948 valid 0.71984, valid PER 21.85%
EPOCH 16, Learning Rate: 0.1125
  batch 50 loss: 0.5312039917707443
  batch 100 loss: 0.5061304885149002
  batch 150 loss: 0.5133579963445664
  batch 200 loss: 0.5489960873126983
  batch 250 loss: 0.5234228992462158
  batch 300 loss: 0.5223365032672882
  batch 350 loss: 0.5261707073450088
  batch 400 loss: 0.5235084623098374
  batch 450 loss: 0.5573168414831161
  batch 500 loss: 0.5206265449523926
  batch 550 loss: 0.5033528685569764
  batch 600 loss: 0.5463913187384606
  batch 650 loss: 0.5477695739269257
  batch 700 loss: 0.4701695144176483
  batch 750 loss: 0.5263677674531937
  batch 800 loss: 0.5073514604568481
  batch 850 loss: 0.504838028550148
  batch 900 loss: 0.5100780862569809
running loss: 12.258538097143173
LOSS train 0.51008 valid 0.71212, valid PER 21.48%
EPOCH 17, Learning Rate: 0.1125
  batch 50 loss: 0.5210790181159973
  batch 100 loss: 0.4788068526983261
  batch 150 loss: 0.5295182460546494
  batch 200 loss: 0.48640330255031583
  batch 250 loss: 0.5053949671983718
  batch 300 loss: 0.4987524712085724
  batch 350 loss: 0.5285123759508132
  batch 400 loss: 0.5123692548274994
  batch 450 loss: 0.5024054741859436
  batch 500 loss: 0.5192642003297806
  batch 550 loss: 0.5111261808872223
  batch 600 loss: 0.5379632925987243
  batch 650 loss: 0.49486090838909147
  batch 700 loss: 0.5221082276105881
  batch 750 loss: 0.48570670992136
  batch 800 loss: 0.5159161335229874
  batch 850 loss: 0.5284460812807084
  batch 900 loss: 0.5191900497674942
running loss: 11.607535541057587
LOSS train 0.51919 valid 0.70392, valid PER 21.32%
EPOCH 18, Learning Rate: 0.1125
  batch 50 loss: 0.47736186265945435
  batch 100 loss: 0.4745466738939285
  batch 150 loss: 0.5295271295309066
  batch 200 loss: 0.49955267548561094
  batch 250 loss: 0.4892511409521103
  batch 300 loss: 0.484265256524086
  batch 350 loss: 0.477283211350441
  batch 400 loss: 0.48540207505226135
  batch 450 loss: 0.5047895467281341
  batch 500 loss: 0.5018925565481186
  batch 550 loss: 0.532109894156456
  batch 600 loss: 0.4942549842596054
  batch 650 loss: 0.4922463673353195
  batch 700 loss: 0.49816387712955473
  batch 750 loss: 0.5084479653835297
  batch 800 loss: 0.5289353114366532
  batch 850 loss: 0.5079927521944047
  batch 900 loss: 0.5029586744308472
running loss: 12.099314391613007
LOSS train 0.50296 valid 0.70750, valid PER 21.22%
EPOCH 19, Learning Rate: 0.05625
  batch 50 loss: 0.4834788304567337
  batch 100 loss: 0.4834353494644165
  batch 150 loss: 0.4849027097225189
  batch 200 loss: 0.46606269299983977
  batch 250 loss: 0.5013985317945481
  batch 300 loss: 0.5106525331735611
  batch 350 loss: 0.5038894855976105
  batch 400 loss: 0.4949475711584091
  batch 450 loss: 0.45516993224620816
  batch 500 loss: 0.4666756147146225
  batch 550 loss: 0.4998201435804367
  batch 600 loss: 0.48839923352003095
  batch 650 loss: 0.5051612263917923
  batch 700 loss: 0.5056815874576569
  batch 750 loss: 0.4700404268503189
  batch 800 loss: 0.4698456716537476
  batch 850 loss: 0.4870110511779785
  batch 900 loss: 0.4757712906599045
running loss: 11.386148661375046
LOSS train 0.47577 valid 0.70399, valid PER 21.12%
EPOCH 20, Learning Rate: 0.05625
  batch 50 loss: 0.4569629293680191
  batch 100 loss: 0.4809801173210144
  batch 150 loss: 0.47148683995008467
  batch 200 loss: 0.48252399146556857
  batch 250 loss: 0.49985781908035276
  batch 300 loss: 0.49534036457538605
  batch 350 loss: 0.4611670017242432
  batch 400 loss: 0.4726890844106674
  batch 450 loss: 0.46676696360111236
  batch 500 loss: 0.48846271276474
  batch 550 loss: 0.4857784724235535
  batch 600 loss: 0.47235808372497556
  batch 650 loss: 0.5076490950584411
  batch 700 loss: 0.4612914264202118
  batch 750 loss: 0.4708885794878006
  batch 800 loss: 0.4964455306529999
  batch 850 loss: 0.49102406919002534
  batch 900 loss: 0.4685536783933639
running loss: 11.650667130947113
LOSS train 0.46855 valid 0.70738, valid PER 21.06%
Training finished in 6.0 minutes.
Model saved to checkpoints/20231210_043033/model_17
Loading model from checkpoints/20231210_043033/model_17
SUB: 14.35%, DEL: 6.94%, INS: 1.89%, COR: 78.72%, PER: 23.18%
