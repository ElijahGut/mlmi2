Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=512, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=0.5, is_bidir=True)
Total number of model parameters is 2240552
EPOCH 1, Learning Rate: 0.5
  batch 50 loss: 6.101240801811218
  batch 100 loss: 3.303674998283386
  batch 150 loss: 3.1423941040039063
  batch 200 loss: 2.908392972946167
  batch 250 loss: 2.754161400794983
  batch 300 loss: 2.608168749809265
  batch 350 loss: 2.4862880325317382
  batch 400 loss: 2.4443738889694213
  batch 450 loss: 2.3698254013061524
  batch 500 loss: 2.267297124862671
  batch 550 loss: 2.216180942058563
  batch 600 loss: 2.1837962079048157
  batch 650 loss: 2.0994131112098695
  batch 700 loss: 2.1076840209960936
  batch 750 loss: 2.0491315126419067
  batch 800 loss: 2.0208655047416686
  batch 850 loss: 1.994242594242096
  batch 900 loss: 1.9711315846443176
LOSS train 1.97113 valid 1.95853, valid PER 69.69%
EPOCH 2, Learning Rate: 0.5
  batch 50 loss: 1.931435568332672
  batch 100 loss: 1.8845576381683349
  batch 150 loss: 1.8411006116867066
  batch 200 loss: 1.837834575176239
  batch 250 loss: 1.8454049229621887
  batch 300 loss: 1.8063373970985412
  batch 350 loss: 1.7228099012374878
  batch 400 loss: 1.7394363141059876
  batch 450 loss: 1.70257883310318
  batch 500 loss: 1.7275527787208558
  batch 550 loss: 1.7288519477844237
  batch 600 loss: 1.6700487160682678
  batch 650 loss: 1.6942199921607972
  batch 700 loss: 1.6607291054725648
  batch 750 loss: 1.6575223636627197
  batch 800 loss: 1.6013452315330505
  batch 850 loss: 1.6065427207946776
  batch 900 loss: 1.6363591194152831
LOSS train 1.63636 valid 1.56917, valid PER 54.90%
EPOCH 3, Learning Rate: 0.5
  batch 50 loss: 1.588184130191803
  batch 100 loss: 1.5561096525192262
  batch 150 loss: 1.5647699737548828
  batch 200 loss: 1.53737286567688
  batch 250 loss: 1.5139618110656738
  batch 300 loss: 1.5152582931518555
  batch 350 loss: 1.5412628507614137
  batch 400 loss: 1.528165729045868
  batch 450 loss: 1.4864119839668275
  batch 500 loss: 1.482182936668396
  batch 550 loss: 1.4883776164054872
  batch 600 loss: 1.4512769412994384
  batch 650 loss: 1.429024977684021
  batch 700 loss: 1.4424999737739563
  batch 750 loss: 1.4965795254707337
  batch 800 loss: 1.4197032070159912
  batch 850 loss: 1.447030370235443
  batch 900 loss: 1.3798017740249633
LOSS train 1.37980 valid 1.39904, valid PER 45.23%
EPOCH 4, Learning Rate: 0.5
  batch 50 loss: 1.3923969817161561
  batch 100 loss: 1.402353925704956
  batch 150 loss: 1.3498274493217468
  batch 200 loss: 1.3917136907577514
  batch 250 loss: 1.3944447898864747
  batch 300 loss: 1.37861252784729
  batch 350 loss: 1.3227437841892242
  batch 400 loss: 1.348381688594818
  batch 450 loss: 1.3482949101924897
  batch 500 loss: 1.3210457420349122
  batch 550 loss: 1.3307399439811707
  batch 600 loss: 1.3504463267326354
  batch 650 loss: 1.3336196851730346
  batch 700 loss: 1.2961604356765748
  batch 750 loss: 1.287432885169983
  batch 800 loss: 1.241242299079895
  batch 850 loss: 1.3077178251743318
  batch 900 loss: 1.3195635056495667
LOSS train 1.31956 valid 1.27573, valid PER 40.98%
EPOCH 5, Learning Rate: 0.5
  batch 50 loss: 1.2664501547813416
  batch 100 loss: 1.2441963624954224
  batch 150 loss: 1.294543375968933
  batch 200 loss: 1.2196875977516175
  batch 250 loss: 1.2198509991168975
  batch 300 loss: 1.240718148946762
  batch 350 loss: 1.2303637409210204
  batch 400 loss: 1.2375025296211242
  batch 450 loss: 1.2307449579238892
  batch 500 loss: 1.2362473952770232
  batch 550 loss: 1.1904614782333374
  batch 600 loss: 1.2692786002159118
  batch 650 loss: 1.1945875811576843
  batch 700 loss: 1.2467896962165832
  batch 750 loss: 1.1782964873313904
  batch 800 loss: 1.2127450692653656
  batch 850 loss: 1.1919503951072692
  batch 900 loss: 1.2242535483837127
LOSS train 1.22425 valid 1.18117, valid PER 37.48%
EPOCH 6, Learning Rate: 0.5
  batch 50 loss: 1.202657653093338
  batch 100 loss: 1.1538395154476166
  batch 150 loss: 1.1443495595455169
  batch 200 loss: 1.1641330695152283
  batch 250 loss: 1.181737574338913
  batch 300 loss: 1.1559739756584166
  batch 350 loss: 1.1556183397769928
  batch 400 loss: 1.1542935073375702
  batch 450 loss: 1.166274436712265
  batch 500 loss: 1.1538351416587829
  batch 550 loss: 1.1828246486186982
  batch 600 loss: 1.1365184879302979
  batch 650 loss: 1.1413121974468232
  batch 700 loss: 1.1519594538211821
  batch 750 loss: 1.1181473302841187
  batch 800 loss: 1.1211908078193664
  batch 850 loss: 1.109310997724533
  batch 900 loss: 1.1472529208660125
LOSS train 1.14725 valid 1.13792, valid PER 37.36%
EPOCH 7, Learning Rate: 0.5
  batch 50 loss: 1.1218202757835387
  batch 100 loss: 1.1120261287689208
  batch 150 loss: 1.1097266745567322
  batch 200 loss: 1.0866269016265868
  batch 250 loss: 1.088887070417404
  batch 300 loss: 1.0619588935375213
  batch 350 loss: 1.0892209672927857
  batch 400 loss: 1.0916609847545624
  batch 450 loss: 1.1015281295776367
  batch 500 loss: 1.0843758332729339
  batch 550 loss: 1.0824873924255372
  batch 600 loss: 1.0792979967594147
  batch 650 loss: 1.074070212841034
  batch 700 loss: 1.0936911880970002
  batch 750 loss: 1.0544879674911498
  batch 800 loss: 1.0613610756397247
  batch 850 loss: 1.0948260271549224
  batch 900 loss: 1.1093325865268708
LOSS train 1.10933 valid 1.10881, valid PER 35.76%
EPOCH 8, Learning Rate: 0.5
  batch 50 loss: 1.0382282078266143
  batch 100 loss: 1.0401801764965057
  batch 150 loss: 1.0308871126174928
  batch 200 loss: 1.0166682171821595
  batch 250 loss: 1.0419606733322144
  batch 300 loss: 0.9814216709136963
  batch 350 loss: 1.0596453750133514
  batch 400 loss: 1.0171780645847321
  batch 450 loss: 1.0398141622543335
  batch 500 loss: 1.0675880241394042
  batch 550 loss: 1.0087780666351318
  batch 600 loss: 1.0341842448711396
  batch 650 loss: 1.0637576985359192
  batch 700 loss: 1.0171269404888152
  batch 750 loss: 1.0155146420001984
  batch 800 loss: 1.0467202472686767
  batch 850 loss: 1.0160737705230714
  batch 900 loss: 1.0106593298912048
LOSS train 1.01066 valid 1.04291, valid PER 32.65%
EPOCH 9, Learning Rate: 0.5
  batch 50 loss: 0.9390895080566406
  batch 100 loss: 1.0106300950050353
  batch 150 loss: 0.9885820269584655
  batch 200 loss: 0.9699070239067078
  batch 250 loss: 0.9980950546264649
  batch 300 loss: 1.003889904022217
  batch 350 loss: 1.0188680708408355
  batch 400 loss: 1.005155827999115
  batch 450 loss: 0.9860183012485504
  batch 500 loss: 0.9741257178783417
  batch 550 loss: 1.0032579350471496
  batch 600 loss: 1.0023599112033843
  batch 650 loss: 0.981526802778244
  batch 700 loss: 0.960486466884613
  batch 750 loss: 0.9637111032009125
  batch 800 loss: 0.9773471975326538
  batch 850 loss: 1.0104428851604461
  batch 900 loss: 0.9507097852230072
LOSS train 0.95071 valid 1.01472, valid PER 32.37%
EPOCH 10, Learning Rate: 0.5
  batch 50 loss: 0.9135989725589753
  batch 100 loss: 0.9324750649929047
  batch 150 loss: 0.9567259657382965
  batch 200 loss: 0.9779687571525574
  batch 250 loss: 0.9538775634765625
  batch 300 loss: 0.9303034746646881
  batch 350 loss: 0.9646914339065552
  batch 400 loss: 0.898187712430954
  batch 450 loss: 0.917579072713852
  batch 500 loss: 0.9481736612319946
  batch 550 loss: 0.9704443526268005
  batch 600 loss: 0.9359629797935486
  batch 650 loss: 0.9215312445163727
  batch 700 loss: 0.9468894231319428
  batch 750 loss: 0.9250607311725616
  batch 800 loss: 0.9486649918556214
  batch 850 loss: 0.9519054460525512
  batch 900 loss: 0.9479949617385864
LOSS train 0.94799 valid 1.01482, valid PER 33.74%
EPOCH 11, Learning Rate: 0.5
  batch 50 loss: 0.8861565935611725
  batch 100 loss: 0.870499382019043
  batch 150 loss: 0.8740829753875733
  batch 200 loss: 0.9375314199924469
  batch 250 loss: 0.9258024859428405
  batch 300 loss: 0.8898301720619202
  batch 350 loss: 0.8938128221035003
  batch 400 loss: 0.9276924431324005
  batch 450 loss: 0.9067392718791961
  batch 500 loss: 0.8824217057228089
  batch 550 loss: 0.892416568994522
  batch 600 loss: 0.880176099538803
  batch 650 loss: 0.9477905869483948
  batch 700 loss: 0.8724499905109405
  batch 750 loss: 0.894337385892868
  batch 800 loss: 0.9239823007583619
  batch 850 loss: 0.943327888250351
  batch 900 loss: 0.9128959989547729
LOSS train 0.91290 valid 0.97616, valid PER 31.18%
EPOCH 12, Learning Rate: 0.5
  batch 50 loss: 0.8632147669792175
  batch 100 loss: 0.8457823121547698
  batch 150 loss: 0.832090413570404
  batch 200 loss: 0.8586621272563935
  batch 250 loss: 0.877046570777893
  batch 300 loss: 0.8633247494697571
  batch 350 loss: 0.8564248096942901
  batch 400 loss: 0.8727205550670624
  batch 450 loss: 0.8791166841983795
  batch 500 loss: 0.887909095287323
  batch 550 loss: 0.8293450319766998
  batch 600 loss: 0.8587298095226288
  batch 650 loss: 0.8825177049636841
  batch 700 loss: 0.8843029260635376
  batch 750 loss: 0.8492884802818298
  batch 800 loss: 0.855463547706604
  batch 850 loss: 0.9190914404392242
  batch 900 loss: 0.9054743719100952
LOSS train 0.90547 valid 0.95614, valid PER 30.93%
EPOCH 13, Learning Rate: 0.5
  batch 50 loss: 0.812098935842514
  batch 100 loss: 0.8276085495948792
  batch 150 loss: 0.8076951396465302
  batch 200 loss: 0.8442633724212647
  batch 250 loss: 0.8200404119491577
  batch 300 loss: 0.8141501152515411
  batch 350 loss: 0.824930522441864
  batch 400 loss: 0.8378777289390564
  batch 450 loss: 0.8491856145858765
  batch 500 loss: 0.8202928841114044
  batch 550 loss: 0.8411696481704712
  batch 600 loss: 0.8293205869197845
  batch 650 loss: 0.8438020718097686
  batch 700 loss: 0.8474845772981644
  batch 750 loss: 0.7954267656803131
  batch 800 loss: 0.827362185716629
  batch 850 loss: 0.8649358081817627
  batch 900 loss: 0.8462031090259552
LOSS train 0.84620 valid 0.95178, valid PER 29.66%
EPOCH 14, Learning Rate: 0.5
  batch 50 loss: 0.7817159867286683
  batch 100 loss: 0.7858135426044464
  batch 150 loss: 0.7978239321708679
  batch 200 loss: 0.7837970447540283
  batch 250 loss: 0.8051005566120147
  batch 300 loss: 0.8311482298374177
  batch 350 loss: 0.7682312536239624
  batch 400 loss: 0.781332488656044
  batch 450 loss: 0.7827399003505707
  batch 500 loss: 0.8198358845710755
  batch 550 loss: 0.8102553367614747
  batch 600 loss: 0.7775270521640778
  batch 650 loss: 0.795144864320755
  batch 700 loss: 0.8421976232528686
  batch 750 loss: 0.7814061892032623
  batch 800 loss: 0.7729638111591339
  batch 850 loss: 0.813155151605606
  batch 900 loss: 0.8219501161575318
LOSS train 0.82195 valid 0.94015, valid PER 29.10%
EPOCH 15, Learning Rate: 0.5
  batch 50 loss: 0.7661882352828979
  batch 100 loss: 0.7418184667825699
  batch 150 loss: 0.7627573674917221
  batch 200 loss: 0.7873175370693207
  batch 250 loss: 0.7963465344905853
  batch 300 loss: 0.7531782221794129
  batch 350 loss: 0.7592169868946076
  batch 400 loss: 0.7480583763122559
  batch 450 loss: 0.7538674712181092
  batch 500 loss: 0.7347619962692261
  batch 550 loss: 0.7682484644651413
  batch 600 loss: 0.7913213813304901
  batch 650 loss: 0.7918660974502564
  batch 700 loss: 0.7878732079267502
  batch 750 loss: 0.777775548696518
  batch 800 loss: 0.7615949833393096
  batch 850 loss: 0.7549362337589264
  batch 900 loss: 0.7778327369689941
LOSS train 0.77783 valid 0.94148, valid PER 29.06%
EPOCH 16, Learning Rate: 0.5
  batch 50 loss: 0.7371367752552033
  batch 100 loss: 0.7044642281532287
  batch 150 loss: 0.7290930122137069
  batch 200 loss: 0.7258665186166763
  batch 250 loss: 0.7612225961685181
  batch 300 loss: 0.733873302936554
  batch 350 loss: 0.744149923324585
  batch 400 loss: 0.7465693831443787
  batch 450 loss: 0.7526780021190643
  batch 500 loss: 0.7257645905017853
  batch 550 loss: 0.732301082611084
  batch 600 loss: 0.7190425407886505
  batch 650 loss: 0.7598582112789154
  batch 700 loss: 0.7235504245758057
  batch 750 loss: 0.7470896887779236
  batch 800 loss: 0.7535820841789246
  batch 850 loss: 0.7439545148611069
  batch 900 loss: 0.7430867660045624
LOSS train 0.74309 valid 0.91751, valid PER 27.83%
EPOCH 17, Learning Rate: 0.5
  batch 50 loss: 0.7022829860448837
  batch 100 loss: 0.7071368819475174
  batch 150 loss: 0.6958641332387924
  batch 200 loss: 0.6860803496837616
  batch 250 loss: 0.7358104300498962
  batch 300 loss: 0.7104208064079285
  batch 350 loss: 0.6804682338237762
  batch 400 loss: 0.7391267907619476
  batch 450 loss: 0.730797027349472
  batch 500 loss: 0.6840739214420318
  batch 550 loss: 0.7016544175148011
  batch 600 loss: 0.747149977684021
  batch 650 loss: 0.6991263842582702
  batch 700 loss: 0.7021746438741684
  batch 750 loss: 0.6943246412277222
  batch 800 loss: 0.7012811827659607
  batch 850 loss: 0.7286106324195862
  batch 900 loss: 0.6962060463428498
LOSS train 0.69621 valid 0.93352, valid PER 27.93%
EPOCH 18, Learning Rate: 0.5
  batch 50 loss: 0.6619123244285583
  batch 100 loss: 0.6674583661556244
  batch 150 loss: 0.7180588924884796
  batch 200 loss: 0.6827948385477066
  batch 250 loss: 0.6870716094970704
  batch 300 loss: 0.6575484770536423
  batch 350 loss: 0.6643737876415252
  batch 400 loss: 0.6730282521247863
  batch 450 loss: 0.7100189626216888
  batch 500 loss: 0.6866595566272735
  batch 550 loss: 0.6896474343538285
  batch 600 loss: 0.6706538558006286
  batch 650 loss: 0.6665197223424911
  batch 700 loss: 0.6964221233129502
  batch 750 loss: 0.6829968172311783
  batch 800 loss: 0.68488017141819
  batch 850 loss: 0.6770100551843643
  batch 900 loss: 0.6862435901165008
LOSS train 0.68624 valid 0.92638, valid PER 28.42%
EPOCH 19, Learning Rate: 0.5
  batch 50 loss: 0.6248357450962067
  batch 100 loss: 0.606425810456276
  batch 150 loss: 0.6361950021982193
  batch 200 loss: 0.6466914874315262
  batch 250 loss: 0.6408572226762772
  batch 300 loss: 0.6498223119974136
  batch 350 loss: 0.6457627540826798
  batch 400 loss: 0.6504679208993912
  batch 450 loss: 0.6727290600538254
  batch 500 loss: 0.6449042177200317
  batch 550 loss: 0.6281030452251435
  batch 600 loss: 0.6436524605751037
  batch 650 loss: 0.700705806016922
  batch 700 loss: 0.6578599220514297
  batch 750 loss: 0.6353776723146438
  batch 800 loss: 0.6657467162609101
  batch 850 loss: 0.6744042712450028
  batch 900 loss: 0.6629949855804443
LOSS train 0.66299 valid 0.94033, valid PER 27.73%
EPOCH 20, Learning Rate: 0.5
  batch 50 loss: 0.6072953981161118
  batch 100 loss: 0.5898476886749268
  batch 150 loss: 0.5959985762834549
  batch 200 loss: 0.6168271625041961
  batch 250 loss: 0.6129647409915924
  batch 300 loss: 0.6350031095743179
  batch 350 loss: 0.6000588458776474
  batch 400 loss: 0.6385822522640229
  batch 450 loss: 0.6198044353723526
  batch 500 loss: 0.6043977904319763
  batch 550 loss: 0.668838923573494
  batch 600 loss: 0.6046631109714508
  batch 650 loss: 0.6321092134714127
  batch 700 loss: 0.6421020126342774
  batch 750 loss: 0.6024282461404801
  batch 800 loss: 0.660234558582306
  batch 850 loss: 0.6448611277341842
  batch 900 loss: 0.6367743855714798
LOSS train 0.63677 valid 0.95278, valid PER 27.56%
Training finished in 8.0 minutes.
Model saved to checkpoints/20231207_193515/model_16
Loading model from checkpoints/20231207_193515/model_16
SUB: 14.64%, DEL: 11.96%, INS: 2.27%, COR: 73.40%, PER: 28.86%
