Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=2, fbank_dims=23, model_dims=128, concat=1, lr=0.1, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 562216
EPOCH 1, Learning Rate: 0.1
  batch 50 loss: 7.310770683288574
  batch 100 loss: 3.3233532381057738
  batch 150 loss: 3.2950561380386354
  batch 200 loss: 3.2767126083374025
  batch 250 loss: 3.2673843622207643
  batch 300 loss: 3.2405784940719604
  batch 350 loss: 3.2371372842788695
  batch 400 loss: 3.243503222465515
  batch 450 loss: 3.2315887928009035
  batch 500 loss: 3.2115242290496826
  batch 550 loss: 3.190426034927368
  batch 600 loss: 3.1614320850372315
  batch 650 loss: 3.1299265575408937
  batch 700 loss: 3.1182293367385863
  batch 750 loss: 3.0697474575042722
  batch 800 loss: 3.037131314277649
  batch 850 loss: 3.0046956586837767
  batch 900 loss: 2.930008473396301
avg val loss: 2.8771743774414062
LOSS train 2.93001 valid 2.87717, valid PER 88.08%
EPOCH 2, Learning Rate: 0.1
  batch 50 loss: 2.8810376644134523
  batch 100 loss: 2.8251027154922483
  batch 150 loss: 2.737072157859802
  batch 200 loss: 2.709147572517395
  batch 250 loss: 2.7056841230392457
  batch 300 loss: 2.6466844415664674
  batch 350 loss: 2.609545965194702
  batch 400 loss: 2.5636093997955323
  batch 450 loss: 2.547463746070862
  batch 500 loss: 2.502995491027832
  batch 550 loss: 2.4992700099945067
  batch 600 loss: 2.444335312843323
  batch 650 loss: 2.396138744354248
  batch 700 loss: 2.3763255071640015
  batch 750 loss: 2.338291187286377
  batch 800 loss: 2.288985185623169
  batch 850 loss: 2.2957545208930967
  batch 900 loss: 2.2582928562164306
avg val loss: 2.214904546737671
LOSS train 2.25829 valid 2.21490, valid PER 77.21%
EPOCH 3, Learning Rate: 0.1
  batch 50 loss: 2.1752105808258055
  batch 100 loss: 2.2330029010772705
  batch 150 loss: 2.229737493991852
  batch 200 loss: 2.137339000701904
  batch 250 loss: 2.153370966911316
  batch 300 loss: 2.118092179298401
  batch 350 loss: 2.11852516412735
  batch 400 loss: 2.081040744781494
  batch 450 loss: 2.043749306201935
  batch 500 loss: 2.006951942443848
  batch 550 loss: 2.008437249660492
  batch 600 loss: 1.9597573232650758
  batch 650 loss: 1.9773017692565917
  batch 700 loss: 1.9425176644325257
  batch 750 loss: 1.9647561812400818
  batch 800 loss: 1.9370628881454468
  batch 850 loss: 1.8807400894165038
  batch 900 loss: 1.8922221565246582
avg val loss: 1.8350707292556763
LOSS train 1.89222 valid 1.83507, valid PER 68.77%
EPOCH 4, Learning Rate: 0.1
  batch 50 loss: 1.8906917881965637
  batch 100 loss: 1.8094234299659728
  batch 150 loss: 1.8166529846191406
  batch 200 loss: 1.8120916533470153
  batch 250 loss: 1.8301821517944337
  batch 300 loss: 1.80096777677536
  batch 350 loss: 1.7635590958595275
  batch 400 loss: 1.7333357906341553
  batch 450 loss: 1.7416623449325561
  batch 500 loss: 1.7671506571769715
  batch 550 loss: 1.7015256452560426
  batch 600 loss: 1.7175690579414367
  batch 650 loss: 1.7459368538856506
  batch 700 loss: 1.7575770926475525
  batch 750 loss: 1.6800034070014953
  batch 800 loss: 1.6911822867393493
  batch 850 loss: 1.6470317888259887
  batch 900 loss: 1.6406137132644654
avg val loss: 1.5850889682769775
LOSS train 1.64061 valid 1.58509, valid PER 58.98%
EPOCH 5, Learning Rate: 0.1
  batch 50 loss: 1.6628145623207091
  batch 100 loss: 1.626698317527771
  batch 150 loss: 1.6322065997123718
  batch 200 loss: 1.6466606378555297
  batch 250 loss: 1.60007559299469
  batch 300 loss: 1.6061927700042724
  batch 350 loss: 1.5844904375076294
  batch 400 loss: 1.5537128567695617
  batch 450 loss: 1.5434557747840882
  batch 500 loss: 1.5315636253356935
  batch 550 loss: 1.5718162703514098
  batch 600 loss: 1.5697066688537598
  batch 650 loss: 1.5417551255226136
  batch 700 loss: 1.530007026195526
  batch 750 loss: 1.527687237262726
  batch 800 loss: 1.5676682448387147
  batch 850 loss: 1.5291991639137268
  batch 900 loss: 1.496953980922699
avg val loss: 1.462816834449768
LOSS train 1.49695 valid 1.46282, valid PER 53.06%
EPOCH 6, Learning Rate: 0.1
  batch 50 loss: 1.484732587337494
  batch 100 loss: 1.4912042045593261
  batch 150 loss: 1.4795381951332092
  batch 200 loss: 1.4277870559692383
  batch 250 loss: 1.4714375352859497
  batch 300 loss: 1.4873700857162475
  batch 350 loss: 1.461476662158966
  batch 400 loss: 1.4359290957450868
  batch 450 loss: 1.472925033569336
  batch 500 loss: 1.411430356502533
  batch 550 loss: 1.4532536935806275
  batch 600 loss: 1.4118138551712036
  batch 650 loss: 1.4158828234672547
  batch 700 loss: 1.3976059079170227
  batch 750 loss: 1.4398530888557435
  batch 800 loss: 1.40747887134552
  batch 850 loss: 1.429936637878418
  batch 900 loss: 1.4284625601768495
avg val loss: 1.3147015571594238
LOSS train 1.42846 valid 1.31470, valid PER 44.54%
EPOCH 7, Learning Rate: 0.1
  batch 50 loss: 1.383686397075653
  batch 100 loss: 1.4134968829154968
  batch 150 loss: 1.3448351955413818
  batch 200 loss: 1.3384363460540771
  batch 250 loss: 1.3756094360351563
  batch 300 loss: 1.3617095685005187
  batch 350 loss: 1.3794226193428039
  batch 400 loss: 1.3471110153198242
  batch 450 loss: 1.3578573656082153
  batch 500 loss: 1.3394095373153687
  batch 550 loss: 1.2970014381408692
  batch 600 loss: 1.3214679741859436
  batch 650 loss: 1.2919926428794861
  batch 700 loss: 1.3448124647140502
  batch 750 loss: 1.3206401646137238
  batch 800 loss: 1.3237888932228088
  batch 850 loss: 1.2804913008213044
  batch 900 loss: 1.30627188205719
avg val loss: 1.2427555322647095
LOSS train 1.30627 valid 1.24276, valid PER 40.13%
EPOCH 8, Learning Rate: 0.1
  batch 50 loss: 1.3226039600372315
  batch 100 loss: 1.2841520357131957
  batch 150 loss: 1.3057262587547303
  batch 200 loss: 1.2701580619812012
  batch 250 loss: 1.250304914712906
  batch 300 loss: 1.2316361927986146
  batch 350 loss: 1.260365722179413
  batch 400 loss: 1.2577199280261993
  batch 450 loss: 1.3044417095184326
  batch 500 loss: 1.2874788892269136
  batch 550 loss: 1.2558274710178374
  batch 600 loss: 1.2070270776748657
  batch 650 loss: 1.2302966976165772
  batch 700 loss: 1.2708210229873658
  batch 750 loss: 1.2593706059455871
  batch 800 loss: 1.233744089603424
  batch 850 loss: 1.2266881799697875
  batch 900 loss: 1.2320190834999085
avg val loss: 1.1589913368225098
LOSS train 1.23202 valid 1.15899, valid PER 35.76%
EPOCH 9, Learning Rate: 0.1
  batch 50 loss: 1.2179066479206084
  batch 100 loss: 1.200806087255478
  batch 150 loss: 1.1850158047676087
  batch 200 loss: 1.1945880353450775
  batch 250 loss: 1.190150488615036
  batch 300 loss: 1.1943650662899017
  batch 350 loss: 1.1684393167495728
  batch 400 loss: 1.2191289615631105
  batch 450 loss: 1.2077599728107453
  batch 500 loss: 1.184770131111145
  batch 550 loss: 1.190970938205719
  batch 600 loss: 1.216927102804184
  batch 650 loss: 1.2001654064655305
  batch 700 loss: 1.1780535221099853
  batch 750 loss: 1.1652126097679139
  batch 800 loss: 1.2174916899204253
  batch 850 loss: 1.2003810930252075
  batch 900 loss: 1.1585176634788512
avg val loss: 1.0975366830825806
LOSS train 1.15852 valid 1.09754, valid PER 34.22%
EPOCH 10, Learning Rate: 0.1
  batch 50 loss: 1.1561408853530883
  batch 100 loss: 1.1726106095314026
  batch 150 loss: 1.179574944972992
  batch 200 loss: 1.15467551112175
  batch 250 loss: 1.1589071357250214
  batch 300 loss: 1.1451352167129516
  batch 350 loss: 1.1363237702846527
  batch 400 loss: 1.1101902627944946
  batch 450 loss: 1.1258396542072295
  batch 500 loss: 1.1375164043903352
  batch 550 loss: 1.1299523663520814
  batch 600 loss: 1.129148050546646
  batch 650 loss: 1.1405715012550355
  batch 700 loss: 1.1655462610721588
  batch 750 loss: 1.144803079366684
  batch 800 loss: 1.1515461432933807
  batch 850 loss: 1.1319997262954713
  batch 900 loss: 1.1195879876613617
avg val loss: 1.073030948638916
LOSS train 1.11959 valid 1.07303, valid PER 33.92%
EPOCH 11, Learning Rate: 0.1
  batch 50 loss: 1.1304068839550019
  batch 100 loss: 1.1046363890171051
  batch 150 loss: 1.0979244601726532
  batch 200 loss: 1.0722632813453674
  batch 250 loss: 1.0880850446224213
  batch 300 loss: 1.0844768738746644
  batch 350 loss: 1.1275876915454865
  batch 400 loss: 1.073435423374176
  batch 450 loss: 1.0951595401763916
  batch 500 loss: 1.1019969999790191
  batch 550 loss: 1.1118357121944427
  batch 600 loss: 1.1100596046447755
  batch 650 loss: 1.1057176780700684
  batch 700 loss: 1.1713421177864074
  batch 750 loss: 1.0784539604187011
  batch 800 loss: 1.0854187941551208
  batch 850 loss: 1.1043127167224884
  batch 900 loss: 1.119244922399521
avg val loss: 1.0275707244873047
LOSS train 1.11924 valid 1.02757, valid PER 32.06%
EPOCH 12, Learning Rate: 0.1
  batch 50 loss: 1.0470769393444062
  batch 100 loss: 1.0426204431056976
  batch 150 loss: 1.0610421431064605
  batch 200 loss: 1.0937780606746674
  batch 250 loss: 1.054579359292984
  batch 300 loss: 1.092307879924774
  batch 350 loss: 1.0641374099254608
  batch 400 loss: 1.0911521100997925
  batch 450 loss: 1.0594587445259094
  batch 500 loss: 1.0944714772701263
  batch 550 loss: 1.0767957866191864
  batch 600 loss: 1.0725037610530854
  batch 650 loss: 1.0711034178733825
  batch 700 loss: 1.0481083512306213
  batch 750 loss: 1.0807775962352753
  batch 800 loss: 1.0201982760429382
  batch 850 loss: 1.0647299444675447
  batch 900 loss: 1.0574473595619203
avg val loss: 1.0121715068817139
LOSS train 1.05745 valid 1.01217, valid PER 31.42%
EPOCH 13, Learning Rate: 0.1
  batch 50 loss: 1.032276166677475
  batch 100 loss: 1.035801944732666
  batch 150 loss: 1.0467156898975372
  batch 200 loss: 1.0057571458816528
  batch 250 loss: 1.0186641037464141
  batch 300 loss: 1.0821567177772522
  batch 350 loss: 1.0197538673877715
  batch 400 loss: 1.0323447799682617
  batch 450 loss: 1.0547479975223542
  batch 500 loss: 1.0334321784973144
  batch 550 loss: 1.0895970737934113
  batch 600 loss: 1.044288536310196
  batch 650 loss: 1.0505732691287994
  batch 700 loss: 1.03404012799263
  batch 750 loss: 0.9737517988681793
  batch 800 loss: 1.036776202917099
  batch 850 loss: 1.0065542376041412
  batch 900 loss: 1.0222807705402375
avg val loss: 0.999141275882721
LOSS train 1.02228 valid 0.99914, valid PER 30.36%
EPOCH 14, Learning Rate: 0.1
  batch 50 loss: 1.001142861843109
  batch 100 loss: 0.9938129842281341
  batch 150 loss: 1.0127361178398133
  batch 200 loss: 1.002594872713089
  batch 250 loss: 1.0028166902065276
  batch 300 loss: 0.9695938491821289
  batch 350 loss: 0.9864437830448151
  batch 400 loss: 1.0371263337135315
  batch 450 loss: 0.9859156739711762
  batch 500 loss: 0.9955677938461304
  batch 550 loss: 1.0043536913394928
  batch 600 loss: 0.9945546817779541
  batch 650 loss: 1.0117420077323913
  batch 700 loss: 1.0301015067100525
  batch 750 loss: 1.0011517584323884
  batch 800 loss: 0.9867699277400971
  batch 850 loss: 1.0531572782993317
  batch 900 loss: 1.004713212251663
avg val loss: 0.9811784029006958
LOSS train 1.00471 valid 0.98118, valid PER 30.27%
EPOCH 15, Learning Rate: 0.1
  batch 50 loss: 0.9567104721069336
  batch 100 loss: 0.9843784999847413
  batch 150 loss: 0.9843401503562927
  batch 200 loss: 1.0125022983551026
  batch 250 loss: 1.010769522190094
  batch 300 loss: 0.9977034640312195
  batch 350 loss: 0.9864665710926056
  batch 400 loss: 0.9958960771560669
  batch 450 loss: 0.9717542231082916
  batch 500 loss: 0.9790218710899353
  batch 550 loss: 1.0058140993118285
  batch 600 loss: 1.0144162011146545
  batch 650 loss: 0.9673458552360534
  batch 700 loss: 0.9455540931224823
  batch 750 loss: 1.0011475801467895
  batch 800 loss: 0.9833910322189331
  batch 850 loss: 0.9636153185367584
  batch 900 loss: 0.9176547884941101
avg val loss: 0.9526975750923157
LOSS train 0.91765 valid 0.95270, valid PER 30.17%
EPOCH 16, Learning Rate: 0.1
  batch 50 loss: 0.9760424172878266
  batch 100 loss: 0.9544045555591584
  batch 150 loss: 0.9540582597255707
  batch 200 loss: 0.9864400410652161
  batch 250 loss: 0.9554845130443573
  batch 300 loss: 0.9640090811252594
  batch 350 loss: 0.9703697025775909
  batch 400 loss: 0.9609290111064911
  batch 450 loss: 0.9755822944641114
  batch 500 loss: 0.9659451901912689
  batch 550 loss: 0.9313775157928467
  batch 600 loss: 0.9665079689025879
  batch 650 loss: 0.9790153920650482
  batch 700 loss: 0.9348924207687378
  batch 750 loss: 0.9655962884426117
  batch 800 loss: 0.9457999682426452
  batch 850 loss: 0.9468682754039764
  batch 900 loss: 0.9539042437076568
avg val loss: 0.9375078082084656
LOSS train 0.95390 valid 0.93751, valid PER 28.82%
EPOCH 17, Learning Rate: 0.1
  batch 50 loss: 0.9617321264743804
  batch 100 loss: 0.9022517657279968
  batch 150 loss: 0.9786046195030212
  batch 200 loss: 0.9041186332702636
  batch 250 loss: 0.9476379895210266
  batch 300 loss: 0.9216517388820649
  batch 350 loss: 0.9410091662406921
  batch 400 loss: 0.9369468080997467
  batch 450 loss: 0.9172002911567688
  batch 500 loss: 0.9569172263145447
  batch 550 loss: 0.9328459405899048
  batch 600 loss: 0.9505151462554932
  batch 650 loss: 0.9295291996002197
  batch 700 loss: 0.9350951278209686
  batch 750 loss: 0.8933076918125152
  batch 800 loss: 0.9482142460346222
  batch 850 loss: 0.9329184722900391
  batch 900 loss: 0.9380778133869171
avg val loss: 0.9291973114013672
LOSS train 0.93808 valid 0.92920, valid PER 28.54%
EPOCH 18, Learning Rate: 0.1
  batch 50 loss: 0.9139017224311828
  batch 100 loss: 0.9021559226512909
  batch 150 loss: 0.9327404415607452
  batch 200 loss: 0.9283694505691529
  batch 250 loss: 0.8985094976425171
  batch 300 loss: 0.8957864975929261
  batch 350 loss: 0.8812010431289673
  batch 400 loss: 0.9073956513404846
  batch 450 loss: 0.9212085604667664
  batch 500 loss: 0.9123320019245148
  batch 550 loss: 0.9521352899074554
  batch 600 loss: 0.9274564039707184
  batch 650 loss: 0.9079565799236298
  batch 700 loss: 0.8992906653881073
  batch 750 loss: 0.9273225510120392
  batch 800 loss: 0.932522920370102
  batch 850 loss: 0.9354775357246399
  batch 900 loss: 0.9069709408283234
avg val loss: 0.9051392674446106
LOSS train 0.90697 valid 0.90514, valid PER 27.67%
EPOCH 19, Learning Rate: 0.1
  batch 50 loss: 0.9045274496078491
  batch 100 loss: 0.9002104079723359
  batch 150 loss: 0.902432472705841
  batch 200 loss: 0.8793670785427093
  batch 250 loss: 0.9183600199222565
  batch 300 loss: 0.8959042572975159
  batch 350 loss: 0.9103699374198914
  batch 400 loss: 0.8977260160446167
  batch 450 loss: 0.8662916469573975
  batch 500 loss: 0.8908514940738678
  batch 550 loss: 0.8970720756053925
  batch 600 loss: 0.8997752702236176
  batch 650 loss: 0.9295014643669128
  batch 700 loss: 0.9184945034980774
  batch 750 loss: 0.8789568388462067
  batch 800 loss: 0.8537714231014252
  batch 850 loss: 0.9059652817249298
  batch 900 loss: 0.8907320129871369
avg val loss: 0.9033963084220886
LOSS train 0.89073 valid 0.90340, valid PER 27.74%
EPOCH 20, Learning Rate: 0.1
  batch 50 loss: 0.840072032213211
  batch 100 loss: 0.8921160483360291
  batch 150 loss: 0.8873190641403198
  batch 200 loss: 0.8864981961250306
  batch 250 loss: 0.9075953757762909
  batch 300 loss: 0.8925406992435455
  batch 350 loss: 0.8541140615940094
  batch 400 loss: 0.8679148435592652
  batch 450 loss: 0.8787357950210571
  batch 500 loss: 0.8920434153079987
  batch 550 loss: 0.8909988176822662
  batch 600 loss: 0.865500215291977
  batch 650 loss: 0.8834506869316101
  batch 700 loss: 0.859240392446518
  batch 750 loss: 0.8505267345905304
  batch 800 loss: 0.8686899173259736
  batch 850 loss: 0.8800008594989777
  batch 900 loss: 0.8542801523208619
avg val loss: 0.8965715765953064
LOSS train 0.85428 valid 0.89657, valid PER 27.59%
Training finished in 5.0 minutes.
Model saved to checkpoints/20231210_135211/model_20
Loading model from checkpoints/20231210_135211/model_20
SUB: 16.72%, DEL: 10.34%, INS: 2.25%, COR: 72.93%, PER: 29.32%
