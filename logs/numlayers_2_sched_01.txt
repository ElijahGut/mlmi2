Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=2, fbank_dims=23, model_dims=128, concat=1, lr=0.1, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 562216
EPOCH 1, Learning Rate: 0.1
  batch 50 loss: 7.310770788192749
  batch 100 loss: 3.323353123664856
  batch 150 loss: 3.29505615234375
  batch 200 loss: 3.2767128324508668
  batch 250 loss: 3.2673844623565675
  batch 300 loss: 3.2405784130096436
  batch 350 loss: 3.2371372175216675
  batch 400 loss: 3.2435033178329467
  batch 450 loss: 3.231588807106018
  batch 500 loss: 3.2115245151519773
  batch 550 loss: 3.190426115989685
  batch 600 loss: 3.161432342529297
  batch 650 loss: 3.1299269342422487
  batch 700 loss: 3.1182295656204224
  batch 750 loss: 3.06974750995636
  batch 800 loss: 3.0371312999725344
  batch 850 loss: 3.0046955251693728
  batch 900 loss: 2.930008511543274
running loss: 69.38688969612122
LOSS train 2.93001 valid 2.87717, valid PER 88.08%
EPOCH 2, Learning Rate: 0.1
  batch 50 loss: 2.881037697792053
  batch 100 loss: 2.8251031827926636
  batch 150 loss: 2.7370724964141844
  batch 200 loss: 2.7091476678848267
  batch 250 loss: 2.705684332847595
  batch 300 loss: 2.6466847038269044
  batch 350 loss: 2.6095458030700684
  batch 400 loss: 2.563609390258789
  batch 450 loss: 2.547463645935059
  batch 500 loss: 2.5029957771301268
  batch 550 loss: 2.499270887374878
  batch 600 loss: 2.444334850311279
  batch 650 loss: 2.3961389684677123
  batch 700 loss: 2.37632529258728
  batch 750 loss: 2.3382905435562136
  batch 800 loss: 2.2889848852157595
  batch 850 loss: 2.2957552337646483
  batch 900 loss: 2.258292670249939
running loss: 54.33456039428711
LOSS train 2.25829 valid 2.21491, valid PER 77.21%
EPOCH 3, Learning Rate: 0.1
  batch 50 loss: 2.1752101588249206
  batch 100 loss: 2.2330019903182983
  batch 150 loss: 2.2297372794151307
  batch 200 loss: 2.1373387670516966
  batch 250 loss: 2.1533711171150207
  batch 300 loss: 2.11809161901474
  batch 350 loss: 2.1185257267951965
  batch 400 loss: 2.081040813922882
  batch 450 loss: 2.0437483835220336
  batch 500 loss: 2.0069518876075745
  batch 550 loss: 2.0084384441375733
  batch 600 loss: 1.9597577667236328
  batch 650 loss: 1.9773015475273132
  batch 700 loss: 1.942519612312317
  batch 750 loss: 1.9647569155693054
  batch 800 loss: 1.937063593864441
  batch 850 loss: 1.880739185810089
  batch 900 loss: 1.8922219157218934
running loss: 46.52074432373047
LOSS train 1.89222 valid 1.83506, valid PER 68.77%
EPOCH 4, Learning Rate: 0.1
  batch 50 loss: 1.8906923365592956
  batch 100 loss: 1.809424910545349
  batch 150 loss: 1.8166534805297851
  batch 200 loss: 1.812091064453125
  batch 250 loss: 1.830183708667755
  batch 300 loss: 1.8009681057929994
  batch 350 loss: 1.7635589027404786
  batch 400 loss: 1.7333370018005372
  batch 450 loss: 1.74166246175766
  batch 500 loss: 1.7671517539024353
  batch 550 loss: 1.701523370742798
  batch 600 loss: 1.7175700664520264
  batch 650 loss: 1.7459400796890259
  batch 700 loss: 1.7575759649276734
  batch 750 loss: 1.6800034260749817
  batch 800 loss: 1.6911819243431092
  batch 850 loss: 1.6470288395881654
  batch 900 loss: 1.640614821910858
running loss: 40.680245995521545
LOSS train 1.64061 valid 1.58509, valid PER 58.98%
EPOCH 5, Learning Rate: 0.1
  batch 50 loss: 1.6628146004676818
  batch 100 loss: 1.6266967391967773
  batch 150 loss: 1.6322051095962524
  batch 200 loss: 1.6466619181632995
  batch 250 loss: 1.600074932575226
  batch 300 loss: 1.6061927199363708
  batch 350 loss: 1.5844884514808655
  batch 400 loss: 1.5537119460105897
  batch 450 loss: 1.5434578323364259
  batch 500 loss: 1.5315633249282836
  batch 550 loss: 1.5718164372444152
  batch 600 loss: 1.5697056794166564
  batch 650 loss: 1.541755530834198
  batch 700 loss: 1.5300069308280946
  batch 750 loss: 1.5276860308647155
  batch 800 loss: 1.5676717495918273
  batch 850 loss: 1.5291975331306458
  batch 900 loss: 1.4969539475440978
running loss: 35.965980648994446
LOSS train 1.49695 valid 1.46281, valid PER 53.07%
EPOCH 6, Learning Rate: 0.1
  batch 50 loss: 1.4847073221206666
  batch 100 loss: 1.4911979174613952
  batch 150 loss: 1.479541015625
  batch 200 loss: 1.4277756524085998
  batch 250 loss: 1.471443326473236
  batch 300 loss: 1.4873671126365662
  batch 350 loss: 1.461472945213318
  batch 400 loss: 1.4359254813194275
  batch 450 loss: 1.47293035030365
  batch 500 loss: 1.411427206993103
  batch 550 loss: 1.4532593059539796
  batch 600 loss: 1.4118124270439147
  batch 650 loss: 1.4158804440498352
  batch 700 loss: 1.3975927424430847
  batch 750 loss: 1.4398487544059753
  batch 800 loss: 1.407475997209549
  batch 850 loss: 1.4299401831626892
  batch 900 loss: 1.4284464931488037
running loss: 32.716689229011536
LOSS train 1.42845 valid 1.31468, valid PER 44.54%
EPOCH 7, Learning Rate: 0.1
  batch 50 loss: 1.3836707806587218
  batch 100 loss: 1.4134952855110168
  batch 150 loss: 1.3448295831680297
  batch 200 loss: 1.33843323469162
  batch 250 loss: 1.3756122231483459
  batch 300 loss: 1.3617033433914185
  batch 350 loss: 1.3794204783439636
  batch 400 loss: 1.3471059370040894
  batch 450 loss: 1.3578777968883515
  batch 500 loss: 1.3393982934951782
  batch 550 loss: 1.2970001327991485
  batch 600 loss: 1.3214601039886475
  batch 650 loss: 1.2920162796974182
  batch 700 loss: 1.3448020124435425
  batch 750 loss: 1.32063716173172
  batch 800 loss: 1.3237992787361146
  batch 850 loss: 1.280490345954895
  batch 900 loss: 1.3062093544006348
running loss: 30.27009105682373
LOSS train 1.30621 valid 1.24194, valid PER 40.12%
EPOCH 8, Learning Rate: 0.1
  batch 50 loss: 1.322154335975647
  batch 100 loss: 1.2848634648323058
  batch 150 loss: 1.3053778862953187
  batch 200 loss: 1.2711912608146667
  batch 250 loss: 1.2536326694488524
  batch 300 loss: 1.2355655217170716
  batch 350 loss: 1.259224820137024
  batch 400 loss: 1.2537942290306092
  batch 450 loss: 1.3051277375221253
  batch 500 loss: 1.28770880818367
  batch 550 loss: 1.2591702461242675
  batch 600 loss: 1.2037777149677276
  batch 650 loss: 1.2306716799736024
  batch 700 loss: 1.2701867294311524
  batch 750 loss: 1.2573969340324402
  batch 800 loss: 1.2350263488292694
  batch 850 loss: 1.2282704269886018
  batch 900 loss: 1.2355104994773864
running loss: 28.965562522411346
LOSS train 1.23551 valid 1.15166, valid PER 35.64%
EPOCH 9, Learning Rate: 0.1
  batch 50 loss: 1.2168937313556671
  batch 100 loss: 1.2038031101226807
  batch 150 loss: 1.1842113769054412
  batch 200 loss: 1.1920664381980897
  batch 250 loss: 1.1824857199192047
  batch 300 loss: 1.1929632472991942
  batch 350 loss: 1.168288640975952
  batch 400 loss: 1.2242034590244293
  batch 450 loss: 1.1997909116744996
  batch 500 loss: 1.1893822538852692
  batch 550 loss: 1.1955066585540772
  batch 600 loss: 1.2128461158275605
  batch 650 loss: 1.2027996110916137
  batch 700 loss: 1.183018332719803
  batch 750 loss: 1.1662441062927247
  batch 800 loss: 1.2223807883262634
  batch 850 loss: 1.2048131847381591
  batch 900 loss: 1.1634607589244843
running loss: 29.035159587860107
LOSS train 1.16346 valid 1.09155, valid PER 33.82%
EPOCH 10, Learning Rate: 0.05
  batch 50 loss: 1.1339623379707335
  batch 100 loss: 1.150441927909851
  batch 150 loss: 1.1479931640625
  batch 200 loss: 1.1327185940742492
  batch 250 loss: 1.1243837261199952
  batch 300 loss: 1.1155102372169494
  batch 350 loss: 1.115192096233368
  batch 400 loss: 1.0841697669029235
  batch 450 loss: 1.0953838288784028
  batch 500 loss: 1.11000328540802
  batch 550 loss: 1.111103551387787
  batch 600 loss: 1.10782958984375
  batch 650 loss: 1.1253755581378937
  batch 700 loss: 1.1428190565109253
  batch 750 loss: 1.1330934274196625
  batch 800 loss: 1.1249969470500947
  batch 850 loss: 1.1129271757602692
  batch 900 loss: 1.0926659321784973
running loss: 27.390993118286133
LOSS train 1.09267 valid 1.05695, valid PER 33.20%
EPOCH 11, Learning Rate: 0.05
  batch 50 loss: 1.118015674352646
  batch 100 loss: 1.0850072920322418
  batch 150 loss: 1.0843196356296538
  batch 200 loss: 1.059196307659149
  batch 250 loss: 1.0746372616291047
  batch 300 loss: 1.0750726974010467
  batch 350 loss: 1.1239556443691254
  batch 400 loss: 1.0619830203056335
  batch 450 loss: 1.0845194828510285
  batch 500 loss: 1.082608050107956
  batch 550 loss: 1.10143749833107
  batch 600 loss: 1.0948536717891693
  batch 650 loss: 1.1073299610614777
  batch 700 loss: 1.1689375257492065
  batch 750 loss: 1.0768307042121887
  batch 800 loss: 1.0795358943939208
  batch 850 loss: 1.0962108623981477
  batch 900 loss: 1.1103848719596863
running loss: 24.87176203727722
LOSS train 1.11038 valid 1.03498, valid PER 32.86%
EPOCH 12, Learning Rate: 0.05
  batch 50 loss: 1.0549095058441162
  batch 100 loss: 1.0447267925739288
  batch 150 loss: 1.0656015348434449
  batch 200 loss: 1.0890139901638032
  batch 250 loss: 1.0594158732891084
  batch 300 loss: 1.0940414571762085
  batch 350 loss: 1.0713582849502563
  batch 400 loss: 1.0956820666790008
  batch 450 loss: 1.0605340015888214
  batch 500 loss: 1.096721246242523
  batch 550 loss: 1.0786170148849488
  batch 600 loss: 1.0668796753883363
  batch 650 loss: 1.0782586073875426
  batch 700 loss: 1.0612658500671386
  batch 750 loss: 1.0854346644878388
  batch 800 loss: 1.0225423276424408
  batch 850 loss: 1.068564041852951
  batch 900 loss: 1.0627935659885406
running loss: 25.037179052829742
LOSS train 1.06279 valid 1.02080, valid PER 31.18%
EPOCH 13, Learning Rate: 0.025
  batch 50 loss: 1.047045521736145
  batch 100 loss: 1.036907833814621
  batch 150 loss: 1.0613024759292602
  batch 200 loss: 0.9924339699745178
  batch 250 loss: 1.0206548714637755
  batch 300 loss: 1.0759685575962066
  batch 350 loss: 1.0135229063034057
  batch 400 loss: 1.0305465960502624
  batch 450 loss: 1.0510541236400603
  batch 500 loss: 1.0369181644916534
  batch 550 loss: 1.0915802216529846
  batch 600 loss: 1.0604806447029114
  batch 650 loss: 1.0469755625724793
  batch 700 loss: 1.0351043403148652
  batch 750 loss: 0.9834505164623261
  batch 800 loss: 1.0406384181976318
  batch 850 loss: 1.0088299596309662
  batch 900 loss: 1.0301418089866639
running loss: 24.221518635749817
LOSS train 1.03014 valid 0.99782, valid PER 30.93%
EPOCH 14, Learning Rate: 0.025
  batch 50 loss: 1.0193447077274322
  batch 100 loss: 1.0113675856590272
  batch 150 loss: 1.0224355733394623
  batch 200 loss: 0.9998276257514953
  batch 250 loss: 1.016394534111023
  batch 300 loss: 0.980602719783783
  batch 350 loss: 1.0088092660903931
  batch 400 loss: 1.0554068970680237
  batch 450 loss: 1.0043480336666106
  batch 500 loss: 1.022102394104004
  batch 550 loss: 1.0195323956012725
  batch 600 loss: 1.011590850353241
  batch 650 loss: 1.0395943486690522
  batch 700 loss: 1.0529876816272736
  batch 750 loss: 1.0202999198436737
  batch 800 loss: 0.9992467689514161
  batch 850 loss: 1.077579926252365
  batch 900 loss: 1.0209624326229096
running loss: 25.052831649780273
LOSS train 1.02096 valid 0.99114, valid PER 31.02%
EPOCH 15, Learning Rate: 0.0125
  batch 50 loss: 0.9912506091594696
  batch 100 loss: 1.0095860826969147
  batch 150 loss: 0.9995442271232605
  batch 200 loss: 1.0270216214656829
  batch 250 loss: 1.0394085693359374
  batch 300 loss: 1.012655018568039
  batch 350 loss: 1.009417335987091
  batch 400 loss: 1.011641491651535
  batch 450 loss: 0.9970518624782563
  batch 500 loss: 1.0088239121437073
  batch 550 loss: 1.0479735493659974
  batch 600 loss: 1.0365307247638702
  batch 650 loss: 0.9913844585418701
  batch 700 loss: 0.9787133359909057
  batch 750 loss: 1.0264242315292358
  batch 800 loss: 1.0031757640838623
  batch 850 loss: 0.989802747964859
  batch 900 loss: 0.9363967752456666
running loss: 23.767227232456207
LOSS train 0.93640 valid 0.98174, valid PER 30.47%
EPOCH 16, Learning Rate: 0.0125
  batch 50 loss: 1.003667573928833
  batch 100 loss: 0.9974438178539277
  batch 150 loss: 0.9997382736206055
  batch 200 loss: 1.021881821155548
  batch 250 loss: 0.985259793996811
  batch 300 loss: 1.004063687324524
  batch 350 loss: 1.018070172071457
  batch 400 loss: 0.9950048017501831
  batch 450 loss: 1.0249568128585815
  batch 500 loss: 0.9951314628124237
  batch 550 loss: 0.9667567634582519
  batch 600 loss: 1.0099510264396667
  batch 650 loss: 1.0173447835445404
  batch 700 loss: 0.9794876635074615
  batch 750 loss: 1.0155485725402833
  batch 800 loss: 0.98619819521904
  batch 850 loss: 0.9931345844268799
  batch 900 loss: 0.9937121725082397
running loss: 23.40792864561081
LOSS train 0.99371 valid 0.97534, valid PER 30.20%
EPOCH 17, Learning Rate: 0.0125
  batch 50 loss: 1.0091865515708924
  batch 100 loss: 0.9654275286197662
  batch 150 loss: 1.0336412870883942
  batch 200 loss: 0.9558417642116547
  batch 250 loss: 1.0009250617027283
  batch 300 loss: 0.9782467555999755
  batch 350 loss: 1.0027805197238921
  batch 400 loss: 1.0025719225406646
  batch 450 loss: 0.9853582501411438
  batch 500 loss: 1.0004909694194795
  batch 550 loss: 0.9910937452316284
  batch 600 loss: 1.0092685472965242
  batch 650 loss: 0.9944136536121368
  batch 700 loss: 0.9923756861686707
  batch 750 loss: 0.9540289223194123
  batch 800 loss: 1.0184543216228485
  batch 850 loss: 1.0015900194644929
  batch 900 loss: 0.99525883436203
running loss: 22.50420606136322
LOSS train 0.99526 valid 0.97534, valid PER 30.04%
EPOCH 18, Learning Rate: 0.0125
  batch 50 loss: 0.989473522901535
  batch 100 loss: 0.9847885370254517
  batch 150 loss: 1.0091749703884125
  batch 200 loss: 1.002048579454422
  batch 250 loss: 0.9747585868835449
  batch 300 loss: 0.9876833605766296
  batch 350 loss: 0.9509055447578431
  batch 400 loss: 0.9736677360534668
  batch 450 loss: 0.9906136345863342
  batch 500 loss: 0.9869514608383179
  batch 550 loss: 1.0289070105552673
  batch 600 loss: 0.991203726530075
  batch 650 loss: 0.9799426066875457
  batch 700 loss: 0.9718963396549225
  batch 750 loss: 1.0048003315925598
  batch 800 loss: 0.9996220684051513
  batch 850 loss: 0.9924077773094178
  batch 900 loss: 0.9898314452171326
running loss: 23.994565784931183
LOSS train 0.98983 valid 0.97122, valid PER 30.19%
EPOCH 19, Learning Rate: 0.00625
  batch 50 loss: 0.9954432654380798
  batch 100 loss: 0.9812096428871154
  batch 150 loss: 0.9736551094055176
  batch 200 loss: 0.9649014031887054
  batch 250 loss: 0.9985566365718842
  batch 300 loss: 0.9851406145095826
  batch 350 loss: 0.9936887836456298
  batch 400 loss: 0.9846759974956513
  batch 450 loss: 0.9549936783313752
  batch 500 loss: 0.9713531720638275
  batch 550 loss: 0.9845784127712249
  batch 600 loss: 0.9908542656898498
  batch 650 loss: 1.0037607216835023
  batch 700 loss: 1.0054132544994354
  batch 750 loss: 0.9602318775653839
  batch 800 loss: 0.9360034656524658
  batch 850 loss: 0.9985405945777893
  batch 900 loss: 0.9718139219284058
running loss: 23.344563364982605
LOSS train 0.97181 valid 0.96905, valid PER 30.30%
EPOCH 20, Learning Rate: 0.003125
  batch 50 loss: 0.9401754665374756
  batch 100 loss: 0.992939532995224
  batch 150 loss: 0.9801902186870575
  batch 200 loss: 0.972536849975586
  batch 250 loss: 1.0053716349601745
  batch 300 loss: 0.9877031147480011
  batch 350 loss: 0.9630731928348542
  batch 400 loss: 0.9638200759887695
  batch 450 loss: 0.963908805847168
  batch 500 loss: 0.989437301158905
  batch 550 loss: 0.9901188766956329
  batch 600 loss: 0.9694394326210022
  batch 650 loss: 0.990806474685669
  batch 700 loss: 0.9563804411888123
  batch 750 loss: 0.945304514169693
  batch 800 loss: 0.9705797433853149
  batch 850 loss: 0.9721868181228638
  batch 900 loss: 0.9559963285923004
running loss: 24.645589292049408
LOSS train 0.95600 valid 0.96282, valid PER 30.00%
Training finished in 5.0 minutes.
Model saved to checkpoints/20231210_043017/model_20
Loading model from checkpoints/20231210_043017/model_20
SUB: 16.40%, DEL: 12.92%, INS: 1.77%, COR: 70.68%, PER: 31.09%
