Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=None, is_bidir=True)
Total number of model parameters is 166952
EPOCH 1, Learning Rate: 0.5
  batch 50 loss: 4.168336486816406
  batch 100 loss: 3.171690969467163
  batch 150 loss: 3.0335014390945436
  batch 200 loss: 2.920676999092102
  batch 250 loss: 2.845626072883606
  batch 300 loss: 2.6675737953186034
  batch 350 loss: 2.5304164409637453
  batch 400 loss: 2.47391357421875
  batch 450 loss: 2.435191798210144
  batch 500 loss: 2.352832179069519
  batch 550 loss: 2.2337597012519836
  batch 600 loss: 2.1399185943603514
  batch 650 loss: 2.0525386333465576
  batch 700 loss: 2.0444712448120117
  batch 750 loss: 1.9827166223526
  batch 800 loss: 1.9559385347366334
  batch 850 loss: 1.9073656368255616
  batch 900 loss: 1.882458243370056
LOSS train 1.88246 valid 1.81916, valid PER 70.29%
EPOCH 2, Learning Rate: 0.5
  batch 50 loss: 1.8305874109268188
  batch 100 loss: 1.7790384411811828
  batch 150 loss: 1.7636429858207703
  batch 200 loss: 1.7411661267280578
  batch 250 loss: 1.773052909374237
  batch 300 loss: 1.7157744479179382
  batch 350 loss: 1.6239651131629944
  batch 400 loss: 1.6521763849258422
  batch 450 loss: 1.6026969814300538
  batch 500 loss: 1.6225980091094971
  batch 550 loss: 1.6351009917259216
  batch 600 loss: 1.5720024108886719
  batch 650 loss: 1.5998506140708924
  batch 700 loss: 1.5955197215080261
  batch 750 loss: 1.5648335194587708
  batch 800 loss: 1.5014327454566956
  batch 850 loss: 1.498163125514984
  batch 900 loss: 1.5168226313591004
LOSS train 1.51682 valid 1.41190, valid PER 47.57%
EPOCH 3, Learning Rate: 0.5
  batch 50 loss: 1.4709884095191956
  batch 100 loss: 1.4573272824287415
  batch 150 loss: 1.4507134819030763
  batch 200 loss: 1.4246806168556214
  batch 250 loss: 1.4031617474555969
  batch 300 loss: 1.3924849033355713
  batch 350 loss: 1.4438385128974915
  batch 400 loss: 1.398233642578125
  batch 450 loss: 1.391594525575638
  batch 500 loss: 1.3791513133049011
  batch 550 loss: 1.3834661531448365
  batch 600 loss: 1.3631211137771606
  batch 650 loss: 1.3191217041015626
  batch 700 loss: 1.3603145170211792
  batch 750 loss: 1.3915702128410339
  batch 800 loss: 1.3095571279525757
  batch 850 loss: 1.3588890218734742
  batch 900 loss: 1.3013517355918884
LOSS train 1.30135 valid 1.31249, valid PER 41.64%
EPOCH 4, Learning Rate: 0.5
  batch 50 loss: 1.322254900932312
  batch 100 loss: 1.3264650583267212
  batch 150 loss: 1.2726982474327087
  batch 200 loss: 1.293905119895935
  batch 250 loss: 1.288117060661316
  batch 300 loss: 1.305089257955551
  batch 350 loss: 1.241696091890335
  batch 400 loss: 1.275393979549408
  batch 450 loss: 1.2643360936641692
  batch 500 loss: 1.2643080842494965
  batch 550 loss: 1.2677193462848664
  batch 600 loss: 1.2864071536064148
  batch 650 loss: 1.2782116973400115
  batch 700 loss: 1.248382362127304
  batch 750 loss: 1.2295048177242278
  batch 800 loss: 1.2181617569923402
  batch 850 loss: 1.2240026414394378
  batch 900 loss: 1.2534056317806244
LOSS train 1.25341 valid 1.17745, valid PER 37.65%
EPOCH 5, Learning Rate: 0.5
  batch 50 loss: 1.1861385631561279
  batch 100 loss: 1.1912737202644348
  batch 150 loss: 1.2551636004447937
  batch 200 loss: 1.1865896034240722
  batch 250 loss: 1.1840368473529816
  batch 300 loss: 1.2260812866687774
  batch 350 loss: 1.2236233592033385
  batch 400 loss: 1.2113967716693879
  batch 450 loss: 1.191711208820343
  batch 500 loss: 1.2013114023208618
  batch 550 loss: 1.1664992940425873
  batch 600 loss: 1.2952732574939727
  batch 650 loss: 1.2224708557128907
  batch 700 loss: 1.2513719201087952
  batch 750 loss: 1.1525980341434479
  batch 800 loss: 1.1932666671276093
  batch 850 loss: 1.20463991522789
  batch 900 loss: 1.1989117527008057
LOSS train 1.19891 valid 1.12955, valid PER 36.31%
EPOCH 6, Learning Rate: 0.5
  batch 50 loss: 1.179671026468277
  batch 100 loss: 1.1335421693325043
  batch 150 loss: 1.1294445836544036
  batch 200 loss: 1.1479823970794678
  batch 250 loss: 1.129798412322998
  batch 300 loss: 1.1410840964317321
  batch 350 loss: 1.148143665790558
  batch 400 loss: 1.1294835233688354
  batch 450 loss: 1.144480630159378
  batch 500 loss: 1.1588281202316284
  batch 550 loss: 1.1661003530025482
  batch 600 loss: 1.123170120716095
  batch 650 loss: 1.1450510573387147
  batch 700 loss: 1.1535567271709442
  batch 750 loss: 1.1201780939102173
  batch 800 loss: 1.1202300143241883
  batch 850 loss: 1.1195268642902374
  batch 900 loss: 1.1522326695919036
LOSS train 1.15223 valid 1.08487, valid PER 35.48%
EPOCH 7, Learning Rate: 0.5
  batch 50 loss: 1.1178750038146972
  batch 100 loss: 1.1289186203479766
  batch 150 loss: 1.1196244263648987
  batch 200 loss: 1.0844888150691987
  batch 250 loss: 1.103125686645508
  batch 300 loss: 1.1133765006065368
  batch 350 loss: 1.0905475461483
  batch 400 loss: 1.109728970527649
  batch 450 loss: 1.104685616493225
  batch 500 loss: 1.0880318975448608
  batch 550 loss: 1.1037525391578675
  batch 600 loss: 1.1263675558567048
  batch 650 loss: 1.1137732481956482
  batch 700 loss: 1.122389452457428
  batch 750 loss: 1.1490080726146699
  batch 800 loss: 1.1293690359592439
  batch 850 loss: 1.1253653693199157
  batch 900 loss: 1.1509788036346436
LOSS train 1.15098 valid 1.09530, valid PER 35.10%
EPOCH 8, Learning Rate: 0.5
  batch 50 loss: 1.0954994893074035
  batch 100 loss: 1.0613634514808654
  batch 150 loss: 1.051262722015381
  batch 200 loss: 1.0344011759757996
  batch 250 loss: 1.061698729991913
  batch 300 loss: 1.0068563115596771
  batch 350 loss: 1.120265371799469
  batch 400 loss: 1.0719978535175323
  batch 450 loss: 1.08804426074028
  batch 500 loss: 1.1086673641204834
  batch 550 loss: 1.0452064657211304
  batch 600 loss: 1.094245651960373
  batch 650 loss: 1.130014909505844
  batch 700 loss: 1.0628024625778199
  batch 750 loss: 1.0817968356609344
  batch 800 loss: 1.1078670144081115
  batch 850 loss: 1.0869583666324616
  batch 900 loss: 1.059208631515503
LOSS train 1.05921 valid 1.04539, valid PER 33.80%
EPOCH 9, Learning Rate: 0.5
  batch 50 loss: 1.0186380565166473
  batch 100 loss: 1.0445238864421844
  batch 150 loss: 1.060450496673584
  batch 200 loss: 1.013335611820221
  batch 250 loss: 1.0570616722106934
  batch 300 loss: 1.0654720044136048
  batch 350 loss: 1.0881191301345825
  batch 400 loss: 1.0793262016773224
  batch 450 loss: 1.0514072263240815
  batch 500 loss: 1.0015184247493745
  batch 550 loss: 1.0594978523254395
  batch 600 loss: 1.0770152497291565
  batch 650 loss: 1.0378273236751556
  batch 700 loss: 1.0237475395202638
  batch 750 loss: 1.0223294711112976
  batch 800 loss: 1.0523007452487945
  batch 850 loss: 1.0758812594413758
  batch 900 loss: 1.0154513835906982
LOSS train 1.01545 valid 1.03023, valid PER 33.01%
EPOCH 10, Learning Rate: 0.5
  batch 50 loss: 0.9640340209007263
  batch 100 loss: 1.0249110054969788
  batch 150 loss: 1.051763769388199
  batch 200 loss: 1.0465383219718933
  batch 250 loss: 1.024515950679779
  batch 300 loss: 0.9892188620567322
  batch 350 loss: 1.0290994846820831
  batch 400 loss: 1.0050517058372497
  batch 450 loss: 1.006612914800644
  batch 500 loss: 1.0465720760822297
  batch 550 loss: 1.0546764814853669
  batch 600 loss: 1.0359092938899994
  batch 650 loss: 1.0030486476421356
  batch 700 loss: 1.003674110174179
  batch 750 loss: 1.0008022260665894
  batch 800 loss: 1.0426487696170808
  batch 850 loss: 1.0516429805755616
  batch 900 loss: 1.0255025053024291
LOSS train 1.02550 valid 1.03720, valid PER 33.91%
EPOCH 11, Learning Rate: 0.5
  batch 50 loss: 0.9851641845703125
  batch 100 loss: 0.9699022138118744
  batch 150 loss: 0.9674112975597382
  batch 200 loss: 1.0168785846233368
  batch 250 loss: 1.0152561593055724
  batch 300 loss: 0.9712296617031098
  batch 350 loss: 0.9942741644382477
  batch 400 loss: 1.0196744441986083
  batch 450 loss: 1.0143512225151061
  batch 500 loss: 0.9837192296981812
  batch 550 loss: 0.9858014702796936
  batch 600 loss: 0.9797409427165985
  batch 650 loss: 1.0495916950702666
  batch 700 loss: 0.9584948945045472
  batch 750 loss: 0.9769291961193085
  batch 800 loss: 1.0071999621391297
  batch 850 loss: 1.0378173613548278
  batch 900 loss: 1.0249323308467866
LOSS train 1.02493 valid 1.00306, valid PER 32.90%
EPOCH 12, Learning Rate: 0.5
  batch 50 loss: 0.9738510274887084
  batch 100 loss: 0.9619924759864807
  batch 150 loss: 0.949909633398056
  batch 200 loss: 0.9415420937538147
  batch 250 loss: 0.9854412531852722
  batch 300 loss: 0.9618186187744141
  batch 350 loss: 0.9539862608909607
  batch 400 loss: 0.9792254710197449
  batch 450 loss: 0.9937050485610962
  batch 500 loss: 1.0121406722068786
  batch 550 loss: 0.9721672630310059
  batch 600 loss: 0.9590021538734436
  batch 650 loss: 1.0233219707012176
  batch 700 loss: 0.9924454116821289
  batch 750 loss: 0.9526316618919373
  batch 800 loss: 0.9653123390674591
  batch 850 loss: 0.9962636125087738
  batch 900 loss: 0.9769215667247773
LOSS train 0.97692 valid 0.98592, valid PER 32.28%
EPOCH 13, Learning Rate: 0.5
  batch 50 loss: 0.9197114181518554
  batch 100 loss: 0.9481909143924713
  batch 150 loss: 0.9065769076347351
  batch 200 loss: 0.951720724105835
  batch 250 loss: 0.9497121250629426
  batch 300 loss: 0.9252504694461823
  batch 350 loss: 0.9386657428741455
  batch 400 loss: 0.974500880241394
  batch 450 loss: 0.9848982751369476
  batch 500 loss: 0.9410311257839203
  batch 550 loss: 0.9549246525764465
  batch 600 loss: 0.9488372135162354
  batch 650 loss: 0.9528692233562469
  batch 700 loss: 0.9529695749282837
  batch 750 loss: 0.8988354277610778
  batch 800 loss: 0.9369521963596344
  batch 850 loss: 0.9837531399726868
  batch 900 loss: 0.9880256187915802
LOSS train 0.98803 valid 1.00378, valid PER 32.30%
EPOCH 14, Learning Rate: 0.5
  batch 50 loss: 0.9426443064212799
  batch 100 loss: 0.9023635268211365
  batch 150 loss: 0.9175314629077911
  batch 200 loss: 0.9446557033061981
  batch 250 loss: 0.9321893453598022
  batch 300 loss: 0.9699866330623627
  batch 350 loss: 0.8976379132270813
  batch 400 loss: 0.9279427778720856
  batch 450 loss: 0.9417007839679719
  batch 500 loss: 0.9499924278259277
  batch 550 loss: 0.9597170066833496
  batch 600 loss: 0.9121723997592927
  batch 650 loss: 0.9591589057445526
  batch 700 loss: 0.9836699426174164
  batch 750 loss: 0.9353355240821838
  batch 800 loss: 0.9042805254459381
  batch 850 loss: 0.9547797071933747
  batch 900 loss: 0.9458770442008972
LOSS train 0.94588 valid 0.99864, valid PER 32.72%
EPOCH 15, Learning Rate: 0.5
  batch 50 loss: 0.9166370546817779
  batch 100 loss: 0.9106714463233948
  batch 150 loss: 0.9249426352977753
  batch 200 loss: 0.9848650753498077
  batch 250 loss: 0.9508388793468475
  batch 300 loss: 0.9430535519123078
  batch 350 loss: 0.9453013014793396
  batch 400 loss: 0.9134354794025421
  batch 450 loss: 0.9297818338871002
  batch 500 loss: 0.9129247522354126
  batch 550 loss: 0.945332168340683
  batch 600 loss: 0.9708868765830994
  batch 650 loss: 0.9495220971107483
  batch 700 loss: 0.9581191909313201
  batch 750 loss: 0.9592935335636139
  batch 800 loss: 0.9295344460010528
  batch 850 loss: 0.8979411327838898
  batch 900 loss: 0.9235638666152954
LOSS train 0.92356 valid 0.97260, valid PER 31.88%
EPOCH 16, Learning Rate: 0.5
  batch 50 loss: 0.9453082144260406
  batch 100 loss: 0.8978057134151459
  batch 150 loss: 0.9028960561752319
  batch 200 loss: 0.9027535259723664
  batch 250 loss: 0.9201990795135498
  batch 300 loss: 0.8977202677726746
  batch 350 loss: 0.9335101842880249
  batch 400 loss: 0.9109202325344086
  batch 450 loss: 0.9421411025524139
  batch 500 loss: 0.8715101110935212
  batch 550 loss: 0.9185190641880036
  batch 600 loss: 0.8965463840961456
  batch 650 loss: 0.9207821595668793
  batch 700 loss: 0.8885580849647522
  batch 750 loss: 0.9076957106590271
  batch 800 loss: 0.91877610206604
  batch 850 loss: 0.9370185518264771
  batch 900 loss: 0.9190358471870422
LOSS train 0.91904 valid 0.94539, valid PER 31.07%
EPOCH 17, Learning Rate: 0.5
  batch 50 loss: 0.8876313936710357
  batch 100 loss: 0.9101986229419708
  batch 150 loss: 0.8878218185901642
  batch 200 loss: 0.8886430644989014
  batch 250 loss: 0.889223084449768
  batch 300 loss: 0.900469012260437
  batch 350 loss: 0.8993271231651306
  batch 400 loss: 0.9611432516574859
  batch 450 loss: 0.9313279843330383
  batch 500 loss: 0.8937597703933716
  batch 550 loss: 0.934632521867752
  batch 600 loss: 0.9623992991447449
  batch 650 loss: 0.8954291200637817
  batch 700 loss: 0.901224091053009
  batch 750 loss: 0.8642229545116424
  batch 800 loss: 0.9017916607856751
  batch 850 loss: 0.9104677522182465
  batch 900 loss: 0.8868079543113708
LOSS train 0.88681 valid 0.96188, valid PER 30.46%
EPOCH 18, Learning Rate: 0.5
  batch 50 loss: 0.8876897251605987
  batch 100 loss: 0.9072739148139953
  batch 150 loss: 0.8762341594696045
  batch 200 loss: 0.885057703256607
  batch 250 loss: 0.8989291441440582
  batch 300 loss: 0.8767617273330689
  batch 350 loss: 0.9110722589492798
  batch 400 loss: 0.8748481667041779
  batch 450 loss: 0.9501368689537049
  batch 500 loss: 0.9145657670497894
  batch 550 loss: 0.9078460943698883
  batch 600 loss: 0.9164653468132019
  batch 650 loss: 0.8945042884349823
  batch 700 loss: 0.9485377454757691
  batch 750 loss: 0.9102294099330902
  batch 800 loss: 0.8806847512722016
  batch 850 loss: 0.9002669167518615
  batch 900 loss: 0.9296778237819672
LOSS train 0.92968 valid 0.98630, valid PER 31.63%
EPOCH 19, Learning Rate: 0.5
  batch 50 loss: 0.8477539479732513
  batch 100 loss: 0.8433812093734742
  batch 150 loss: 0.8603784692287445
  batch 200 loss: 0.8719343757629394
  batch 250 loss: 0.8931123447418213
  batch 300 loss: 0.88116375207901
  batch 350 loss: 0.8827596867084503
  batch 400 loss: 0.964728078842163
  batch 450 loss: 0.9436991369724274
  batch 500 loss: 0.9589154303073884
  batch 550 loss: 0.917498551607132
  batch 600 loss: 0.9231405973434448
  batch 650 loss: 0.977055002450943
  batch 700 loss: 0.9316239154338837
  batch 750 loss: 0.9076005268096924
  batch 800 loss: 0.9230485808849335
  batch 850 loss: 0.9374044120311738
  batch 900 loss: 0.9142562711238861
LOSS train 0.91426 valid 0.97516, valid PER 31.58%
EPOCH 20, Learning Rate: 0.5
  batch 50 loss: 0.8601940619945526
  batch 100 loss: 0.8587997317314148
  batch 150 loss: 0.839697209596634
  batch 200 loss: 0.8679304087162018
  batch 250 loss: 0.8682684588432312
  batch 300 loss: 0.9323859679698944
  batch 350 loss: 0.8880155158042907
  batch 400 loss: 0.9124388027191163
  batch 450 loss: 0.9339724040031433
  batch 500 loss: 0.9242817950248718
  batch 550 loss: 0.9491531360149383
  batch 600 loss: 0.8991981685161591
  batch 650 loss: 0.9465980112552643
  batch 700 loss: 0.9549476361274719
  batch 750 loss: 0.918234510421753
  batch 800 loss: 0.9459669363498687
  batch 850 loss: 0.9112172734737396
  batch 900 loss: 0.9280322551727295
LOSS train 0.92803 valid 1.03088, valid PER 32.65%
Training finished in 3.0 minutes.
Model saved to checkpoints/20231207_181418/model_16
Loading model from checkpoints/20231207_181418/model_16
SUB: 15.99%, DEL: 15.04%, INS: 1.35%, COR: 68.97%, PER: 32.38%
