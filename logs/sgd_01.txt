Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.1, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 166952
EPOCH 1, Learning Rate: 0.1
  batch 50 loss: 7.7459714984893795
  batch 100 loss: 3.295362267494202
  batch 150 loss: 3.248691749572754
  batch 200 loss: 3.2207879257202148
  batch 250 loss: 3.184152855873108
  batch 300 loss: 3.1301203918457032
  batch 350 loss: 3.0840154027938844
  batch 400 loss: 3.032951846122742
  batch 450 loss: 2.9606709384918215
  batch 500 loss: 2.863920907974243
  batch 550 loss: 2.796695799827576
  batch 600 loss: 2.742771124839783
  batch 650 loss: 2.674844012260437
  batch 700 loss: 2.651069793701172
  batch 750 loss: 2.6015339374542235
  batch 800 loss: 2.570527458190918
  batch 850 loss: 2.54691002368927
  batch 900 loss: 2.4970034313201905
LOSS train 2.49700 valid 2.46948, valid PER 81.88%
EPOCH 2, Learning Rate: 0.1
  batch 50 loss: 2.469251341819763
  batch 100 loss: 2.419891610145569
  batch 150 loss: 2.364733452796936
  batch 200 loss: 2.3525835037231446
  batch 250 loss: 2.3469011545181275
  batch 300 loss: 2.3219754219055178
  batch 350 loss: 2.254878842830658
  batch 400 loss: 2.2604932093620302
  batch 450 loss: 2.223956723213196
  batch 500 loss: 2.202768223285675
  batch 550 loss: 2.2133582425117493
  batch 600 loss: 2.1618653249740603
  batch 650 loss: 2.1717596650123596
  batch 700 loss: 2.1412470531463623
  batch 750 loss: 2.136174259185791
  batch 800 loss: 2.0814195442199708
  batch 850 loss: 2.075209176540375
  batch 900 loss: 2.0832577967643737
LOSS train 2.08326 valid 2.04713, valid PER 76.53%
EPOCH 3, Learning Rate: 0.1
  batch 50 loss: 2.05993896484375
  batch 100 loss: 2.0070077872276304
  batch 150 loss: 2.023909137248993
  batch 200 loss: 2.0009516668319702
  batch 250 loss: 1.9707911109924316
  batch 300 loss: 1.9686854195594787
  batch 350 loss: 1.9918121957778931
  batch 400 loss: 1.9525083255767823
  batch 450 loss: 1.906639232635498
  batch 500 loss: 1.9146945762634278
  batch 550 loss: 1.8974924731254577
  batch 600 loss: 1.8638218760490417
  batch 650 loss: 1.8458688688278198
  batch 700 loss: 1.8593598842620849
  batch 750 loss: 1.896014051437378
  batch 800 loss: 1.829048626422882
  batch 850 loss: 1.8540533947944642
  batch 900 loss: 1.7943976402282715
LOSS train 1.79440 valid 1.78251, valid PER 68.12%
EPOCH 4, Learning Rate: 0.1
  batch 50 loss: 1.7962833380699157
  batch 100 loss: 1.8087106585502624
  batch 150 loss: 1.7650011491775512
  batch 200 loss: 1.7932137036323548
  batch 250 loss: 1.7847160077095032
  batch 300 loss: 1.7745734190940856
  batch 350 loss: 1.7150030064582824
  batch 400 loss: 1.746301293373108
  batch 450 loss: 1.7332941269874573
  batch 500 loss: 1.699188663959503
  batch 550 loss: 1.7185846543312073
  batch 600 loss: 1.7306743192672729
  batch 650 loss: 1.72631032705307
  batch 700 loss: 1.69281081199646
  batch 750 loss: 1.662559552192688
  batch 800 loss: 1.6393213653564453
  batch 850 loss: 1.6721501684188842
  batch 900 loss: 1.6941878986358643
LOSS train 1.69419 valid 1.64776, valid PER 63.50%
EPOCH 5, Learning Rate: 0.1
  batch 50 loss: 1.6510109663009644
  batch 100 loss: 1.6404113125801087
  batch 150 loss: 1.6615381240844727
  batch 200 loss: 1.602423746585846
  batch 250 loss: 1.6139019298553468
  batch 300 loss: 1.628921661376953
  batch 350 loss: 1.6146514081954957
  batch 400 loss: 1.6161485648155212
  batch 450 loss: 1.6035075187683105
  batch 500 loss: 1.612757568359375
  batch 550 loss: 1.5402616620063783
  batch 600 loss: 1.6204334425926208
  batch 650 loss: 1.5734599876403808
  batch 700 loss: 1.601805956363678
  batch 750 loss: 1.5492274236679078
  batch 800 loss: 1.571020257472992
  batch 850 loss: 1.5673395776748658
  batch 900 loss: 1.577200825214386
LOSS train 1.57720 valid 1.51596, valid PER 57.80%
EPOCH 6, Learning Rate: 0.1
  batch 50 loss: 1.5685594987869262
  batch 100 loss: 1.5275449347496033
  batch 150 loss: 1.5212650513648986
  batch 200 loss: 1.5201735496520996
  batch 250 loss: 1.5442047882080079
  batch 300 loss: 1.513731906414032
  batch 350 loss: 1.5222275161743164
  batch 400 loss: 1.507105495929718
  batch 450 loss: 1.5180325198173523
  batch 500 loss: 1.495647897720337
  batch 550 loss: 1.5166322135925292
  batch 600 loss: 1.485597138404846
  batch 650 loss: 1.498243546485901
  batch 700 loss: 1.4916774654388427
  batch 750 loss: 1.476009657382965
  batch 800 loss: 1.4566806936264038
  batch 850 loss: 1.4685958886146546
  batch 900 loss: 1.4835510921478272
LOSS train 1.48355 valid 1.44826, valid PER 55.83%
EPOCH 7, Learning Rate: 0.1
  batch 50 loss: 1.4532545876502991
  batch 100 loss: 1.4724907517433166
  batch 150 loss: 1.4539617943763732
  batch 200 loss: 1.4440389561653137
  batch 250 loss: 1.451260416507721
  batch 300 loss: 1.4152594637870788
  batch 350 loss: 1.4243143939971923
  batch 400 loss: 1.437086012363434
  batch 450 loss: 1.4311553835868835
  batch 500 loss: 1.4203003859519958
  batch 550 loss: 1.4187856340408325
  batch 600 loss: 1.43160537481308
  batch 650 loss: 1.4151989150047302
  batch 700 loss: 1.4231663060188293
  batch 750 loss: 1.407720046043396
  batch 800 loss: 1.4067856025695802
  batch 850 loss: 1.4194172143936157
  batch 900 loss: 1.4553629708290101
LOSS train 1.45536 valid 1.40831, valid PER 51.97%
EPOCH 8, Learning Rate: 0.1
  batch 50 loss: 1.4039281630516052
  batch 100 loss: 1.3939001011848449
  batch 150 loss: 1.3837387323379517
  batch 200 loss: 1.3556975150108337
  batch 250 loss: 1.3822074842453003
  batch 300 loss: 1.309959924221039
  batch 350 loss: 1.3882844543457031
  batch 400 loss: 1.3605418491363526
  batch 450 loss: 1.3742920804023742
  batch 500 loss: 1.4002606129646302
  batch 550 loss: 1.3278223085403442
  batch 600 loss: 1.3736562085151673
  batch 650 loss: 1.3983540034294129
  batch 700 loss: 1.3426865363121032
  batch 750 loss: 1.3468548202514647
  batch 800 loss: 1.3568754267692567
  batch 850 loss: 1.3607391834259033
  batch 900 loss: 1.3372394704818726
LOSS train 1.33724 valid 1.30838, valid PER 45.34%
EPOCH 9, Learning Rate: 0.1
  batch 50 loss: 1.288052728176117
  batch 100 loss: 1.3394174003601074
  batch 150 loss: 1.330186927318573
  batch 200 loss: 1.2981787943840026
  batch 250 loss: 1.3369413208961487
  batch 300 loss: 1.3288341784477233
  batch 350 loss: 1.344565864801407
  batch 400 loss: 1.325755832195282
  batch 450 loss: 1.3331313824653626
  batch 500 loss: 1.286870846748352
  batch 550 loss: 1.3188188433647157
  batch 600 loss: 1.3359512186050415
  batch 650 loss: 1.2899555373191833
  batch 700 loss: 1.286362601518631
  batch 750 loss: 1.2974686229228973
  batch 800 loss: 1.318378963470459
  batch 850 loss: 1.3217919445037842
  batch 900 loss: 1.2633627617359162
LOSS train 1.26336 valid 1.24609, valid PER 41.38%
EPOCH 10, Learning Rate: 0.1
  batch 50 loss: 1.250392234325409
  batch 100 loss: 1.2678209221363068
  batch 150 loss: 1.296847550868988
  batch 200 loss: 1.2861749410629273
  batch 250 loss: 1.2829371011257171
  batch 300 loss: 1.2582143926620484
  batch 350 loss: 1.257595295906067
  batch 400 loss: 1.2408438563346862
  batch 450 loss: 1.2356676292419433
  batch 500 loss: 1.2672365975379944
  batch 550 loss: 1.2842554330825806
  batch 600 loss: 1.2543076598644256
  batch 650 loss: 1.2381911754608155
  batch 700 loss: 1.2560435247421264
  batch 750 loss: 1.254092972278595
  batch 800 loss: 1.265040909051895
  batch 850 loss: 1.2483370089530945
  batch 900 loss: 1.265328016281128
LOSS train 1.26533 valid 1.23054, valid PER 41.73%
EPOCH 11, Learning Rate: 0.1
  batch 50 loss: 1.2274564039707183
  batch 100 loss: 1.2293848419189453
  batch 150 loss: 1.2112192475795747
  batch 200 loss: 1.2613972735404968
  batch 250 loss: 1.2307555520534514
  batch 300 loss: 1.1970467293262481
  batch 350 loss: 1.2367045128345489
  batch 400 loss: 1.2341122913360596
  batch 450 loss: 1.2183128488063812
  batch 500 loss: 1.1973157107830048
  batch 550 loss: 1.2172776198387145
  batch 600 loss: 1.198844885826111
  batch 650 loss: 1.2518055438995361
  batch 700 loss: 1.19872660279274
  batch 750 loss: 1.1950111949443818
  batch 800 loss: 1.2634250509738922
  batch 850 loss: 1.2278422832489013
  batch 900 loss: 1.2379574716091155
LOSS train 1.23796 valid 1.16763, valid PER 38.55%
EPOCH 12, Learning Rate: 0.1
  batch 50 loss: 1.2000916028022766
  batch 100 loss: 1.188371512889862
  batch 150 loss: 1.1792702865600586
  batch 200 loss: 1.1865323889255524
  batch 250 loss: 1.2162513291835786
  batch 300 loss: 1.1921611320972443
  batch 350 loss: 1.1857622468471527
  batch 400 loss: 1.2039364528656007
  batch 450 loss: 1.1907916247844696
  batch 500 loss: 1.2163658249378204
  batch 550 loss: 1.1391199457645416
  batch 600 loss: 1.142318114042282
  batch 650 loss: 1.2101334917545319
  batch 700 loss: 1.1781635141372682
  batch 750 loss: 1.1811476373672485
  batch 800 loss: 1.1520536386966704
  batch 850 loss: 1.205732947587967
  batch 900 loss: 1.206151555776596
LOSS train 1.20615 valid 1.14129, valid PER 37.12%
EPOCH 13, Learning Rate: 0.1
  batch 50 loss: 1.1352846944332122
  batch 100 loss: 1.1815907216072083
  batch 150 loss: 1.1359986424446107
  batch 200 loss: 1.1606953144073486
  batch 250 loss: 1.1592368328571319
  batch 300 loss: 1.136846262216568
  batch 350 loss: 1.160959973335266
  batch 400 loss: 1.171139384508133
  batch 450 loss: 1.172758799791336
  batch 500 loss: 1.1471344172954558
  batch 550 loss: 1.1598574459552764
  batch 600 loss: 1.1510710728168487
  batch 650 loss: 1.1562526416778565
  batch 700 loss: 1.170879831314087
  batch 750 loss: 1.1210619592666626
  batch 800 loss: 1.1412528157234192
  batch 850 loss: 1.1902867364883423
  batch 900 loss: 1.1609329974651337
LOSS train 1.16093 valid 1.13857, valid PER 37.12%
EPOCH 14, Learning Rate: 0.1
  batch 50 loss: 1.1384953045845032
  batch 100 loss: 1.1508715271949768
  batch 150 loss: 1.105444585084915
  batch 200 loss: 1.1531116652488709
  batch 250 loss: 1.1334738731384277
  batch 300 loss: 1.1516042375564575
  batch 350 loss: 1.1157413411140442
  batch 400 loss: 1.1289939618110656
  batch 450 loss: 1.113866914510727
  batch 500 loss: 1.133273788690567
  batch 550 loss: 1.1475247895717622
  batch 600 loss: 1.1147704172134398
  batch 650 loss: 1.1452336061000823
  batch 700 loss: 1.1538907992839813
  batch 750 loss: 1.1209932911396026
  batch 800 loss: 1.0639210939407349
  batch 850 loss: 1.1440289032459259
  batch 900 loss: 1.115001184940338
LOSS train 1.11500 valid 1.11395, valid PER 36.58%
EPOCH 15, Learning Rate: 0.1
  batch 50 loss: 1.1392221438884735
  batch 100 loss: 1.0920347487926483
  batch 150 loss: 1.1072192668914795
  batch 200 loss: 1.1391957688331604
  batch 250 loss: 1.1229518032073975
  batch 300 loss: 1.09045796751976
  batch 350 loss: 1.1058844137191772
  batch 400 loss: 1.1043819642066957
  batch 450 loss: 1.10025141954422
  batch 500 loss: 1.0755887615680695
  batch 550 loss: 1.112743468284607
  batch 600 loss: 1.1171722054481505
  batch 650 loss: 1.1159639275074005
  batch 700 loss: 1.1236771607398988
  batch 750 loss: 1.0983318150043488
  batch 800 loss: 1.0854445850849153
  batch 850 loss: 1.0861957812309264
  batch 900 loss: 1.1113702964782715
LOSS train 1.11137 valid 1.09821, valid PER 36.00%
EPOCH 16, Learning Rate: 0.1
  batch 50 loss: 1.1030029046535492
  batch 100 loss: 1.0576275610923767
  batch 150 loss: 1.0734823942184448
  batch 200 loss: 1.0777912640571594
  batch 250 loss: 1.1013657796382903
  batch 300 loss: 1.0902686262130736
  batch 350 loss: 1.1083334052562714
  batch 400 loss: 1.1019283175468444
  batch 450 loss: 1.114600191116333
  batch 500 loss: 1.0585784912109375
  batch 550 loss: 1.115935480594635
  batch 600 loss: 1.0881548893451691
  batch 650 loss: 1.0874841463565827
  batch 700 loss: 1.0639192962646484
  batch 750 loss: 1.090129598379135
  batch 800 loss: 1.1057975029945373
  batch 850 loss: 1.0568328046798705
  batch 900 loss: 1.0689135575294495
LOSS train 1.06891 valid 1.06196, valid PER 34.27%
EPOCH 17, Learning Rate: 0.1
  batch 50 loss: 1.0703549838066102
  batch 100 loss: 1.0859631729125976
  batch 150 loss: 1.0528831791877746
  batch 200 loss: 1.0479819560050965
  batch 250 loss: 1.0590316486358642
  batch 300 loss: 1.0740326166152954
  batch 350 loss: 1.033356773853302
  batch 400 loss: 1.0950225806236267
  batch 450 loss: 1.0950898480415345
  batch 500 loss: 1.061480301618576
  batch 550 loss: 1.08404123544693
  batch 600 loss: 1.1281233716011048
  batch 650 loss: 1.0608829593658446
  batch 700 loss: 1.055222556591034
  batch 750 loss: 1.0390505135059356
  batch 800 loss: 1.0522532773017883
  batch 850 loss: 1.0393294823169708
  batch 900 loss: 1.0325769758224488
LOSS train 1.03258 valid 1.05736, valid PER 34.02%
EPOCH 18, Learning Rate: 0.1
  batch 50 loss: 1.0426476800441742
  batch 100 loss: 1.0675004398822785
  batch 150 loss: 1.0674471163749695
  batch 200 loss: 1.0394252932071686
  batch 250 loss: 1.0443579208850862
  batch 300 loss: 1.051036810874939
  batch 350 loss: 1.0505104970932007
  batch 400 loss: 1.023688702583313
  batch 450 loss: 1.0821090817451477
  batch 500 loss: 1.0659984397888183
  batch 550 loss: 1.053124315738678
  batch 600 loss: 1.033587282896042
  batch 650 loss: 1.0213849282264709
  batch 700 loss: 1.0714194226264953
  batch 750 loss: 1.0387236022949218
  batch 800 loss: 1.037329363822937
  batch 850 loss: 1.0234760069847106
  batch 900 loss: 1.069121879339218
LOSS train 1.06912 valid 1.07291, valid PER 34.84%
EPOCH 19, Learning Rate: 0.1
  batch 50 loss: 0.9885607719421386
  batch 100 loss: 1.0073896038532257
  batch 150 loss: 1.0218257105350494
  batch 200 loss: 1.037930029630661
  batch 250 loss: 1.0488579988479614
  batch 300 loss: 1.020975695848465
  batch 350 loss: 1.0289527809619903
  batch 400 loss: 1.051600043773651
  batch 450 loss: 1.0453603076934814
  batch 500 loss: 1.0376698970794678
  batch 550 loss: 1.0074639558792113
  batch 600 loss: 1.038866972923279
  batch 650 loss: 1.0796499061584472
  batch 700 loss: 1.014847308397293
  batch 750 loss: 0.9923619270324707
  batch 800 loss: 1.045479997396469
  batch 850 loss: 1.0396939647197723
  batch 900 loss: 1.0500071096420287
LOSS train 1.05001 valid 1.04780, valid PER 33.89%
EPOCH 20, Learning Rate: 0.1
  batch 50 loss: 1.0070421528816222
  batch 100 loss: 1.003706030845642
  batch 150 loss: 0.9926413416862487
  batch 200 loss: 1.0305733489990234
  batch 250 loss: 1.0049900138378143
  batch 300 loss: 1.0380491924285888
  batch 350 loss: 0.983543553352356
  batch 400 loss: 1.0092996644973755
  batch 450 loss: 1.0178732585906982
  batch 500 loss: 0.9786880159378052
  batch 550 loss: 1.052221293449402
  batch 600 loss: 0.9921696972846985
  batch 650 loss: 1.0207110750675201
  batch 700 loss: 1.0270952093601227
  batch 750 loss: 0.9971675765514374
  batch 800 loss: 1.0393278086185456
  batch 850 loss: 1.039227248430252
  batch 900 loss: 1.0401489818096161
LOSS train 1.04015 valid 1.02813, valid PER 32.90%
Training finished in 3.0 minutes.
Model saved to checkpoints/20231210_033508/model_20
Loading model from checkpoints/20231210_033508/model_20
SUB: 15.92%, DEL: 16.99%, INS: 1.45%, COR: 67.09%, PER: 34.35%
