Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=512, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 2240552
EPOCH 1, Learning Rate: 0.5
  batch 50 loss: 4.82469783782959
  batch 100 loss: 3.246851644515991
  batch 150 loss: 3.0867361545562746
  batch 200 loss: 2.785787444114685
  batch 250 loss: 2.5827296018600463
  batch 300 loss: 2.4172764110565184
  batch 350 loss: 2.3054465579986574
  batch 400 loss: 2.2687465810775755
  batch 450 loss: 2.1788132047653197
  batch 500 loss: 2.0716077136993407
  batch 550 loss: 2.0399753403663636
  batch 600 loss: 1.963459451198578
  batch 650 loss: 1.8947614526748657
  batch 700 loss: 1.8862864685058593
  batch 750 loss: 1.8265661001205444
  batch 800 loss: 1.8170525074005126
  batch 850 loss: 1.7639921522140503
  batch 900 loss: 1.7436865973472595
avg val loss: 1.7138665914535522
LOSS train 1.74369 valid 1.71387, valid PER 58.61%
EPOCH 2, Learning Rate: 0.5
  batch 50 loss: 1.7217126822471618
  batch 100 loss: 1.6369163012504577
  batch 150 loss: 1.6490127372741699
  batch 200 loss: 1.6482213854789733
  batch 250 loss: 1.6353123784065247
  batch 300 loss: 1.5996806883811951
  batch 350 loss: 1.5095729804039002
  batch 400 loss: 1.5239159059524536
  batch 450 loss: 1.4949630975723267
  batch 500 loss: 1.5284597969055176
  batch 550 loss: 1.5306112217903136
  batch 600 loss: 1.4633830499649048
  batch 650 loss: 1.4913079857826232
  batch 700 loss: 1.4697549915313721
  batch 750 loss: 1.4496870279312133
  batch 800 loss: 1.4003162312507629
  batch 850 loss: 1.3954562425613404
  batch 900 loss: 1.4207192254066467
avg val loss: 1.3560514450073242
LOSS train 1.42072 valid 1.35605, valid PER 43.26%
EPOCH 3, Learning Rate: 0.5
  batch 50 loss: 1.3533396577835084
  batch 100 loss: 1.3510089087486268
  batch 150 loss: 1.3355808138847352
  batch 200 loss: 1.3163338565826417
  batch 250 loss: 1.3059838283061982
  batch 300 loss: 1.3046730947494507
  batch 350 loss: 1.3491618847846985
  batch 400 loss: 1.3255119514465332
  batch 450 loss: 1.2857805871963501
  batch 500 loss: 1.2830737924575806
  batch 550 loss: 1.2890538275241852
  batch 600 loss: 1.2493615174293518
  batch 650 loss: 1.2389258670806884
  batch 700 loss: 1.265080382823944
  batch 750 loss: 1.3144990968704224
  batch 800 loss: 1.2458569538593292
  batch 850 loss: 1.2572345507144929
  batch 900 loss: 1.186171008348465
avg val loss: 1.21787428855896
LOSS train 1.18617 valid 1.21787, valid PER 37.44%
EPOCH 4, Learning Rate: 0.5
  batch 50 loss: 1.1788871228694915
  batch 100 loss: 1.210307959318161
  batch 150 loss: 1.1532850253582
  batch 200 loss: 1.1891910207271577
  batch 250 loss: 1.2030954897403716
  batch 300 loss: 1.1940921568870544
  batch 350 loss: 1.1277835702896117
  batch 400 loss: 1.1610907924175262
  batch 450 loss: 1.1495232462882996
  batch 500 loss: 1.1479006552696227
  batch 550 loss: 1.1607178497314452
  batch 600 loss: 1.189549823999405
  batch 650 loss: 1.1466085505485535
  batch 700 loss: 1.1225330924987793
  batch 750 loss: 1.1155528593063355
  batch 800 loss: 1.0707972133159638
  batch 850 loss: 1.128960189819336
  batch 900 loss: 1.1512347757816315
avg val loss: 1.1204280853271484
LOSS train 1.15123 valid 1.12043, valid PER 35.07%
EPOCH 5, Learning Rate: 0.5
  batch 50 loss: 1.055046204328537
  batch 100 loss: 1.0603383696079254
  batch 150 loss: 1.110903251171112
  batch 200 loss: 1.0368290030956269
  batch 250 loss: 1.0493290853500366
  batch 300 loss: 1.0525596952438354
  batch 350 loss: 1.0416488122940064
  batch 400 loss: 1.0528957808017732
  batch 450 loss: 1.054916044473648
  batch 500 loss: 1.0542904877662658
  batch 550 loss: 1.0297471117973327
  batch 600 loss: 1.0987004518508912
  batch 650 loss: 1.0417661988735198
  batch 700 loss: 1.0771775233745575
  batch 750 loss: 0.9959902465343475
  batch 800 loss: 1.0299439096450806
  batch 850 loss: 1.0345371711254119
  batch 900 loss: 1.0379925847053528
avg val loss: 1.0214471817016602
LOSS train 1.03799 valid 1.02145, valid PER 31.89%
EPOCH 6, Learning Rate: 0.5
  batch 50 loss: 1.0061383986473083
  batch 100 loss: 0.9469039189815521
  batch 150 loss: 0.9431193768978119
  batch 200 loss: 0.9716750609874726
  batch 250 loss: 1.0186230313777924
  batch 300 loss: 0.9864536190032959
  batch 350 loss: 0.9694547307491302
  batch 400 loss: 0.9810539960861206
  batch 450 loss: 1.0088012182712556
  batch 500 loss: 0.9839960718154908
  batch 550 loss: 0.9921554386615753
  batch 600 loss: 0.9449019765853882
  batch 650 loss: 0.9964409041404724
  batch 700 loss: 0.9753185224533081
  batch 750 loss: 0.9539905178546906
  batch 800 loss: 0.9563171815872192
  batch 850 loss: 0.9515590023994446
  batch 900 loss: 0.9741097474098206
avg val loss: 1.0244033336639404
LOSS train 0.97411 valid 1.02440, valid PER 31.93%
EPOCH 7, Learning Rate: 0.25
  batch 50 loss: 0.8749341630935669
  batch 100 loss: 0.8559035396575928
  batch 150 loss: 0.8434864830970764
  batch 200 loss: 0.8189625191688538
  batch 250 loss: 0.8137977397441865
  batch 300 loss: 0.8165669238567352
  batch 350 loss: 0.8252580010890961
  batch 400 loss: 0.8300210273265839
  batch 450 loss: 0.8481341564655304
  batch 500 loss: 0.8192966079711914
  batch 550 loss: 0.8191214692592621
  batch 600 loss: 0.8274117743968964
  batch 650 loss: 0.8164134323596954
  batch 700 loss: 0.8410053646564484
  batch 750 loss: 0.8070393991470337
  batch 800 loss: 0.8195635116100312
  batch 850 loss: 0.8386166083812714
  batch 900 loss: 0.854784209728241
avg val loss: 0.9184255003929138
LOSS train 0.85478 valid 0.91843, valid PER 29.20%
EPOCH 8, Learning Rate: 0.25
  batch 50 loss: 0.7746710801124572
  batch 100 loss: 0.7705307483673096
  batch 150 loss: 0.766744327545166
  batch 200 loss: 0.7810699099302292
  batch 250 loss: 0.7750854116678237
  batch 300 loss: 0.7296208000183105
  batch 350 loss: 0.8019133865833282
  batch 400 loss: 0.7762496459484101
  batch 450 loss: 0.7933092784881591
  batch 500 loss: 0.8210262131690979
  batch 550 loss: 0.7521150386333466
  batch 600 loss: 0.812219705581665
  batch 650 loss: 0.8234436726570129
  batch 700 loss: 0.7672847586870194
  batch 750 loss: 0.7812365055084228
  batch 800 loss: 0.8123422926664352
  batch 850 loss: 0.772039383649826
  batch 900 loss: 0.7898562979698182
avg val loss: 0.9092032313346863
LOSS train 0.78986 valid 0.90920, valid PER 27.85%
EPOCH 9, Learning Rate: 0.25
  batch 50 loss: 0.7119547826051712
  batch 100 loss: 0.7436941397190094
  batch 150 loss: 0.7470138311386109
  batch 200 loss: 0.7286653196811677
  batch 250 loss: 0.7660845673084259
  batch 300 loss: 0.7449758684635163
  batch 350 loss: 0.7813934063911439
  batch 400 loss: 0.7710060834884643
  batch 450 loss: 0.7490504515171051
  batch 500 loss: 0.739928332567215
  batch 550 loss: 0.771905699968338
  batch 600 loss: 0.7696758162975311
  batch 650 loss: 0.7587019741535187
  batch 700 loss: 0.7190309888124466
  batch 750 loss: 0.7384006828069687
  batch 800 loss: 0.7661359667778015
  batch 850 loss: 0.7726593327522278
  batch 900 loss: 0.7156899178028107
avg val loss: 0.8956324458122253
LOSS train 0.71569 valid 0.89563, valid PER 27.97%
EPOCH 10, Learning Rate: 0.25
  batch 50 loss: 0.6755023354291916
  batch 100 loss: 0.6927689254283905
  batch 150 loss: 0.7207678186893464
  batch 200 loss: 0.7215377938747406
  batch 250 loss: 0.7267446339130401
  batch 300 loss: 0.6904920715093613
  batch 350 loss: 0.7166585659980774
  batch 400 loss: 0.6899196153879166
  batch 450 loss: 0.6906171131134033
  batch 500 loss: 0.7183956968784332
  batch 550 loss: 0.738875436782837
  batch 600 loss: 0.723464338183403
  batch 650 loss: 0.7108220815658569
  batch 700 loss: 0.7226174342632293
  batch 750 loss: 0.7036281061172486
  batch 800 loss: 0.7401414382457733
  batch 850 loss: 0.7287908405065536
  batch 900 loss: 0.7299457001686096
avg val loss: 0.9055349826812744
LOSS train 0.72995 valid 0.90553, valid PER 28.00%
EPOCH 11, Learning Rate: 0.125
  batch 50 loss: 0.63042904317379
  batch 100 loss: 0.597468432188034
  batch 150 loss: 0.614207597374916
  batch 200 loss: 0.6690069490671158
  batch 250 loss: 0.6474542474746704
  batch 300 loss: 0.6116616052389144
  batch 350 loss: 0.6370381933450698
  batch 400 loss: 0.639410012960434
  batch 450 loss: 0.63556225836277
  batch 500 loss: 0.6142520177364349
  batch 550 loss: 0.6314497005939483
  batch 600 loss: 0.6204923915863038
  batch 650 loss: 0.6640411204099655
  batch 700 loss: 0.6009769427776337
  batch 750 loss: 0.6256273651123047
  batch 800 loss: 0.6424979907274246
  batch 850 loss: 0.6664010411500931
  batch 900 loss: 0.6575579929351807
avg val loss: 0.8666485548019409
LOSS train 0.65756 valid 0.86665, valid PER 26.32%
EPOCH 12, Learning Rate: 0.125
  batch 50 loss: 0.6002515077590942
  batch 100 loss: 0.594726921916008
  batch 150 loss: 0.5432126957178116
  batch 200 loss: 0.6031119680404663
  batch 250 loss: 0.6080713534355163
  batch 300 loss: 0.5866737824678421
  batch 350 loss: 0.5898887556791306
  batch 400 loss: 0.6091791945695877
  batch 450 loss: 0.6166175550222397
  batch 500 loss: 0.6214828062057495
  batch 550 loss: 0.5761850580573082
  batch 600 loss: 0.6020720756053924
  batch 650 loss: 0.6254335302114487
  batch 700 loss: 0.6224403506517411
  batch 750 loss: 0.5986199539899826
  batch 800 loss: 0.6102794289588929
  batch 850 loss: 0.649758677482605
  batch 900 loss: 0.6444468307495117
avg val loss: 0.8718728423118591
LOSS train 0.64445 valid 0.87187, valid PER 26.62%
EPOCH 13, Learning Rate: 0.0625
  batch 50 loss: 0.5525815904140472
  batch 100 loss: 0.5637497210502624
  batch 150 loss: 0.5354858613014222
  batch 200 loss: 0.5579455494880676
  batch 250 loss: 0.5477244192361832
  batch 300 loss: 0.5481995534896851
  batch 350 loss: 0.5391366511583329
  batch 400 loss: 0.5578187316656112
  batch 450 loss: 0.5511972963809967
  batch 500 loss: 0.5380579590797424
  batch 550 loss: 0.5677616959810257
  batch 600 loss: 0.5430922305583954
  batch 650 loss: 0.5704876393079757
  batch 700 loss: 0.573220272064209
  batch 750 loss: 0.5192231768369675
  batch 800 loss: 0.5512144339084625
  batch 850 loss: 0.5693887299299241
  batch 900 loss: 0.5717206829786301
avg val loss: 0.8677244186401367
LOSS train 0.57172 valid 0.86772, valid PER 25.82%
EPOCH 14, Learning Rate: 0.03125
  batch 50 loss: 0.5220360910892486
  batch 100 loss: 0.5091378176212311
  batch 150 loss: 0.5143994069099427
  batch 200 loss: 0.5079361605644226
  batch 250 loss: 0.5274272620677948
  batch 300 loss: 0.5521397399902344
  batch 350 loss: 0.5087459522485733
  batch 400 loss: 0.5331030416488648
  batch 450 loss: 0.5223568791151046
  batch 500 loss: 0.5320997858047485
  batch 550 loss: 0.5426861137151718
  batch 600 loss: 0.5046823561191559
  batch 650 loss: 0.5253843766450882
  batch 700 loss: 0.5548352748155594
  batch 750 loss: 0.5139920234680175
  batch 800 loss: 0.49957805931568144
  batch 850 loss: 0.5285760658979416
  batch 900 loss: 0.5354515981674194
avg val loss: 0.8692210912704468
LOSS train 0.53545 valid 0.86922, valid PER 25.77%
EPOCH 15, Learning Rate: 0.015625
  batch 50 loss: 0.5135175144672394
  batch 100 loss: 0.5033226925134658
  batch 150 loss: 0.5123147881031036
  batch 200 loss: 0.5192854934930802
  batch 250 loss: 0.5255442333221435
  batch 300 loss: 0.5049845910072327
  batch 350 loss: 0.521320983171463
  batch 400 loss: 0.5081369608640671
  batch 450 loss: 0.49335300326347353
  batch 500 loss: 0.48649585008621216
  batch 550 loss: 0.5022068512439728
  batch 600 loss: 0.5236621916294097
  batch 650 loss: 0.5288959538936615
  batch 700 loss: 0.5160000199079513
  batch 750 loss: 0.5159548872709274
  batch 800 loss: 0.4980626308917999
  batch 850 loss: 0.48492616951465606
  batch 900 loss: 0.5022625213861466
avg val loss: 0.8701436519622803
LOSS train 0.50226 valid 0.87014, valid PER 25.83%
EPOCH 16, Learning Rate: 0.0078125
  batch 50 loss: 0.5127384293079377
  batch 100 loss: 0.4867863303422928
  batch 150 loss: 0.5121818363666535
  batch 200 loss: 0.5019180220365524
  batch 250 loss: 0.5203832691907883
  batch 300 loss: 0.4884341061115265
  batch 350 loss: 0.5063221794366837
  batch 400 loss: 0.5119138890504837
  batch 450 loss: 0.5175956416130066
  batch 500 loss: 0.48665219247341157
  batch 550 loss: 0.48827246904373167
  batch 600 loss: 0.5000658303499221
  batch 650 loss: 0.5137431162595749
  batch 700 loss: 0.48441154599189756
  batch 750 loss: 0.4960323178768158
  batch 800 loss: 0.5091231220960617
  batch 850 loss: 0.49994182884693145
  batch 900 loss: 0.5017579686641693
avg val loss: 0.8702512979507446
LOSS train 0.50176 valid 0.87025, valid PER 25.64%
EPOCH 17, Learning Rate: 0.00390625
  batch 50 loss: 0.5107871985435486
  batch 100 loss: 0.5118993043899536
  batch 150 loss: 0.48864962041378024
  batch 200 loss: 0.48437042713165285
  batch 250 loss: 0.5327514833211899
  batch 300 loss: 0.49765943467617035
  batch 350 loss: 0.4845911818742752
  batch 400 loss: 0.5172445929050445
  batch 450 loss: 0.5069228065013885
  batch 500 loss: 0.46954443871974944
  batch 550 loss: 0.4764298576116562
  batch 600 loss: 0.5104826772212983
  batch 650 loss: 0.49626731634140014
  batch 700 loss: 0.4943637180328369
  batch 750 loss: 0.4832537069916725
  batch 800 loss: 0.4821783113479614
  batch 850 loss: 0.507635572552681
  batch 900 loss: 0.48637362360954284
avg val loss: 0.8713023066520691
LOSS train 0.48637 valid 0.87130, valid PER 25.71%
EPOCH 18, Learning Rate: 0.001953125
  batch 50 loss: 0.4853549951314926
  batch 100 loss: 0.498707475066185
  batch 150 loss: 0.5207826536893845
  batch 200 loss: 0.500405421257019
  batch 250 loss: 0.5335204219818115
  batch 300 loss: 0.4812804067134857
  batch 350 loss: 0.4901107490062714
  batch 400 loss: 0.49101617753505705
  batch 450 loss: 0.5084794563055038
  batch 500 loss: 0.48699463069438936
  batch 550 loss: 0.5113713020086288
  batch 600 loss: 0.4890636718273163
  batch 650 loss: 0.47332067966461183
  batch 700 loss: 0.4960785919427872
  batch 750 loss: 0.48953613221645353
  batch 800 loss: 0.4904770964384079
  batch 850 loss: 0.4776718804240227
  batch 900 loss: 0.4991969579458237
avg val loss: 0.8713317513465881
LOSS train 0.49920 valid 0.87133, valid PER 25.66%
EPOCH 19, Learning Rate: 0.0009765625
  batch 50 loss: 0.4956136804819107
  batch 100 loss: 0.48328865885734557
  batch 150 loss: 0.4962723791599274
  batch 200 loss: 0.49759582698345184
  batch 250 loss: 0.4885120463371277
  batch 300 loss: 0.49661425113677976
  batch 350 loss: 0.49345412611961365
  batch 400 loss: 0.48453500866889954
  batch 450 loss: 0.5061214673519134
  batch 500 loss: 0.48591827154159545
  batch 550 loss: 0.47757806241512296
  batch 600 loss: 0.4808195811510086
  batch 650 loss: 0.5273416286706925
  batch 700 loss: 0.4846601241827011
  batch 750 loss: 0.4803801727294922
  batch 800 loss: 0.49230526983737943
  batch 850 loss: 0.4994252759218216
  batch 900 loss: 0.4979348796606064
avg val loss: 0.8715975880622864
LOSS train 0.49793 valid 0.87160, valid PER 25.70%
EPOCH 20, Learning Rate: 0.00048828125
  batch 50 loss: 0.5094104164838791
  batch 100 loss: 0.49544532865285873
  batch 150 loss: 0.48183725237846375
  batch 200 loss: 0.48982750713825224
  batch 250 loss: 0.4807706594467163
  batch 300 loss: 0.505287709236145
  batch 350 loss: 0.47051890194416046
  batch 400 loss: 0.5027777349948883
  batch 450 loss: 0.49587819635868075
  batch 500 loss: 0.4706736260652542
  batch 550 loss: 0.5186483645439148
  batch 600 loss: 0.47883793771266936
  batch 650 loss: 0.48821729719638823
  batch 700 loss: 0.49811342418193816
  batch 750 loss: 0.46930710583925245
  batch 800 loss: 0.5066965943574906
  batch 850 loss: 0.4948420977592468
  batch 900 loss: 0.4906521612405777
avg val loss: 0.8716034293174744
LOSS train 0.49065 valid 0.87160, valid PER 25.74%
Training finished in 8.0 minutes.
Model saved to checkpoints/20231210_135512/model_11
Loading model from checkpoints/20231210_135512/model_11
SUB: 14.58%, DEL: 21.60%, INS: 1.40%, COR: 63.81%, PER: 37.59%
