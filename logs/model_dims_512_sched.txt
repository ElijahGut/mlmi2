Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=512, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 2240552
EPOCH 1, Learning Rate: 0.5
  batch 50 loss: 4.819530115127564
  batch 100 loss: 3.247200951576233
  batch 150 loss: 3.0914895725250244
  batch 200 loss: 2.7813798713684084
  batch 250 loss: 2.58009512424469
  batch 300 loss: 2.4163039541244506
  batch 350 loss: 2.305046248435974
  batch 400 loss: 2.266467070579529
  batch 450 loss: 2.1818296813964846
  batch 500 loss: 2.0718367981910704
  batch 550 loss: 2.0417795538902284
  batch 600 loss: 1.962792570590973
  batch 650 loss: 1.8939026665687562
  batch 700 loss: 1.887998297214508
  batch 750 loss: 1.8270029354095458
  batch 800 loss: 1.8167645645141601
  batch 850 loss: 1.7635497403144837
  batch 900 loss: 1.744075481891632
running loss: 40.80908238887787
LOSS train 1.74408 valid 1.72118, valid PER 58.79%
EPOCH 2, Learning Rate: 0.5
  batch 50 loss: 1.7212473702430726
  batch 100 loss: 1.6360290098190307
  batch 150 loss: 1.6510265135765076
  batch 200 loss: 1.6444785165786744
  batch 250 loss: 1.6357617449760438
  batch 300 loss: 1.5980629062652587
  batch 350 loss: 1.5083934712409972
  batch 400 loss: 1.5246733856201171
  batch 450 loss: 1.4969387555122375
  batch 500 loss: 1.5327792000770568
  batch 550 loss: 1.5341224408149718
  batch 600 loss: 1.4660451531410217
  batch 650 loss: 1.4916172695159913
  batch 700 loss: 1.4704181456565857
  batch 750 loss: 1.4522640633583068
  batch 800 loss: 1.4009234976768494
  batch 850 loss: 1.3978497242927552
  batch 900 loss: 1.4198916625976563
running loss: 33.0277886390686
LOSS train 1.41989 valid 1.35927, valid PER 43.01%
EPOCH 3, Learning Rate: 0.5
  batch 50 loss: 1.3540374612808228
  batch 100 loss: 1.3511878967285156
  batch 150 loss: 1.3354323029518127
  batch 200 loss: 1.3102332878112792
  batch 250 loss: 1.3057414174079895
  batch 300 loss: 1.3067280292510985
  batch 350 loss: 1.3506486773490907
  batch 400 loss: 1.3255791389942169
  batch 450 loss: 1.2867019665241242
  batch 500 loss: 1.2843713545799256
  batch 550 loss: 1.2847583842277528
  batch 600 loss: 1.251812571287155
  batch 650 loss: 1.2405701541900636
  batch 700 loss: 1.2618718135356903
  batch 750 loss: 1.3172879779338837
  batch 800 loss: 1.2499657142162324
  batch 850 loss: 1.2537209391593933
  batch 900 loss: 1.183649516105652
running loss: 28.416241347789764
LOSS train 1.18365 valid 1.20684, valid PER 37.23%
EPOCH 4, Learning Rate: 0.5
  batch 50 loss: 1.1819647812843324
  batch 100 loss: 1.212850924730301
  batch 150 loss: 1.157205879688263
  batch 200 loss: 1.1900329267978669
  batch 250 loss: 1.2010307908058167
  batch 300 loss: 1.196378264427185
  batch 350 loss: 1.1213032042980193
  batch 400 loss: 1.165226401090622
  batch 450 loss: 1.1506212878227233
  batch 500 loss: 1.1481326699256897
  batch 550 loss: 1.158280600309372
  batch 600 loss: 1.192609100341797
  batch 650 loss: 1.1505825233459472
  batch 700 loss: 1.1305083072185516
  batch 750 loss: 1.1156643426418305
  batch 800 loss: 1.0655264973640441
  batch 850 loss: 1.1289233434200288
  batch 900 loss: 1.1514566397666932
running loss: 26.35257738828659
LOSS train 1.15146 valid 1.10503, valid PER 34.37%
EPOCH 5, Learning Rate: 0.5
  batch 50 loss: 1.052572989463806
  batch 100 loss: 1.0553147137165069
  batch 150 loss: 1.1084552347660064
  batch 200 loss: 1.0345282471179962
  batch 250 loss: 1.0440011656284331
  batch 300 loss: 1.0583562922477723
  batch 350 loss: 1.0446094954013825
  batch 400 loss: 1.057142230272293
  batch 450 loss: 1.053545126914978
  batch 500 loss: 1.045372292995453
  batch 550 loss: 1.0295297384262085
  batch 600 loss: 1.1098351001739502
  batch 650 loss: 1.0334969508647918
  batch 700 loss: 1.082562210559845
  batch 750 loss: 0.9923445296287536
  batch 800 loss: 1.0371417939662932
  batch 850 loss: 1.0300139021873473
  batch 900 loss: 1.0429154551029205
running loss: 24.14413321018219
LOSS train 1.04292 valid 1.02229, valid PER 31.78%
EPOCH 6, Learning Rate: 0.5
  batch 50 loss: 1.0091183841228486
  batch 100 loss: 0.9506545150279999
  batch 150 loss: 0.9447568726539611
  batch 200 loss: 0.9739936423301697
  batch 250 loss: 1.016304006576538
  batch 300 loss: 0.9819455111026764
  batch 350 loss: 0.9746616566181183
  batch 400 loss: 0.9763562667369843
  batch 450 loss: 1.0097328507900238
  batch 500 loss: 0.9871077644824982
  batch 550 loss: 0.9936548411846161
  batch 600 loss: 0.9449006235599517
  batch 650 loss: 1.0013302636146546
  batch 700 loss: 0.9746648418903351
  batch 750 loss: 0.9525316321849823
  batch 800 loss: 0.9555447638034821
  batch 850 loss: 0.941689544916153
  batch 900 loss: 0.9690551781654357
running loss: 22.836661159992218
LOSS train 0.96906 valid 1.02175, valid PER 31.72%
EPOCH 7, Learning Rate: 0.5
  batch 50 loss: 0.9137520122528077
  batch 100 loss: 0.9236818432807923
  batch 150 loss: 0.91195925116539
  batch 200 loss: 0.8882681012153626
  batch 250 loss: 0.8856566298007965
  batch 300 loss: 0.890220913887024
  batch 350 loss: 0.9050965988636017
  batch 400 loss: 0.9038750886917114
  batch 450 loss: 0.9289470624923706
  batch 500 loss: 0.9095915877819061
  batch 550 loss: 0.9101351809501648
  batch 600 loss: 0.9089376187324524
  batch 650 loss: 0.9040948724746705
  batch 700 loss: 0.9236664843559265
  batch 750 loss: 0.8842299258708954
  batch 800 loss: 0.8904629230499268
  batch 850 loss: 0.9288320100307464
  batch 900 loss: 0.9319516825675964
running loss: 21.05514097213745
LOSS train 0.93195 valid 0.95332, valid PER 29.97%
EPOCH 8, Learning Rate: 0.5
  batch 50 loss: 0.8421668314933777
  batch 100 loss: 0.8259208548069
  batch 150 loss: 0.8332014083862305
  batch 200 loss: 0.8400275182723999
  batch 250 loss: 0.8434751927852631
  batch 300 loss: 0.8132301092147827
  batch 350 loss: 0.8747049343585968
  batch 400 loss: 0.8366081631183624
  batch 450 loss: 0.8522332525253296
  batch 500 loss: 0.9010622525215148
  batch 550 loss: 0.8044425296783447
  batch 600 loss: 0.8793299210071563
  batch 650 loss: 0.8976220178604126
  batch 700 loss: 0.8329100930690765
  batch 750 loss: 0.8450383138656616
  batch 800 loss: 0.8782983499765397
  batch 850 loss: 0.8354933106899262
  batch 900 loss: 0.8512404716014862
running loss: 20.26767188310623
LOSS train 0.85124 valid 0.94783, valid PER 28.48%
EPOCH 9, Learning Rate: 0.5
  batch 50 loss: 0.7496048176288604
  batch 100 loss: 0.7978435426950454
  batch 150 loss: 0.7900897371768951
  batch 200 loss: 0.7795588088035583
  batch 250 loss: 0.8103606766462326
  batch 300 loss: 0.8051389253139496
  batch 350 loss: 0.8375489628314972
  batch 400 loss: 0.8133490353822708
  batch 450 loss: 0.8055608916282654
  batch 500 loss: 0.7889753109216691
  batch 550 loss: 0.8217609775066376
  batch 600 loss: 0.8340001440048218
  batch 650 loss: 0.8066462481021881
  batch 700 loss: 0.7756372332572937
  batch 750 loss: 0.7915914940834046
  batch 800 loss: 0.8053930842876434
  batch 850 loss: 0.8277835166454315
  batch 900 loss: 0.7731898045539856
running loss: 19.874611139297485
LOSS train 0.77319 valid 0.93097, valid PER 28.79%
EPOCH 10, Learning Rate: 0.5
  batch 50 loss: 0.7011612462997436
  batch 100 loss: 0.7318242311477661
  batch 150 loss: 0.7375300711393357
  batch 200 loss: 0.7687622272968292
  batch 250 loss: 0.7597988343238831
  batch 300 loss: 0.7225688898563385
  batch 350 loss: 0.7571624314785004
  batch 400 loss: 0.7294234126806259
  batch 450 loss: 0.7304548156261444
  batch 500 loss: 0.765520870089531
  batch 550 loss: 0.7809317326545715
  batch 600 loss: 0.7635266435146332
  batch 650 loss: 0.7418219697475433
  batch 700 loss: 0.7658032763004303
  batch 750 loss: 0.7423506033420563
  batch 800 loss: 0.7820790910720825
  batch 850 loss: 0.7605088686943055
  batch 900 loss: 0.7619396591186524
running loss: 18.485317766666412
LOSS train 0.76194 valid 0.94376, valid PER 29.33%
EPOCH 11, Learning Rate: 0.5
  batch 50 loss: 0.6678666800260544
  batch 100 loss: 0.654082378745079
  batch 150 loss: 0.666918580532074
  batch 200 loss: 0.7399766725301743
  batch 250 loss: 0.718795828819275
  batch 300 loss: 0.6807395893335343
  batch 350 loss: 0.6990164190530777
  batch 400 loss: 0.7247532737255097
  batch 450 loss: 0.7051886415481567
  batch 500 loss: 0.7097050470113754
  batch 550 loss: 0.7186377894878387
  batch 600 loss: 0.7053435832262039
  batch 650 loss: 0.753524888753891
  batch 700 loss: 0.6841535007953644
  batch 750 loss: 0.7047206854820252
  batch 800 loss: 0.7393087249994278
  batch 850 loss: 0.7608843863010406
  batch 900 loss: 0.7484302270412445
running loss: 17.315640598535538
LOSS train 0.74843 valid 0.89456, valid PER 27.00%
EPOCH 12, Learning Rate: 0.5
  batch 50 loss: 0.6393097513914108
  batch 100 loss: 0.6351681053638458
  batch 150 loss: 0.5994671058654785
  batch 200 loss: 0.6692390465736389
  batch 250 loss: 0.6699045866727829
  batch 300 loss: 0.6338936132192612
  batch 350 loss: 0.6531275534629821
  batch 400 loss: 0.6865286660194397
  batch 450 loss: 0.678413615822792
  batch 500 loss: 0.6939613002538682
  batch 550 loss: 0.6473484712839127
  batch 600 loss: 0.6744298225641251
  batch 650 loss: 0.6891856449842453
  batch 700 loss: 0.703930813074112
  batch 750 loss: 0.6602881264686584
  batch 800 loss: 0.6758907866477967
  batch 850 loss: 0.7238984143733979
  batch 900 loss: 0.7173322725296021
running loss: 16.610226452350616
LOSS train 0.71733 valid 0.88688, valid PER 26.62%
EPOCH 13, Learning Rate: 0.5
  batch 50 loss: 0.5910700762271881
  batch 100 loss: 0.6018222677707672
  batch 150 loss: 0.586770835518837
  batch 200 loss: 0.6173971772193909
  batch 250 loss: 0.6115159887075424
  batch 300 loss: 0.6007992947101592
  batch 350 loss: 0.610195814371109
  batch 400 loss: 0.6327333807945251
  batch 450 loss: 0.6232114964723587
  batch 500 loss: 0.6192645728588104
  batch 550 loss: 0.6532199048995971
  batch 600 loss: 0.6243530195951462
  batch 650 loss: 0.6565686225891113
  batch 700 loss: 0.650169979929924
  batch 750 loss: 0.5924205189943313
  batch 800 loss: 0.6355388861894607
  batch 850 loss: 0.6591134136915207
  batch 900 loss: 0.6571335029602051
running loss: 16.5292187333107
LOSS train 0.65713 valid 0.89942, valid PER 26.38%
EPOCH 14, Learning Rate: 0.5
  batch 50 loss: 0.5612560707330704
  batch 100 loss: 0.5422777152061462
  batch 150 loss: 0.5633829194307327
  batch 200 loss: 0.5625224900245667
  batch 250 loss: 0.565354762673378
  batch 300 loss: 0.6077667650580406
  batch 350 loss: 0.5707550865411758
  batch 400 loss: 0.5830024862289429
  batch 450 loss: 0.5879252868890762
  batch 500 loss: 0.6049188500642777
  batch 550 loss: 0.6114944952726364
  batch 600 loss: 0.5689200204610825
  batch 650 loss: 0.6008628457784653
  batch 700 loss: 0.6372973030805588
  batch 750 loss: 0.5732701587677002
  batch 800 loss: 0.578114640712738
  batch 850 loss: 0.6172590577602386
  batch 900 loss: 0.6158874213695527
running loss: 15.500123888254166
LOSS train 0.61589 valid 0.91084, valid PER 26.54%
EPOCH 15, Learning Rate: 0.5
  batch 50 loss: 0.5080699855089188
  batch 100 loss: 0.5246351140737534
  batch 150 loss: 0.5346286636590958
  batch 200 loss: 0.553736081123352
  batch 250 loss: 0.5525696188211441
  batch 300 loss: 0.5496866607666016
  batch 350 loss: 0.5534616565704346
  batch 400 loss: 0.5564452612400055
  batch 450 loss: 0.5510647225379944
  batch 500 loss: 0.5398625612258912
  batch 550 loss: 0.5612296056747437
  batch 600 loss: 0.5951177549362182
  batch 650 loss: 0.5857508993148803
  batch 700 loss: 0.575532124042511
  batch 750 loss: 0.5785983222723007
  batch 800 loss: 0.5604285413026809
  batch 850 loss: 0.5523330622911453
  batch 900 loss: 0.5773480987548828
running loss: 13.553138732910156
LOSS train 0.57735 valid 0.93124, valid PER 26.36%
EPOCH 16, Learning Rate: 0.5
  batch 50 loss: 0.4911948949098587
  batch 100 loss: 0.4814063638448715
  batch 150 loss: 0.49630461871623993
  batch 200 loss: 0.4903502893447876
  batch 250 loss: 0.5276044052839279
  batch 300 loss: 0.5041102570295334
  batch 350 loss: 0.5203953361511231
  batch 400 loss: 0.5215214991569519
  batch 450 loss: 0.5268410140275955
  batch 500 loss: 0.49833702981472017
  batch 550 loss: 0.5038992148637772
  batch 600 loss: 0.5251187366247178
  batch 650 loss: 0.5587053573131562
  batch 700 loss: 0.5235527259111404
  batch 750 loss: 0.5534678554534912
  batch 800 loss: 0.55698262155056
  batch 850 loss: 0.5545669215917587
  batch 900 loss: 0.5442262369394303
running loss: 13.043001800775528
LOSS train 0.54423 valid 0.92959, valid PER 26.26%
EPOCH 17, Learning Rate: 0.5
  batch 50 loss: 0.4627252876758575
  batch 100 loss: 0.4617991709709168
  batch 150 loss: 0.45229303538799287
  batch 200 loss: 0.4630133202672005
  batch 250 loss: 0.504985316991806
  batch 300 loss: 0.4818978667259216
  batch 350 loss: 0.4711172753572464
  batch 400 loss: 0.5029390245676041
  batch 450 loss: 0.49552410662174223
  batch 500 loss: 0.4722230762243271
  batch 550 loss: 0.48557957768440246
  batch 600 loss: 0.5054186427593231
  batch 650 loss: 0.49977135539054873
  batch 700 loss: 0.49945250689983367
  batch 750 loss: 0.4802071464061737
  batch 800 loss: 0.4970182126760483
  batch 850 loss: 0.505422260761261
  batch 900 loss: 0.49260402619838717
running loss: 12.788036733865738
LOSS train 0.49260 valid 0.92793, valid PER 25.47%
EPOCH 18, Learning Rate: 0.5
  batch 50 loss: 0.4090228259563446
  batch 100 loss: 0.42266122668981554
  batch 150 loss: 0.4655162900686264
  batch 200 loss: 0.44308828115463256
  batch 250 loss: 0.46606153428554536
  batch 300 loss: 0.4293737953901291
  batch 350 loss: 0.44727640628814697
  batch 400 loss: 0.4434786680340767
  batch 450 loss: 0.4674085718393326
  batch 500 loss: 0.4514749854803085
  batch 550 loss: 0.47746920824050904
  batch 600 loss: 0.4671241122484207
  batch 650 loss: 0.44543173342943193
  batch 700 loss: 0.45881431221961977
  batch 750 loss: 0.46832049190998076
  batch 800 loss: 0.4528162711858749
  batch 850 loss: 0.4786033111810684
  batch 900 loss: 0.4879880279302597
running loss: 11.714686959981918
LOSS train 0.48799 valid 0.94796, valid PER 25.69%
EPOCH 19, Learning Rate: 0.5
  batch 50 loss: 0.3993313202261925
  batch 100 loss: 0.3816303825378418
  batch 150 loss: 0.3809698450565338
  batch 200 loss: 0.4023032397031784
  batch 250 loss: 0.39816994160413743
  batch 300 loss: 0.4130453649163246
  batch 350 loss: 0.4078906172513962
  batch 400 loss: 0.42471343517303467
  batch 450 loss: 0.43975519835948945
  batch 500 loss: 0.4283232280611992
  batch 550 loss: 0.42411125004291533
  batch 600 loss: 0.42033314377069475
  batch 650 loss: 0.46860839784145353
  batch 700 loss: 0.4293781936168671
  batch 750 loss: 0.42667777597904205
  batch 800 loss: 0.4474515333771706
  batch 850 loss: 0.4526991331577301
  batch 900 loss: 0.44081482887268064
running loss: 10.566840410232544
LOSS train 0.44081 valid 0.97388, valid PER 26.12%
EPOCH 20, Learning Rate: 0.5
  batch 50 loss: 0.36558322340250016
  batch 100 loss: 0.35387659013271333
  batch 150 loss: 0.35678845793008807
  batch 200 loss: 0.3781354138255119
  batch 250 loss: 0.37510122269392016
  batch 300 loss: 0.38541001617908477
  batch 350 loss: 0.3695851600170135
  batch 400 loss: 0.384080191552639
  batch 450 loss: 0.4002203696966171
  batch 500 loss: 0.37444940268993376
  batch 550 loss: 0.4362445938587189
  batch 600 loss: 0.4074063986539841
  batch 650 loss: 0.4109776622056961
  batch 700 loss: 0.42031279325485227
  batch 750 loss: 0.37754474222660067
  batch 800 loss: 0.4306320559978485
  batch 850 loss: 0.4280403870344162
  batch 900 loss: 0.4186388337612152
running loss: 10.090790688991547
LOSS train 0.41864 valid 1.03109, valid PER 25.68%
Training finished in 9.0 minutes.
Model saved to checkpoints/20231210_035807/model_12
Loading model from checkpoints/20231210_035807/model_12
SUB: 14.33%, DEL: 11.26%, INS: 2.21%, COR: 74.41%, PER: 27.80%
