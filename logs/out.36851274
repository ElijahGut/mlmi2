Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.1, optimiser='sgd', grad_clip=None)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.167357501983642
  batch 100 loss: 3.177679557800293
  batch 150 loss: 3.034065432548523
  batch 200 loss: 2.9180173015594484
  batch 250 loss: 2.83626962184906
  batch 300 loss: 2.6839871978759766
  batch 350 loss: 2.565197854042053
  batch 400 loss: 2.4646384620666506
  batch 450 loss: 2.4552144861221312
  batch 500 loss: 2.348204915523529
  batch 550 loss: 2.3263227272033693
  batch 600 loss: 2.1263525080680847
  batch 650 loss: 2.0337803602218627
  batch 700 loss: 2.01303715467453
  batch 750 loss: 1.9488483119010924
  batch 800 loss: 1.9280304598808289
  batch 850 loss: 1.8585135126113892
  batch 900 loss: 1.845025932788849
LOSS train 1.84503 valid 1.85104, valid PER 70.35%
EPOCH 2:
  batch 50 loss: 1.7876216506958007
  batch 100 loss: 1.7274880623817443
  batch 150 loss: 1.709457437992096
  batch 200 loss: 1.7010525679588318
  batch 250 loss: 1.7209260439872742
  batch 300 loss: 1.668881094455719
  batch 350 loss: 1.5665583205223084
  batch 400 loss: 1.594084348678589
  batch 450 loss: 1.5459321069717407
  batch 500 loss: 1.5638482666015625
  batch 550 loss: 1.571033000946045
  batch 600 loss: 1.5157767367362975
  batch 650 loss: 1.5440324640274048
  batch 700 loss: 1.5363975977897644
  batch 750 loss: 1.4926066422462463
  batch 800 loss: 1.4296598410606385
  batch 850 loss: 1.4248063993453979
  batch 900 loss: 1.4503507781028748
LOSS train 1.45035 valid 1.44351, valid PER 45.35%
EPOCH 3:
  batch 50 loss: 1.396870982646942
  batch 100 loss: 1.391358871459961
  batch 150 loss: 1.3821173095703125
  batch 200 loss: 1.3288603973388673
  batch 250 loss: 1.339447009563446
  batch 300 loss: 1.3263414692878723
  batch 350 loss: 1.3751827812194823
  batch 400 loss: 1.3476536178588867
  batch 450 loss: 1.3362422037124633
  batch 500 loss: 1.3010132598876953
  batch 550 loss: 1.3299714303016663
  batch 600 loss: 1.3066854226589202
  batch 650 loss: 1.2481521415710448
  batch 700 loss: 1.30234912276268
  batch 750 loss: 1.325378190279007
  batch 800 loss: 1.2693705224990846
  batch 850 loss: 1.2877220010757446
  batch 900 loss: 1.2383353185653687
LOSS train 1.23834 valid 1.34422, valid PER 40.07%
EPOCH 4:
  batch 50 loss: 1.2256718838214875
  batch 100 loss: 1.2429006719589233
  batch 150 loss: 1.1838167703151703
  batch 200 loss: 1.2654992210865021
  batch 250 loss: 1.2417252349853516
  batch 300 loss: 1.238933149576187
  batch 350 loss: 1.1750450134277344
  batch 400 loss: 1.2156566095352173
  batch 450 loss: 1.1961264300346375
  batch 500 loss: 1.1803679621219636
  batch 550 loss: 1.2055052137374878
  batch 600 loss: 1.2300853729248047
  batch 650 loss: 1.2177167963981628
  batch 700 loss: 1.1866765749454498
  batch 750 loss: 1.1545658123493194
  batch 800 loss: 1.1220674657821654
  batch 850 loss: 1.1784378933906554
  batch 900 loss: 1.2239039361476898
LOSS train 1.22390 valid 1.17478, valid PER 36.70%
EPOCH 5:
  batch 50 loss: 1.1056648707389831
  batch 100 loss: 1.1288584220409392
  batch 150 loss: 1.2043065547943115
  batch 200 loss: 1.1038988399505616
  batch 250 loss: 1.130458732843399
  batch 300 loss: 1.162521605491638
  batch 350 loss: 1.155103611946106
  batch 400 loss: 1.137179661989212
  batch 450 loss: 1.1567662024497987
  batch 500 loss: 1.13752317070961
  batch 550 loss: 1.1155390107631684
  batch 600 loss: 1.207115685939789
  batch 650 loss: 1.153705461025238
  batch 700 loss: 1.20106769323349
  batch 750 loss: 1.1512208187580109
  batch 800 loss: 1.1476196801662446
  batch 850 loss: 1.1715343046188353
  batch 900 loss: 1.1357663464546204
LOSS train 1.13577 valid 1.11468, valid PER 35.44%
EPOCH 6:
  batch 50 loss: 1.1293227303028106
  batch 100 loss: 1.0730196845531463
  batch 150 loss: 1.0595081698894502
  batch 200 loss: 1.0720441722869873
  batch 250 loss: 1.1213076269626618
  batch 300 loss: 1.0773336470127106
  batch 350 loss: 1.1189041936397552
  batch 400 loss: 1.065268646478653
  batch 450 loss: 1.1097375631332398
  batch 500 loss: 1.0826102268695832
  batch 550 loss: 1.113984773159027
  batch 600 loss: 1.0584663796424865
  batch 650 loss: 1.0759576570987701
  batch 700 loss: 1.0689858758449555
  batch 750 loss: 1.0581190812587737
  batch 800 loss: 1.063285984992981
  batch 850 loss: 1.0492099058628082
  batch 900 loss: 1.0692814779281616
LOSS train 1.06928 valid 1.07779, valid PER 34.46%
EPOCH 7:
  batch 50 loss: 1.052933806180954
  batch 100 loss: 1.0608239412307738
  batch 150 loss: 1.046259229183197
  batch 200 loss: 1.0231869554519653
  batch 250 loss: 1.0424592661857606
  batch 300 loss: 1.014033054113388
  batch 350 loss: 1.0542876255512237
  batch 400 loss: 1.0350983679294585
  batch 450 loss: 1.0329515624046326
  batch 500 loss: 1.0465270793437957
  batch 550 loss: 1.081048494577408
  batch 600 loss: 1.0165906846523285
  batch 650 loss: 1.0255083179473876
  batch 700 loss: 1.0542008006572723
  batch 750 loss: 1.009153118133545
  batch 800 loss: 1.0008407056331634
  batch 850 loss: 1.169152582883835
  batch 900 loss: 1.1030131888389587
LOSS train 1.10301 valid 1.08077, valid PER 35.16%
EPOCH 8:
  batch 50 loss: 1.0259144055843352
  batch 100 loss: 1.0171218514442444
  batch 150 loss: 0.9856359052658081
  batch 200 loss: 0.9813833272457123
  batch 250 loss: 0.9957743287086487
  batch 300 loss: 0.9485263466835022
  batch 350 loss: 1.0193833303451538
  batch 400 loss: 1.0125585639476775
  batch 450 loss: 1.0357103335857392
  batch 500 loss: 1.045337905883789
  batch 550 loss: 1.008163869380951
  batch 600 loss: 1.0265062129497529
  batch 650 loss: 1.0361724996566772
  batch 700 loss: 0.9976117968559265
  batch 750 loss: 1.0005919170379638
  batch 800 loss: 1.0169687366485596
  batch 850 loss: 1.0229606544971466
  batch 900 loss: 1.003001093864441
LOSS train 1.00300 valid 1.01898, valid PER 32.48%
EPOCH 9:
  batch 50 loss: 0.9364474201202393
  batch 100 loss: 0.9669607889652252
  batch 150 loss: 0.983930585384369
  batch 200 loss: 0.9621645605564118
  batch 250 loss: 0.9546954190731048
  batch 300 loss: 0.981944682598114
  batch 350 loss: 0.9855966377258301
  batch 400 loss: 0.9827637338638305
  batch 450 loss: 0.9767269349098205
  batch 500 loss: 0.946665860414505
  batch 550 loss: 0.9888321781158447
  batch 600 loss: 1.0102373218536378
  batch 650 loss: 0.9780451345443726
  batch 700 loss: 0.9727480840682984
  batch 750 loss: 0.9943695557117462
  batch 800 loss: 1.0118530094623566
  batch 850 loss: 0.9994351470470428
  batch 900 loss: 0.9740325808525085
LOSS train 0.97403 valid 1.01131, valid PER 32.26%
EPOCH 10:
  batch 50 loss: 0.9170256507396698
  batch 100 loss: 0.936213196516037
  batch 150 loss: 0.9488441395759583
  batch 200 loss: 0.9915944182872772
  batch 250 loss: 0.9687874937057495
  batch 300 loss: 0.9326701319217682
  batch 350 loss: 1.0044108021259308
  batch 400 loss: 0.9487938833236694
  batch 450 loss: 0.9467022037506103
  batch 500 loss: 1.0138727033138275
  batch 550 loss: 0.9890546572208404
  batch 600 loss: 0.9649502873420716
  batch 650 loss: 0.9809550702571869
  batch 700 loss: 1.0196212911605835
  batch 750 loss: 0.9615470170974731
  batch 800 loss: 0.975659419298172
  batch 850 loss: 0.9891176724433899
  batch 900 loss: 0.9708005380630493
LOSS train 0.97080 valid 1.00578, valid PER 32.83%
EPOCH 11:
  batch 50 loss: 0.9236690592765808
  batch 100 loss: 0.9335726201534271
  batch 150 loss: 0.9310408520698548
  batch 200 loss: 0.9731654834747314
  batch 250 loss: 0.9539665329456329
  batch 300 loss: 0.9059760928153991
  batch 350 loss: 0.9354203569889069
  batch 400 loss: 0.9591781449317932
  batch 450 loss: 0.9429240357875824
  batch 500 loss: 0.929598115682602
  batch 550 loss: 0.9223531806468963
  batch 600 loss: 0.8992943239212036
  batch 650 loss: 0.9628133034706116
  batch 700 loss: 0.8899213886260986
  batch 750 loss: 0.9571489882469177
  batch 800 loss: 0.962110333442688
  batch 850 loss: 0.9706793880462646
  batch 900 loss: 0.9564272594451905
LOSS train 0.95643 valid 0.99848, valid PER 31.68%
EPOCH 12:
  batch 50 loss: 0.9209017848968506
  batch 100 loss: 0.881709371805191
  batch 150 loss: 0.8653925216197967
  batch 200 loss: 0.8838258123397827
  batch 250 loss: 0.8996587479114533
  batch 300 loss: 0.8876896905899048
  batch 350 loss: 0.8766963684558868
  batch 400 loss: 0.9286109375953674
  batch 450 loss: 0.9225737142562866
  batch 500 loss: 0.9469425594806671
  batch 550 loss: 0.857250452041626
  batch 600 loss: 0.8754467189311981
  batch 650 loss: 0.9238962495326996
  batch 700 loss: 0.9115556120872498
  batch 750 loss: 0.8969291877746582
  batch 800 loss: 0.9186218476295471
  batch 850 loss: 0.9985405683517456
  batch 900 loss: 0.9619269979000091
LOSS train 0.96193 valid 1.03346, valid PER 33.12%
EPOCH 13:
  batch 50 loss: 0.9059428417682648
  batch 100 loss: 0.9125137257575989
  batch 150 loss: 0.861517242193222
  batch 200 loss: 0.8763766014575958
  batch 250 loss: 0.9183501076698303
  batch 300 loss: 0.9060497951507568
  batch 350 loss: 0.8944980919361114
  batch 400 loss: 0.9087151432037354
  batch 450 loss: 0.9347668623924256
  batch 500 loss: 0.9024388515949249
  batch 550 loss: 0.9077634143829346
  batch 600 loss: 0.8842529129981994
  batch 650 loss: 0.9093311154842376
  batch 700 loss: 0.9147476494312287
  batch 750 loss: 0.8674741661548615
  batch 800 loss: 0.8711131393909455
  batch 850 loss: 0.9183851206302642
  batch 900 loss: 0.9261920154094696
LOSS train 0.92619 valid 0.97716, valid PER 31.00%
EPOCH 14:
  batch 50 loss: 0.8906958985328675
  batch 100 loss: 0.8505532228946686
  batch 150 loss: 0.8708049082756042
  batch 200 loss: 0.8672106182575225
  batch 250 loss: 0.8689084768295288
  batch 300 loss: 0.8961579251289368
  batch 350 loss: 0.8502402329444885
  batch 400 loss: 0.8479471063613891
  batch 450 loss: 0.8404593062400818
  batch 500 loss: 0.8686516880989075
  batch 550 loss: 0.8704773247241974
  batch 600 loss: 0.849234465956688
  batch 650 loss: 0.8807501232624054
  batch 700 loss: 0.8795144951343536
  batch 750 loss: 0.8476455998420716
  batch 800 loss: 0.8447262692451477
  batch 850 loss: 0.9369066202640534
  batch 900 loss: 0.9296523559093476
LOSS train 0.92965 valid 0.98746, valid PER 31.94%
EPOCH 15:
  batch 50 loss: 0.8779206895828247
  batch 100 loss: 0.8550349879264831
  batch 150 loss: 0.8559871041774749
  batch 200 loss: 0.9144561886787415
  batch 250 loss: 0.8686250865459442
  batch 300 loss: 0.8583828771114349
  batch 350 loss: 0.8624506223201752
  batch 400 loss: 0.8218018269538879
  batch 450 loss: 0.8574197816848755
  batch 500 loss: 0.8214546865224839
  batch 550 loss: 0.8488202071189881
  batch 600 loss: 0.8843370950222016
  batch 650 loss: 0.874834133386612
  batch 700 loss: 0.900649139881134
  batch 750 loss: 0.8964474785327912
  batch 800 loss: 0.8969274640083313
  batch 850 loss: 0.9316968810558319
  batch 900 loss: 0.8984429717063904
LOSS train 0.89844 valid 1.00255, valid PER 32.10%
EPOCH 16:
  batch 50 loss: 0.885451363325119
  batch 100 loss: 0.8252395331859589
  batch 150 loss: 0.8206780457496643
  batch 200 loss: 0.8324270534515381
  batch 250 loss: 0.882290461063385
  batch 300 loss: 0.8571739494800568
  batch 350 loss: 0.8774071633815765
  batch 400 loss: 0.8551790916919708
  batch 450 loss: 0.881794102191925
  batch 500 loss: 0.8252097320556641
  batch 550 loss: 0.8702544820308685
  batch 600 loss: 0.8614576649665833
  batch 650 loss: 0.8464123809337616
  batch 700 loss: 0.8334914565086364
  batch 750 loss: 0.8481321537494659
  batch 800 loss: 0.8822449743747711
  batch 850 loss: 0.8508324599266053
  batch 900 loss: 0.8464576709270477
LOSS train 0.84646 valid 0.94989, valid PER 30.20%
EPOCH 17:
  batch 50 loss: 0.8375282937288284
  batch 100 loss: 0.834784426689148
  batch 150 loss: 0.8195030069351197
  batch 200 loss: 0.8109448182582856
  batch 250 loss: 0.833930195569992
  batch 300 loss: 0.8363081181049347
  batch 350 loss: 0.8179435205459594
  batch 400 loss: 0.8607141304016114
  batch 450 loss: 0.8434027230739594
  batch 500 loss: 0.8083572709560394
  batch 550 loss: 0.8480527782440186
  batch 600 loss: 0.8985638880729675
  batch 650 loss: 0.8360054469108582
  batch 700 loss: 0.8404362440109253
  batch 750 loss: 0.822684406042099
  batch 800 loss: 0.8339276695251465
  batch 850 loss: 0.8497670066356658
  batch 900 loss: 0.8212791323661804
LOSS train 0.82128 valid 0.96925, valid PER 30.52%
EPOCH 18:
  batch 50 loss: 0.825604841709137
  batch 100 loss: 0.8444876909255982
  batch 150 loss: 0.8671421122550964
  batch 200 loss: 0.8287167924642563
  batch 250 loss: 0.8331252717971802
  batch 300 loss: 0.8064562034606934
  batch 350 loss: 0.8729178357124329
  batch 400 loss: 0.8029583096504211
  batch 450 loss: 0.8825689888000489
  batch 500 loss: 0.823149551153183
  batch 550 loss: 0.8151194536685944
  batch 600 loss: 0.8113401448726654
  batch 650 loss: 0.8241100168228149
  batch 700 loss: 0.8483844542503357
  batch 750 loss: 0.8179224604368209
  batch 800 loss: 0.8247139114141464
  batch 850 loss: 0.828654910326004
  batch 900 loss: 0.863075670003891
LOSS train 0.86308 valid 0.96802, valid PER 30.70%
EPOCH 19:
  batch 50 loss: 0.759325008392334
  batch 100 loss: 0.7815902829170227
  batch 150 loss: 0.8024216532707215
  batch 200 loss: 0.8090416812896728
  batch 250 loss: 0.8208129441738129
  batch 300 loss: 0.8195237469673157
  batch 350 loss: 0.7875812005996704
  batch 400 loss: 0.8316138231754303
  batch 450 loss: 0.8263663530349732
  batch 500 loss: 0.8093940877914428
  batch 550 loss: 0.8212626528739929
  batch 600 loss: 0.8433665990829468
  batch 650 loss: 0.9353836846351623
  batch 700 loss: 0.8429031014442444
  batch 750 loss: 0.8134480738639831
  batch 800 loss: 0.8513756632804871
  batch 850 loss: 0.8840091454982758
  batch 900 loss: 0.8293369889259339
LOSS train 0.82934 valid 0.97797, valid PER 31.05%
EPOCH 20:
  batch 50 loss: 0.8024550688266754
  batch 100 loss: 0.7786178195476532
  batch 150 loss: 0.7759000605344772
  batch 200 loss: 0.8038418066501617
  batch 250 loss: 0.7969425392150878
  batch 300 loss: 0.8070354294776917
  batch 350 loss: 0.883181049823761
  batch 400 loss: 0.8756985926628112
  batch 450 loss: 0.868783382177353
  batch 500 loss: 0.8362944352626801
  batch 550 loss: 0.8996483075618744
  batch 600 loss: 0.8089712083339691
  batch 650 loss: 0.8384115159511566
  batch 700 loss: 0.8367627060413361
  batch 750 loss: 0.8182889819145203
  batch 800 loss: 0.8384155285358429
  batch 850 loss: 0.8224935257434844
  batch 900 loss: 0.8356434845924378
LOSS train 0.83564 valid 0.94867, valid PER 30.15%
Training finished in 5.0 minutes.
Model saved to checkpoints/20231207_172449/model_20
Loading model from checkpoints/20231207_172449/model_20
SUB: 15.40%, DEL: 13.69%, INS: 2.02%, COR: 70.90%, PER: 31.11%
