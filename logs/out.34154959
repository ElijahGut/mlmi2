Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.1, optimiser='sgd')
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.164659695625305
  batch 100 loss: 3.1752304935455324
  batch 150 loss: 3.028200478553772
  batch 200 loss: 2.908307795524597
  batch 250 loss: 2.8160798835754393
  batch 300 loss: 2.8217694759368896
  batch 350 loss: 2.5568658399581907
  batch 400 loss: 2.47830970287323
  batch 450 loss: 2.428416724205017
  batch 500 loss: 2.3446427273750303
  batch 550 loss: 2.311241784095764
  batch 600 loss: 2.134418752193451
  batch 650 loss: 2.0111509680747988
  batch 700 loss: 1.9919447660446168
  batch 750 loss: 1.9295987844467164
  batch 800 loss: 1.902683310508728
  batch 850 loss: 1.8351586294174194
  batch 900 loss: 1.8463250946998597
LOSS train 1.84633 valid 1.81144, valid PER 69.70%
EPOCH 2:
  batch 50 loss: 1.7905201673507691
  batch 100 loss: 1.7476225757598878
  batch 150 loss: 1.6270153665542602
  batch 200 loss: 1.674454095363617
  batch 250 loss: 1.6375389885902405
  batch 300 loss: 1.6091015696525575
  batch 350 loss: 1.6078250336647033
  batch 400 loss: 1.5581274437904358
  batch 450 loss: 1.5624638032913207
  batch 500 loss: 1.535818612575531
  batch 550 loss: 1.4870019721984864
  batch 600 loss: 1.4961805653572082
  batch 650 loss: 1.4316181874275207
  batch 700 loss: 1.4792841911315917
  batch 750 loss: 1.4278412246704102
  batch 800 loss: 1.3797297525405883
  batch 850 loss: 1.419772264957428
  batch 900 loss: 1.3513015127182006
LOSS train 1.35130 valid 1.39661, valid PER 44.89%
EPOCH 3:
  batch 50 loss: 1.2851632595062257
  batch 100 loss: 1.3739168930053711
  batch 150 loss: 1.362477707862854
  batch 200 loss: 1.280439029932022
  batch 250 loss: 1.3121623659133912
  batch 300 loss: 1.3031913828849793
  batch 350 loss: 1.3250375509262085
  batch 400 loss: 1.2931821846961975
  batch 450 loss: 1.2608834195137024
  batch 500 loss: 1.261312415599823
  batch 550 loss: 1.277881977558136
  batch 600 loss: 1.1858230388164521
  batch 650 loss: 1.251040086746216
  batch 700 loss: 1.2511308121681213
  batch 750 loss: 1.2775388026237489
  batch 800 loss: 1.2590398371219635
  batch 850 loss: 1.2329200935363769
  batch 900 loss: 1.2341762614250182
LOSS train 1.23418 valid 1.20372, valid PER 38.87%
EPOCH 4:
  batch 50 loss: 1.2242199158668519
  batch 100 loss: 1.147527780532837
  batch 150 loss: 1.1800365149974823
  batch 200 loss: 1.2209920966625214
  batch 250 loss: 1.2065941989421844
  batch 300 loss: 1.1920725154876708
  batch 350 loss: 1.190182636976242
  batch 400 loss: 1.1404245460033418
  batch 450 loss: 1.151251701116562
  batch 500 loss: 1.1793831837177278
  batch 550 loss: 1.1340443837642669
  batch 600 loss: 1.1112933850288391
  batch 650 loss: 1.1971085000038146
  batch 700 loss: 1.2060672068595886
  batch 750 loss: 1.1486250317096711
  batch 800 loss: 1.1374723505973816
  batch 850 loss: 1.1096407389640808
  batch 900 loss: 1.1304865729808808
LOSS train 1.13049 valid 1.15342, valid PER 35.82%
EPOCH 5:
  batch 50 loss: 1.102854243516922
  batch 100 loss: 1.085848171710968
  batch 150 loss: 1.1199272382259369
  batch 200 loss: 1.1270600056648254
  batch 250 loss: 1.0963907790184022
  batch 300 loss: 1.1001396119594573
  batch 350 loss: 1.0649395549297334
  batch 400 loss: 1.0547356593608856
  batch 450 loss: 1.0482985007762908
  batch 500 loss: 1.0581099486351013
  batch 550 loss: 1.0860024273395539
  batch 600 loss: 1.1279600358009338
  batch 650 loss: 1.1102755355834961
  batch 700 loss: 1.0736087214946748
  batch 750 loss: 1.0426264190673828
  batch 800 loss: 1.1094629240036011
  batch 850 loss: 1.0868524026870727
  batch 900 loss: 1.0651159548759461
LOSS train 1.06512 valid 1.08650, valid PER 34.20%
EPOCH 6:
  batch 50 loss: 1.024880827665329
  batch 100 loss: 1.0327815198898316
  batch 150 loss: 1.0267086577415467
  batch 200 loss: 0.9856172108650207
  batch 250 loss: 1.0882402074337005
  batch 300 loss: 1.061266965866089
  batch 350 loss: 1.0577149736881255
  batch 400 loss: 1.0391091513633728
  batch 450 loss: 1.0423217070102693
  batch 500 loss: 0.9918329739570617
  batch 550 loss: 1.031746872663498
  batch 600 loss: 1.017264757156372
  batch 650 loss: 0.9898811554908753
  batch 700 loss: 0.9716046011447906
  batch 750 loss: 1.0243673026561737
  batch 800 loss: 1.0252359211444855
  batch 850 loss: 1.0383603513240813
  batch 900 loss: 1.02185289144516
LOSS train 1.02185 valid 1.07878, valid PER 33.94%
EPOCH 7:
  batch 50 loss: 0.9627477490901947
  batch 100 loss: 1.0377365517616273
  batch 150 loss: 0.9637785851955414
  batch 200 loss: 0.9726420223712922
  batch 250 loss: 1.0211642897129058
  batch 300 loss: 0.9834280192852021
  batch 350 loss: 1.022438895702362
  batch 400 loss: 0.9710326683521271
  batch 450 loss: 0.9689225041866303
  batch 500 loss: 0.9899034380912781
  batch 550 loss: 0.9490109050273895
  batch 600 loss: 1.01843208193779
  batch 650 loss: 0.9715128672122956
  batch 700 loss: 1.0126246976852418
  batch 750 loss: 0.9729451358318328
  batch 800 loss: 0.9917683482170105
  batch 850 loss: 0.9565402352809906
  batch 900 loss: 0.954630640745163
LOSS train 0.95463 valid 1.04872, valid PER 33.17%
EPOCH 8:
  batch 50 loss: 0.9487492227554322
  batch 100 loss: 0.9206208193302154
  batch 150 loss: 0.9490221512317657
  batch 200 loss: 0.9716117143630981
  batch 250 loss: 0.9631397104263306
  batch 300 loss: 0.9561428964138031
  batch 350 loss: 0.9542713713645935
  batch 400 loss: 0.935651843547821
  batch 450 loss: 0.9863661062717438
  batch 500 loss: 0.953379567861557
  batch 550 loss: 0.9389537203311921
  batch 600 loss: 0.8931414031982422
  batch 650 loss: 0.9237487077713012
  batch 700 loss: 0.9712460792064667
  batch 750 loss: 0.9668711233139038
  batch 800 loss: 0.9486333084106445
  batch 850 loss: 0.9077883625030517
  batch 900 loss: 0.9293577790260314
LOSS train 0.92936 valid 1.03282, valid PER 31.87%
EPOCH 9:
  batch 50 loss: 0.8901737260818482
  batch 100 loss: 0.875092797279358
  batch 150 loss: 0.8790323293209076
  batch 200 loss: 0.8613715052604676
  batch 250 loss: 0.8670049405097962
  batch 300 loss: 0.8841330862045288
  batch 350 loss: 0.8707835447788238
  batch 400 loss: 0.9222195541858673
  batch 450 loss: 0.9138486993312835
  batch 500 loss: 0.9087597465515137
  batch 550 loss: 0.9019201338291168
  batch 600 loss: 0.9464889121055603
  batch 650 loss: 0.9142185401916504
  batch 700 loss: 0.9039288938045502
  batch 750 loss: 0.8948202729225159
  batch 800 loss: 0.9360662317276001
  batch 850 loss: 0.9319464898109436
  batch 900 loss: 0.9249499642848968
LOSS train 0.92495 valid 0.98886, valid PER 31.09%
EPOCH 10:
  batch 50 loss: 0.8703232645988465
  batch 100 loss: 0.8918416559696197
  batch 150 loss: 0.9228772497177125
  batch 200 loss: 0.90097198843956
  batch 250 loss: 0.8875363206863404
  batch 300 loss: 0.8916773641109467
  batch 350 loss: 0.8931265044212341
  batch 400 loss: 0.8675751030445099
  batch 450 loss: 0.8646204626560211
  batch 500 loss: 0.8751089036464691
  batch 550 loss: 0.8759931790828704
  batch 600 loss: 0.8690851199626922
  batch 650 loss: 0.8838412237167358
  batch 700 loss: 0.8927014887332916
  batch 750 loss: 0.9054787242412567
  batch 800 loss: 0.8797043895721436
  batch 850 loss: 0.8825842201709747
  batch 900 loss: 0.8538547420501709
LOSS train 0.85385 valid 1.01418, valid PER 31.29%
EPOCH 11:
  batch 50 loss: 0.8382632851600647
  batch 100 loss: 0.8263190460205078
  batch 150 loss: 0.8451857388019561
  batch 200 loss: 0.8031976866722107
  batch 250 loss: 0.8521139860153198
  batch 300 loss: 0.8711155784130097
  batch 350 loss: 0.8807971030473709
  batch 400 loss: 0.8429811429977417
  batch 450 loss: 0.8480366253852845
  batch 500 loss: 0.8515049040317535
  batch 550 loss: 0.8606271851062774
  batch 600 loss: 0.8444208526611328
  batch 650 loss: 0.8804641735553741
  batch 700 loss: 0.9222232973575593
  batch 750 loss: 0.8589012134075165
  batch 800 loss: 0.8841735756397248
  batch 850 loss: 0.8694707059860229
  batch 900 loss: 0.8836398649215699
LOSS train 0.88364 valid 0.98270, valid PER 30.09%
EPOCH 12:
  batch 50 loss: 0.7937883722782135
  batch 100 loss: 0.8195358431339264
  batch 150 loss: 0.8475386357307434
  batch 200 loss: 0.8727384674549102
  batch 250 loss: 0.8411139297485352
  batch 300 loss: 0.8731793928146362
  batch 350 loss: 0.835114119052887
  batch 400 loss: 0.883737655878067
  batch 450 loss: 0.8518361079692841
  batch 500 loss: 0.8536256241798401
  batch 550 loss: 0.8549135196208953
  batch 600 loss: 0.8578940212726593
  batch 650 loss: 0.8523177480697632
  batch 700 loss: 0.8199410903453827
  batch 750 loss: 0.8558877384662629
  batch 800 loss: 0.8060280406475067
  batch 850 loss: 0.8604972720146179
  batch 900 loss: 0.8470301699638366
LOSS train 0.84703 valid 1.01014, valid PER 30.84%
EPOCH 13:
  batch 50 loss: 0.7824633312225342
  batch 100 loss: 0.7957510590553284
  batch 150 loss: 0.8164253926277161
  batch 200 loss: 0.7727898716926574
  batch 250 loss: 0.7929670155048371
  batch 300 loss: 0.8370962888002396
  batch 350 loss: 0.7954012870788574
  batch 400 loss: 0.7954999530315399
  batch 450 loss: 0.8231188929080964
  batch 500 loss: 0.7882919871807098
  batch 550 loss: 0.8559813523292541
  batch 600 loss: 0.8169218277931214
  batch 650 loss: 0.8070212268829345
  batch 700 loss: 0.8313478982448578
  batch 750 loss: 0.7945027631521225
  batch 800 loss: 0.8169407796859741
  batch 850 loss: 0.8006825494766235
  batch 900 loss: 0.810636561512947
LOSS train 0.81064 valid 0.96806, valid PER 29.68%
EPOCH 14:
  batch 50 loss: 0.7720266675949097
  batch 100 loss: 0.742719087600708
  batch 150 loss: 0.7535832667350769
  batch 200 loss: 0.7726677882671357
  batch 250 loss: 0.7813841950893402
  batch 300 loss: 0.7547003674507141
  batch 350 loss: 0.7603259038925171
  batch 400 loss: 0.8222623980045318
  batch 450 loss: 0.7802356338500976
  batch 500 loss: 0.805953631401062
  batch 550 loss: 0.7918566179275512
  batch 600 loss: 0.7880427217483521
  batch 650 loss: 0.7889629697799683
  batch 700 loss: 0.8138163638114929
  batch 750 loss: 0.8082293200492859
  batch 800 loss: 0.8024142217636109
  batch 850 loss: 0.8154130458831788
  batch 900 loss: 0.8142697703838349
LOSS train 0.81427 valid 0.97535, valid PER 30.02%
EPOCH 15:
  batch 50 loss: 0.7332285487651825
  batch 100 loss: 0.7422528111934662
  batch 150 loss: 0.7197450435161591
  batch 200 loss: 0.7603598642349243
  batch 250 loss: 0.7781970977783204
  batch 300 loss: 0.7919668686389923
  batch 350 loss: 0.7837253081798553
  batch 400 loss: 0.7932879614830017
  batch 450 loss: 0.7747322499752045
  batch 500 loss: 0.772784196138382
  batch 550 loss: 0.8226499253511429
  batch 600 loss: 0.8372095966339111
  batch 650 loss: 0.7994063830375672
  batch 700 loss: 0.7755652236938476
  batch 750 loss: 0.7970218014717102
  batch 800 loss: 0.7826999127864838
  batch 850 loss: 0.7623864161968231
  batch 900 loss: 0.7415355914831161
LOSS train 0.74154 valid 0.95688, valid PER 29.52%
EPOCH 16:
  batch 50 loss: 0.733397388458252
  batch 100 loss: 0.7187147092819214
  batch 150 loss: 0.7454566442966462
  batch 200 loss: 0.752169851064682
  batch 250 loss: 0.7315630865097046
  batch 300 loss: 0.7516378295421601
  batch 350 loss: 0.7466323161125183
  batch 400 loss: 0.735714795589447
  batch 450 loss: 0.7626712882518768
  batch 500 loss: 0.7333936285972595
  batch 550 loss: 0.7265812158584595
  batch 600 loss: 0.7554604482650756
  batch 650 loss: 0.7561849117279053
  batch 700 loss: 0.7257069313526153
  batch 750 loss: 0.7719810307025909
  batch 800 loss: 0.7888254058361054
  batch 850 loss: 0.7505301594734192
  batch 900 loss: 0.7767233383655548
LOSS train 0.77672 valid 0.97821, valid PER 29.77%
EPOCH 17:
  batch 50 loss: 0.7438735866546631
  batch 100 loss: 0.6951915580034256
  batch 150 loss: 0.770949677824974
  batch 200 loss: 0.7061005318164826
  batch 250 loss: 0.744457689523697
  batch 300 loss: 0.7434139454364777
  batch 350 loss: 0.7545990782976151
  batch 400 loss: 0.7626814126968384
  batch 450 loss: 0.7336694312095642
  batch 500 loss: 0.7261752080917359
  batch 550 loss: 0.7217791521549225
  batch 600 loss: 0.7720427232980728
  batch 650 loss: 0.7308627009391785
  batch 700 loss: 0.7511042642593384
  batch 750 loss: 0.7183597558736801
  batch 800 loss: 0.7506211268901825
  batch 850 loss: 0.7240214502811432
  batch 900 loss: 0.768423353433609
LOSS train 0.76842 valid 0.98012, valid PER 29.80%
EPOCH 18:
  batch 50 loss: 0.7073864424228669
  batch 100 loss: 0.6994672244787217
  batch 150 loss: 0.7395406788587571
  batch 200 loss: 0.7314404493570328
  batch 250 loss: 0.6853049105405807
  batch 300 loss: 0.7355398190021515
  batch 350 loss: 0.7155480408668518
  batch 400 loss: 0.7601285421848297
  batch 450 loss: 0.7519915288686753
  batch 500 loss: 0.7575392341613769
  batch 550 loss: 0.7768486428260803
  batch 600 loss: 0.7473841154575348
  batch 650 loss: 0.7249704372882843
  batch 700 loss: 0.7210492068529128
  batch 750 loss: 0.7467456144094468
  batch 800 loss: 0.7626743650436402
  batch 850 loss: 0.7372889316082001
  batch 900 loss: 0.7545819509029389
LOSS train 0.75458 valid 0.95994, valid PER 28.80%
EPOCH 19:
  batch 50 loss: 0.6846976238489151
  batch 100 loss: 0.6833022958040238
  batch 150 loss: 0.6788334900140762
  batch 200 loss: 0.6901982176303864
  batch 250 loss: 0.7231762886047364
  batch 300 loss: 0.7590267169475555
  batch 350 loss: 0.7322752410173416
  batch 400 loss: 0.7143285137414932
  batch 450 loss: 0.7026051545143127
  batch 500 loss: 0.7140938401222229
  batch 550 loss: 0.7423335856199265
  batch 600 loss: 0.7806543898582459
  batch 650 loss: 0.7498524248600006
  batch 700 loss: 0.7788539576530457
  batch 750 loss: 0.7115765476226806
  batch 800 loss: 0.7538359546661377
  batch 850 loss: 0.7689189183712005
  batch 900 loss: 0.7446793210506439
LOSS train 0.74468 valid 0.99828, valid PER 29.62%
EPOCH 20:
  batch 50 loss: 0.6896054768562316
  batch 100 loss: 0.7354524236917496
  batch 150 loss: 0.6986641269922257
  batch 200 loss: 0.6903832989931107
  batch 250 loss: 0.7301768386363983
  batch 300 loss: 0.7111015141010284
  batch 350 loss: 0.6707864832878113
  batch 400 loss: 0.7180209195613861
  batch 450 loss: 0.7033961200714112
  batch 500 loss: 0.7292034208774567
  batch 550 loss: 0.7223695123195648
  batch 600 loss: 0.7192550164461136
  batch 650 loss: 0.7678022575378418
  batch 700 loss: 0.723070125579834
  batch 750 loss: 0.7133100575208664
  batch 800 loss: 0.7295046877861023
  batch 850 loss: 0.73522332072258
  batch 900 loss: 0.7531288695335389
LOSS train 0.75313 valid 1.00258, valid PER 30.21%
Training finished in 4.0 minutes.
Model saved to checkpoints/20231129_164215/model_15
Loading model from checkpoints/20231129_164215/model_15
SUB: 17.03%, DEL: 12.25%, INS: 2.18%, COR: 70.72%, PER: 31.46%
