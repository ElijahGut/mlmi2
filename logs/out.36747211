Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.1, optimiser='sgd', grad_clip=None)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.167357983589173
  batch 100 loss: 3.1776888179779053
  batch 150 loss: 3.0340717887878417
  batch 200 loss: 2.9180564212799074
  batch 250 loss: 2.915362639427185
  batch 300 loss: 2.6969663381576536
  batch 350 loss: 2.506179494857788
  batch 400 loss: 2.4121050119400023
  batch 450 loss: 2.315843324661255
  batch 500 loss: 2.1930409836769105
  batch 550 loss: 2.1247258973121643
  batch 600 loss: 2.087298274040222
  batch 650 loss: 1.9915840578079225
  batch 700 loss: 1.9759005904197693
  batch 750 loss: 1.925390555858612
  batch 800 loss: 1.904190866947174
  batch 850 loss: 1.850671830177307
  batch 900 loss: 1.8304894495010375
LOSS train 1.83049 valid 1.79459, valid PER 69.30%
EPOCH 2:
  batch 50 loss: 1.7761271357536317
  batch 100 loss: 1.7073336386680602
  batch 150 loss: 1.6980208802223205
  batch 200 loss: 1.689070999622345
  batch 250 loss: 1.7165114188194275
  batch 300 loss: 1.6450394701957702
  batch 350 loss: 1.5710056972503663
  batch 400 loss: 1.5616776609420777
  batch 450 loss: 1.5134554123878479
  batch 500 loss: 1.55174551486969
  batch 550 loss: 1.5659534692764283
  batch 600 loss: 1.5056042003631591
  batch 650 loss: 1.5050644588470459
  batch 700 loss: 1.4894332599639892
  batch 750 loss: 1.470602502822876
  batch 800 loss: 1.4109449124336242
  batch 850 loss: 1.4016644144058228
  batch 900 loss: 1.430949649810791
LOSS train 1.43095 valid 1.36173, valid PER 44.17%
EPOCH 3:
  batch 50 loss: 1.3774056935310364
  batch 100 loss: 1.3716192746162414
  batch 150 loss: 1.3779036331176757
  batch 200 loss: 1.3480228519439696
  batch 250 loss: 1.323951425552368
  batch 300 loss: 1.3235023403167725
  batch 350 loss: 1.3782357048988343
  batch 400 loss: 1.3312684845924379
  batch 450 loss: 1.3045068240165711
  batch 500 loss: 1.311319580078125
  batch 550 loss: 1.326409866809845
  batch 600 loss: 1.2960538804531097
  batch 650 loss: 1.2488323855400085
  batch 700 loss: 1.2922098755836486
  batch 750 loss: 1.37054004073143
  batch 800 loss: 1.2920023608207702
  batch 850 loss: 1.3017592668533324
  batch 900 loss: 1.2677157139778137
LOSS train 1.26772 valid 1.26454, valid PER 39.28%
EPOCH 4:
  batch 50 loss: 1.2169754540920257
  batch 100 loss: 1.2599065160751344
  batch 150 loss: 1.1952118110656738
  batch 200 loss: 1.2440293622016907
  batch 250 loss: 1.22717937707901
  batch 300 loss: 1.251081051826477
  batch 350 loss: 1.2015950560569764
  batch 400 loss: 1.2284202456474305
  batch 450 loss: 1.207481200695038
  batch 500 loss: 1.2099707841873169
  batch 550 loss: 1.3071799743175507
  batch 600 loss: 1.2518063580989838
  batch 650 loss: 1.2489055609703064
  batch 700 loss: 1.1907207727432252
  batch 750 loss: 1.1604609835147857
  batch 800 loss: 1.1423715698719024
  batch 850 loss: 1.1716227900981904
  batch 900 loss: 1.2072779083251952
LOSS train 1.20728 valid 1.16549, valid PER 36.87%
EPOCH 5:
  batch 50 loss: 1.1215962731838227
  batch 100 loss: 1.1352204120159148
  batch 150 loss: 1.209265617132187
  batch 200 loss: 1.1228951919078827
  batch 250 loss: 1.1616173827648162
  batch 300 loss: 1.1457325160503387
  batch 350 loss: 1.1885599982738495
  batch 400 loss: 1.1476538240909577
  batch 450 loss: 1.1238295674324035
  batch 500 loss: 1.182492710351944
  batch 550 loss: 1.1418184065818786
  batch 600 loss: 1.2036589944362641
  batch 650 loss: 1.1389165115356445
  batch 700 loss: 1.1530707740783692
  batch 750 loss: 1.1243323194980621
  batch 800 loss: 1.143643822669983
  batch 850 loss: 1.1458695697784425
  batch 900 loss: 1.1569715297222138
LOSS train 1.15697 valid 1.13815, valid PER 34.93%
EPOCH 6:
  batch 50 loss: 1.1238539099693299
  batch 100 loss: 1.088772531747818
  batch 150 loss: 1.082926105260849
  batch 200 loss: 1.0971899819374085
  batch 250 loss: 1.122150069475174
  batch 300 loss: 1.0803718769550323
  batch 350 loss: 1.0843218660354614
  batch 400 loss: 1.0986891841888429
  batch 450 loss: 1.1350974023342133
  batch 500 loss: 1.1011529076099396
  batch 550 loss: 1.159446655511856
  batch 600 loss: 1.1136979639530182
  batch 650 loss: 1.1074718236923218
  batch 700 loss: 1.1096254515647888
  batch 750 loss: 1.0809968507289887
  batch 800 loss: 1.068867369890213
  batch 850 loss: 1.05270632147789
  batch 900 loss: 1.0902424299716948
LOSS train 1.09024 valid 1.13301, valid PER 36.14%
EPOCH 7:
  batch 50 loss: 1.0890766441822053
  batch 100 loss: 1.0834334254264832
  batch 150 loss: 1.0711788141727447
  batch 200 loss: 1.0402664065361023
  batch 250 loss: 1.0528679168224335
  batch 300 loss: 1.032901999950409
  batch 350 loss: 1.0466364920139313
  batch 400 loss: 1.069517434835434
  batch 450 loss: 1.0607263219356537
  batch 500 loss: 1.048141040802002
  batch 550 loss: 1.03887549161911
  batch 600 loss: 1.0392303311824798
  batch 650 loss: 1.0635875141620637
  batch 700 loss: 1.0831958532333374
  batch 750 loss: 1.039120795726776
  batch 800 loss: 1.055272752046585
  batch 850 loss: 1.0500143706798553
  batch 900 loss: 1.091088480949402
LOSS train 1.09109 valid 1.04884, valid PER 34.10%
EPOCH 8:
  batch 50 loss: 1.0126103949546814
  batch 100 loss: 1.0080053877830506
  batch 150 loss: 0.9990153324604034
  batch 200 loss: 0.9730350279808044
  batch 250 loss: 0.9890084564685822
  batch 300 loss: 0.9431149518489838
  batch 350 loss: 1.028228042125702
  batch 400 loss: 0.9803370761871338
  batch 450 loss: 1.0109703290462493
  batch 500 loss: 1.0500365054607392
  batch 550 loss: 0.9938598620891571
  batch 600 loss: 1.0200872528553009
  batch 650 loss: 1.0395027220249176
  batch 700 loss: 0.976306198835373
  batch 750 loss: 0.9961449110507965
  batch 800 loss: 1.0268316793441772
  batch 850 loss: 1.0287107396125794
  batch 900 loss: 1.0169817304611206
LOSS train 1.01698 valid 1.02118, valid PER 32.54%
EPOCH 9:
  batch 50 loss: 0.9368945956230164
  batch 100 loss: 0.9681510162353516
  batch 150 loss: 1.0025546264648437
  batch 200 loss: 0.9581111693382263
  batch 250 loss: 0.9858338689804077
  batch 300 loss: 0.9828349542617798
  batch 350 loss: 0.9875544488430024
  batch 400 loss: 0.9875182008743286
  batch 450 loss: 0.9618995141983032
  batch 500 loss: 0.9727765691280365
  batch 550 loss: 1.0126011717319487
  batch 600 loss: 1.0356634616851808
  batch 650 loss: 0.9893895018100739
  batch 700 loss: 0.9592575860023499
  batch 750 loss: 0.9537535524368286
  batch 800 loss: 0.9885950136184692
  batch 850 loss: 0.9789226365089416
  batch 900 loss: 0.9475575876235962
LOSS train 0.94756 valid 1.02725, valid PER 32.54%
EPOCH 10:
  batch 50 loss: 0.910410453081131
  batch 100 loss: 0.9420135045051574
  batch 150 loss: 0.9569757449626922
  batch 200 loss: 0.9693819153308868
  batch 250 loss: 0.954645859003067
  batch 300 loss: 0.9107598757743836
  batch 350 loss: 0.9570669150352478
  batch 400 loss: 0.8889350044727325
  batch 450 loss: 0.9184589326381684
  batch 500 loss: 0.9520579612255097
  batch 550 loss: 0.9614538216590881
  batch 600 loss: 0.9396536862850189
  batch 650 loss: 0.9355575251579284
  batch 700 loss: 0.9485829067230225
  batch 750 loss: 0.9464475727081298
  batch 800 loss: 0.9591378450393677
  batch 850 loss: 0.9525479865074158
  batch 900 loss: 0.9710418021678925
LOSS train 0.97104 valid 1.04104, valid PER 33.61%
EPOCH 11:
  batch 50 loss: 0.8977874827384948
  batch 100 loss: 0.8694949007034302
  batch 150 loss: 0.8906700646877289
  batch 200 loss: 0.9499137973785401
  batch 250 loss: 0.9234330570697784
  batch 300 loss: 0.8889746236801147
  batch 350 loss: 0.9083025848865509
  batch 400 loss: 0.9264537763595581
  batch 450 loss: 0.9276687371730804
  batch 500 loss: 0.917211480140686
  batch 550 loss: 0.9118590342998505
  batch 600 loss: 0.8807635033130645
  batch 650 loss: 0.9747481620311738
  batch 700 loss: 0.9020275568962097
  batch 750 loss: 0.9221663630008697
  batch 800 loss: 0.941964544057846
  batch 850 loss: 0.9548266756534577
  batch 900 loss: 0.9603473567962646
LOSS train 0.96035 valid 0.98228, valid PER 30.94%
EPOCH 12:
  batch 50 loss: 0.9074803495407104
  batch 100 loss: 0.8811430144309997
  batch 150 loss: 0.8674347102642059
  batch 200 loss: 0.8884082198143005
  batch 250 loss: 0.8989649856090546
  batch 300 loss: 0.8836486077308655
  batch 350 loss: 0.9040816342830658
  batch 400 loss: 0.9003556144237518
  batch 450 loss: 0.9042430913448334
  batch 500 loss: 0.9302385795116425
  batch 550 loss: 0.8582571029663086
  batch 600 loss: 0.8682133746147156
  batch 650 loss: 0.9184490466117858
  batch 700 loss: 0.9195758271217346
  batch 750 loss: 0.8737585353851318
  batch 800 loss: 0.8793170142173767
  batch 850 loss: 0.9093472254276276
  batch 900 loss: 0.929116986989975
LOSS train 0.92912 valid 0.96062, valid PER 30.95%
EPOCH 13:
  batch 50 loss: 0.8602094948291779
  batch 100 loss: 0.8865198934078217
  batch 150 loss: 0.8626536095142364
  batch 200 loss: 0.8832209801673889
  batch 250 loss: 0.8622835755348206
  batch 300 loss: 0.8727406018972397
  batch 350 loss: 0.8820575404167176
  batch 400 loss: 0.9065398061275483
  batch 450 loss: 0.8939100456237793
  batch 500 loss: 0.8498143720626831
  batch 550 loss: 0.8774720418453217
  batch 600 loss: 0.8588248276710511
  batch 650 loss: 0.9095567429065704
  batch 700 loss: 0.8954676645994186
  batch 750 loss: 0.8594599986076354
  batch 800 loss: 0.8952325570583344
  batch 850 loss: 0.9097487795352935
  batch 900 loss: 0.9095893061161041
LOSS train 0.90959 valid 0.97601, valid PER 30.38%
EPOCH 14:
  batch 50 loss: 0.8597475242614746
  batch 100 loss: 0.8462731003761291
  batch 150 loss: 0.8386599922180176
  batch 200 loss: 0.8639898478984833
  batch 250 loss: 0.8877975499629974
  batch 300 loss: 0.9057246494293213
  batch 350 loss: 0.8564413940906525
  batch 400 loss: 0.8702090406417846
  batch 450 loss: 0.8624266648292541
  batch 500 loss: 0.8870280039310455
  batch 550 loss: 0.8742281079292298
  batch 600 loss: 0.8483937388658523
  batch 650 loss: 0.8924022150039673
  batch 700 loss: 0.9034942555427551
  batch 750 loss: 0.8696074664592743
  batch 800 loss: 0.8515864145755768
  batch 850 loss: 0.9221351265907287
  batch 900 loss: 0.8780803120136261
LOSS train 0.87808 valid 0.97126, valid PER 30.64%
EPOCH 15:
  batch 50 loss: 0.8452723836898803
  batch 100 loss: 0.836896887421608
  batch 150 loss: 0.8291489017009736
  batch 200 loss: 0.9011682283878326
  batch 250 loss: 0.872833251953125
  batch 300 loss: 0.8541697978973388
  batch 350 loss: 0.8563479471206665
  batch 400 loss: 0.8478057372570038
  batch 450 loss: 0.8492611384391785
  batch 500 loss: 0.8089703929424286
  batch 550 loss: 0.8706345570087433
  batch 600 loss: 0.875487458705902
  batch 650 loss: 0.8804916024208069
  batch 700 loss: 0.8733890450000763
  batch 750 loss: 0.9100444304943085
  batch 800 loss: 0.8871563744544982
  batch 850 loss: 0.873585592508316
  batch 900 loss: 0.8753553122282028
LOSS train 0.87536 valid 0.99883, valid PER 32.02%
EPOCH 16:
  batch 50 loss: 0.8647374320030212
  batch 100 loss: 0.8170687735080719
  batch 150 loss: 0.8242580938339233
  batch 200 loss: 0.8279273045063019
  batch 250 loss: 0.8632113826274872
  batch 300 loss: 0.8539493680000305
  batch 350 loss: 0.8672934234142303
  batch 400 loss: 0.8557346069812775
  batch 450 loss: 0.8820825207233429
  batch 500 loss: 0.8268434309959412
  batch 550 loss: 0.8451350605487824
  batch 600 loss: 0.8249901175498963
  batch 650 loss: 0.8644810461997986
  batch 700 loss: 0.8261953544616699
  batch 750 loss: 0.8595617592334748
  batch 800 loss: 0.8596770083904266
  batch 850 loss: 0.8399605596065521
  batch 900 loss: 0.8394628250598908
LOSS train 0.83946 valid 0.95852, valid PER 29.90%
EPOCH 17:
  batch 50 loss: 0.8395375609397888
  batch 100 loss: 0.808168181180954
  batch 150 loss: 0.7986791086196899
  batch 200 loss: 0.8062503468990326
  batch 250 loss: 0.8356753599643707
  batch 300 loss: 0.8296099627017974
  batch 350 loss: 0.8093686509132385
  batch 400 loss: 0.8653982555866242
  batch 450 loss: 0.845004369020462
  batch 500 loss: 0.8126127350330353
  batch 550 loss: 0.8730970084667206
  batch 600 loss: 0.8823633301258087
  batch 650 loss: 0.8304392492771149
  batch 700 loss: 0.8285126256942749
  batch 750 loss: 0.8121001136302948
  batch 800 loss: 0.846196346282959
  batch 850 loss: 0.8559432983398437
  batch 900 loss: 0.8098823255300522
LOSS train 0.80988 valid 0.98528, valid PER 30.39%
EPOCH 18:
  batch 50 loss: 0.8102254343032836
  batch 100 loss: 0.8430654752254486
  batch 150 loss: 0.8362807643413543
  batch 200 loss: 0.83313252389431
  batch 250 loss: 0.8451765787601471
  batch 300 loss: 0.858486350774765
  batch 350 loss: 0.94283527135849
  batch 400 loss: 0.9391980910301209
  batch 450 loss: 0.9716863489151001
  batch 500 loss: 0.9242232954502105
  batch 550 loss: 0.89008633852005
  batch 600 loss: 0.8625586402416229
  batch 650 loss: 0.8454575443267822
  batch 700 loss: 0.8909491550922394
  batch 750 loss: 0.8576203870773316
  batch 800 loss: 0.8439443528652191
  batch 850 loss: 0.8467377185821533
  batch 900 loss: 0.8867446136474609
LOSS train 0.88674 valid 0.97524, valid PER 31.10%
EPOCH 19:
  batch 50 loss: 0.771154066324234
  batch 100 loss: 0.7832847714424134
  batch 150 loss: 0.7969388377666473
  batch 200 loss: 0.8205319702625274
  batch 250 loss: 0.8656421685218811
  batch 300 loss: 0.8499062514305115
  batch 350 loss: 0.8140205144882202
  batch 400 loss: 0.8139938366413116
  batch 450 loss: 0.8354920625686646
  batch 500 loss: 0.8344301664829255
  batch 550 loss: 0.8042043805122375
  batch 600 loss: 0.8269767332077026
  batch 650 loss: 0.8895491445064545
  batch 700 loss: 0.8077126562595367
  batch 750 loss: 0.8114742636680603
  batch 800 loss: 0.864293577671051
  batch 850 loss: 0.8263934022188186
  batch 900 loss: 0.8201289331912994
LOSS train 0.82013 valid 0.98822, valid PER 30.41%
EPOCH 20:
  batch 50 loss: 0.7828784251213073
  batch 100 loss: 0.7844664084911347
  batch 150 loss: 0.7867463481426239
  batch 200 loss: 0.815018093585968
  batch 250 loss: 0.8209915435314179
  batch 300 loss: 0.8128255307674408
  batch 350 loss: 0.7806308209896088
  batch 400 loss: 0.7913354444503784
  batch 450 loss: 0.8099117016792298
  batch 500 loss: 0.7822842240333557
  batch 550 loss: 0.8442992413043976
  batch 600 loss: 0.8053481554985047
  batch 650 loss: 0.8387268912792206
  batch 700 loss: 0.8212851512432099
  batch 750 loss: 0.826203988790512
  batch 800 loss: 0.8368412804603577
  batch 850 loss: 0.8440039098262787
  batch 900 loss: 0.8429076719284058
LOSS train 0.84291 valid 0.96568, valid PER 29.91%
Training finished in 3.0 minutes.
Model saved to checkpoints/20231206_202455/model_16
Loading model from checkpoints/20231206_202455/model_16
SUB: 16.95%, DEL: 12.98%, INS: 2.00%, COR: 70.07%, PER: 31.93%
