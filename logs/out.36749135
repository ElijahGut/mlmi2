Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.1, optimiser='sgd', grad_clip=None)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.167360138893128
  batch 100 loss: 3.1776906299591063
  batch 150 loss: 3.0340744829177857
  batch 200 loss: 2.9180561208724978
  batch 250 loss: 2.8833710145950318
  batch 300 loss: 2.6192121934890746
  batch 350 loss: 2.488053879737854
  batch 400 loss: 2.4064865207672117
  batch 450 loss: 2.317496557235718
  batch 500 loss: 2.1893091297149656
  batch 550 loss: 2.1230774760246276
  batch 600 loss: 2.0903614711761476
  batch 650 loss: 1.9924418663978576
  batch 700 loss: 1.9759803557395934
  batch 750 loss: 1.926801664829254
  batch 800 loss: 1.898578670024872
  batch 850 loss: 1.8446079063415528
  batch 900 loss: 1.845050392150879
LOSS train 1.84505 valid 1.79302, valid PER 69.00%
EPOCH 2:
  batch 50 loss: 1.7761607146263123
  batch 100 loss: 1.7128121185302734
  batch 150 loss: 1.6946972703933716
  batch 200 loss: 1.6937700390815735
  batch 250 loss: 1.7223001790046693
  batch 300 loss: 1.6446289443969726
  batch 350 loss: 1.5701625680923461
  batch 400 loss: 1.5592856431007385
  batch 450 loss: 1.5172407054901123
  batch 500 loss: 1.5567676067352294
  batch 550 loss: 1.554408645629883
  batch 600 loss: 1.4841068267822266
  batch 650 loss: 1.5300900912284852
  batch 700 loss: 1.4886596274375916
  batch 750 loss: 1.4708695089817048
  batch 800 loss: 1.4007943153381348
  batch 850 loss: 1.414079041481018
  batch 900 loss: 1.4225384259223939
LOSS train 1.42254 valid 1.35454, valid PER 43.13%
EPOCH 3:
  batch 50 loss: 1.3890284824371337
  batch 100 loss: 1.3680197048187255
  batch 150 loss: 1.3605674695968628
  batch 200 loss: 1.3327534866333008
  batch 250 loss: 1.3298785018920898
  batch 300 loss: 1.3191844749450683
  batch 350 loss: 1.3644940781593322
  batch 400 loss: 1.3217431712150574
  batch 450 loss: 1.3177931606769562
  batch 500 loss: 1.3443058633804321
  batch 550 loss: 1.3311600995063781
  batch 600 loss: 1.3071316874027252
  batch 650 loss: 1.2764014303684235
  batch 700 loss: 1.2784673392772674
  batch 750 loss: 1.3167333102226257
  batch 800 loss: 1.2767619371414185
  batch 850 loss: 1.302850387096405
  batch 900 loss: 1.2396596789360046
LOSS train 1.23966 valid 1.23689, valid PER 38.29%
EPOCH 4:
  batch 50 loss: 1.204095003604889
  batch 100 loss: 1.244968786239624
  batch 150 loss: 1.177963056564331
  batch 200 loss: 1.2383849346637725
  batch 250 loss: 1.2469603967666627
  batch 300 loss: 1.2576518321037293
  batch 350 loss: 1.1645080590248107
  batch 400 loss: 1.2169944512844086
  batch 450 loss: 1.1972503447532654
  batch 500 loss: 1.1988981914520265
  batch 550 loss: 1.217100979089737
  batch 600 loss: 1.2349767470359803
  batch 650 loss: 1.2333078014850616
  batch 700 loss: 1.18866223692894
  batch 750 loss: 1.186244205236435
  batch 800 loss: 1.1251225352287293
  batch 850 loss: 1.1672685515880585
  batch 900 loss: 1.2102898454666138
LOSS train 1.21029 valid 1.16047, valid PER 36.55%
EPOCH 5:
  batch 50 loss: 1.1010821855068207
  batch 100 loss: 1.1209952592849732
  batch 150 loss: 1.1969578576087951
  batch 200 loss: 1.1161820530891418
  batch 250 loss: 1.1601601898670197
  batch 300 loss: 1.1402352786064147
  batch 350 loss: 1.1398366522789
  batch 400 loss: 1.1311010539531707
  batch 450 loss: 1.1286040651798248
  batch 500 loss: 1.1451104271411896
  batch 550 loss: 1.073511040210724
  batch 600 loss: 1.1713555765151977
  batch 650 loss: 1.1781059169769288
  batch 700 loss: 1.1738025510311128
  batch 750 loss: 1.0724232470989228
  batch 800 loss: 1.13441122174263
  batch 850 loss: 1.1376574051380157
  batch 900 loss: 1.1352451479434966
LOSS train 1.13525 valid 1.10677, valid PER 34.40%
EPOCH 6:
  batch 50 loss: 1.1057411468029021
  batch 100 loss: 1.0663906729221344
  batch 150 loss: 1.0429008424282074
  batch 200 loss: 1.0720128631591797
  batch 250 loss: 1.091109014749527
  batch 300 loss: 1.0555164802074433
  batch 350 loss: 1.0719141888618469
  batch 400 loss: 1.0544621062278747
  batch 450 loss: 1.118901927471161
  batch 500 loss: 1.071540813446045
  batch 550 loss: 1.0929207634925842
  batch 600 loss: 1.084005138874054
  batch 650 loss: 1.094471787214279
  batch 700 loss: 1.0903419363498688
  batch 750 loss: 1.041439425945282
  batch 800 loss: 1.0610035097599029
  batch 850 loss: 1.0529388117790222
  batch 900 loss: 1.0524603533744812
LOSS train 1.05246 valid 1.09652, valid PER 34.44%
EPOCH 7:
  batch 50 loss: 1.053826892375946
  batch 100 loss: 1.0504387354850768
  batch 150 loss: 1.0430338490009308
  batch 200 loss: 1.0451602435112
  batch 250 loss: 1.026211950778961
  batch 300 loss: 1.029580943584442
  batch 350 loss: 1.007058756351471
  batch 400 loss: 1.0470506739616394
  batch 450 loss: 1.0444790852069854
  batch 500 loss: 1.0399213922023773
  batch 550 loss: 1.037414035797119
  batch 600 loss: 1.0316438257694245
  batch 650 loss: 1.0358163726329803
  batch 700 loss: 1.046461479663849
  batch 750 loss: 1.003678092956543
  batch 800 loss: 1.0109077548980714
  batch 850 loss: 1.0593323004245758
  batch 900 loss: 1.080366028547287
LOSS train 1.08037 valid 1.06513, valid PER 33.71%
EPOCH 8:
  batch 50 loss: 1.0022739636898041
  batch 100 loss: 1.0095193994045257
  batch 150 loss: 1.0070703947544097
  batch 200 loss: 0.9658854138851166
  batch 250 loss: 0.9804315948486328
  batch 300 loss: 0.9351820278167725
  batch 350 loss: 1.0153702080249787
  batch 400 loss: 0.9948061072826385
  batch 450 loss: 1.0014838409423827
  batch 500 loss: 1.0350247240066528
  batch 550 loss: 0.9693429231643677
  batch 600 loss: 0.9966393840312958
  batch 650 loss: 1.0312740814685821
  batch 700 loss: 0.9714632368087769
  batch 750 loss: 0.9705858206748963
  batch 800 loss: 0.9829659545421601
  batch 850 loss: 1.0063414371013641
  batch 900 loss: 0.9838396453857422
LOSS train 0.98384 valid 1.03309, valid PER 32.95%
EPOCH 9:
  batch 50 loss: 0.9341943395137787
  batch 100 loss: 0.9657038724422455
  batch 150 loss: 0.9909390234947204
  batch 200 loss: 0.935604236125946
  batch 250 loss: 0.9804206418991089
  batch 300 loss: 0.9570519876480102
  batch 350 loss: 0.9824702560901641
  batch 400 loss: 0.9563153004646301
  batch 450 loss: 0.967897710800171
  batch 500 loss: 0.9183693850040435
  batch 550 loss: 0.9785821187496185
  batch 600 loss: 1.0007719326019286
  batch 650 loss: 0.9898725306987762
  batch 700 loss: 0.9818301618099212
  batch 750 loss: 0.9632114291191101
  batch 800 loss: 0.9866909408569335
  batch 850 loss: 0.9903181576728821
  batch 900 loss: 0.95686558842659
LOSS train 0.95687 valid 1.19293, valid PER 35.96%
EPOCH 10:
  batch 50 loss: 1.0102823793888092
  batch 100 loss: 0.974947019815445
  batch 150 loss: 0.9646324563026428
  batch 200 loss: 0.9917824506759644
  batch 250 loss: 0.9663003325462342
  batch 300 loss: 0.9168469488620759
  batch 350 loss: 0.9721209990978241
  batch 400 loss: 0.9109125781059265
  batch 450 loss: 0.9274169278144836
  batch 500 loss: 0.9506600058078766
  batch 550 loss: 0.9783392620086669
  batch 600 loss: 0.9643955373764038
  batch 650 loss: 0.9363662981987
  batch 700 loss: 0.9558774280548096
  batch 750 loss: 0.9530179166793823
  batch 800 loss: 0.9789237558841706
  batch 850 loss: 0.971900818347931
  batch 900 loss: 0.956009932756424
LOSS train 0.95601 valid 1.03284, valid PER 33.25%
EPOCH 11:
  batch 50 loss: 0.892509161233902
  batch 100 loss: 0.8832336223125458
  batch 150 loss: 0.8963683986663818
  batch 200 loss: 0.9387882494926453
  batch 250 loss: 0.949873821735382
  batch 300 loss: 0.9169989395141601
  batch 350 loss: 0.9405632317066193
  batch 400 loss: 0.9590070605278015
  batch 450 loss: 0.9593522584438324
  batch 500 loss: 0.9293511927127838
  batch 550 loss: 0.9273436760902405
  batch 600 loss: 0.9034993410110473
  batch 650 loss: 0.996272087097168
  batch 700 loss: 0.9020667970180511
  batch 750 loss: 0.9063099825382233
  batch 800 loss: 0.9493657529354096
  batch 850 loss: 0.9888633513450622
  batch 900 loss: 0.9627959883213043
LOSS train 0.96280 valid 0.99759, valid PER 31.49%
EPOCH 12:
  batch 50 loss: 0.8988983070850373
  batch 100 loss: 0.8984054827690124
  batch 150 loss: 0.8940966057777405
  batch 200 loss: 0.8942651355266571
  batch 250 loss: 0.9320021617412567
  batch 300 loss: 0.903311094045639
  batch 350 loss: 0.9119349503517151
  batch 400 loss: 0.9199029338359833
  batch 450 loss: 0.9157351350784302
  batch 500 loss: 0.9194560432434082
  batch 550 loss: 0.8484793734550476
  batch 600 loss: 0.8816175842285157
  batch 650 loss: 0.9210958588123321
  batch 700 loss: 0.9237686538696289
  batch 750 loss: 0.8878721010684967
  batch 800 loss: 0.8944364327192307
  batch 850 loss: 0.9129767322540283
  batch 900 loss: 0.9461531579494477
LOSS train 0.94615 valid 0.97277, valid PER 31.44%
EPOCH 13:
  batch 50 loss: 0.8539947462081909
  batch 100 loss: 0.8908928263187409
  batch 150 loss: 0.9376380956172943
  batch 200 loss: 0.906084064245224
  batch 250 loss: 0.8750615572929382
  batch 300 loss: 0.8700612235069275
  batch 350 loss: 0.8900942516326904
  batch 400 loss: 0.893356271982193
  batch 450 loss: 0.9008192801475525
  batch 500 loss: 0.8442554795742034
  batch 550 loss: 0.8788024139404297
  batch 600 loss: 0.8809707140922547
  batch 650 loss: 0.8779181838035583
  batch 700 loss: 0.8978402507305145
  batch 750 loss: 0.8590532410144806
  batch 800 loss: 0.8854164946079254
  batch 850 loss: 0.9002166891098022
  batch 900 loss: 0.9146656095981598
LOSS train 0.91467 valid 1.01203, valid PER 31.69%
EPOCH 14:
  batch 50 loss: 0.8757938385009766
  batch 100 loss: 0.8469584143161774
  batch 150 loss: 0.8400476145744323
  batch 200 loss: 0.8552008330821991
  batch 250 loss: 0.8645535492897034
  batch 300 loss: 0.8953726160526275
  batch 350 loss: 0.8427968513965607
  batch 400 loss: 0.890542219877243
  batch 450 loss: 0.8627258706092834
  batch 500 loss: 0.8896536457538605
  batch 550 loss: 0.9004138660430908
  batch 600 loss: 0.8729452884197235
  batch 650 loss: 0.896635251045227
  batch 700 loss: 0.9096580457687378
  batch 750 loss: 0.8676514780521393
  batch 800 loss: 0.8542483901977539
  batch 850 loss: 0.9115866291522979
  batch 900 loss: 0.8727048361301422
LOSS train 0.87270 valid 0.97401, valid PER 31.52%
EPOCH 15:
  batch 50 loss: 0.8440690112113952
  batch 100 loss: 0.8381395924091339
  batch 150 loss: 0.8431181526184082
  batch 200 loss: 0.9004336297512054
  batch 250 loss: 0.8537788462638854
  batch 300 loss: 0.8786717617511749
  batch 350 loss: 0.8584366142749786
  batch 400 loss: 0.9523868107795715
  batch 450 loss: 0.9483205354213715
  batch 500 loss: 0.8570113408565522
  batch 550 loss: 0.8913579356670379
  batch 600 loss: 0.8930734038352967
  batch 650 loss: 0.9075423538684845
  batch 700 loss: 0.9038884830474854
  batch 750 loss: 0.873222713470459
  batch 800 loss: 0.8644245684146881
  batch 850 loss: 0.8422908186912537
  batch 900 loss: 0.8685513508319854
LOSS train 0.86855 valid 0.97463, valid PER 31.00%
EPOCH 16:
  batch 50 loss: 0.8563715326786041
  batch 100 loss: 0.8090394592285156
  batch 150 loss: 0.8216371619701386
  batch 200 loss: 0.8324302232265473
  batch 250 loss: 0.877752023935318
  batch 300 loss: 0.8372942471504211
  batch 350 loss: 0.8819728076457978
  batch 400 loss: 0.8600833439826965
  batch 450 loss: 0.8907557094097137
  batch 500 loss: 0.8225602030754089
  batch 550 loss: 0.8619116175174714
  batch 600 loss: 0.8479405963420867
  batch 650 loss: 0.8931876790523529
  batch 700 loss: 0.8364754855632782
  batch 750 loss: 0.8462811827659606
  batch 800 loss: 0.8327494549751282
  batch 850 loss: 0.850233280658722
  batch 900 loss: 0.8483200573921204
LOSS train 0.84832 valid 0.97877, valid PER 30.82%
EPOCH 17:
  batch 50 loss: 0.8459237098693848
  batch 100 loss: 0.8405712151527405
  batch 150 loss: 0.8346981173753738
  batch 200 loss: 0.8310164165496826
  batch 250 loss: 0.8335002171993255
  batch 300 loss: 0.8332407903671265
  batch 350 loss: 0.7888534510135651
  batch 400 loss: 0.8929944002628326
  batch 450 loss: 0.8719306814670563
  batch 500 loss: 0.8370012974739075
  batch 550 loss: 0.8724463319778443
  batch 600 loss: 0.8970399594306946
  batch 650 loss: 0.8450826346874237
  batch 700 loss: 0.8371529722213745
  batch 750 loss: 0.8175120627880097
  batch 800 loss: 0.830808082818985
  batch 850 loss: 0.8308587408065796
  batch 900 loss: 0.8106629115343094
LOSS train 0.81066 valid 0.94685, valid PER 29.54%
EPOCH 18:
  batch 50 loss: 0.8011528122425079
  batch 100 loss: 0.8167562246322632
  batch 150 loss: 0.8205696737766266
  batch 200 loss: 0.7936891674995422
  batch 250 loss: 0.8107723987102509
  batch 300 loss: 0.7918949949741364
  batch 350 loss: 0.8523272514343262
  batch 400 loss: 0.8150624632835388
  batch 450 loss: 0.8659726071357727
  batch 500 loss: 0.8414515447616577
  batch 550 loss: 0.8596894943714142
  batch 600 loss: 0.8158528608083725
  batch 650 loss: 0.8102068853378296
  batch 700 loss: 0.8330047082901001
  batch 750 loss: 0.8189614951610565
  batch 800 loss: 0.8338310408592224
  batch 850 loss: 0.841099693775177
  batch 900 loss: 0.8593273723125457
LOSS train 0.85933 valid 0.95842, valid PER 30.48%
EPOCH 19:
  batch 50 loss: 0.7572760498523712
  batch 100 loss: 0.7637488830089569
  batch 150 loss: 0.7881919825077057
  batch 200 loss: 0.7924482989311218
  batch 250 loss: 0.8124287152290344
  batch 300 loss: 0.8202857935428619
  batch 350 loss: 0.7975690579414367
  batch 400 loss: 0.8112458527088166
  batch 450 loss: 0.8156950438022613
  batch 500 loss: 0.8434678769111633
  batch 550 loss: 0.8043999743461608
  batch 600 loss: 0.801865770816803
  batch 650 loss: 0.9308043980598449
  batch 700 loss: 0.9560714983940124
  batch 750 loss: 0.8838390123844146
  batch 800 loss: 0.8976981949806213
  batch 850 loss: 0.8837477624416351
  batch 900 loss: 0.862106261253357
LOSS train 0.86211 valid 0.97189, valid PER 30.78%
EPOCH 20:
  batch 50 loss: 0.8166380405426026
  batch 100 loss: 0.8009289860725403
  batch 150 loss: 0.8018753230571747
  batch 200 loss: 0.8073532652854919
  batch 250 loss: 0.8191142177581787
  batch 300 loss: 0.8372635114192962
  batch 350 loss: 0.7776348602771759
  batch 400 loss: 0.8166555428504944
  batch 450 loss: 0.8221578705310821
  batch 500 loss: 0.785932548046112
  batch 550 loss: 0.8910619819164276
  batch 600 loss: 0.7867372179031372
  batch 650 loss: 0.8416313874721527
  batch 700 loss: 0.8345200729370117
  batch 750 loss: 0.828004680275917
  batch 800 loss: 0.8474176716804505
  batch 850 loss: 0.8227185678482055
  batch 900 loss: 0.8296963262557984
LOSS train 0.82970 valid 0.97857, valid PER 30.10%
Training finished in 9.0 minutes.
Model saved to checkpoints/20231206_204914/model_17
Loading model from checkpoints/20231206_204914/model_17
SUB: 17.48%, DEL: 11.94%, INS: 2.28%, COR: 70.58%, PER: 31.70%
