Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=512, concat=1, lr=0.1, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 2240552
EPOCH 1, Learning Rate: 0.1
  batch 50 loss: 7.575983381271362
  batch 100 loss: 3.2915182542800903
  batch 150 loss: 3.2516705226898193
  batch 200 loss: 3.213766756057739
  batch 250 loss: 3.1493919134140014
  batch 300 loss: 3.0402799367904665
  batch 350 loss: 2.950597949028015
  batch 400 loss: 2.8870822620391845
  batch 450 loss: 2.8181779479980467
  batch 500 loss: 2.7171339082717894
  batch 550 loss: 2.6586394929885864
  batch 600 loss: 2.611887979507446
  batch 650 loss: 2.550929136276245
  batch 700 loss: 2.5442946767807006
  batch 750 loss: 2.4916499710083007
  batch 800 loss: 2.4672495031356814
  batch 850 loss: 2.439134469032288
  batch 900 loss: 2.397923889160156
avg val loss: 2.3678362369537354
LOSS train 2.39792 valid 2.36784, valid PER 79.80%
EPOCH 2, Learning Rate: 0.1
  batch 50 loss: 2.3740450668334963
  batch 100 loss: 2.3222959184646608
  batch 150 loss: 2.2650717592239378
  batch 200 loss: 2.25542236328125
  batch 250 loss: 2.2612534809112548
  batch 300 loss: 2.2217047142982485
  batch 350 loss: 2.148682403564453
  batch 400 loss: 2.165633237361908
  batch 450 loss: 2.1252154779434203
  batch 500 loss: 2.1152978801727294
  batch 550 loss: 2.1227673935890197
  batch 600 loss: 2.073195893764496
  batch 650 loss: 2.090377824306488
  batch 700 loss: 2.0556822276115416
  batch 750 loss: 2.0549666142463683
  batch 800 loss: 1.9831290578842162
  batch 850 loss: 1.9925934171676636
  batch 900 loss: 2.00208411693573
avg val loss: 1.9501714706420898
LOSS train 2.00208 valid 1.95017, valid PER 72.84%
EPOCH 3, Learning Rate: 0.1
  batch 50 loss: 1.9758883666992189
  batch 100 loss: 1.9301377320289612
  batch 150 loss: 1.9340162324905394
  batch 200 loss: 1.9289735293388366
  batch 250 loss: 1.8940665745735168
  batch 300 loss: 1.8852958846092225
  batch 350 loss: 1.9073601388931274
  batch 400 loss: 1.8830497694015502
  batch 450 loss: 1.8200615310668946
  batch 500 loss: 1.8408473873138427
  batch 550 loss: 1.825344250202179
  batch 600 loss: 1.792223834991455
  batch 650 loss: 1.7755531215667724
  batch 700 loss: 1.7937048411369323
  batch 750 loss: 1.815360107421875
  batch 800 loss: 1.7557684397697448
  batch 850 loss: 1.7717319703102112
  batch 900 loss: 1.7218232893943786
avg val loss: 1.7402024269104004
LOSS train 1.72182 valid 1.74020, valid PER 64.18%
EPOCH 4, Learning Rate: 0.1
  batch 50 loss: 1.7373030090332031
  batch 100 loss: 1.7369709277153016
  batch 150 loss: 1.6871476936340333
  batch 200 loss: 1.725800302028656
  batch 250 loss: 1.7136575150489808
  batch 300 loss: 1.7165072894096374
  batch 350 loss: 1.6392383909225463
  batch 400 loss: 1.6732678318023682
  batch 450 loss: 1.667922923564911
  batch 500 loss: 1.6366884684562684
  batch 550 loss: 1.649451675415039
  batch 600 loss: 1.6619587182998656
  batch 650 loss: 1.6515564799308777
  batch 700 loss: 1.6198016905784607
  batch 750 loss: 1.5932622265815735
  batch 800 loss: 1.5584370112419128
  batch 850 loss: 1.599610059261322
  batch 900 loss: 1.6193367218971253
avg val loss: 1.6023588180541992
LOSS train 1.61934 valid 1.60236, valid PER 59.22%
EPOCH 5, Learning Rate: 0.1
  batch 50 loss: 1.5797296690940856
  batch 100 loss: 1.5627377891540528
  batch 150 loss: 1.5830419898033141
  batch 200 loss: 1.5221137356758119
  batch 250 loss: 1.5361031126976012
  batch 300 loss: 1.544236228466034
  batch 350 loss: 1.5433476305007934
  batch 400 loss: 1.5251508593559264
  batch 450 loss: 1.5249256849288941
  batch 500 loss: 1.5341300344467164
  batch 550 loss: 1.4670974922180176
  batch 600 loss: 1.538422577381134
  batch 650 loss: 1.4801155924797058
  batch 700 loss: 1.5260198664665223
  batch 750 loss: 1.45636647939682
  batch 800 loss: 1.4931431913375854
  batch 850 loss: 1.4866939687728882
  batch 900 loss: 1.5035890030860901
avg val loss: 1.4465978145599365
LOSS train 1.50359 valid 1.44660, valid PER 51.95%
EPOCH 6, Learning Rate: 0.1
  batch 50 loss: 1.488082559108734
  batch 100 loss: 1.4427747869491576
  batch 150 loss: 1.4411841487884522
  batch 200 loss: 1.4499964642524719
  batch 250 loss: 1.465590133666992
  batch 300 loss: 1.4234152746200561
  batch 350 loss: 1.430065336227417
  batch 400 loss: 1.4184304785728454
  batch 450 loss: 1.4350975418090821
  batch 500 loss: 1.40955007314682
  batch 550 loss: 1.4363509345054626
  batch 600 loss: 1.4059638714790343
  batch 650 loss: 1.4083282661437988
  batch 700 loss: 1.4064936447143555
  batch 750 loss: 1.3785232567787171
  batch 800 loss: 1.3755066251754762
  batch 850 loss: 1.3600963282585143
  batch 900 loss: 1.3880020236968995
avg val loss: 1.4015867710113525
LOSS train 1.38800 valid 1.40159, valid PER 53.10%
EPOCH 7, Learning Rate: 0.1
  batch 50 loss: 1.3821264243125915
  batch 100 loss: 1.3812021803855896
  batch 150 loss: 1.3581666207313539
  batch 200 loss: 1.3413546836376191
  batch 250 loss: 1.3492345941066741
  batch 300 loss: 1.323809106349945
  batch 350 loss: 1.3342730903625488
  batch 400 loss: 1.3450724458694459
  batch 450 loss: 1.341595585346222
  batch 500 loss: 1.3249061489105225
  batch 550 loss: 1.3235644578933716
  batch 600 loss: 1.3366497898101806
  batch 650 loss: 1.321035532951355
  batch 700 loss: 1.3262485837936402
  batch 750 loss: 1.299796850681305
  batch 800 loss: 1.2988999342918397
  batch 850 loss: 1.3190474486351014
  batch 900 loss: 1.3448293423652649
avg val loss: 1.2930244207382202
LOSS train 1.34483 valid 1.29302, valid PER 45.86%
EPOCH 8, Learning Rate: 0.1
  batch 50 loss: 1.2915385138988495
  batch 100 loss: 1.2924010491371154
  batch 150 loss: 1.2817284727096558
  batch 200 loss: 1.2482662749290467
  batch 250 loss: 1.2856568324565887
  batch 300 loss: 1.2170002961158752
  batch 350 loss: 1.2870586180686951
  batch 400 loss: 1.2618389987945557
  batch 450 loss: 1.2612310326099396
  batch 500 loss: 1.2910092520713805
  batch 550 loss: 1.2264675331115722
  batch 600 loss: 1.2692304944992066
  batch 650 loss: 1.2867372417449952
  batch 700 loss: 1.2542634677886964
  batch 750 loss: 1.2404978239536286
  batch 800 loss: 1.257412452697754
  batch 850 loss: 1.2500963878631592
  batch 900 loss: 1.237422925233841
avg val loss: 1.2270342111587524
LOSS train 1.23742 valid 1.22703, valid PER 39.79%
EPOCH 9, Learning Rate: 0.1
  batch 50 loss: 1.179194005727768
  batch 100 loss: 1.2384324836730958
  batch 150 loss: 1.2186234831809997
  batch 200 loss: 1.1974480080604553
  batch 250 loss: 1.20863618850708
  batch 300 loss: 1.2334834337234497
  batch 350 loss: 1.2365623831748962
  batch 400 loss: 1.2238411569595338
  batch 450 loss: 1.2017964804172516
  batch 500 loss: 1.1972341871261596
  batch 550 loss: 1.2126547598838806
  batch 600 loss: 1.21507927775383
  batch 650 loss: 1.1885620641708374
  batch 700 loss: 1.1863309669494628
  batch 750 loss: 1.1841719734668732
  batch 800 loss: 1.195110538005829
  batch 850 loss: 1.208600082397461
  batch 900 loss: 1.1653755140304565
avg val loss: 1.1705275774002075
LOSS train 1.16538 valid 1.17053, valid PER 38.98%
EPOCH 10, Learning Rate: 0.1
  batch 50 loss: 1.1235866177082061
  batch 100 loss: 1.1448865711688996
  batch 150 loss: 1.1875391447544097
  batch 200 loss: 1.181670330762863
  batch 250 loss: 1.1738459694385528
  batch 300 loss: 1.1394550800323486
  batch 350 loss: 1.1713218653202058
  batch 400 loss: 1.1233602750301361
  batch 450 loss: 1.114065774679184
  batch 500 loss: 1.170378395318985
  batch 550 loss: 1.1850046908855438
  batch 600 loss: 1.13495565533638
  batch 650 loss: 1.1245268738269807
  batch 700 loss: 1.1312120127677918
  batch 750 loss: 1.1289313507080079
  batch 800 loss: 1.143229318857193
  batch 850 loss: 1.1456874108314514
  batch 900 loss: 1.1415167331695557
avg val loss: 1.1456336975097656
LOSS train 1.14152 valid 1.14563, valid PER 38.55%
EPOCH 11, Learning Rate: 0.1
  batch 50 loss: 1.0976844429969788
  batch 100 loss: 1.087894959449768
  batch 150 loss: 1.0869182932376862
  batch 200 loss: 1.1404408431053161
  batch 250 loss: 1.12388876080513
  batch 300 loss: 1.0889714086055755
  batch 350 loss: 1.1065042400360108
  batch 400 loss: 1.118212068080902
  batch 450 loss: 1.100774519443512
  batch 500 loss: 1.0790843963623047
  batch 550 loss: 1.0956084108352662
  batch 600 loss: 1.0745843303203584
  batch 650 loss: 1.1289229249954225
  batch 700 loss: 1.0712398719787597
  batch 750 loss: 1.071640009880066
  batch 800 loss: 1.120420595407486
  batch 850 loss: 1.1201282572746276
  batch 900 loss: 1.1105923128128052
avg val loss: 1.0920219421386719
LOSS train 1.11059 valid 1.09202, valid PER 35.46%
EPOCH 12, Learning Rate: 0.1
  batch 50 loss: 1.07390540599823
  batch 100 loss: 1.0630601847171783
  batch 150 loss: 1.0422030329704284
  batch 200 loss: 1.0623742485046386
  batch 250 loss: 1.0911600923538207
  batch 300 loss: 1.0536914682388305
  batch 350 loss: 1.0577313220500946
  batch 400 loss: 1.088193302154541
  batch 450 loss: 1.0774608314037324
  batch 500 loss: 1.0892401099205018
  batch 550 loss: 1.015477409362793
  batch 600 loss: 1.034189555644989
  batch 650 loss: 1.0846113753318787
  batch 700 loss: 1.0591503632068635
  batch 750 loss: 1.0367219460010528
  batch 800 loss: 1.0429553890228271
  batch 850 loss: 1.0740156412124633
  batch 900 loss: 1.0873223924636841
avg val loss: 1.0607811212539673
LOSS train 1.08732 valid 1.06078, valid PER 35.39%
EPOCH 13, Learning Rate: 0.1
  batch 50 loss: 1.0205352818965912
  batch 100 loss: 1.043671555519104
  batch 150 loss: 1.0134191930294036
  batch 200 loss: 1.0362543916702271
  batch 250 loss: 1.0214173793792725
  batch 300 loss: 1.0199685907363891
  batch 350 loss: 1.0275567948818207
  batch 400 loss: 1.0385216736793519
  batch 450 loss: 1.0384360671043396
  batch 500 loss: 0.9947707784175873
  batch 550 loss: 1.0379832911491393
  batch 600 loss: 1.0066067361831665
  batch 650 loss: 1.0241376161575317
  batch 700 loss: 1.044774487018585
  batch 750 loss: 0.9829030001163482
  batch 800 loss: 1.0222431552410125
  batch 850 loss: 1.0500009500980376
  batch 900 loss: 1.0263930916786195
avg val loss: 1.0629667043685913
LOSS train 1.02639 valid 1.06297, valid PER 34.36%
EPOCH 14, Learning Rate: 0.05
  batch 50 loss: 0.9838755261898041
  batch 100 loss: 0.9822599029541016
  batch 150 loss: 0.9608691000938415
  batch 200 loss: 0.9578178560733795
  batch 250 loss: 0.9581357955932617
  batch 300 loss: 0.9818246650695801
  batch 350 loss: 0.9427588021755219
  batch 400 loss: 0.9430537152290345
  batch 450 loss: 0.9494885802268982
  batch 500 loss: 0.9734260380268097
  batch 550 loss: 0.9768716847896576
  batch 600 loss: 0.9314434742927551
  batch 650 loss: 0.94646036028862
  batch 700 loss: 0.985339869260788
  batch 750 loss: 0.9455994498729706
  batch 800 loss: 0.904012942314148
  batch 850 loss: 0.9659400951862335
  batch 900 loss: 0.9703918886184693
avg val loss: 1.0109058618545532
LOSS train 0.97039 valid 1.01091, valid PER 32.67%
EPOCH 15, Learning Rate: 0.05
  batch 50 loss: 0.9598640942573547
  batch 100 loss: 0.9348144364356995
  batch 150 loss: 0.9436466789245606
  batch 200 loss: 0.96627965092659
  batch 250 loss: 0.9616889786720276
  batch 300 loss: 0.9267072558403016
  batch 350 loss: 0.9453983771800994
  batch 400 loss: 0.9193772518634796
  batch 450 loss: 0.929329799413681
  batch 500 loss: 0.9020970261096954
  batch 550 loss: 0.9347698664665223
  batch 600 loss: 0.9559553611278534
  batch 650 loss: 0.9641736841201782
  batch 700 loss: 0.9529359519481659
  batch 750 loss: 0.941539204120636
  batch 800 loss: 0.9286607050895691
  batch 850 loss: 0.9141411256790161
  batch 900 loss: 0.9366779673099518
avg val loss: 1.0048991441726685
LOSS train 0.93668 valid 1.00490, valid PER 31.85%
EPOCH 16, Learning Rate: 0.05
  batch 50 loss: 0.9400385081768036
  batch 100 loss: 0.8841294455528259
  batch 150 loss: 0.9078123331069946
  batch 200 loss: 0.9202611255645752
  batch 250 loss: 0.9422525990009308
  batch 300 loss: 0.9173052096366883
  batch 350 loss: 0.9418633592128753
  batch 400 loss: 0.9304545271396637
  batch 450 loss: 0.9425536668300629
  batch 500 loss: 0.8908329451084137
  batch 550 loss: 0.9315579307079315
  batch 600 loss: 0.9092676889896393
  batch 650 loss: 0.930148093700409
  batch 700 loss: 0.9089225363731385
  batch 750 loss: 0.9325812101364136
  batch 800 loss: 0.9355901205539703
  batch 850 loss: 0.9079880011081696
  batch 900 loss: 0.898538544178009
avg val loss: 0.987916111946106
LOSS train 0.89854 valid 0.98792, valid PER 31.49%
EPOCH 17, Learning Rate: 0.05
  batch 50 loss: 0.9246910774707794
  batch 100 loss: 0.92486079454422
  batch 150 loss: 0.9096714091300965
  batch 200 loss: 0.8821534311771393
  batch 250 loss: 0.9102236509323121
  batch 300 loss: 0.9133828938007355
  batch 350 loss: 0.8721258234977722
  batch 400 loss: 0.9402389264106751
  batch 450 loss: 0.9299349188804626
  batch 500 loss: 0.881454701423645
  batch 550 loss: 0.903885828256607
  batch 600 loss: 0.9518227565288544
  batch 650 loss: 0.89514377951622
  batch 700 loss: 0.8819791746139526
  batch 750 loss: 0.8835049760341644
  batch 800 loss: 0.881239743232727
  batch 850 loss: 0.9087102448940277
  batch 900 loss: 0.884613173007965
avg val loss: 0.978388249874115
LOSS train 0.88461 valid 0.97839, valid PER 30.45%
EPOCH 18, Learning Rate: 0.05
  batch 50 loss: 0.8886932790279388
  batch 100 loss: 0.8923622441291809
  batch 150 loss: 0.8963838362693787
  batch 200 loss: 0.8882603752613067
  batch 250 loss: 0.886184139251709
  batch 300 loss: 0.8644758880138397
  batch 350 loss: 0.8879114031791687
  batch 400 loss: 0.86566366314888
  batch 450 loss: 0.9247126066684723
  batch 500 loss: 0.8946115171909332
  batch 550 loss: 0.906278042793274
  batch 600 loss: 0.8792956495285034
  batch 650 loss: 0.8651770448684692
  batch 700 loss: 0.9034165513515472
  batch 750 loss: 0.8822854638099671
  batch 800 loss: 0.8743675565719604
  batch 850 loss: 0.8727297282218933
  batch 900 loss: 0.9013809394836426
avg val loss: 0.9754366874694824
LOSS train 0.90138 valid 0.97544, valid PER 30.54%
EPOCH 19, Learning Rate: 0.05
  batch 50 loss: 0.8469163644313812
  batch 100 loss: 0.8473971903324127
  batch 150 loss: 0.8622022306919098
  batch 200 loss: 0.8770420098304749
  batch 250 loss: 0.8740021204948425
  batch 300 loss: 0.8780622994899749
  batch 350 loss: 0.8655227744579315
  batch 400 loss: 0.8740238559246063
  batch 450 loss: 0.8973857843875885
  batch 500 loss: 0.8707836508750916
  batch 550 loss: 0.850982323884964
  batch 600 loss: 0.8713185966014863
  batch 650 loss: 0.9141711735725403
  batch 700 loss: 0.8695635163784027
  batch 750 loss: 0.8332460260391236
  batch 800 loss: 0.8846088027954102
  batch 850 loss: 0.8932692658901215
  batch 900 loss: 0.8795668935775757
avg val loss: 0.9800136089324951
LOSS train 0.87957 valid 0.98001, valid PER 30.94%
EPOCH 20, Learning Rate: 0.025
  batch 50 loss: 0.8425151097774506
  batch 100 loss: 0.8334338092803955
  batch 150 loss: 0.8220620727539063
  batch 200 loss: 0.8542467951774597
  batch 250 loss: 0.8266195738315583
  batch 300 loss: 0.8506222283840179
  batch 350 loss: 0.8039519739151001
  batch 400 loss: 0.8301734793186187
  batch 450 loss: 0.8111451828479767
  batch 500 loss: 0.8015029394626617
  batch 550 loss: 0.8802519750595093
  batch 600 loss: 0.8030907666683197
  batch 650 loss: 0.8358506190776825
  batch 700 loss: 0.8348273408412933
  batch 750 loss: 0.8233423173427582
  batch 800 loss: 0.8618319523334503
  batch 850 loss: 0.8380675482749939
  batch 900 loss: 0.8553867721557618
avg val loss: 0.9575471878051758
LOSS train 0.85539 valid 0.95755, valid PER 29.70%
Training finished in 8.0 minutes.
Model saved to checkpoints/20231210_135512/model_20
Loading model from checkpoints/20231210_135512/model_20
SUB: 15.22%, DEL: 14.95%, INS: 1.84%, COR: 69.83%, PER: 32.00%
