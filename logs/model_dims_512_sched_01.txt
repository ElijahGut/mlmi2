Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=512, concat=1, lr=0.1, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 2240552
EPOCH 1, Learning Rate: 0.1
  batch 50 loss: 7.575984582901001
  batch 100 loss: 3.2915183401107786
  batch 150 loss: 3.251670479774475
  batch 200 loss: 3.2137671232223513
  batch 250 loss: 3.1493922328948973
  batch 300 loss: 3.040280652046204
  batch 350 loss: 2.9505985498428347
  batch 400 loss: 2.8870826673507692
  batch 450 loss: 2.8181782150268555
  batch 500 loss: 2.717134222984314
  batch 550 loss: 2.6586393213272093
  batch 600 loss: 2.6118880844116212
  batch 650 loss: 2.550928792953491
  batch 700 loss: 2.544294681549072
  batch 750 loss: 2.4916495275497437
  batch 800 loss: 2.4672490215301512
  batch 850 loss: 2.439134330749512
  batch 900 loss: 2.3979235458374024
running loss: 56.68201208114624
LOSS train 2.39792 valid 2.36784, valid PER 79.80%
EPOCH 2, Learning Rate: 0.1
  batch 50 loss: 2.3740452480316163
  batch 100 loss: 2.322295517921448
  batch 150 loss: 2.265071523189545
  batch 200 loss: 2.2554217267036436
  batch 250 loss: 2.261253390312195
  batch 300 loss: 2.2217044734954836
  batch 350 loss: 2.1486825704574586
  batch 400 loss: 2.1656314635276797
  batch 450 loss: 2.1252148246765135
  batch 500 loss: 2.115297954082489
  batch 550 loss: 2.122766740322113
  batch 600 loss: 2.0731941270828247
  batch 650 loss: 2.09037807226181
  batch 700 loss: 2.0556808948516845
  batch 750 loss: 2.054966013431549
  batch 800 loss: 1.9831284976005554
  batch 850 loss: 1.9925934123992919
  batch 900 loss: 2.002084205150604
running loss: 47.551958322525024
LOSS train 2.00208 valid 1.95017, valid PER 72.84%
EPOCH 3, Learning Rate: 0.1
  batch 50 loss: 1.9758888006210327
  batch 100 loss: 1.9301382327079772
  batch 150 loss: 1.9340144038200378
  batch 200 loss: 1.9289736127853394
  batch 250 loss: 1.8940666699409485
  batch 300 loss: 1.8852948141098023
  batch 350 loss: 1.9073599219322204
  batch 400 loss: 1.8830489754676818
  batch 450 loss: 1.8200634217262268
  batch 500 loss: 1.8408472800254823
  batch 550 loss: 1.8253446292877198
  batch 600 loss: 1.792224555015564
  batch 650 loss: 1.775553891658783
  batch 700 loss: 1.7937047815322875
  batch 750 loss: 1.8153595733642578
  batch 800 loss: 1.755768985748291
  batch 850 loss: 1.7717324829101562
  batch 900 loss: 1.7218237686157227
running loss: 41.43498611450195
LOSS train 1.72182 valid 1.74020, valid PER 64.18%
EPOCH 4, Learning Rate: 0.1
  batch 50 loss: 1.7373054218292237
  batch 100 loss: 1.7369711184501648
  batch 150 loss: 1.6871479415893555
  batch 200 loss: 1.7257987213134767
  batch 250 loss: 1.7136574220657348
  batch 300 loss: 1.7165078973770143
  batch 350 loss: 1.63923837184906
  batch 400 loss: 1.6732691097259522
  batch 450 loss: 1.6679235792160034
  batch 500 loss: 1.6366890811920165
  batch 550 loss: 1.6494512367248535
  batch 600 loss: 1.661959912776947
  batch 650 loss: 1.65155912399292
  batch 700 loss: 1.6198011302947999
  batch 750 loss: 1.5932619738578797
  batch 800 loss: 1.5584363746643066
  batch 850 loss: 1.5996098852157592
  batch 900 loss: 1.6193387293815613
running loss: 38.117668986320496
LOSS train 1.61934 valid 1.60236, valid PER 59.22%
EPOCH 5, Learning Rate: 0.1
  batch 50 loss: 1.5797304177284242
  batch 100 loss: 1.562736828327179
  batch 150 loss: 1.583041558265686
  batch 200 loss: 1.5221125602722168
  batch 250 loss: 1.5361034512519836
  batch 300 loss: 1.5442357087135314
  batch 350 loss: 1.543347897529602
  batch 400 loss: 1.5251496458053588
  batch 450 loss: 1.5249249625205994
  batch 500 loss: 1.5341300654411316
  batch 550 loss: 1.4670969939231873
  batch 600 loss: 1.5384218788146973
  batch 650 loss: 1.4801148176193237
  batch 700 loss: 1.5260207176208496
  batch 750 loss: 1.4563657879829406
  batch 800 loss: 1.4931373548507691
  batch 850 loss: 1.486691312789917
  batch 900 loss: 1.5035936617851258
running loss: 35.01198470592499
LOSS train 1.50359 valid 1.44661, valid PER 51.95%
EPOCH 6, Learning Rate: 0.1
  batch 50 loss: 1.4880845832824707
  batch 100 loss: 1.442775797843933
  batch 150 loss: 1.441183762550354
  batch 200 loss: 1.4499967837333678
  batch 250 loss: 1.4655871319770812
  batch 300 loss: 1.423412129878998
  batch 350 loss: 1.430064606666565
  batch 400 loss: 1.4184281277656554
  batch 450 loss: 1.4351009368896483
  batch 500 loss: 1.4095481395721436
  batch 550 loss: 1.4363510990142823
  batch 600 loss: 1.4059635043144225
  batch 650 loss: 1.4083291888237
  batch 700 loss: 1.40649263381958
  batch 750 loss: 1.3785222244262696
  batch 800 loss: 1.3755066466331483
  batch 850 loss: 1.3600941061973573
  batch 900 loss: 1.3880015397071839
running loss: 33.088730692863464
LOSS train 1.38800 valid 1.40158, valid PER 53.10%
EPOCH 7, Learning Rate: 0.1
  batch 50 loss: 1.3821288633346558
  batch 100 loss: 1.3812003183364867
  batch 150 loss: 1.3581654500961304
  batch 200 loss: 1.341354374885559
  batch 250 loss: 1.349233363866806
  batch 300 loss: 1.3238053441047668
  batch 350 loss: 1.3342731523513793
  batch 400 loss: 1.3450737142562865
  batch 450 loss: 1.3415934574604034
  batch 500 loss: 1.324906852245331
  batch 550 loss: 1.3235602474212647
  batch 600 loss: 1.3366530656814575
  batch 650 loss: 1.3210362029075622
  batch 700 loss: 1.3262510883808136
  batch 750 loss: 1.2997951889038086
  batch 800 loss: 1.2988962960243224
  batch 850 loss: 1.3190478420257568
  batch 900 loss: 1.3448332452774048
running loss: 30.627146363258362
LOSS train 1.34483 valid 1.29302, valid PER 45.87%
EPOCH 8, Learning Rate: 0.1
  batch 50 loss: 1.2915354788303375
  batch 100 loss: 1.292400246858597
  batch 150 loss: 1.2817222952842713
  batch 200 loss: 1.2482660806179047
  batch 250 loss: 1.2856562316417695
  batch 300 loss: 1.217002319097519
  batch 350 loss: 1.2870585072040557
  batch 400 loss: 1.2618380665779114
  batch 450 loss: 1.2612369561195373
  batch 500 loss: 1.2910089898109436
  batch 550 loss: 1.2264697515964509
  batch 600 loss: 1.2692288970947265
  batch 650 loss: 1.2867334294319153
  batch 700 loss: 1.254262000322342
  batch 750 loss: 1.2404995572566986
  batch 800 loss: 1.2574140322208405
  batch 850 loss: 1.2500957095623015
  batch 900 loss: 1.2374261832237243
running loss: 29.759552478790283
LOSS train 1.23743 valid 1.22702, valid PER 39.79%
EPOCH 9, Learning Rate: 0.1
  batch 50 loss: 1.1791856622695922
  batch 100 loss: 1.2384353852272034
  batch 150 loss: 1.21861913561821
  batch 200 loss: 1.197435132265091
  batch 250 loss: 1.208646742105484
  batch 300 loss: 1.2334777116775513
  batch 350 loss: 1.23655078291893
  batch 400 loss: 1.2238575828075409
  batch 450 loss: 1.201830952167511
  batch 500 loss: 1.1972055196762086
  batch 550 loss: 1.2126414322853087
  batch 600 loss: 1.2150860679149629
  batch 650 loss: 1.1885867738723754
  batch 700 loss: 1.1863322460651398
  batch 750 loss: 1.1841875672340394
  batch 800 loss: 1.1951312303543091
  batch 850 loss: 1.2085776340961456
  batch 900 loss: 1.1653876292705536
running loss: 29.169268786907196
LOSS train 1.16539 valid 1.17055, valid PER 39.01%
EPOCH 10, Learning Rate: 0.1
  batch 50 loss: 1.1235734033584595
  batch 100 loss: 1.1447918474674226
  batch 150 loss: 1.1872016978263855
  batch 200 loss: 1.181544198989868
  batch 250 loss: 1.1746466052532196
  batch 300 loss: 1.139913227558136
  batch 350 loss: 1.1698854172229767
  batch 400 loss: 1.12286780834198
  batch 450 loss: 1.114158834218979
  batch 500 loss: 1.171087954044342
  batch 550 loss: 1.1845811915397644
  batch 600 loss: 1.134570163488388
  batch 650 loss: 1.1250298988819123
  batch 700 loss: 1.1306105589866637
  batch 750 loss: 1.1290967643260956
  batch 800 loss: 1.1444543719291687
  batch 850 loss: 1.1451647508144378
  batch 900 loss: 1.139988750219345
running loss: 27.302211046218872
LOSS train 1.13999 valid 1.14537, valid PER 38.60%
EPOCH 11, Learning Rate: 0.1
  batch 50 loss: 1.098702335357666
  batch 100 loss: 1.0847282636165618
  batch 150 loss: 1.0838013088703156
  batch 200 loss: 1.1393745732307434
  batch 250 loss: 1.1258380341529846
  batch 300 loss: 1.0862572598457336
  batch 350 loss: 1.1058532655239106
  batch 400 loss: 1.1154743671417235
  batch 450 loss: 1.0977076637744902
  batch 500 loss: 1.078397009372711
  batch 550 loss: 1.0925078809261322
  batch 600 loss: 1.0774916589260102
  batch 650 loss: 1.1302110290527343
  batch 700 loss: 1.0701183378696442
  batch 750 loss: 1.0762616288661957
  batch 800 loss: 1.1187632763385773
  batch 850 loss: 1.1205225574970246
  batch 900 loss: 1.1134664022922516
running loss: 25.781700491905212
LOSS train 1.11347 valid 1.09165, valid PER 35.74%
EPOCH 12, Learning Rate: 0.1
  batch 50 loss: 1.0777979218959808
  batch 100 loss: 1.0636185145378112
  batch 150 loss: 1.0434406757354737
  batch 200 loss: 1.0616699039936066
  batch 250 loss: 1.09125443816185
  batch 300 loss: 1.0581047129631043
  batch 350 loss: 1.0552746808528901
  batch 400 loss: 1.0878652811050415
  batch 450 loss: 1.0794602811336518
  batch 500 loss: 1.086003416776657
  batch 550 loss: 1.011872066259384
  batch 600 loss: 1.0359982764720916
  batch 650 loss: 1.0840454518795013
  batch 700 loss: 1.057009699344635
  batch 750 loss: 1.035917032957077
  batch 800 loss: 1.0363818955421449
  batch 850 loss: 1.074518723487854
  batch 900 loss: 1.0860634851455688
running loss: 24.585974395275116
LOSS train 1.08606 valid 1.05845, valid PER 34.93%
EPOCH 13, Learning Rate: 0.1
  batch 50 loss: 1.0148708927631378
  batch 100 loss: 1.0406623041629792
  batch 150 loss: 1.011685757637024
  batch 200 loss: 1.042403210401535
  batch 250 loss: 1.0227711510658264
  batch 300 loss: 1.015663936138153
  batch 350 loss: 1.0231934404373169
  batch 400 loss: 1.0379109489917755
  batch 450 loss: 1.0369499719142914
  batch 500 loss: 0.9948652827739716
  batch 550 loss: 1.0410602807998657
  batch 600 loss: 1.0053365743160247
  batch 650 loss: 1.026719788312912
  batch 700 loss: 1.0413708794116974
  batch 750 loss: 0.9832530307769776
  batch 800 loss: 1.0145020914077758
  batch 850 loss: 1.0492521274089812
  batch 900 loss: 1.0252508199214936
running loss: 25.40142512321472
LOSS train 1.02525 valid 1.05339, valid PER 34.03%
EPOCH 14, Learning Rate: 0.05
  batch 50 loss: 0.9814569330215455
  batch 100 loss: 0.9835678732395172
  batch 150 loss: 0.9544809186458587
  batch 200 loss: 0.9552799069881439
  batch 250 loss: 0.9594141912460327
  batch 300 loss: 0.9790322518348694
  batch 350 loss: 0.9426413476467133
  batch 400 loss: 0.9385037291049957
  batch 450 loss: 0.9491018533706665
  batch 500 loss: 0.9714791190624237
  batch 550 loss: 0.9763606476783753
  batch 600 loss: 0.9265397226810456
  batch 650 loss: 0.9466062772274018
  batch 700 loss: 0.9842879021167755
  batch 750 loss: 0.942967392206192
  batch 800 loss: 0.9061395680904388
  batch 850 loss: 0.9678902328014374
  batch 900 loss: 0.9711553573608398
running loss: 23.864619731903076
LOSS train 0.97116 valid 1.01361, valid PER 32.57%
EPOCH 15, Learning Rate: 0.05
  batch 50 loss: 0.957895964384079
  batch 100 loss: 0.9339083182811737
  batch 150 loss: 0.938661640882492
  batch 200 loss: 0.9642547225952148
  batch 250 loss: 0.962107253074646
  batch 300 loss: 0.9246646273136139
  batch 350 loss: 0.940038298368454
  batch 400 loss: 0.9206880915164948
  batch 450 loss: 0.9266547596454621
  batch 500 loss: 0.9041982853412628
  batch 550 loss: 0.9368661832809448
  batch 600 loss: 0.9521486806869507
  batch 650 loss: 0.9666631972789764
  batch 700 loss: 0.9538100934028626
  batch 750 loss: 0.9380746424198151
  batch 800 loss: 0.9255915236473083
  batch 850 loss: 0.9107568073272705
  batch 900 loss: 0.9414845514297485
running loss: 21.814236342906952
LOSS train 0.94148 valid 1.00094, valid PER 31.71%
EPOCH 16, Learning Rate: 0.05
  batch 50 loss: 0.93436279296875
  batch 100 loss: 0.8834080231189728
  batch 150 loss: 0.9086777102947236
  batch 200 loss: 0.9242172825336457
  batch 250 loss: 0.9454175508022309
  batch 300 loss: 0.9143361496925354
  batch 350 loss: 0.940036712884903
  batch 400 loss: 0.9285540390014648
  batch 450 loss: 0.9421570575237275
  batch 500 loss: 0.8894180917739868
  batch 550 loss: 0.9272163820266723
  batch 600 loss: 0.9154806244373321
  batch 650 loss: 0.9258826553821564
  batch 700 loss: 0.9081170952320099
  batch 750 loss: 0.930422215461731
  batch 800 loss: 0.9293674254417419
  batch 850 loss: 0.9081195628643036
  batch 900 loss: 0.8964791285991669
running loss: 21.418029010295868
LOSS train 0.89648 valid 0.98953, valid PER 31.51%
EPOCH 17, Learning Rate: 0.05
  batch 50 loss: 0.9246139812469483
  batch 100 loss: 0.9271063315868378
  batch 150 loss: 0.9076397597789765
  batch 200 loss: 0.8826259207725525
  batch 250 loss: 0.9084466218948364
  batch 300 loss: 0.9083913230895996
  batch 350 loss: 0.8675696980953217
  batch 400 loss: 0.9454240715503692
  batch 450 loss: 0.9319386792182922
  batch 500 loss: 0.8854051184654236
  batch 550 loss: 0.9054882049560546
  batch 600 loss: 0.9481140804290772
  batch 650 loss: 0.8916446828842163
  batch 700 loss: 0.8801119387149811
  batch 750 loss: 0.8802447128295898
  batch 800 loss: 0.8852173721790314
  batch 850 loss: 0.9131193637847901
  batch 900 loss: 0.8841485416889191
running loss: 22.625769555568695
LOSS train 0.88415 valid 0.98117, valid PER 30.76%
EPOCH 18, Learning Rate: 0.025
  batch 50 loss: 0.8773022663593292
  batch 100 loss: 0.8778219079971313
  batch 150 loss: 0.880635644197464
  batch 200 loss: 0.8693015432357788
  batch 250 loss: 0.8624407863616943
  batch 300 loss: 0.8408853685855866
  batch 350 loss: 0.8642534589767457
  batch 400 loss: 0.8428877246379852
  batch 450 loss: 0.9039564645290374
  batch 500 loss: 0.8761832058429718
  batch 550 loss: 0.8813462209701538
  batch 600 loss: 0.8575676989555359
  batch 650 loss: 0.8443705940246582
  batch 700 loss: 0.8863590097427368
  batch 750 loss: 0.8589511942863465
  batch 800 loss: 0.850987720489502
  batch 850 loss: 0.8497752726078034
  batch 900 loss: 0.8759735333919525
running loss: 20.551338016986847
LOSS train 0.87597 valid 0.96621, valid PER 30.58%
EPOCH 19, Learning Rate: 0.025
  batch 50 loss: 0.8322770535945893
  batch 100 loss: 0.8287473666667938
  batch 150 loss: 0.8422951769828796
  batch 200 loss: 0.8595061695575714
  batch 250 loss: 0.8565257024765015
  batch 300 loss: 0.8638852107524871
  batch 350 loss: 0.84878977060318
  batch 400 loss: 0.8550046873092652
  batch 450 loss: 0.8766058433055878
  batch 500 loss: 0.8530443644523621
  batch 550 loss: 0.83463991522789
  batch 600 loss: 0.8567082691192627
  batch 650 loss: 0.8956801760196685
  batch 700 loss: 0.85275151014328
  batch 750 loss: 0.8172690093517303
  batch 800 loss: 0.8658590424060821
  batch 850 loss: 0.8813007640838623
  batch 900 loss: 0.86297687292099
running loss: 20.58227527141571
LOSS train 0.86298 valid 0.97001, valid PER 30.47%
EPOCH 20, Learning Rate: 0.0125
  batch 50 loss: 0.8365889775753022
  batch 100 loss: 0.8294552981853485
  batch 150 loss: 0.8233732724189758
  batch 200 loss: 0.8545404553413392
  batch 250 loss: 0.8269822275638581
  batch 300 loss: 0.8485376012325286
  batch 350 loss: 0.8014520072937011
  batch 400 loss: 0.832384638786316
  batch 450 loss: 0.8088167548179627
  batch 500 loss: 0.7974168050289154
  batch 550 loss: 0.880556412935257
  batch 600 loss: 0.7998129010200501
  batch 650 loss: 0.8349998474121094
  batch 700 loss: 0.834491868019104
  batch 750 loss: 0.8229878604412079
  batch 800 loss: 0.8607854926586151
  batch 850 loss: 0.8388432657718659
  batch 900 loss: 0.855208283662796
running loss: 20.127085030078888
LOSS train 0.85521 valid 0.96215, valid PER 30.12%
Training finished in 8.0 minutes.
Model saved to checkpoints/20231210_043329/model_20
Loading model from checkpoints/20231210_043329/model_20
SUB: 15.04%, DEL: 15.40%, INS: 1.85%, COR: 69.55%, PER: 32.30%
