Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.1, optimiser='sgd')
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.164660835266114
  batch 100 loss: 3.175236258506775
  batch 150 loss: 3.0282037687301635
  batch 200 loss: 2.90831289768219
  batch 250 loss: 2.816085410118103
  batch 300 loss: 2.8673627376556396
  batch 350 loss: 2.5636994457244873
  batch 400 loss: 2.4722129678726197
  batch 450 loss: 2.4376158237457277
  batch 500 loss: 2.315710144042969
  batch 550 loss: 2.248806552886963
  batch 600 loss: 2.1237207555770876
  batch 650 loss: 2.007653169631958
  batch 700 loss: 1.9954535484313964
  batch 750 loss: 1.928324043750763
  batch 800 loss: 1.904880805015564
  batch 850 loss: 1.8347418928146362
  batch 900 loss: 1.840866436958313
LOSS train 1.84087 valid 1.80760, valid PER 69.73%
EPOCH 2:
  batch 50 loss: 1.789363284111023
  batch 100 loss: 1.7462832713127137
  batch 150 loss: 1.633331730365753
  batch 200 loss: 1.6707955718040466
  batch 250 loss: 1.6428789377212525
  batch 300 loss: 1.6191106534004212
  batch 350 loss: 1.6168157529830933
  batch 400 loss: 1.562348165512085
  batch 450 loss: 1.57511647939682
  batch 500 loss: 1.5433259010314941
  batch 550 loss: 1.5000138807296752
  batch 600 loss: 1.5157030415534973
  batch 650 loss: 1.4507815194129945
  batch 700 loss: 1.4921248507499696
  batch 750 loss: 1.4340255379676818
  batch 800 loss: 1.4091277599334717
  batch 850 loss: 1.4194623494148255
  batch 900 loss: 1.3608041572570801
LOSS train 1.36080 valid 1.39025, valid PER 44.74%
EPOCH 3:
  batch 50 loss: 1.2969477462768555
  batch 100 loss: 1.370375406742096
  batch 150 loss: 1.3871582984924316
  batch 200 loss: 1.2853134024143218
  batch 250 loss: 1.3219782209396362
  batch 300 loss: 1.3042696619033813
  batch 350 loss: 1.335551815032959
  batch 400 loss: 1.3243112826347352
  batch 450 loss: 1.290236713886261
  batch 500 loss: 1.254234149456024
  batch 550 loss: 1.2735106754302978
  batch 600 loss: 1.2049599075317383
  batch 650 loss: 1.2357963085174561
  batch 700 loss: 1.2380545961856841
  batch 750 loss: 1.2527775037288666
  batch 800 loss: 1.264363625049591
  batch 850 loss: 1.2415901446342468
  batch 900 loss: 1.254404261112213
LOSS train 1.25440 valid 1.21671, valid PER 38.11%
EPOCH 4:
  batch 50 loss: 1.2169647991657258
  batch 100 loss: 1.1681635522842406
  batch 150 loss: 1.191143901348114
  batch 200 loss: 1.1587356996536256
  batch 250 loss: 1.1446796190738677
  batch 300 loss: 1.1651688981056214
  batch 350 loss: 1.1824801719188691
  batch 400 loss: 1.1290176701545716
  batch 450 loss: 1.133534655570984
  batch 500 loss: 1.173655172586441
  batch 550 loss: 1.1318252766132355
  batch 600 loss: 1.0932795548439025
  batch 650 loss: 1.2000386261940001
  batch 700 loss: 1.2270014631748198
  batch 750 loss: 1.163932696580887
  batch 800 loss: 1.1175934100151061
  batch 850 loss: 1.1229237687587739
  batch 900 loss: 1.128253868818283
LOSS train 1.12825 valid 1.14300, valid PER 35.78%
EPOCH 5:
  batch 50 loss: 1.0891858088970183
  batch 100 loss: 1.0854322814941406
  batch 150 loss: 1.0878269648551941
  batch 200 loss: 1.1102997851371765
  batch 250 loss: 1.0688876616954803
  batch 300 loss: 1.0852318918704986
  batch 350 loss: 1.03920467376709
  batch 400 loss: 1.0535355925559997
  batch 450 loss: 1.0423648464679718
  batch 500 loss: 1.0451119482517242
  batch 550 loss: 1.0846520137786866
  batch 600 loss: 1.101565692424774
  batch 650 loss: 1.0644864571094512
  batch 700 loss: 1.0725679099559784
  batch 750 loss: 1.054340866804123
  batch 800 loss: 1.0887420237064362
  batch 850 loss: 1.0983988118171693
  batch 900 loss: 1.0504676377773285
LOSS train 1.05047 valid 1.09351, valid PER 34.45%
EPOCH 6:
  batch 50 loss: 1.0602126121520996
  batch 100 loss: 1.078916481733322
  batch 150 loss: 1.0421892428398132
  batch 200 loss: 1.0140973675251006
  batch 250 loss: 1.051583616733551
  batch 300 loss: 1.0500802421569824
  batch 350 loss: 1.084045969247818
  batch 400 loss: 1.0149565744400024
  batch 450 loss: 1.0451872766017913
  batch 500 loss: 0.9932997643947601
  batch 550 loss: 1.0457216036319732
  batch 600 loss: 1.0128062665462494
  batch 650 loss: 0.9830267763137818
  batch 700 loss: 0.9696005129814148
  batch 750 loss: 1.0111540579795837
  batch 800 loss: 1.0222920346260072
  batch 850 loss: 1.0308006370067597
  batch 900 loss: 1.0350283014774322
LOSS train 1.03503 valid 1.06367, valid PER 34.24%
EPOCH 7:
  batch 50 loss: 0.9468290257453919
  batch 100 loss: 1.0059679090976714
  batch 150 loss: 0.9457773578166961
  batch 200 loss: 0.9882544016838074
  batch 250 loss: 1.0327939069271088
  batch 300 loss: 0.9823343026638031
  batch 350 loss: 0.9915779852867126
  batch 400 loss: 0.9609795379638671
  batch 450 loss: 0.966411292552948
  batch 500 loss: 0.987402447462082
  batch 550 loss: 0.9614843308925629
  batch 600 loss: 0.9823892033100128
  batch 650 loss: 0.9511482441425323
  batch 700 loss: 0.9898643672466279
  batch 750 loss: 0.9603907430171966
  batch 800 loss: 0.9819295167922973
  batch 850 loss: 0.9596058952808381
  batch 900 loss: 1.010310139656067
LOSS train 1.01031 valid 1.07642, valid PER 33.58%
EPOCH 8:
  batch 50 loss: 0.9602288699150086
  batch 100 loss: 0.916613792181015
  batch 150 loss: 0.951581425666809
  batch 200 loss: 0.93689812541008
  batch 250 loss: 0.9293509995937348
  batch 300 loss: 0.923180205821991
  batch 350 loss: 0.9481069862842559
  batch 400 loss: 0.9412643373012543
  batch 450 loss: 0.9957075619697571
  batch 500 loss: 0.9369940340518952
  batch 550 loss: 0.9568584656715393
  batch 600 loss: 0.8816205704212189
  batch 650 loss: 0.9390088129043579
  batch 700 loss: 0.9548639225959777
  batch 750 loss: 0.9554726982116699
  batch 800 loss: 0.9507740426063538
  batch 850 loss: 0.915115475654602
  batch 900 loss: 0.9127963721752167
LOSS train 0.91280 valid 1.04540, valid PER 32.02%
EPOCH 9:
  batch 50 loss: 0.8943478810787201
  batch 100 loss: 0.8827617633342743
  batch 150 loss: 0.9008502650260926
  batch 200 loss: 0.8618168854713439
  batch 250 loss: 0.8813139081001282
  batch 300 loss: 0.9231041145324707
  batch 350 loss: 0.8777939200401306
  batch 400 loss: 0.9523963117599488
  batch 450 loss: 0.9846204257011414
  batch 500 loss: 0.9137322568893432
  batch 550 loss: 0.9218619215488434
  batch 600 loss: 0.9494902956485748
  batch 650 loss: 0.9261595988273621
  batch 700 loss: 0.8980479896068573
  batch 750 loss: 0.8878621768951416
  batch 800 loss: 0.9229749524593354
  batch 850 loss: 0.9473386979103089
  batch 900 loss: 0.903503110408783
LOSS train 0.90350 valid 1.00161, valid PER 30.28%
EPOCH 10:
  batch 50 loss: 0.860650053024292
  batch 100 loss: 0.871153439283371
  batch 150 loss: 0.8981335949897766
  batch 200 loss: 0.8733066511154175
  batch 250 loss: 0.879352605342865
  batch 300 loss: 0.8693529558181763
  batch 350 loss: 0.8865777134895325
  batch 400 loss: 0.8605302226543426
  batch 450 loss: 0.8553661262989044
  batch 500 loss: 0.8851238095760345
  batch 550 loss: 0.8813794684410096
  batch 600 loss: 0.8910718727111816
  batch 650 loss: 0.89299680352211
  batch 700 loss: 0.8974146187305451
  batch 750 loss: 0.9134721791744232
  batch 800 loss: 0.8860751795768738
  batch 850 loss: 0.8603814947605133
  batch 900 loss: 0.8515945053100586
LOSS train 0.85159 valid 0.99012, valid PER 31.21%
EPOCH 11:
  batch 50 loss: 0.8510691154003144
  batch 100 loss: 0.8089419281482697
  batch 150 loss: 0.8335644239187241
  batch 200 loss: 0.8066904711723327
  batch 250 loss: 0.8175005078315735
  batch 300 loss: 0.8329101419448852
  batch 350 loss: 0.9002631866931915
  batch 400 loss: 0.8666448247432709
  batch 450 loss: 0.841474199295044
  batch 500 loss: 0.8452400553226471
  batch 550 loss: 0.8560949516296387
  batch 600 loss: 0.8337458515167236
  batch 650 loss: 0.8492169892787933
  batch 700 loss: 0.9936197924613953
  batch 750 loss: 0.9360929274559021
  batch 800 loss: 0.9476681816577911
  batch 850 loss: 0.9442135334014893
  batch 900 loss: 0.9551113486289978
LOSS train 0.95511 valid 1.01763, valid PER 31.31%
EPOCH 12:
  batch 50 loss: 0.8518954181671142
  batch 100 loss: 0.838039549589157
  batch 150 loss: 0.8732936823368073
  batch 200 loss: 0.8780769801139832
  batch 250 loss: 0.8538605010509491
  batch 300 loss: 0.8896105670928955
  batch 350 loss: 0.8525901997089386
  batch 400 loss: 0.8802909278869628
  batch 450 loss: 0.8605371963977814
  batch 500 loss: 0.8762247169017792
  batch 550 loss: 0.87808767080307
  batch 600 loss: 0.8642657971382142
  batch 650 loss: 0.8631204307079315
  batch 700 loss: 0.8628189587593078
  batch 750 loss: 0.8717903339862824
  batch 800 loss: 0.8382093489170075
  batch 850 loss: 0.854550814628601
  batch 900 loss: 0.8644240736961365
LOSS train 0.86442 valid 0.99867, valid PER 30.64%
EPOCH 13:
  batch 50 loss: 0.8090024280548096
  batch 100 loss: 0.8002636933326721
  batch 150 loss: 0.8277175045013427
  batch 200 loss: 0.7841130650043487
  batch 250 loss: 0.8166638731956481
  batch 300 loss: 0.8432757633924485
  batch 350 loss: 0.7907302451133728
  batch 400 loss: 0.796802603006363
  batch 450 loss: 0.8367695033550262
  batch 500 loss: 0.8298222863674164
  batch 550 loss: 0.8669817662239074
  batch 600 loss: 0.8613545453548431
  batch 650 loss: 0.8542867255210876
  batch 700 loss: 0.8552444338798523
  batch 750 loss: 0.8209028089046478
  batch 800 loss: 0.8374118602275848
  batch 850 loss: 0.8675257760286331
  batch 900 loss: 0.9085362076759338
LOSS train 0.90854 valid 1.00449, valid PER 30.46%
EPOCH 14:
  batch 50 loss: 0.8659592360258103
  batch 100 loss: 0.8140374445915222
  batch 150 loss: 0.8465822076797486
  batch 200 loss: 0.8378150343894959
  batch 250 loss: 0.8245322275161743
  batch 300 loss: 0.812650808095932
  batch 350 loss: 0.8298536610603332
  batch 400 loss: 0.8976464748382569
  batch 450 loss: 0.8379785192012786
  batch 500 loss: 0.8609745132923127
  batch 550 loss: 0.8519516134262085
  batch 600 loss: 0.8176089680194855
  batch 650 loss: 0.8239106678962708
  batch 700 loss: 0.8368486642837525
  batch 750 loss: 0.8645265996456146
  batch 800 loss: 0.8219665467739106
  batch 850 loss: 0.8605419886112213
  batch 900 loss: 0.8637893152236938
LOSS train 0.86379 valid 0.98620, valid PER 30.56%
EPOCH 15:
  batch 50 loss: 0.7578369677066803
  batch 100 loss: 0.7929404199123382
  batch 150 loss: 0.7740045821666718
  batch 200 loss: 0.7976667219400406
  batch 250 loss: 0.8126517748832702
  batch 300 loss: 0.8047804939746857
  batch 350 loss: 0.7790147721767425
  batch 400 loss: 0.8266967821121216
  batch 450 loss: 0.8077345144748688
  batch 500 loss: 0.7901634711027146
  batch 550 loss: 0.839600168466568
  batch 600 loss: 0.8440452289581298
  batch 650 loss: 0.799836375117302
  batch 700 loss: 0.7864004242420196
  batch 750 loss: 0.8204460966587067
  batch 800 loss: 0.8045036941766739
  batch 850 loss: 0.8006569015979766
  batch 900 loss: 0.7617581623792649
LOSS train 0.76176 valid 0.95945, valid PER 29.70%
EPOCH 16:
  batch 50 loss: 0.7553360676765442
  batch 100 loss: 0.7246745955944062
  batch 150 loss: 0.7834987115859985
  batch 200 loss: 0.8000542998313904
  batch 250 loss: 0.7640329849720001
  batch 300 loss: 0.8074472784996033
  batch 350 loss: 0.7858153986930847
  batch 400 loss: 0.7697091484069825
  batch 450 loss: 0.7882549035549163
  batch 500 loss: 0.7551099216938019
  batch 550 loss: 0.7703001785278321
  batch 600 loss: 0.7989129745960235
  batch 650 loss: 0.7872241222858429
  batch 700 loss: 0.753806654214859
  batch 750 loss: 0.7976318192481995
  batch 800 loss: 0.7926396751403808
  batch 850 loss: 0.7761082589626312
  batch 900 loss: 0.7673384535312653
LOSS train 0.76734 valid 0.96775, valid PER 29.74%
EPOCH 17:
  batch 50 loss: 0.7454054498672485
  batch 100 loss: 0.6961774396896362
  batch 150 loss: 0.7733742928504944
  batch 200 loss: 0.7341715598106384
  batch 250 loss: 0.7715234100818634
  batch 300 loss: 0.746151933670044
  batch 350 loss: 0.762160222530365
  batch 400 loss: 0.8101825284957885
  batch 450 loss: 0.7920779061317443
  batch 500 loss: 0.781737003326416
  batch 550 loss: 0.8203437972068787
  batch 600 loss: 0.8006513059139252
  batch 650 loss: 0.7970509922504425
  batch 700 loss: 0.7808374345302582
  batch 750 loss: 0.7742026889324188
  batch 800 loss: 0.7970363438129425
  batch 850 loss: 0.7552347302436828
  batch 900 loss: 0.7713464725017548
LOSS train 0.77135 valid 0.97503, valid PER 29.36%
EPOCH 18:
  batch 50 loss: 0.7405733561515808
  batch 100 loss: 0.7282510673999787
  batch 150 loss: 0.7698829340934753
  batch 200 loss: 0.7280427998304367
  batch 250 loss: 0.7903017294406891
  batch 300 loss: 0.7827881157398224
  batch 350 loss: 0.7561101734638214
  batch 400 loss: 0.7797475975751876
  batch 450 loss: 0.8193135774135589
  batch 500 loss: 0.8196195244789124
  batch 550 loss: 0.8226255905628205
  batch 600 loss: 0.7961698246002197
  batch 650 loss: 0.7637006747722626
  batch 700 loss: 0.855408787727356
  batch 750 loss: 0.8628244459629059
  batch 800 loss: 0.8834643805027008
  batch 850 loss: 0.8417464518547058
  batch 900 loss: 0.8318601977825165
LOSS train 0.83186 valid 1.02649, valid PER 30.54%
EPOCH 19:
  batch 50 loss: 0.7910836040973663
  batch 100 loss: 0.7894813573360443
  batch 150 loss: 0.765527013540268
  batch 200 loss: 0.7491681265830994
  batch 250 loss: 0.8035548257827759
  batch 300 loss: 0.7980450117588043
  batch 350 loss: 0.7908190441131592
  batch 400 loss: 0.7752188694477081
  batch 450 loss: 0.7374521136283875
  batch 500 loss: 0.7577200365066529
  batch 550 loss: 0.7641569918394089
  batch 600 loss: 0.7771979117393494
  batch 650 loss: 0.7904940211772918
  batch 700 loss: 0.7868688523769378
  batch 750 loss: 0.7524298202991485
  batch 800 loss: 0.7442246460914612
  batch 850 loss: 0.7634710812568665
  batch 900 loss: 0.7502902019023895
LOSS train 0.75029 valid 0.97800, valid PER 30.06%
EPOCH 20:
  batch 50 loss: 0.6913971596956253
  batch 100 loss: 0.7478240871429443
  batch 150 loss: 0.7147228753566742
  batch 200 loss: 0.7324342548847198
  batch 250 loss: 0.7376175826787948
  batch 300 loss: 0.7106027698516846
  batch 350 loss: 0.7359347802400589
  batch 400 loss: 0.7612090086936951
  batch 450 loss: 0.7523945188522339
  batch 500 loss: 0.7483845829963685
  batch 550 loss: 0.7513804161548614
  batch 600 loss: 0.7560229706764221
  batch 650 loss: 0.789954651594162
  batch 700 loss: 0.7417150270938874
  batch 750 loss: 0.710995819568634
  batch 800 loss: 0.7322684127092361
  batch 850 loss: 0.7460717099905014
  batch 900 loss: 0.7262585896253586
LOSS train 0.72626 valid 0.96777, valid PER 29.64%
Training finished in 4.0 minutes.
Model saved to checkpoints/20231129_164215/model_15
Loading model from checkpoints/20231129_164215/model_15
SUB: 17.03%, DEL: 12.25%, INS: 2.18%, COR: 70.72%, PER: 31.46%
