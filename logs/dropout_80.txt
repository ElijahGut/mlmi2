Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.8, optimiser='sgd')
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.171219868659973
  batch 100 loss: 3.1780846881866456
  batch 150 loss: 3.0975841331481933
  batch 200 loss: 3.015917592048645
  batch 250 loss: 2.9571457529067993
  batch 300 loss: 2.78349347114563
  batch 350 loss: 2.7060132932662966
  batch 400 loss: 2.6116879034042357
  batch 450 loss: 2.6135511302948
  batch 500 loss: 2.8485466194152833
  batch 550 loss: 2.4746082067489623
  batch 600 loss: 2.3961563682556153
  batch 650 loss: 2.335917296409607
  batch 700 loss: 2.317824082374573
  batch 750 loss: 2.285552587509155
  batch 800 loss: 2.2481545066833495
  batch 850 loss: 2.2328137707710267
  batch 900 loss: 2.1958960103988647
LOSS train 2.19590 valid 2.20331, valid PER 80.50%
EPOCH 2:
  batch 50 loss: 2.1903671503067015
  batch 100 loss: 2.1405909657478333
  batch 150 loss: 2.1055656051635743
  batch 200 loss: 2.1240253829956055
  batch 250 loss: 2.1224043583869934
  batch 300 loss: 2.087310137748718
  batch 350 loss: 2.018411476612091
  batch 400 loss: 2.0415697574615477
  batch 450 loss: 2.00911431312561
  batch 500 loss: 2.0220837450027465
  batch 550 loss: 2.025168788433075
  batch 600 loss: 1.9815717840194702
  batch 650 loss: 1.9913268399238586
  batch 700 loss: 1.9889874696731566
  batch 750 loss: 1.9703669786453246
  batch 800 loss: 1.9330401277542115
  batch 850 loss: 1.9436311364173888
  batch 900 loss: 1.9510835194587708
LOSS train 1.95108 valid 1.85385, valid PER 74.82%
EPOCH 3:
  batch 50 loss: 1.9283477926254273
  batch 100 loss: 1.8948278975486756
  batch 150 loss: 1.9159068083763122
  batch 200 loss: 1.9010347723960876
  batch 250 loss: 1.8700638270378114
  batch 300 loss: 1.8688158249855042
  batch 350 loss: 1.915441267490387
  batch 400 loss: 1.8747874116897583
  batch 450 loss: 1.8629111170768737
  batch 500 loss: 1.8565080261230469
  batch 550 loss: 1.8572579503059388
  batch 600 loss: 1.8419647312164307
  batch 650 loss: 1.8120840978622437
  batch 700 loss: 1.8343168687820435
  batch 750 loss: 1.856553304195404
  batch 800 loss: 1.8011161279678345
  batch 850 loss: 1.8283825182914735
  batch 900 loss: 1.8044713163375854
LOSS train 1.80447 valid 1.74465, valid PER 72.42%
EPOCH 4:
  batch 50 loss: 1.7955520582199096
  batch 100 loss: 1.81822105884552
  batch 150 loss: 1.7554406714439392
  batch 200 loss: 1.7995204257965087
  batch 250 loss: 1.7799649477005004
  batch 300 loss: 1.7937952971458435
  batch 350 loss: 1.7384489941596986
  batch 400 loss: 1.773165349960327
  batch 450 loss: 1.7593083882331848
  batch 500 loss: 1.7528494882583618
  batch 550 loss: 1.7633987641334534
  batch 600 loss: 1.787406723499298
  batch 650 loss: 1.7779825305938721
  batch 700 loss: 1.7280263566970826
  batch 750 loss: 1.7103830647468568
  batch 800 loss: 1.6846180582046508
  batch 850 loss: 1.7195764112472534
  batch 900 loss: 1.7598738527297975
LOSS train 1.75987 valid 1.66643, valid PER 66.18%
EPOCH 5:
  batch 50 loss: 1.6999851155281067
  batch 100 loss: 1.7023631072044372
  batch 150 loss: 1.7271545791625977
  batch 200 loss: 1.6701557755470275
  batch 250 loss: 1.6779293084144593
  batch 300 loss: 1.695835919380188
  batch 350 loss: 1.6850729441642762
  batch 400 loss: 1.6872602796554566
  batch 450 loss: 1.6711192750930786
  batch 500 loss: 1.6955758595466615
  batch 550 loss: 1.646883659362793
  batch 600 loss: 1.706255009174347
  batch 650 loss: 1.6652951431274414
  batch 700 loss: 1.6902906894683838
  batch 750 loss: 1.6367146730422975
  batch 800 loss: 1.6603330779075622
  batch 850 loss: 1.6600168395042418
  batch 900 loss: 1.6716239333152771
LOSS train 1.67162 valid 1.56301, valid PER 64.44%
EPOCH 6:
  batch 50 loss: 1.6587609791755675
  batch 100 loss: 1.6457188439369201
  batch 150 loss: 1.6318032264709472
  batch 200 loss: 1.6377666068077088
  batch 250 loss: 1.6539915442466735
  batch 300 loss: 1.6136144232749938
  batch 350 loss: 1.6292034769058228
  batch 400 loss: 1.611605055332184
  batch 450 loss: 1.6593493008613587
  batch 500 loss: 1.615361785888672
  batch 550 loss: 1.6329485559463501
  batch 600 loss: 1.601654314994812
  batch 650 loss: 1.6385566210746765
  batch 700 loss: 1.6248304629325867
  batch 750 loss: 1.5988422083854674
  batch 800 loss: 1.5911200070381164
  batch 850 loss: 1.5774051022529603
  batch 900 loss: 1.6117613554000854
LOSS train 1.61176 valid 1.54731, valid PER 60.99%
EPOCH 7:
  batch 50 loss: 1.6301111674308777
  batch 100 loss: 1.6121907472610473
  batch 150 loss: 1.6006505513191223
  batch 200 loss: 1.5731322813034057
  batch 250 loss: 1.5827760314941406
  batch 300 loss: 1.5627279114723205
  batch 350 loss: 1.5802351832389832
  batch 400 loss: 1.5732470774650573
  batch 450 loss: 1.5840295958518982
  batch 500 loss: 1.5827535581588745
  batch 550 loss: 1.5490517020225525
  batch 600 loss: 1.587199671268463
  batch 650 loss: 1.580124101638794
  batch 700 loss: 1.5871819519996644
  batch 750 loss: 1.577665467262268
  batch 800 loss: 1.5632997584342956
  batch 850 loss: 1.5839435768127441
  batch 900 loss: 1.6081856894493103
LOSS train 1.60819 valid 1.52184, valid PER 61.01%
EPOCH 8:
  batch 50 loss: 1.550247504711151
  batch 100 loss: 1.5488274574279786
  batch 150 loss: 1.551188280582428
  batch 200 loss: 1.5083814001083373
  batch 250 loss: 1.5475228357315063
  batch 300 loss: 1.4886324667930604
  batch 350 loss: 1.5514707732200623
  batch 400 loss: 1.5430958199501037
  batch 450 loss: 1.5511266684532166
  batch 500 loss: 1.5840657663345337
  batch 550 loss: 1.513134765625
  batch 600 loss: 1.5282984900474548
  batch 650 loss: 1.590185022354126
  batch 700 loss: 1.518679280281067
  batch 750 loss: 1.520116527080536
  batch 800 loss: 1.556151614189148
  batch 850 loss: 1.5538602375984192
  batch 900 loss: 1.54393034696579
LOSS train 1.54393 valid 1.37887, valid PER 56.61%
EPOCH 9:
  batch 50 loss: 1.4786244201660157
  batch 100 loss: 1.5147882604599
  batch 150 loss: 1.5241285514831544
  batch 200 loss: 1.5017430448532105
  batch 250 loss: 1.5145854401588439
  batch 300 loss: 1.5163734912872315
  batch 350 loss: 1.5270318794250488
  batch 400 loss: 1.5172570848464966
  batch 450 loss: 1.521209213733673
  batch 500 loss: 1.481506757736206
  batch 550 loss: 1.516543481349945
  batch 600 loss: 1.519555571079254
  batch 650 loss: 1.4850977206230163
  batch 700 loss: 1.494088625907898
  batch 750 loss: 1.503746154308319
  batch 800 loss: 1.5335748672485352
  batch 850 loss: 1.5234982752799988
  batch 900 loss: 1.4775589561462403
LOSS train 1.47756 valid 1.36469, valid PER 54.74%
EPOCH 10:
  batch 50 loss: 1.4552297568321229
  batch 100 loss: 1.480284857749939
  batch 150 loss: 1.501199197769165
  batch 200 loss: 1.5108328127861024
  batch 250 loss: 1.492113220691681
  batch 300 loss: 1.4594854807853699
  batch 350 loss: 1.4902071452140808
  batch 400 loss: 1.436078429222107
  batch 450 loss: 1.4545058107376099
  batch 500 loss: 1.4772987651824951
  batch 550 loss: 1.5038192629814149
  batch 600 loss: 1.4609045386314392
  batch 650 loss: 1.4511807203292846
  batch 700 loss: 1.4643526077270508
  batch 750 loss: 1.4710273814201356
  batch 800 loss: 1.469562313556671
  batch 850 loss: 1.482850682735443
  batch 900 loss: 1.4868155670166017
LOSS train 1.48682 valid 1.36899, valid PER 55.94%
EPOCH 11:
  batch 50 loss: 1.445566999912262
  batch 100 loss: 1.4499389052391052
  batch 150 loss: 1.4104029488563539
  batch 200 loss: 1.4760615587234498
  batch 250 loss: 1.4448017859458924
  batch 300 loss: 1.4194404888153076
  batch 350 loss: 1.456099123954773
  batch 400 loss: 1.4652160692214966
  batch 450 loss: 1.455196626186371
  batch 500 loss: 1.4381208777427674
  batch 550 loss: 1.433714928627014
  batch 600 loss: 1.4327700436115265
  batch 650 loss: 1.4991439533233644
  batch 700 loss: 1.4070280957221986
  batch 750 loss: 1.4377720546722412
  batch 800 loss: 1.4729324316978454
  batch 850 loss: 1.4921479439735412
  batch 900 loss: 1.4644300484657287
LOSS train 1.46443 valid 1.35550, valid PER 52.78%
EPOCH 12:
  batch 50 loss: 1.4672541427612305
  batch 100 loss: 1.4404472208023071
  batch 150 loss: 1.4311124610900878
  batch 200 loss: 1.418536434173584
  batch 250 loss: 1.4762996768951415
  batch 300 loss: 1.4387360405921936
  batch 350 loss: 1.4279643845558168
  batch 400 loss: 1.4288979053497315
  batch 450 loss: 1.4434697151184082
  batch 500 loss: 1.453354969024658
  batch 550 loss: 1.367422776222229
  batch 600 loss: 1.4092612218856813
  batch 650 loss: 1.455893087387085
  batch 700 loss: 1.4286311340332032
  batch 750 loss: 1.4237210416793824
  batch 800 loss: 1.4005820226669312
  batch 850 loss: 1.4469616413116455
  batch 900 loss: 1.450125904083252
LOSS train 1.45013 valid 1.35432, valid PER 52.53%
EPOCH 13:
  batch 50 loss: 1.3897435235977174
  batch 100 loss: 1.418672695159912
  batch 150 loss: 1.4008388566970824
  batch 200 loss: 1.4116265749931336
  batch 250 loss: 1.4147717881202697
  batch 300 loss: 1.3641521167755126
  batch 350 loss: 1.3989073133468628
  batch 400 loss: 1.4431582975387574
  batch 450 loss: 1.4175308179855346
  batch 500 loss: 1.3893157362937927
  batch 550 loss: 1.407214467525482
  batch 600 loss: 1.3981180906295776
  batch 650 loss: 1.4081528067588807
  batch 700 loss: 1.4236055421829223
  batch 750 loss: 1.3837715554237366
  batch 800 loss: 1.39319589138031
  batch 850 loss: 1.4455617952346802
  batch 900 loss: 1.4219367861747743
LOSS train 1.42194 valid 1.37186, valid PER 52.58%
EPOCH 14:
  batch 50 loss: 1.3793994522094726
  batch 100 loss: 1.3962338876724243
  batch 150 loss: 1.371564311981201
  batch 200 loss: 1.3883610439300538
  batch 250 loss: 1.3968804740905763
  batch 300 loss: 1.418472011089325
  batch 350 loss: 1.364393880367279
  batch 400 loss: 1.3689850521087648
  batch 450 loss: 1.3538119459152222
  batch 500 loss: 1.3975667715072633
  batch 550 loss: 1.4137903881072997
  batch 600 loss: 1.3770754957199096
  batch 650 loss: 1.3982372093200683
  batch 700 loss: 1.4203274154663086
  batch 750 loss: 1.36872008562088
  batch 800 loss: 1.3445741748809814
  batch 850 loss: 1.4172522354125976
  batch 900 loss: 1.4227073121070861
LOSS train 1.42271 valid 1.27634, valid PER 48.95%
EPOCH 15:
  batch 50 loss: 1.3599934816360473
  batch 100 loss: 1.3390773022174836
  batch 150 loss: 1.3665044450759887
  batch 200 loss: 1.4091814351081848
  batch 250 loss: 1.3833253979682922
  batch 300 loss: 1.361658501625061
  batch 350 loss: 1.3776288175582885
  batch 400 loss: 1.354773190021515
  batch 450 loss: 1.3513800740242004
  batch 500 loss: 1.3182718443870545
  batch 550 loss: 1.3600011014938354
  batch 600 loss: 1.399643943309784
  batch 650 loss: 1.4187150192260742
  batch 700 loss: 1.3702407991886139
  batch 750 loss: 1.3805080223083497
  batch 800 loss: 1.3610826110839844
  batch 850 loss: 1.3595983171463013
  batch 900 loss: 1.3889647030830383
LOSS train 1.38896 valid 1.31106, valid PER 50.90%
EPOCH 16:
  batch 50 loss: 1.3915119290351867
  batch 100 loss: 1.3198278903961183
  batch 150 loss: 1.3515905940532684
  batch 200 loss: 1.3569937467575073
  batch 250 loss: 1.370230655670166
  batch 300 loss: 1.3865195322036743
  batch 350 loss: 1.365597321987152
  batch 400 loss: 1.3692263793945312
  batch 450 loss: 1.3950892448425294
  batch 500 loss: 1.3273587107658387
  batch 550 loss: 1.367991602420807
  batch 600 loss: 1.3297486412525177
  batch 650 loss: 1.374035849571228
  batch 700 loss: 1.3091550517082213
  batch 750 loss: 1.3549892735481261
  batch 800 loss: 1.3756266689300538
  batch 850 loss: 1.35514972448349
  batch 900 loss: 1.3344824409484863
LOSS train 1.33448 valid 1.24594, valid PER 48.16%
EPOCH 17:
  batch 50 loss: 1.343809380531311
  batch 100 loss: 1.3363251459598542
  batch 150 loss: 1.3382480716705323
  batch 200 loss: 1.3176298904418946
  batch 250 loss: 1.352237002849579
  batch 300 loss: 1.3389944124221802
  batch 350 loss: 1.3030394315719604
  batch 400 loss: 1.3736530113220216
  batch 450 loss: 1.376270091533661
  batch 500 loss: 1.3147921133041383
  batch 550 loss: 1.3527938365936278
  batch 600 loss: 1.3809906578063964
  batch 650 loss: 1.3389653778076172
  batch 700 loss: 1.553733744621277
  batch 750 loss: 1.4259035849571229
  batch 800 loss: 1.4097990345954896
  batch 850 loss: 1.3928530836105346
  batch 900 loss: 1.3729985451698303
LOSS train 1.37300 valid 1.27151, valid PER 47.73%
EPOCH 18:
  batch 50 loss: 1.381142988204956
  batch 100 loss: 1.3812978196144103
  batch 150 loss: 1.381090714931488
  batch 200 loss: 1.34968519449234
  batch 250 loss: 1.351233172416687
  batch 300 loss: 1.354593734741211
  batch 350 loss: 1.3875049352645874
  batch 400 loss: 1.3072373700141906
  batch 450 loss: 1.4021466493606567
  batch 500 loss: 1.3632758808135987
  batch 550 loss: 1.3349356961250305
  batch 600 loss: 1.3249186515808105
  batch 650 loss: 1.3251442456245421
  batch 700 loss: 1.3854396772384643
  batch 750 loss: 1.3317556047439576
  batch 800 loss: 1.3385389709472657
  batch 850 loss: 1.3063237595558166
  batch 900 loss: 1.3580087852478027
LOSS train 1.35801 valid 1.30640, valid PER 48.67%
EPOCH 19:
  batch 50 loss: 1.288375780582428
  batch 100 loss: 1.2997286605834961
  batch 150 loss: 1.3144415020942688
  batch 200 loss: 1.3276411080360413
  batch 250 loss: 1.3516353380680084
  batch 300 loss: 1.3413276743888856
  batch 350 loss: 1.3118483924865723
  batch 400 loss: 1.3382519555091859
  batch 450 loss: 1.3278823184967041
  batch 500 loss: 1.3275794363021851
  batch 550 loss: 1.3089547657966614
  batch 600 loss: 1.3151096200942993
  batch 650 loss: 1.3766175782680512
  batch 700 loss: 1.3054108786582947
  batch 750 loss: 1.2900805902481078
  batch 800 loss: 1.3260979509353639
  batch 850 loss: 1.3248096680641175
  batch 900 loss: 1.3135755205154418
LOSS train 1.31358 valid 1.24841, valid PER 46.33%
EPOCH 20:
  batch 50 loss: 1.3047432231903076
  batch 100 loss: 1.3060996377468108
  batch 150 loss: 1.2838640451431274
  batch 200 loss: 1.3096230983734132
  batch 250 loss: 1.3107229471206665
  batch 300 loss: 1.3182076382637025
  batch 350 loss: 1.3102772891521455
  batch 400 loss: 1.3060436034202576
  batch 450 loss: 1.3043815994262695
  batch 500 loss: 1.2852439486980438
  batch 550 loss: 1.3697860741615295
  batch 600 loss: 1.2813301980495453
  batch 650 loss: 1.3161772894859314
  batch 700 loss: 1.3245424342155456
  batch 750 loss: 1.2983246970176696
  batch 800 loss: 1.3382278156280518
  batch 850 loss: 1.3494797432422638
  batch 900 loss: 1.3473301553726196
LOSS train 1.34733 valid 1.22941, valid PER 45.53%
Training finished in 6.0 minutes.
Model saved to checkpoints/20231206_160607/model_20
Loading model from checkpoints/20231206_160607/model_20
SUB: 7.72%, DEL: 40.28%, INS: 0.21%, COR: 52.01%, PER: 48.20%
