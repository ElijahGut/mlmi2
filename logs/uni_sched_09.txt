Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.9, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 83496
EPOCH 1, Learning Rate: 0.9
  batch 50 loss: 4.4303303241729735
  batch 100 loss: 3.1905765247344973
  batch 150 loss: 3.0106576824188234
  batch 200 loss: 2.697713351249695
  batch 250 loss: 2.5031158113479615
  batch 300 loss: 2.389619550704956
  batch 350 loss: 2.2694099378585815
  batch 400 loss: 2.238921618461609
  batch 450 loss: 2.173757314682007
  batch 500 loss: 2.0734081959724424
  batch 550 loss: 2.053354468345642
  batch 600 loss: 1.9859491229057311
  batch 650 loss: 1.9423881673812866
  batch 700 loss: 1.9600020837783814
  batch 750 loss: 1.8977409195899964
  batch 800 loss: 1.8888654017448425
  batch 850 loss: 1.8384349322319031
  batch 900 loss: 1.8251010823249816
running loss: 43.13781201839447
LOSS train 1.82510 valid 1.74108, valid PER 64.10%
EPOCH 2, Learning Rate: 0.9
  batch 50 loss: 1.7838372993469238
  batch 100 loss: 1.7529216885566712
  batch 150 loss: 1.721534481048584
  batch 200 loss: 1.7564657020568848
  batch 250 loss: 1.7417608141899108
  batch 300 loss: 1.741763780117035
  batch 350 loss: 1.627438793182373
  batch 400 loss: 1.6707395839691161
  batch 450 loss: 1.5897451758384704
  batch 500 loss: 1.647610499858856
  batch 550 loss: 1.6522287607192994
  batch 600 loss: 1.584140295982361
  batch 650 loss: 1.6316579174995423
  batch 700 loss: 1.585583655834198
  batch 750 loss: 1.6040709733963012
  batch 800 loss: 1.5586519932746887
  batch 850 loss: 1.5952135729789734
  batch 900 loss: 1.559623486995697
running loss: 37.17747914791107
LOSS train 1.55962 valid 1.51950, valid PER 51.13%
EPOCH 3, Learning Rate: 0.9
  batch 50 loss: 1.5493933415412904
  batch 100 loss: 1.5365995907783507
  batch 150 loss: 1.525709958076477
  batch 200 loss: 1.520371413230896
  batch 250 loss: 1.5125071859359742
  batch 300 loss: 1.4927887940406799
  batch 350 loss: 1.5994185924530029
  batch 400 loss: 1.5343509793281556
  batch 450 loss: 1.5384654545783996
  batch 500 loss: 1.5395552921295166
  batch 550 loss: 1.495908100605011
  batch 600 loss: 1.4952362704277038
  batch 650 loss: 1.4318473291397096
  batch 700 loss: 1.4841185188293458
  batch 750 loss: 1.547154140472412
  batch 800 loss: 1.4937351965904235
  batch 850 loss: 1.5329093718528748
  batch 900 loss: 1.481588203907013
running loss: 35.13664162158966
LOSS train 1.48159 valid 1.39406, valid PER 44.33%
EPOCH 4, Learning Rate: 0.9
  batch 50 loss: 1.515080876350403
  batch 100 loss: 1.5039154863357544
  batch 150 loss: 1.471219575405121
  batch 200 loss: 1.4573194694519043
  batch 250 loss: 1.5389839339256286
  batch 300 loss: 1.5390300369262695
  batch 350 loss: 1.4129569029808045
  batch 400 loss: 1.5169878721237182
  batch 450 loss: 1.473505940437317
  batch 500 loss: 1.4706471967697143
  batch 550 loss: 1.5416928720474243
  batch 600 loss: 1.504815742969513
  batch 650 loss: 1.5571007990837098
  batch 700 loss: 1.4883871340751649
  batch 750 loss: 1.421168739795685
  batch 800 loss: 1.4307347023487091
  batch 850 loss: 1.4494984579086303
  batch 900 loss: 1.493849164247513
running loss: 36.16942346096039
LOSS train 1.49385 valid 1.46198, valid PER 44.97%
EPOCH 5, Learning Rate: 0.45
  batch 50 loss: 1.4310554432868958
  batch 100 loss: 1.3547729861736297
  batch 150 loss: 1.3425519466400146
  batch 200 loss: 1.3012864708900451
  batch 250 loss: 1.285819436311722
  batch 300 loss: 1.3174002265930176
  batch 350 loss: 1.3579794263839722
  batch 400 loss: 1.3354308462142945
  batch 450 loss: 1.3368510127067565
  batch 500 loss: 1.3567315745353699
  batch 550 loss: 1.2841722226142884
  batch 600 loss: 1.3476255559921264
  batch 650 loss: 1.2932094430923462
  batch 700 loss: 1.3618610906600952
  batch 750 loss: 1.3391101789474487
  batch 800 loss: 1.323522527217865
  batch 850 loss: 1.3516700196266174
  batch 900 loss: 1.343693265914917
running loss: 31.229877710342407
LOSS train 1.34369 valid 1.25134, valid PER 40.21%
EPOCH 6, Learning Rate: 0.45
  batch 50 loss: 1.3045541739463806
  batch 100 loss: 1.2802067852020265
  batch 150 loss: 1.2736837434768677
  batch 200 loss: 1.2749482798576355
  batch 250 loss: 1.3223833787441253
  batch 300 loss: 1.27941308259964
  batch 350 loss: 1.2742906665802003
  batch 400 loss: 1.2738374006748199
  batch 450 loss: 1.307433694601059
  batch 500 loss: 1.2825269985198975
  batch 550 loss: 1.3089484214782714
  batch 600 loss: 1.2697377634048461
  batch 650 loss: 1.3090079808235169
  batch 700 loss: 1.2910829198360443
  batch 750 loss: 1.2476693511009216
  batch 800 loss: 1.2682983362674713
  batch 850 loss: 1.2849370384216308
  batch 900 loss: 1.2822544193267822
running loss: 29.77869153022766
LOSS train 1.28225 valid 1.22139, valid PER 39.26%
EPOCH 7, Learning Rate: 0.45
  batch 50 loss: 1.2756052780151368
  batch 100 loss: 1.3051556277275085
  batch 150 loss: 1.2764197850227357
  batch 200 loss: 1.244379336833954
  batch 250 loss: 1.2668072748184205
  batch 300 loss: 1.2219212651252747
  batch 350 loss: 1.2620377230644226
  batch 400 loss: 1.2793960165977478
  batch 450 loss: 1.2660395395755768
  batch 500 loss: 1.2637509834766387
  batch 550 loss: 1.2348343920707703
  batch 600 loss: 1.266171669960022
  batch 650 loss: 1.260180537700653
  batch 700 loss: 1.2640881168842315
  batch 750 loss: 1.2400586485862732
  batch 800 loss: 1.2270812511444091
  batch 850 loss: 1.2706087553501129
  batch 900 loss: 1.2863629603385924
running loss: 30.61382704973221
LOSS train 1.28636 valid 1.28277, valid PER 40.71%
EPOCH 8, Learning Rate: 0.225
  batch 50 loss: 1.2778314018249513
  batch 100 loss: 1.227247484922409
  batch 150 loss: 1.2106448793411255
  batch 200 loss: 1.1817342019081116
  batch 250 loss: 1.222033635377884
  batch 300 loss: 1.1713362455368042
  batch 350 loss: 1.213892252445221
  batch 400 loss: 1.1678603827953338
  batch 450 loss: 1.2029239118099213
  batch 500 loss: 1.2241156661510468
  batch 550 loss: 1.1809345090389252
  batch 600 loss: 1.2060722136497497
  batch 650 loss: 1.2301387786865234
  batch 700 loss: 1.1747004830837249
  batch 750 loss: 1.1848202562332153
  batch 800 loss: 1.1940622103214265
  batch 850 loss: 1.1809495878219605
  batch 900 loss: 1.166810804605484
running loss: 28.490579843521118
LOSS train 1.16681 valid 1.14851, valid PER 36.78%
EPOCH 9, Learning Rate: 0.225
  batch 50 loss: 1.1354377138614655
  batch 100 loss: 1.1995302629470825
  batch 150 loss: 1.174890203475952
  batch 200 loss: 1.1261621415615082
  batch 250 loss: 1.1552757918834686
  batch 300 loss: 1.1681820726394654
  batch 350 loss: 1.1852396142482757
  batch 400 loss: 1.1620928645133972
  batch 450 loss: 1.1676382851600646
  batch 500 loss: 1.1153662431240081
  batch 550 loss: 1.1751452422142028
  batch 600 loss: 1.178638104200363
  batch 650 loss: 1.1439973628520965
  batch 700 loss: 1.1413101279735565
  batch 750 loss: 1.1670498836040497
  batch 800 loss: 1.180782254934311
  batch 850 loss: 1.2011853969097137
  batch 900 loss: 1.1384439373016357
running loss: 27.547600269317627
LOSS train 1.13844 valid 1.12630, valid PER 35.87%
EPOCH 10, Learning Rate: 0.225
  batch 50 loss: 1.1043209505081177
  batch 100 loss: 1.1388546669483184
  batch 150 loss: 1.1821239292621613
  batch 200 loss: 1.1740550267696381
  batch 250 loss: 1.1671580576896667
  batch 300 loss: 1.0943120300769806
  batch 350 loss: 1.145296084880829
  batch 400 loss: 1.1316976237297058
  batch 450 loss: 1.1191624009609222
  batch 500 loss: 1.1815105772018433
  batch 550 loss: 1.1743799078464507
  batch 600 loss: 1.1423381567001343
  batch 650 loss: 1.133574538230896
  batch 700 loss: 1.1530801248550415
  batch 750 loss: 1.1303895807266235
  batch 800 loss: 1.1380947315692902
  batch 850 loss: 1.1679656672477723
  batch 900 loss: 1.165946012735367
running loss: 28.158024668693542
LOSS train 1.16595 valid 1.12205, valid PER 35.88%
EPOCH 11, Learning Rate: 0.1125
  batch 50 loss: 1.1119912469387054
  batch 100 loss: 1.0785402417182923
  batch 150 loss: 1.0896682715415955
  batch 200 loss: 1.1264145243167878
  batch 250 loss: 1.116471449136734
  batch 300 loss: 1.0549183559417725
  batch 350 loss: 1.10419935464859
  batch 400 loss: 1.1162747871875762
  batch 450 loss: 1.1111236929893493
  batch 500 loss: 1.0749993777275086
  batch 550 loss: 1.0932835495471955
  batch 600 loss: 1.083414055109024
  batch 650 loss: 1.1235300540924071
  batch 700 loss: 1.0540686893463134
  batch 750 loss: 1.0835150945186616
  batch 800 loss: 1.122504588365555
  batch 850 loss: 1.130022975206375
  batch 900 loss: 1.1133586478233337
running loss: 26.08490490913391
LOSS train 1.11336 valid 1.08710, valid PER 35.34%
EPOCH 12, Learning Rate: 0.1125
  batch 50 loss: 1.0947428047657013
  batch 100 loss: 1.0652288472652436
  batch 150 loss: 1.058778223991394
  batch 200 loss: 1.0669436991214751
  batch 250 loss: 1.0941579854488372
  batch 300 loss: 1.0845905137062073
  batch 350 loss: 1.0933925437927245
  batch 400 loss: 1.1215700590610505
  batch 450 loss: 1.090865557193756
  batch 500 loss: 1.1086267495155335
  batch 550 loss: 1.0343937730789186
  batch 600 loss: 1.0698192846775054
  batch 650 loss: 1.1177979171276093
  batch 700 loss: 1.1078649497032165
  batch 750 loss: 1.0678241670131683
  batch 800 loss: 1.0599843561649323
  batch 850 loss: 1.1199314272403718
  batch 900 loss: 1.1465233969688415
running loss: 26.169976592063904
LOSS train 1.14652 valid 1.09914, valid PER 34.76%
EPOCH 13, Learning Rate: 0.05625
  batch 50 loss: 1.0707400238513947
  batch 100 loss: 1.096257756948471
  batch 150 loss: 1.0401361787319183
  batch 200 loss: 1.0857240545749665
  batch 250 loss: 1.0622245395183563
  batch 300 loss: 1.0503399538993836
  batch 350 loss: 1.0561426043510438
  batch 400 loss: 1.0837976050376892
  batch 450 loss: 1.1006579518318176
  batch 500 loss: 1.0322128486633302
  batch 550 loss: 1.0526925122737885
  batch 600 loss: 1.0603876638412475
  batch 650 loss: 1.0709348344802856
  batch 700 loss: 1.0584462094306946
  batch 750 loss: 1.0397177374362945
  batch 800 loss: 1.048960063457489
  batch 850 loss: 1.0999111223220825
  batch 900 loss: 1.0837235832214356
running loss: 26.29453593492508
LOSS train 1.08372 valid 1.06793, valid PER 34.50%
EPOCH 14, Learning Rate: 0.028125
  batch 50 loss: 1.0355245792865753
  batch 100 loss: 1.0695109295845031
  batch 150 loss: 1.0573431301116942
  batch 200 loss: 1.0588819408416748
  batch 250 loss: 1.0529019331932068
  batch 300 loss: 1.0743273210525512
  batch 350 loss: 1.0320743656158446
  batch 400 loss: 1.0346267688274384
  batch 450 loss: 1.0297486460208893
  batch 500 loss: 1.056092348098755
  batch 550 loss: 1.0657745790481568
  batch 600 loss: 1.0330563712120056
  batch 650 loss: 1.0575320863723754
  batch 700 loss: 1.0609529364109038
  batch 750 loss: 1.0142559015750885
  batch 800 loss: 0.9852776384353638
  batch 850 loss: 1.057685774564743
  batch 900 loss: 1.0449323129653931
running loss: 26.171623647212982
LOSS train 1.04493 valid 1.06277, valid PER 34.06%
EPOCH 15, Learning Rate: 0.0140625
  batch 50 loss: 1.056120707988739
  batch 100 loss: 1.0250738632678986
  batch 150 loss: 1.0269222819805146
  batch 200 loss: 1.0569492375850678
  batch 250 loss: 1.0372757601737976
  batch 300 loss: 1.0274015522003175
  batch 350 loss: 1.0287272083759307
  batch 400 loss: 1.027212541103363
  batch 450 loss: 1.0271366930007935
  batch 500 loss: 1.001334284543991
  batch 550 loss: 1.032263320684433
  batch 600 loss: 1.0550152385234832
  batch 650 loss: 1.053731507062912
  batch 700 loss: 1.068041617870331
  batch 750 loss: 1.030750595331192
  batch 800 loss: 1.0405027902126311
  batch 850 loss: 1.0044307911396027
  batch 900 loss: 1.039195204973221
running loss: 24.577654719352722
LOSS train 1.03920 valid 1.05741, valid PER 33.91%
EPOCH 16, Learning Rate: 0.0140625
  batch 50 loss: 1.0669973134994506
  batch 100 loss: 1.011179460287094
  batch 150 loss: 1.0213099133968353
  batch 200 loss: 1.0341732597351074
  batch 250 loss: 1.054715541601181
  batch 300 loss: 1.0382119381427766
  batch 350 loss: 1.0628241693973541
  batch 400 loss: 1.0230729508399963
  batch 450 loss: 1.0568886005878448
  batch 500 loss: 1.006495976448059
  batch 550 loss: 1.038019107580185
  batch 600 loss: 1.0498950099945068
  batch 650 loss: 1.0262468647956848
  batch 700 loss: 1.0081650304794312
  batch 750 loss: 1.022164157629013
  batch 800 loss: 1.0230420684814454
  batch 850 loss: 1.0174996864795685
  batch 900 loss: 1.026726267337799
running loss: 24.100733160972595
LOSS train 1.02673 valid 1.05932, valid PER 33.88%
EPOCH 17, Learning Rate: 0.0140625
  batch 50 loss: 1.049472644329071
  batch 100 loss: 1.0195830619335176
  batch 150 loss: 1.0127499437332153
  batch 200 loss: 1.0168750858306885
  batch 250 loss: 1.0320824551582337
  batch 300 loss: 1.0228760600090028
  batch 350 loss: 1.0093503832817077
  batch 400 loss: 1.0797884595394134
  batch 450 loss: 1.0310157680511474
  batch 500 loss: 1.028756548166275
  batch 550 loss: 1.0399362170696258
  batch 600 loss: 1.0643806087970733
  batch 650 loss: 1.0248085415363313
  batch 700 loss: 1.023485815525055
  batch 750 loss: 1.0187475657463074
  batch 800 loss: 0.9948536324501037
  batch 850 loss: 1.0188425719738006
  batch 900 loss: 1.0085743808746337
running loss: 25.46468812227249
LOSS train 1.00857 valid 1.05494, valid PER 33.96%
EPOCH 18, Learning Rate: 0.00703125
  batch 50 loss: 1.0291724252700805
  batch 100 loss: 1.033971720933914
  batch 150 loss: 1.0563246655464171
  batch 200 loss: 1.0472551167011261
  batch 250 loss: 1.0398158872127532
  batch 300 loss: 1.0173720800876618
  batch 350 loss: 1.051112620830536
  batch 400 loss: 0.9932615172863006
  batch 450 loss: 1.0533514034748077
  batch 500 loss: 1.0294405794143677
  batch 550 loss: 1.0141717886924744
  batch 600 loss: 0.9885922515392304
  batch 650 loss: 0.991276388168335
  batch 700 loss: 1.0545708155632019
  batch 750 loss: 1.0175970304012298
  batch 800 loss: 1.0237185037136078
  batch 850 loss: 0.9980086612701417
  batch 900 loss: 1.0306030249595641
running loss: 23.652005672454834
LOSS train 1.03060 valid 1.05451, valid PER 33.80%
EPOCH 19, Learning Rate: 0.00703125
  batch 50 loss: 1.0073757421970368
  batch 100 loss: 0.9869363820552826
  batch 150 loss: 1.0217992317676545
  batch 200 loss: 1.0246199429035188
  batch 250 loss: 1.0663428962230683
  batch 300 loss: 1.0163919281959535
  batch 350 loss: 1.0109667360782624
  batch 400 loss: 1.032635872364044
  batch 450 loss: 1.0367694532871246
  batch 500 loss: 1.0292706787586212
  batch 550 loss: 1.0156395518779755
  batch 600 loss: 1.0370074617862701
  batch 650 loss: 1.056853972673416
  batch 700 loss: 0.9993728518486023
  batch 750 loss: 1.0009341645240784
  batch 800 loss: 1.0231042075157166
  batch 850 loss: 1.037827959060669
  batch 900 loss: 1.0330171442031861
running loss: 24.55920547246933
LOSS train 1.03302 valid 1.05345, valid PER 33.75%
EPOCH 20, Learning Rate: 0.003515625
  batch 50 loss: 0.9985554230213165
  batch 100 loss: 1.038284500837326
  batch 150 loss: 1.0152554881572724
  batch 200 loss: 1.0287012696266173
  batch 250 loss: 1.0096750116348268
  batch 300 loss: 1.032823132276535
  batch 350 loss: 0.989143785238266
  batch 400 loss: 1.0088197684288025
  batch 450 loss: 1.0071849572658538
  batch 500 loss: 1.013655948638916
  batch 550 loss: 1.059088681936264
  batch 600 loss: 0.9793668484687805
  batch 650 loss: 1.0107359516620635
  batch 700 loss: 1.0399041724205018
  batch 750 loss: 1.017588871717453
  batch 800 loss: 1.050433555841446
  batch 850 loss: 1.038879337310791
  batch 900 loss: 1.0265128588676453
running loss: 24.25442224740982
LOSS train 1.02651 valid 1.05277, valid PER 33.74%
Training finished in 2.0 minutes.
Model saved to checkpoints/20231210_042039/model_20
Loading model from checkpoints/20231210_042039/model_20
SUB: 19.31%, DEL: 13.72%, INS: 1.42%, COR: 66.97%, PER: 34.45%
