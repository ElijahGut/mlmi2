Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.9, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 83496
EPOCH 1, Learning Rate: 0.9
  batch 50 loss: 4.430331730842591
  batch 100 loss: 3.1905772256851197
  batch 150 loss: 3.010523586273193
  batch 200 loss: 2.698449945449829
  batch 250 loss: 2.5032590341567995
  batch 300 loss: 2.3894051265716554
  batch 350 loss: 2.269390516281128
  batch 400 loss: 2.2385899925231936
  batch 450 loss: 2.173739447593689
  batch 500 loss: 2.0724801325798037
  batch 550 loss: 2.048216252326965
  batch 600 loss: 1.9852017116546632
  batch 650 loss: 1.9430407166481019
  batch 700 loss: 1.9540959334373473
  batch 750 loss: 1.9041066336631776
  batch 800 loss: 1.8906840753555298
  batch 850 loss: 1.8419137358665467
  batch 900 loss: 1.8253173041343689
avg val loss: 1.7421189546585083
LOSS train 1.82532 valid 1.74212, valid PER 64.82%
EPOCH 2, Learning Rate: 0.9
  batch 50 loss: 1.7787732076644898
  batch 100 loss: 1.749693853855133
  batch 150 loss: 1.7205600357055664
  batch 200 loss: 1.7622294950485229
  batch 250 loss: 1.75124764919281
  batch 300 loss: 1.732565450668335
  batch 350 loss: 1.6466984415054322
  batch 400 loss: 1.6756409120559692
  batch 450 loss: 1.614819962978363
  batch 500 loss: 1.6380902099609376
  batch 550 loss: 1.644466905593872
  batch 600 loss: 1.5878742003440858
  batch 650 loss: 1.630055468082428
  batch 700 loss: 1.5900101995468139
  batch 750 loss: 1.6012253046035767
  batch 800 loss: 1.5515637946128846
  batch 850 loss: 1.5714157176017762
  batch 900 loss: 1.582324366569519
avg val loss: 1.4536832571029663
LOSS train 1.58232 valid 1.45368, valid PER 49.69%
EPOCH 3, Learning Rate: 0.9
  batch 50 loss: 1.55181556224823
  batch 100 loss: 1.527330596446991
  batch 150 loss: 1.5138497376441955
  batch 200 loss: 1.496628303527832
  batch 250 loss: 1.4953925609588623
  batch 300 loss: 1.5148236012458802
  batch 350 loss: 1.6091387224197389
  batch 400 loss: 1.5580997776985168
  batch 450 loss: 1.5604451394081116
  batch 500 loss: 1.523866765499115
  batch 550 loss: 1.5247006702423096
  batch 600 loss: 1.524200472831726
  batch 650 loss: 1.4786904120445252
  batch 700 loss: 1.4914764809608458
  batch 750 loss: 1.5530965876579286
  batch 800 loss: 1.459432783126831
  batch 850 loss: 1.4757848286628723
  batch 900 loss: 1.4497064566612243
avg val loss: 1.462118148803711
LOSS train 1.44971 valid 1.46212, valid PER 45.05%
EPOCH 4, Learning Rate: 0.45
  batch 50 loss: 1.399346947669983
  batch 100 loss: 1.3944830346107482
  batch 150 loss: 1.3383964109420776
  batch 200 loss: 1.361234039068222
  batch 250 loss: 1.3907988214492797
  batch 300 loss: 1.4188621401786805
  batch 350 loss: 1.3139309680461884
  batch 400 loss: 1.383385524749756
  batch 450 loss: 1.370470814704895
  batch 500 loss: 1.3452341341972351
  batch 550 loss: 1.3863752484321594
  batch 600 loss: 1.390371735095978
  batch 650 loss: 1.3845263957977294
  batch 700 loss: 1.3293031811714173
  batch 750 loss: 1.2996777999401092
  batch 800 loss: 1.3167060935497283
  batch 850 loss: 1.352305142879486
  batch 900 loss: 1.3849875330924988
avg val loss: 1.278868556022644
LOSS train 1.38499 valid 1.27887, valid PER 41.18%
EPOCH 5, Learning Rate: 0.45
  batch 50 loss: 1.3511830186843872
  batch 100 loss: 1.3322821497917174
  batch 150 loss: 1.388207914829254
  batch 200 loss: 1.2837292551994324
  batch 250 loss: 1.2902883732318877
  batch 300 loss: 1.3396452069282532
  batch 350 loss: 1.3465273952484131
  batch 400 loss: 1.3289922094345092
  batch 450 loss: 1.3454226851463318
  batch 500 loss: 1.39025062084198
  batch 550 loss: 1.3343663668632508
  batch 600 loss: 1.3589948046207427
  batch 650 loss: 1.3156360602378845
  batch 700 loss: 1.363950524330139
  batch 750 loss: 1.305672149658203
  batch 800 loss: 1.3227758717536926
  batch 850 loss: 1.3438120865821839
  batch 900 loss: 1.3344106245040894
avg val loss: 1.3379310369491577
LOSS train 1.33441 valid 1.33793, valid PER 43.05%
EPOCH 6, Learning Rate: 0.225
  batch 50 loss: 1.3227141284942627
  batch 100 loss: 1.2814371132850646
  batch 150 loss: 1.2426857316493989
  batch 200 loss: 1.252387331724167
  batch 250 loss: 1.2605459523200988
  batch 300 loss: 1.260780279636383
  batch 350 loss: 1.263543999195099
  batch 400 loss: 1.236503301858902
  batch 450 loss: 1.2670338702201844
  batch 500 loss: 1.2541073775291443
  batch 550 loss: 1.2651123237609863
  batch 600 loss: 1.2270715403556824
  batch 650 loss: 1.270210723876953
  batch 700 loss: 1.2343780744075774
  batch 750 loss: 1.2236389064788817
  batch 800 loss: 1.246792997121811
  batch 850 loss: 1.2204356575012207
  batch 900 loss: 1.218767682313919
avg val loss: 1.2096377611160278
LOSS train 1.21877 valid 1.20964, valid PER 39.64%
EPOCH 7, Learning Rate: 0.225
  batch 50 loss: 1.2322008788585663
  batch 100 loss: 1.2388113844394684
  batch 150 loss: 1.2096014940738677
  batch 200 loss: 1.1956611120700835
  batch 250 loss: 1.2068975782394409
  batch 300 loss: 1.2007572317123414
  batch 350 loss: 1.208323185443878
  batch 400 loss: 1.233688132762909
  batch 450 loss: 1.1869310283660888
  batch 500 loss: 1.1967035245895385
  batch 550 loss: 1.1882479417324066
  batch 600 loss: 1.221834373474121
  batch 650 loss: 1.1931371355056763
  batch 700 loss: 1.2027843308448791
  batch 750 loss: 1.1904051840305327
  batch 800 loss: 1.1690790152549744
  batch 850 loss: 1.2253417336940766
  batch 900 loss: 1.250130259990692
avg val loss: 1.1813838481903076
LOSS train 1.25013 valid 1.18138, valid PER 38.54%
EPOCH 8, Learning Rate: 0.225
  batch 50 loss: 1.2123152804374695
  batch 100 loss: 1.1849676442146302
  batch 150 loss: 1.165228453874588
  batch 200 loss: 1.1603425562381744
  batch 250 loss: 1.2070259511470796
  batch 300 loss: 1.1560537815093994
  batch 350 loss: 1.2135789263248444
  batch 400 loss: 1.1657865273952484
  batch 450 loss: 1.1815751636028289
  batch 500 loss: 1.2125293242931365
  batch 550 loss: 1.1557562828063965
  batch 600 loss: 1.2017371869087219
  batch 650 loss: 1.237195053100586
  batch 700 loss: 1.1666471612453462
  batch 750 loss: 1.195011339187622
  batch 800 loss: 1.1954632461071015
  batch 850 loss: 1.203834549188614
  batch 900 loss: 1.1664633500576018
avg val loss: 1.1661670207977295
LOSS train 1.16646 valid 1.16617, valid PER 37.49%
EPOCH 9, Learning Rate: 0.225
  batch 50 loss: 1.1210915505886079
  batch 100 loss: 1.1790389502048493
  batch 150 loss: 1.1770768356323242
  batch 200 loss: 1.115627270936966
  batch 250 loss: 1.1557025289535523
  batch 300 loss: 1.154339759349823
  batch 350 loss: 1.1868126559257508
  batch 400 loss: 1.1654554116725921
  batch 450 loss: 1.1857180047035216
  batch 500 loss: 1.1445869088172913
  batch 550 loss: 1.1902273404598236
  batch 600 loss: 1.1777660489082336
  batch 650 loss: 1.1914399588108062
  batch 700 loss: 1.1843677532672883
  batch 750 loss: 1.1820709598064423
  batch 800 loss: 1.1958437550067902
  batch 850 loss: 1.2149974346160888
  batch 900 loss: 1.1685041964054108
avg val loss: 1.1524745225906372
LOSS train 1.16850 valid 1.15247, valid PER 37.11%
EPOCH 10, Learning Rate: 0.225
  batch 50 loss: 1.1207211709022522
  batch 100 loss: 1.1539204072952272
  batch 150 loss: 1.2394852066040039
  batch 200 loss: 1.1804553949832917
  batch 250 loss: 1.1780568623542786
  batch 300 loss: 1.116121257543564
  batch 350 loss: 1.171429625749588
  batch 400 loss: 1.140983464717865
  batch 450 loss: 1.129326583147049
  batch 500 loss: 1.2213620638847351
  batch 550 loss: 1.1826901197433473
  batch 600 loss: 1.1519932174682617
  batch 650 loss: 1.158002141714096
  batch 700 loss: 1.1797705972194672
  batch 750 loss: 1.1639364898204803
  batch 800 loss: 1.1683277320861816
  batch 850 loss: 1.1944442892074585
  batch 900 loss: 1.1855531334877014
avg val loss: 1.1453534364700317
LOSS train 1.18555 valid 1.14535, valid PER 37.38%
EPOCH 11, Learning Rate: 0.225
  batch 50 loss: 1.1356913328170777
  batch 100 loss: 1.1017934334278108
  batch 150 loss: 1.1264527082443236
  batch 200 loss: 1.167402334213257
  batch 250 loss: 1.1787377047538756
  batch 300 loss: 1.1316904926300049
  batch 350 loss: 1.1776709187030792
  batch 400 loss: 1.1675645208358765
  batch 450 loss: 1.1609506797790528
  batch 500 loss: 1.1168545997142791
  batch 550 loss: 1.130710368156433
  batch 600 loss: 1.114654449224472
  batch 650 loss: 1.1943513476848602
  batch 700 loss: 1.0929199540615082
  batch 750 loss: 1.1423545634746552
  batch 800 loss: 1.1809834420681
  batch 850 loss: 1.2000314593315125
  batch 900 loss: 1.1728530097007752
avg val loss: 1.1604788303375244
LOSS train 1.17285 valid 1.16048, valid PER 37.65%
EPOCH 12, Learning Rate: 0.1125
  batch 50 loss: 1.1582154083251952
  batch 100 loss: 1.1206248211860657
  batch 150 loss: 1.0919349265098572
  batch 200 loss: 1.1010766220092774
  batch 250 loss: 1.117427053451538
  batch 300 loss: 1.1014146637916564
  batch 350 loss: 1.1140362775325776
  batch 400 loss: 1.1274136459827424
  batch 450 loss: 1.106106346845627
  batch 500 loss: 1.1322110426425933
  batch 550 loss: 1.0464786577224732
  batch 600 loss: 1.0754709577560424
  batch 650 loss: 1.1137471079826355
  batch 700 loss: 1.0801392436027526
  batch 750 loss: 1.0814431083202363
  batch 800 loss: 1.053354960680008
  batch 850 loss: 1.1101224935054779
  batch 900 loss: 1.1338321518898011
avg val loss: 1.1086074113845825
LOSS train 1.13383 valid 1.10861, valid PER 35.54%
EPOCH 13, Learning Rate: 0.1125
  batch 50 loss: 1.0771312880516053
  batch 100 loss: 1.1214457046985626
  batch 150 loss: 1.0650987422466278
  batch 200 loss: 1.0993234455585479
  batch 250 loss: 1.0746450209617615
  batch 300 loss: 1.0865432298183442
  batch 350 loss: 1.0815247976779938
  batch 400 loss: 1.1126294648647308
  batch 450 loss: 1.111919800043106
  batch 500 loss: 1.0499933540821076
  batch 550 loss: 1.072106510400772
  batch 600 loss: 1.0803213572502137
  batch 650 loss: 1.0784157514572144
  batch 700 loss: 1.0618983864784242
  batch 750 loss: 1.0557212674617766
  batch 800 loss: 1.077339652776718
  batch 850 loss: 1.1271746623516083
  batch 900 loss: 1.1197437524795533
avg val loss: 1.0969606637954712
LOSS train 1.11974 valid 1.09696, valid PER 35.24%
EPOCH 14, Learning Rate: 0.1125
  batch 50 loss: 1.062347081899643
  batch 100 loss: 1.099950934648514
  batch 150 loss: 1.082853789329529
  batch 200 loss: 1.0609858679771422
  batch 250 loss: 1.0683057618141174
  batch 300 loss: 1.0995542907714844
  batch 350 loss: 1.0459213006496428
  batch 400 loss: 1.0527264809608459
  batch 450 loss: 1.050934944152832
  batch 500 loss: 1.0828590595722198
  batch 550 loss: 1.078553742170334
  batch 600 loss: 1.0542937564849852
  batch 650 loss: 1.0862245976924896
  batch 700 loss: 1.0799789583683015
  batch 750 loss: 1.044830206632614
  batch 800 loss: 1.009613834619522
  batch 850 loss: 1.100436087846756
  batch 900 loss: 1.068503783941269
avg val loss: 1.089843511581421
LOSS train 1.06850 valid 1.08984, valid PER 34.86%
EPOCH 15, Learning Rate: 0.1125
  batch 50 loss: 1.0661720061302185
  batch 100 loss: 1.0479411089420319
  batch 150 loss: 1.0547288680076599
  batch 200 loss: 1.0882524573802947
  batch 250 loss: 1.0615414106845855
  batch 300 loss: 1.0594715583324432
  batch 350 loss: 1.0734195935726165
  batch 400 loss: 1.0427903962135314
  batch 450 loss: 1.0393578696250916
  batch 500 loss: 1.0229947280883789
  batch 550 loss: 1.0679576241970061
  batch 600 loss: 1.0634810638427734
  batch 650 loss: 1.076411212682724
  batch 700 loss: 1.0843612301349639
  batch 750 loss: 1.0565757715702058
  batch 800 loss: 1.0471069812774658
  batch 850 loss: 1.0365317368507385
  batch 900 loss: 1.071919059753418
avg val loss: 1.0863600969314575
LOSS train 1.07192 valid 1.08636, valid PER 35.16%
EPOCH 16, Learning Rate: 0.1125
  batch 50 loss: 1.0844738614559173
  batch 100 loss: 1.0241526412963866
  batch 150 loss: 1.0323323607444763
  batch 200 loss: 1.0368552172183991
  batch 250 loss: 1.0748893964290618
  batch 300 loss: 1.0682920706272125
  batch 350 loss: 1.0614666283130645
  batch 400 loss: 1.0459883761405946
  batch 450 loss: 1.0710699462890625
  batch 500 loss: 1.0458152449131013
  batch 550 loss: 1.047292457818985
  batch 600 loss: 1.0636685764789582
  batch 650 loss: 1.0496706223487855
  batch 700 loss: 1.0252183604240417
  batch 750 loss: 1.0307200014591218
  batch 800 loss: 1.0482939326763152
  batch 850 loss: 1.0433981788158417
  batch 900 loss: 1.0486873281002045
avg val loss: 1.0723750591278076
LOSS train 1.04869 valid 1.07238, valid PER 34.09%
EPOCH 17, Learning Rate: 0.1125
  batch 50 loss: 1.0442679369449615
  batch 100 loss: 1.0393341636657716
  batch 150 loss: 1.0238138616085053
  batch 200 loss: 1.0434583926200867
  batch 250 loss: 1.0594173192977905
  batch 300 loss: 1.0659987103939057
  batch 350 loss: 1.0311284399032592
  batch 400 loss: 1.1039688909053802
  batch 450 loss: 1.0658588850498198
  batch 500 loss: 1.0484385013580322
  batch 550 loss: 1.068224046230316
  batch 600 loss: 1.1017106461524964
  batch 650 loss: 1.0601136589050293
  batch 700 loss: 1.0679428255558014
  batch 750 loss: 1.0385708904266358
  batch 800 loss: 1.0295747935771942
  batch 850 loss: 1.0449531185626983
  batch 900 loss: 1.0197567677497863
avg val loss: 1.0791665315628052
LOSS train 1.01976 valid 1.07917, valid PER 34.45%
EPOCH 18, Learning Rate: 0.05625
  batch 50 loss: 1.0361730480194091
  batch 100 loss: 1.04003084897995
  batch 150 loss: 1.0444386208057403
  batch 200 loss: 1.0289705741405486
  batch 250 loss: 1.0418730998039245
  batch 300 loss: 1.0214855802059173
  batch 350 loss: 1.0383601629734038
  batch 400 loss: 0.9988933908939361
  batch 450 loss: 1.0576716220378877
  batch 500 loss: 1.0223501467704772
  batch 550 loss: 1.0059501397609711
  batch 600 loss: 0.9978164780139923
  batch 650 loss: 0.9879277646541595
  batch 700 loss: 1.0523773181438445
  batch 750 loss: 1.008225907087326
  batch 800 loss: 1.0129277062416078
  batch 850 loss: 0.9936110246181488
  batch 900 loss: 1.0292583429813384
avg val loss: 1.0544272661209106
LOSS train 1.02926 valid 1.05443, valid PER 33.72%
EPOCH 19, Learning Rate: 0.05625
  batch 50 loss: 0.9822821164131165
  batch 100 loss: 0.9773340439796447
  batch 150 loss: 0.9964624464511871
  batch 200 loss: 1.017343419790268
  batch 250 loss: 1.0375245416164398
  batch 300 loss: 1.0046849405765534
  batch 350 loss: 0.9909434223175049
  batch 400 loss: 1.0155450224876403
  batch 450 loss: 1.0195428264141082
  batch 500 loss: 1.0368956410884858
  batch 550 loss: 1.0002400803565978
  batch 600 loss: 1.0398765408992767
  batch 650 loss: 1.0456850385665895
  batch 700 loss: 0.9786517119407654
  batch 750 loss: 0.9804192554950714
  batch 800 loss: 1.0294122552871705
  batch 850 loss: 1.0330929815769196
  batch 900 loss: 1.0064014244079589
avg val loss: 1.0517624616622925
LOSS train 1.00640 valid 1.05176, valid PER 33.42%
EPOCH 20, Learning Rate: 0.05625
  batch 50 loss: 0.9898494291305542
  batch 100 loss: 1.0175288140773773
  batch 150 loss: 1.0101907825469971
  batch 200 loss: 0.9970475542545318
  batch 250 loss: 1.0056138491630555
  batch 300 loss: 1.0266877603530884
  batch 350 loss: 0.9770806884765625
  batch 400 loss: 0.9961775755882263
  batch 450 loss: 0.9907512199878693
  batch 500 loss: 0.9995221507549286
  batch 550 loss: 1.048439427614212
  batch 600 loss: 0.9478692126274109
  batch 650 loss: 1.001658856868744
  batch 700 loss: 1.022218768596649
  batch 750 loss: 1.0109070360660553
  batch 800 loss: 1.0241843342781067
  batch 850 loss: 1.0113454401493072
  batch 900 loss: 0.9979791080951691
avg val loss: 1.0474324226379395
LOSS train 0.99798 valid 1.04743, valid PER 33.35%
Training finished in 3.0 minutes.
Model saved to checkpoints/20231210_140806/model_20
Loading model from checkpoints/20231210_140806/model_20
SUB: 19.16%, DEL: 14.07%, INS: 1.38%, COR: 66.77%, PER: 34.60%
