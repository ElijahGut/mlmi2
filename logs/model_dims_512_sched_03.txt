Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=512, concat=1, lr=0.3, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 2240552
EPOCH 1, Learning Rate: 0.3
  batch 50 loss: 5.194853215217591
  batch 100 loss: 3.2680160284042357
  batch 150 loss: 3.041122317314148
  batch 200 loss: 2.850920195579529
  batch 250 loss: 2.7205354976654053
  batch 300 loss: 2.550536093711853
  batch 350 loss: 2.4349607753753664
  batch 400 loss: 2.394649429321289
  batch 450 loss: 2.3187090158462524
  batch 500 loss: 2.228364450931549
  batch 550 loss: 2.1944227766990663
  batch 600 loss: 2.1171198773384092
  batch 650 loss: 2.0494754099845887
  batch 700 loss: 2.0508019614219664
  batch 750 loss: 1.9857472825050353
  batch 800 loss: 1.958349039554596
  batch 850 loss: 1.93076993227005
  batch 900 loss: 1.9076837682724
avg val loss: 1.8727130889892578
LOSS train 1.90768 valid 1.87271, valid PER 65.92%
EPOCH 2, Learning Rate: 0.3
  batch 50 loss: 1.8700936102867127
  batch 100 loss: 1.8040597534179688
  batch 150 loss: 1.786462218761444
  batch 200 loss: 1.7821987342834473
  batch 250 loss: 1.7755286955833436
  batch 300 loss: 1.7375808668136596
  batch 350 loss: 1.653835995197296
  batch 400 loss: 1.6642237997055054
  batch 450 loss: 1.6341800260543824
  batch 500 loss: 1.6637388634681702
  batch 550 loss: 1.6674656796455383
  batch 600 loss: 1.5963525867462158
  batch 650 loss: 1.6195533466339112
  batch 700 loss: 1.596823534965515
  batch 750 loss: 1.5867745208740234
  batch 800 loss: 1.5291719794273377
  batch 850 loss: 1.5182191610336304
  batch 900 loss: 1.5565101242065429
avg val loss: 1.4823790788650513
LOSS train 1.55651 valid 1.48238, valid PER 49.15%
EPOCH 3, Learning Rate: 0.3
  batch 50 loss: 1.48851895570755
  batch 100 loss: 1.4765676021575929
  batch 150 loss: 1.4776820921897889
  batch 200 loss: 1.4475339651107788
  batch 250 loss: 1.4283209943771362
  batch 300 loss: 1.4250970220565795
  batch 350 loss: 1.4593150401115418
  batch 400 loss: 1.441863281726837
  batch 450 loss: 1.4007156896591186
  batch 500 loss: 1.393615415096283
  batch 550 loss: 1.3970986795425415
  batch 600 loss: 1.368582614660263
  batch 650 loss: 1.344789047241211
  batch 700 loss: 1.3645345520973207
  batch 750 loss: 1.4159567749500275
  batch 800 loss: 1.3413163113594055
  batch 850 loss: 1.3485750508308412
  batch 900 loss: 1.2928084826469421
avg val loss: 1.3151026964187622
LOSS train 1.29281 valid 1.31510, valid PER 41.23%
EPOCH 4, Learning Rate: 0.3
  batch 50 loss: 1.2955168795585632
  batch 100 loss: 1.3204559063911439
  batch 150 loss: 1.258319160938263
  batch 200 loss: 1.2979109215736389
  batch 250 loss: 1.3094393610954285
  batch 300 loss: 1.2857342839241028
  batch 350 loss: 1.2312817037105561
  batch 400 loss: 1.2668912088871003
  batch 450 loss: 1.2602673149108887
  batch 500 loss: 1.237324538230896
  batch 550 loss: 1.245425614118576
  batch 600 loss: 1.2638562524318695
  batch 650 loss: 1.2435232996940613
  batch 700 loss: 1.20900656580925
  batch 750 loss: 1.2025370109081268
  batch 800 loss: 1.1539228582382202
  batch 850 loss: 1.2180407023429871
  batch 900 loss: 1.2385512804985046
avg val loss: 1.180592656135559
LOSS train 1.23855 valid 1.18059, valid PER 37.08%
EPOCH 5, Learning Rate: 0.3
  batch 50 loss: 1.1647598052024841
  batch 100 loss: 1.1543392884731292
  batch 150 loss: 1.2063023591041564
  batch 200 loss: 1.126048994064331
  batch 250 loss: 1.139408802986145
  batch 300 loss: 1.1516180193424226
  batch 350 loss: 1.1368110799789428
  batch 400 loss: 1.1557066857814788
  batch 450 loss: 1.1401003515720367
  batch 500 loss: 1.1456518399715423
  batch 550 loss: 1.1090000915527343
  batch 600 loss: 1.1836772096157073
  batch 650 loss: 1.1184295833110809
  batch 700 loss: 1.1631606948375701
  batch 750 loss: 1.086805638074875
  batch 800 loss: 1.122642228603363
  batch 850 loss: 1.1041500234603883
  batch 900 loss: 1.1310636043548583
avg val loss: 1.0985556840896606
LOSS train 1.13106 valid 1.09856, valid PER 33.90%
EPOCH 6, Learning Rate: 0.3
  batch 50 loss: 1.1096911573410033
  batch 100 loss: 1.0567310059070587
  batch 150 loss: 1.0423809885978699
  batch 200 loss: 1.071691436767578
  batch 250 loss: 1.0998815977573395
  batch 300 loss: 1.0813776862621307
  batch 350 loss: 1.0567336916923522
  batch 400 loss: 1.0616459012031556
  batch 450 loss: 1.0791274893283844
  batch 500 loss: 1.0744232952594757
  batch 550 loss: 1.081047500371933
  batch 600 loss: 1.0431187987327575
  batch 650 loss: 1.0704622626304627
  batch 700 loss: 1.054277640581131
  batch 750 loss: 1.030949386358261
  batch 800 loss: 1.0452579033374787
  batch 850 loss: 1.031500790119171
  batch 900 loss: 1.0611983847618103
avg val loss: 1.0665435791015625
LOSS train 1.06120 valid 1.06654, valid PER 33.94%
EPOCH 7, Learning Rate: 0.3
  batch 50 loss: 1.0249522173404693
  batch 100 loss: 0.9996447908878326
  batch 150 loss: 1.0135581517219543
  batch 200 loss: 0.9952217268943787
  batch 250 loss: 0.9872397303581237
  batch 300 loss: 0.970568699836731
  batch 350 loss: 0.995526955127716
  batch 400 loss: 0.995810672044754
  batch 450 loss: 1.017102016210556
  batch 500 loss: 0.9957357954978943
  batch 550 loss: 0.989630469083786
  batch 600 loss: 0.9917164313793182
  batch 650 loss: 0.9871771001815796
  batch 700 loss: 1.0040810799598694
  batch 750 loss: 0.9664660596847534
  batch 800 loss: 0.978453620672226
  batch 850 loss: 1.0047208321094514
  batch 900 loss: 1.017663027048111
avg val loss: 1.027006983757019
LOSS train 1.01766 valid 1.02701, valid PER 32.33%
EPOCH 8, Learning Rate: 0.3
  batch 50 loss: 0.9410219264030456
  batch 100 loss: 0.9279129803180695
  batch 150 loss: 0.9347518563270569
  batch 200 loss: 0.9206910240650177
  batch 250 loss: 0.9452777040004731
  batch 300 loss: 0.8832835555076599
  batch 350 loss: 0.9697825384140014
  batch 400 loss: 0.9261103320121765
  batch 450 loss: 0.9444985747337341
  batch 500 loss: 0.9740956163406372
  batch 550 loss: 0.9094203054904938
  batch 600 loss: 0.9555169475078583
  batch 650 loss: 0.9768705689907073
  batch 700 loss: 0.9257503187656403
  batch 750 loss: 0.9231030058860779
  batch 800 loss: 0.9500022184848785
  batch 850 loss: 0.926836154460907
  batch 900 loss: 0.9255298852920533
avg val loss: 0.9784396290779114
LOSS train 0.92553 valid 0.97844, valid PER 30.36%
EPOCH 9, Learning Rate: 0.3
  batch 50 loss: 0.8418064713478088
  batch 100 loss: 0.906411737203598
  batch 150 loss: 0.8866382539272308
  batch 200 loss: 0.8671070218086243
  batch 250 loss: 0.9001048254966736
  batch 300 loss: 0.88902951836586
  batch 350 loss: 0.9157727348804474
  batch 400 loss: 0.9101955449581146
  batch 450 loss: 0.8814896929264069
  batch 500 loss: 0.8738613867759705
  batch 550 loss: 0.9144518530368805
  batch 600 loss: 0.916942744255066
  batch 650 loss: 0.8836428689956665
  batch 700 loss: 0.8585139238834381
  batch 750 loss: 0.8693517076969147
  batch 800 loss: 0.8867237496376038
  batch 850 loss: 0.9116013014316559
  batch 900 loss: 0.8585975301265717
avg val loss: 0.9564662575721741
LOSS train 0.85860 valid 0.95647, valid PER 30.06%
EPOCH 10, Learning Rate: 0.3
  batch 50 loss: 0.7899660801887513
  batch 100 loss: 0.8243393278121949
  batch 150 loss: 0.8392424207925796
  batch 200 loss: 0.8633640909194946
  batch 250 loss: 0.8453456091880799
  batch 300 loss: 0.825174081325531
  batch 350 loss: 0.8550434005260468
  batch 400 loss: 0.8067722374200821
  batch 450 loss: 0.8111499750614166
  batch 500 loss: 0.8541696560382843
  batch 550 loss: 0.8673721432685852
  batch 600 loss: 0.8400041460990906
  batch 650 loss: 0.8298760044574738
  batch 700 loss: 0.8487975871562958
  batch 750 loss: 0.8225013816356659
  batch 800 loss: 0.8639371740818024
  batch 850 loss: 0.848675264120102
  batch 900 loss: 0.8442767870426178
avg val loss: 0.9616588950157166
LOSS train 0.84428 valid 0.96166, valid PER 30.92%
EPOCH 11, Learning Rate: 0.15
  batch 50 loss: 0.7421118998527527
  batch 100 loss: 0.7101482629776001
  batch 150 loss: 0.7090202295780181
  batch 200 loss: 0.7668378454446793
  batch 250 loss: 0.7440828061103821
  batch 300 loss: 0.7013509845733643
  batch 350 loss: 0.7243733495473862
  batch 400 loss: 0.7359109574556351
  batch 450 loss: 0.734782201051712
  batch 500 loss: 0.7181016635894776
  batch 550 loss: 0.7250455421209335
  batch 600 loss: 0.7072450226545334
  batch 650 loss: 0.7653991520404816
  batch 700 loss: 0.6994768244028091
  batch 750 loss: 0.7233486503362656
  batch 800 loss: 0.7413041341304779
  batch 850 loss: 0.7646653127670288
  batch 900 loss: 0.7486052405834198
avg val loss: 0.8761729001998901
LOSS train 0.74861 valid 0.87617, valid PER 27.28%
EPOCH 12, Learning Rate: 0.15
  batch 50 loss: 0.6916252100467681
  batch 100 loss: 0.677161962389946
  batch 150 loss: 0.6501387673616409
  batch 200 loss: 0.6917121243476868
  batch 250 loss: 0.7024173992872238
  batch 300 loss: 0.679314308166504
  batch 350 loss: 0.6791916185617447
  batch 400 loss: 0.6863090324401856
  batch 450 loss: 0.7086658871173859
  batch 500 loss: 0.7018040049076081
  batch 550 loss: 0.6592083978652954
  batch 600 loss: 0.6893508046865463
  batch 650 loss: 0.7205664193630219
  batch 700 loss: 0.7167465031147003
  batch 750 loss: 0.6857335162162781
  batch 800 loss: 0.694207563996315
  batch 850 loss: 0.7504437708854675
  batch 900 loss: 0.7385039961338044
avg val loss: 0.8842389583587646
LOSS train 0.73850 valid 0.88424, valid PER 27.40%
EPOCH 13, Learning Rate: 0.075
  batch 50 loss: 0.6315854269266129
  batch 100 loss: 0.6359865874052048
  batch 150 loss: 0.6155278146266937
  batch 200 loss: 0.6491721504926682
  batch 250 loss: 0.6185531628131866
  batch 300 loss: 0.6061835736036301
  batch 350 loss: 0.6238474172353744
  batch 400 loss: 0.6400159722566605
  batch 450 loss: 0.6282495355606079
  batch 500 loss: 0.614830977320671
  batch 550 loss: 0.6406734102964401
  batch 600 loss: 0.6220653522014618
  batch 650 loss: 0.6417169564962387
  batch 700 loss: 0.6512989473342895
  batch 750 loss: 0.6045501577854157
  batch 800 loss: 0.6276330542564392
  batch 850 loss: 0.6527720236778259
  batch 900 loss: 0.647781503200531
avg val loss: 0.8614604473114014
LOSS train 0.64778 valid 0.86146, valid PER 26.01%
EPOCH 14, Learning Rate: 0.075
  batch 50 loss: 0.5961420500278473
  batch 100 loss: 0.5940133965015412
  batch 150 loss: 0.5997435301542282
  batch 200 loss: 0.5958846855163574
  batch 250 loss: 0.6125296145677567
  batch 300 loss: 0.6387553256750107
  batch 350 loss: 0.5779859882593155
  batch 400 loss: 0.6019579011201859
  batch 450 loss: 0.6036411625146866
  batch 500 loss: 0.6256343752145768
  batch 550 loss: 0.6259619981050492
  batch 600 loss: 0.5801325744390488
  batch 650 loss: 0.6079752594232559
  batch 700 loss: 0.6430888772010803
  batch 750 loss: 0.5987189722061157
  batch 800 loss: 0.5798176050186157
  batch 850 loss: 0.6175633591413497
  batch 900 loss: 0.6233645433187485
avg val loss: 0.8751997351646423
LOSS train 0.62336 valid 0.87520, valid PER 26.10%
EPOCH 15, Learning Rate: 0.0375
  batch 50 loss: 0.580274059176445
  batch 100 loss: 0.5657630747556687
  batch 150 loss: 0.5758513331413269
  batch 200 loss: 0.5905621278285981
  batch 250 loss: 0.5964774161577224
  batch 300 loss: 0.5671214210987091
  batch 350 loss: 0.5742393893003463
  batch 400 loss: 0.566432872414589
  batch 450 loss: 0.5568557929992676
  batch 500 loss: 0.5402475011348724
  batch 550 loss: 0.5651213181018829
  batch 600 loss: 0.5817386931180955
  batch 650 loss: 0.5845089823007583
  batch 700 loss: 0.5961252862215042
  batch 750 loss: 0.5751562362909317
  batch 800 loss: 0.5567918479442596
  batch 850 loss: 0.5549490839242935
  batch 900 loss: 0.5754861414432526
avg val loss: 0.8672159910202026
LOSS train 0.57549 valid 0.86722, valid PER 25.74%
EPOCH 16, Learning Rate: 0.01875
  batch 50 loss: 0.5639120405912399
  batch 100 loss: 0.5309913259744644
  batch 150 loss: 0.547617147564888
  batch 200 loss: 0.5436389148235321
  batch 250 loss: 0.5634497326612472
  batch 300 loss: 0.5461425256729125
  batch 350 loss: 0.5538986420631409
  batch 400 loss: 0.5695763152837753
  batch 450 loss: 0.5633387106657028
  batch 500 loss: 0.528035044670105
  batch 550 loss: 0.5315885108709335
  batch 600 loss: 0.5389067375659943
  batch 650 loss: 0.5656973445415496
  batch 700 loss: 0.5415319460630417
  batch 750 loss: 0.5518666619062423
  batch 800 loss: 0.5560840505361557
  batch 850 loss: 0.5483703094720841
  batch 900 loss: 0.5517444616556167
avg val loss: 0.8683807253837585
LOSS train 0.55174 valid 0.86838, valid PER 25.55%
EPOCH 17, Learning Rate: 0.009375
  batch 50 loss: 0.558475067615509
  batch 100 loss: 0.5534846854209899
  batch 150 loss: 0.5390175187587738
  batch 200 loss: 0.5257440614700317
  batch 250 loss: 0.5722539329528809
  batch 300 loss: 0.5382334488630295
  batch 350 loss: 0.5219018763303757
  batch 400 loss: 0.5671437960863114
  batch 450 loss: 0.5521977818012238
  batch 500 loss: 0.5127812933921814
  batch 550 loss: 0.5244231569766998
  batch 600 loss: 0.5545461344718933
  batch 650 loss: 0.5300558823347091
  batch 700 loss: 0.5328099584579468
  batch 750 loss: 0.5291592347621917
  batch 800 loss: 0.5173419284820556
  batch 850 loss: 0.5571741443872452
  batch 900 loss: 0.5244483816623687
avg val loss: 0.867931604385376
LOSS train 0.52445 valid 0.86793, valid PER 25.52%
EPOCH 18, Learning Rate: 0.0046875
  batch 50 loss: 0.5296898370981217
  batch 100 loss: 0.5395174324512482
  batch 150 loss: 0.5659074038267136
  batch 200 loss: 0.5370410633087158
  batch 250 loss: 0.547626427412033
  batch 300 loss: 0.5213803493976593
  batch 350 loss: 0.5329147297143936
  batch 400 loss: 0.5260571438074112
  batch 450 loss: 0.5481528300046921
  batch 500 loss: 0.5289125508069992
  batch 550 loss: 0.5484949773550034
  batch 600 loss: 0.5272235226631165
  batch 650 loss: 0.5155169767141342
  batch 700 loss: 0.5371169751882553
  batch 750 loss: 0.5325875109434128
  batch 800 loss: 0.5299775671958923
  batch 850 loss: 0.5163464432954789
  batch 900 loss: 0.5320366597175599
avg val loss: 0.8685500621795654
LOSS train 0.53204 valid 0.86855, valid PER 25.45%
EPOCH 19, Learning Rate: 0.00234375
  batch 50 loss: 0.5271133929491043
  batch 100 loss: 0.517358751296997
  batch 150 loss: 0.5292312264442444
  batch 200 loss: 0.5411294656991958
  batch 250 loss: 0.5330617600679397
  batch 300 loss: 0.5422462904453278
  batch 350 loss: 0.5244033509492874
  batch 400 loss: 0.5329315823316574
  batch 450 loss: 0.5457752472162247
  batch 500 loss: 0.5213967365026474
  batch 550 loss: 0.5105864042043686
  batch 600 loss: 0.5231994897127151
  batch 650 loss: 0.5571852958202362
  batch 700 loss: 0.5197761976718902
  batch 750 loss: 0.5088764524459839
  batch 800 loss: 0.5271111357212067
  batch 850 loss: 0.5363091397285461
  batch 900 loss: 0.526620871424675
avg val loss: 0.8692227005958557
LOSS train 0.52662 valid 0.86922, valid PER 25.46%
EPOCH 20, Learning Rate: 0.001171875
  batch 50 loss: 0.5425797468423843
  batch 100 loss: 0.5258328396081925
  batch 150 loss: 0.5191243904829025
  batch 200 loss: 0.5295324277877808
  batch 250 loss: 0.522834906578064
  batch 300 loss: 0.5383068299293519
  batch 350 loss: 0.5012404626607895
  batch 400 loss: 0.5345705974102021
  batch 450 loss: 0.5282602870464325
  batch 500 loss: 0.5054862588644028
  batch 550 loss: 0.5599614214897156
  batch 600 loss: 0.5098286956548691
  batch 650 loss: 0.5273434448242188
  batch 700 loss: 0.5328425812721252
  batch 750 loss: 0.5162471842765808
  batch 800 loss: 0.5501211500167846
  batch 850 loss: 0.5270459479093552
  batch 900 loss: 0.521166381239891
avg val loss: 0.8694327473640442
LOSS train 0.52117 valid 0.86943, valid PER 25.48%
Training finished in 8.0 minutes.
Model saved to checkpoints/20231210_135512/model_13
Loading model from checkpoints/20231210_135512/model_13
SUB: 14.40%, DEL: 11.61%, INS: 2.21%, COR: 73.99%, PER: 28.22%
