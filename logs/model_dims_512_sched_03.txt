Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=512, concat=1, lr=0.3, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 2240552
EPOCH 1, Learning Rate: 0.3
  batch 50 loss: 5.194857048988342
  batch 100 loss: 3.2680176973342894
  batch 150 loss: 3.041104865074158
  batch 200 loss: 2.8508993434906005
  batch 250 loss: 2.7205138492584227
  batch 300 loss: 2.5505188941955566
  batch 350 loss: 2.434948196411133
  batch 400 loss: 2.3946392345428467
  batch 450 loss: 2.3186961078643797
  batch 500 loss: 2.2283563470840453
  batch 550 loss: 2.194414255619049
  batch 600 loss: 2.117110137939453
  batch 650 loss: 2.049466972351074
  batch 700 loss: 2.0507928228378294
  batch 750 loss: 1.9857420086860658
  batch 800 loss: 1.9583415150642396
  batch 850 loss: 1.9307591891288758
  batch 900 loss: 1.9076774668693544
running loss: 44.40842008590698
LOSS train 1.90768 valid 1.87269, valid PER 65.92%
EPOCH 2, Learning Rate: 0.3
  batch 50 loss: 1.8700859999656678
  batch 100 loss: 1.8040506076812743
  batch 150 loss: 1.7864565563201904
  batch 200 loss: 1.7821961951255798
  batch 250 loss: 1.7755219531059265
  batch 300 loss: 1.7375737404823304
  batch 350 loss: 1.6538238501548768
  batch 400 loss: 1.6642094802856446
  batch 450 loss: 1.6341773056983948
  batch 500 loss: 1.663729510307312
  batch 550 loss: 1.6673922324180603
  batch 600 loss: 1.5963716697692871
  batch 650 loss: 1.619463849067688
  batch 700 loss: 1.5968665862083435
  batch 750 loss: 1.5869087290763855
  batch 800 loss: 1.5293481731414795
  batch 850 loss: 1.5182563090324401
  batch 900 loss: 1.5561837005615233
running loss: 35.88784682750702
LOSS train 1.55618 valid 1.48211, valid PER 49.08%
EPOCH 3, Learning Rate: 0.3
  batch 50 loss: 1.4884216713905334
  batch 100 loss: 1.476509585380554
  batch 150 loss: 1.4779207611083984
  batch 200 loss: 1.447673227787018
  batch 250 loss: 1.428411500453949
  batch 300 loss: 1.4250419116020203
  batch 350 loss: 1.4588068771362304
  batch 400 loss: 1.4423341679573058
  batch 450 loss: 1.4005964732170104
  batch 500 loss: 1.3928374433517456
  batch 550 loss: 1.3974924397468567
  batch 600 loss: 1.3695332932472228
  batch 650 loss: 1.344896550178528
  batch 700 loss: 1.3643225526809692
  batch 750 loss: 1.4161796903610229
  batch 800 loss: 1.3424313259124756
  batch 850 loss: 1.3487596917152405
  batch 900 loss: 1.2915558552742004
running loss: 30.83682084083557
LOSS train 1.29156 valid 1.31543, valid PER 41.09%
EPOCH 4, Learning Rate: 0.3
  batch 50 loss: 1.2954901909828187
  batch 100 loss: 1.3210487127304078
  batch 150 loss: 1.2585263848304749
  batch 200 loss: 1.2985704374313354
  batch 250 loss: 1.3093106770515441
  batch 300 loss: 1.2852377343177794
  batch 350 loss: 1.232498950958252
  batch 400 loss: 1.2658546662330628
  batch 450 loss: 1.2603119277954102
  batch 500 loss: 1.2368899810314178
  batch 550 loss: 1.2442410480976105
  batch 600 loss: 1.2653407263755798
  batch 650 loss: 1.2424666142463685
  batch 700 loss: 1.2106351459026337
  batch 750 loss: 1.2019148540496827
  batch 800 loss: 1.1529718351364135
  batch 850 loss: 1.2178922522068023
  batch 900 loss: 1.2391005766391754
running loss: 28.689216256141663
LOSS train 1.23910 valid 1.17847, valid PER 37.08%
EPOCH 5, Learning Rate: 0.3
  batch 50 loss: 1.164084665775299
  batch 100 loss: 1.1556663513183594
  batch 150 loss: 1.2054233574867248
  batch 200 loss: 1.1250268352031707
  batch 250 loss: 1.1380690431594849
  batch 300 loss: 1.1516147541999817
  batch 350 loss: 1.1367419350147248
  batch 400 loss: 1.1558559155464172
  batch 450 loss: 1.1390977716445922
  batch 500 loss: 1.1453177762031554
  batch 550 loss: 1.1076972460746766
  batch 600 loss: 1.183168979883194
  batch 650 loss: 1.1186811435222626
  batch 700 loss: 1.1610246253013612
  batch 750 loss: 1.0862449765205384
  batch 800 loss: 1.1247901141643524
  batch 850 loss: 1.1040455436706542
  batch 900 loss: 1.1302300763130189
running loss: 26.35312908887863
LOSS train 1.13023 valid 1.09904, valid PER 34.04%
EPOCH 6, Learning Rate: 0.3
  batch 50 loss: 1.1097105276584625
  batch 100 loss: 1.0561725831031799
  batch 150 loss: 1.042765394449234
  batch 200 loss: 1.0728798234462738
  batch 250 loss: 1.1002361750602723
  batch 300 loss: 1.082565016746521
  batch 350 loss: 1.0580177533626556
  batch 400 loss: 1.0609540522098542
  batch 450 loss: 1.0791880416870117
  batch 500 loss: 1.0734650635719298
  batch 550 loss: 1.0810805940628052
  batch 600 loss: 1.0401363205909728
  batch 650 loss: 1.0706631970405578
  batch 700 loss: 1.0588899743556976
  batch 750 loss: 1.0327933514118195
  batch 800 loss: 1.042892736196518
  batch 850 loss: 1.03055570602417
  batch 900 loss: 1.0626116442680358
running loss: 24.722807586193085
LOSS train 1.06261 valid 1.06973, valid PER 33.78%
EPOCH 7, Learning Rate: 0.3
  batch 50 loss: 1.0212854325771332
  batch 100 loss: 1.002409461736679
  batch 150 loss: 1.0132750940322877
  batch 200 loss: 0.9939813387393951
  batch 250 loss: 0.9893365752696991
  batch 300 loss: 0.96975621342659
  batch 350 loss: 0.9993192338943482
  batch 400 loss: 0.9955300033092499
  batch 450 loss: 1.017634037733078
  batch 500 loss: 0.9938324177265168
  batch 550 loss: 0.9881561064720153
  batch 600 loss: 0.9942965590953827
  batch 650 loss: 0.9872319269180297
  batch 700 loss: 1.0050146722793578
  batch 750 loss: 0.9653860342502594
  batch 800 loss: 0.9789058482646942
  batch 850 loss: 1.0003665339946748
  batch 900 loss: 1.0166522324085236
running loss: 22.380712807178497
LOSS train 1.01665 valid 1.02951, valid PER 32.47%
EPOCH 8, Learning Rate: 0.3
  batch 50 loss: 0.9419437003135681
  batch 100 loss: 0.9274224865436554
  batch 150 loss: 0.930447188615799
  batch 200 loss: 0.9222291851043701
  batch 250 loss: 0.9463589346408844
  batch 300 loss: 0.8814070379734039
  batch 350 loss: 0.9681422567367554
  batch 400 loss: 0.9235756874084473
  batch 450 loss: 0.944525672197342
  batch 500 loss: 0.9770670783519745
  batch 550 loss: 0.9137151408195495
  batch 600 loss: 0.9543936538696289
  batch 650 loss: 0.9759377789497375
  batch 700 loss: 0.9239999604225159
  batch 750 loss: 0.9217509055137634
  batch 800 loss: 0.9497661328315735
  batch 850 loss: 0.9260026919841766
  batch 900 loss: 0.9268852066993714
running loss: 21.91321712732315
LOSS train 0.92689 valid 0.97795, valid PER 30.12%
EPOCH 9, Learning Rate: 0.3
  batch 50 loss: 0.8441274440288544
  batch 100 loss: 0.9044044768810272
  batch 150 loss: 0.8846720671653747
  batch 200 loss: 0.8687518036365509
  batch 250 loss: 0.9010396921634674
  batch 300 loss: 0.8899127531051636
  batch 350 loss: 0.9135957443714142
  batch 400 loss: 0.9113895463943481
  batch 450 loss: 0.8852698230743408
  batch 500 loss: 0.8729243683815002
  batch 550 loss: 0.9150057280063629
  batch 600 loss: 0.914185528755188
  batch 650 loss: 0.8832113671302796
  batch 700 loss: 0.8562720537185669
  batch 750 loss: 0.8683871638774872
  batch 800 loss: 0.8869108641147614
  batch 850 loss: 0.9089911460876465
  batch 900 loss: 0.8567050528526307
running loss: 21.96781486272812
LOSS train 0.85671 valid 0.95416, valid PER 29.60%
EPOCH 10, Learning Rate: 0.15
  batch 50 loss: 0.7623425424098969
  batch 100 loss: 0.7764864110946655
  batch 150 loss: 0.7843245548009873
  batch 200 loss: 0.8026402449607849
  batch 250 loss: 0.7826814603805542
  batch 300 loss: 0.7572238981723786
  batch 350 loss: 0.783489248752594
  batch 400 loss: 0.7351732552051544
  batch 450 loss: 0.7449205511808396
  batch 500 loss: 0.783956972360611
  batch 550 loss: 0.793112394809723
  batch 600 loss: 0.7703442645072937
  batch 650 loss: 0.7560610151290894
  batch 700 loss: 0.7807634902000428
  batch 750 loss: 0.7550035905838013
  batch 800 loss: 0.7895557522773743
  batch 850 loss: 0.7759630179405212
  batch 900 loss: 0.7806251895427704
running loss: 18.537364065647125
LOSS train 0.78063 valid 0.90815, valid PER 29.04%
EPOCH 11, Learning Rate: 0.15
  batch 50 loss: 0.7186247372627258
  batch 100 loss: 0.7036252558231354
  batch 150 loss: 0.7110179960727692
  batch 200 loss: 0.7738201105594635
  batch 250 loss: 0.7479666149616242
  batch 300 loss: 0.7057309728860856
  batch 350 loss: 0.7311658096313477
  batch 400 loss: 0.7448181200027466
  batch 450 loss: 0.7458613407611847
  batch 500 loss: 0.7286004000902175
  batch 550 loss: 0.7323061221837998
  batch 600 loss: 0.7205090284347534
  batch 650 loss: 0.7812823247909546
  batch 700 loss: 0.7096916019916535
  batch 750 loss: 0.7349880677461624
  batch 800 loss: 0.7569783139228821
  batch 850 loss: 0.7792399740219116
  batch 900 loss: 0.7612153053283691
running loss: 17.743545472621918
LOSS train 0.76122 valid 0.88381, valid PER 27.58%
EPOCH 12, Learning Rate: 0.15
  batch 50 loss: 0.708298129439354
  batch 100 loss: 0.6928173768520355
  batch 150 loss: 0.6647107863426208
  batch 200 loss: 0.7035934889316559
  batch 250 loss: 0.7185723441839218
  batch 300 loss: 0.6908532285690308
  batch 350 loss: 0.6911453849077225
  batch 400 loss: 0.7064812380075455
  batch 450 loss: 0.7224842250347138
  batch 500 loss: 0.7194093215465546
  batch 550 loss: 0.671098421216011
  batch 600 loss: 0.703575792312622
  batch 650 loss: 0.7353969097137452
  batch 700 loss: 0.7310220068693161
  batch 750 loss: 0.699676508307457
  batch 800 loss: 0.7108236485719681
  batch 850 loss: 0.7651494234800339
  batch 900 loss: 0.7525429546833038
running loss: 17.220728754997253
LOSS train 0.75254 valid 0.89275, valid PER 27.73%
EPOCH 13, Learning Rate: 0.15
  batch 50 loss: 0.6590198403596879
  batch 100 loss: 0.6688124841451645
  batch 150 loss: 0.6486421078443527
  batch 200 loss: 0.694306508898735
  batch 250 loss: 0.6670157724618911
  batch 300 loss: 0.6514630961418152
  batch 350 loss: 0.673834216594696
  batch 400 loss: 0.6892933112382889
  batch 450 loss: 0.6808787614107132
  batch 500 loss: 0.6681688553094864
  batch 550 loss: 0.6983055454492569
  batch 600 loss: 0.6762561917304992
  batch 650 loss: 0.6996141409873963
  batch 700 loss: 0.7079817605018616
  batch 750 loss: 0.6604679501056672
  batch 800 loss: 0.6842705857753754
  batch 850 loss: 0.7104756367206574
  batch 900 loss: 0.7062835532426834
running loss: 17.52875417470932
LOSS train 0.70628 valid 0.88753, valid PER 27.06%
EPOCH 14, Learning Rate: 0.075
  batch 50 loss: 0.6191032385826111
  batch 100 loss: 0.609933180809021
  batch 150 loss: 0.6149331039190292
  batch 200 loss: 0.6060150635242462
  batch 250 loss: 0.6193389636278153
  batch 300 loss: 0.6444597119092941
  batch 350 loss: 0.5824437975883484
  batch 400 loss: 0.6090431421995163
  batch 450 loss: 0.6094139271974564
  batch 500 loss: 0.6265679365396499
  batch 550 loss: 0.6321470534801483
  batch 600 loss: 0.5854717946052551
  batch 650 loss: 0.6132743501663208
  batch 700 loss: 0.6498127955198288
  batch 750 loss: 0.599097370505333
  batch 800 loss: 0.5848368793725968
  batch 850 loss: 0.6211379086971283
  batch 900 loss: 0.6273706495761872
running loss: 15.462510406970978
LOSS train 0.62737 valid 0.87970, valid PER 26.31%
EPOCH 15, Learning Rate: 0.075
  batch 50 loss: 0.5900121343135833
  batch 100 loss: 0.5818715745210647
  batch 150 loss: 0.5918557184934616
  batch 200 loss: 0.6019710046052933
  batch 250 loss: 0.6166941010951996
  batch 300 loss: 0.5828766983747482
  batch 350 loss: 0.5935119551420212
  batch 400 loss: 0.5871908390522003
  batch 450 loss: 0.5782384353876114
  batch 500 loss: 0.5605293977260589
  batch 550 loss: 0.5898879033327102
  batch 600 loss: 0.6035060840845108
  batch 650 loss: 0.6071678411960602
  batch 700 loss: 0.6138685774803162
  batch 750 loss: 0.60213616669178
  batch 800 loss: 0.5808027613162995
  batch 850 loss: 0.5803098219633103
  batch 900 loss: 0.6009006750583649
running loss: 13.77375140786171
LOSS train 0.60090 valid 0.87849, valid PER 26.00%
EPOCH 16, Learning Rate: 0.075
  batch 50 loss: 0.5748707562685013
  batch 100 loss: 0.5440758085250854
  batch 150 loss: 0.5592185837030411
  batch 200 loss: 0.5633015561103821
  batch 250 loss: 0.5843772339820862
  batch 300 loss: 0.5694147223234176
  batch 350 loss: 0.5758375430107117
  batch 400 loss: 0.5945247554779053
  batch 450 loss: 0.5897362858057023
  batch 500 loss: 0.552272049188614
  batch 550 loss: 0.5572493076324463
  batch 600 loss: 0.5628919500112534
  batch 650 loss: 0.5894860619306564
  batch 700 loss: 0.5712246334552765
  batch 750 loss: 0.579533925652504
  batch 800 loss: 0.5874657243490219
  batch 850 loss: 0.5776757740974426
  batch 900 loss: 0.5780590981245041
running loss: 13.626149594783783
LOSS train 0.57806 valid 0.88599, valid PER 25.91%
EPOCH 17, Learning Rate: 0.075
  batch 50 loss: 0.5583592468500137
  batch 100 loss: 0.5583385354280472
  batch 150 loss: 0.5435536098480225
  batch 200 loss: 0.5400471830368042
  batch 250 loss: 0.5880370795726776
  batch 300 loss: 0.5553777199983597
  batch 350 loss: 0.5360952723026275
  batch 400 loss: 0.5843991827964783
  batch 450 loss: 0.5668187707662582
  batch 500 loss: 0.5318518543243408
  batch 550 loss: 0.5448707830905914
  batch 600 loss: 0.5813535481691361
  batch 650 loss: 0.5483959770202637
  batch 700 loss: 0.5542346405982971
  batch 750 loss: 0.5475487196445465
  batch 800 loss: 0.5380780988931656
  batch 850 loss: 0.5806201839447022
  batch 900 loss: 0.5470327311754226
running loss: 14.164858251810074
LOSS train 0.54703 valid 0.89351, valid PER 25.94%
EPOCH 18, Learning Rate: 0.0375
  batch 50 loss: 0.5122043591737747
  batch 100 loss: 0.5227329248189926
  batch 150 loss: 0.5512025099992752
  batch 200 loss: 0.518515065908432
  batch 250 loss: 0.5291658240556717
  batch 300 loss: 0.5060442274808884
  batch 350 loss: 0.5118291044235229
  batch 400 loss: 0.5155141526460647
  batch 450 loss: 0.5357047790288925
  batch 500 loss: 0.5158829218149186
  batch 550 loss: 0.5315612655878067
  batch 600 loss: 0.513982806801796
  batch 650 loss: 0.5003511381149292
  batch 700 loss: 0.5215528464317322
  batch 750 loss: 0.517865104675293
  batch 800 loss: 0.5174652653932571
  batch 850 loss: 0.5035871487855911
  batch 900 loss: 0.5193765139579773
running loss: 12.271303653717041
LOSS train 0.51938 valid 0.88964, valid PER 25.75%
EPOCH 19, Learning Rate: 0.0375
  batch 50 loss: 0.49460576474666595
  batch 100 loss: 0.4892177766561508
  batch 150 loss: 0.49432960510253904
  batch 200 loss: 0.5136352813243866
  batch 250 loss: 0.5053889852762222
  batch 300 loss: 0.5173219221830369
  batch 350 loss: 0.4995333641767502
  batch 400 loss: 0.5072361874580383
  batch 450 loss: 0.5189767217636109
  batch 500 loss: 0.4937038993835449
  batch 550 loss: 0.49243306756019595
  batch 600 loss: 0.49782423257827757
  batch 650 loss: 0.5378470134735107
  batch 700 loss: 0.4974834191799164
  batch 750 loss: 0.4869648969173431
  batch 800 loss: 0.5062147182226181
  batch 850 loss: 0.5137585484981537
  batch 900 loss: 0.5090886145830155
running loss: 12.034105896949768
LOSS train 0.50909 valid 0.89851, valid PER 25.62%
EPOCH 20, Learning Rate: 0.0375
  batch 50 loss: 0.4993972969055176
  batch 100 loss: 0.4835631603002548
  batch 150 loss: 0.4755408209562302
  batch 200 loss: 0.4880434954166412
  batch 250 loss: 0.48565158069133757
  batch 300 loss: 0.5032138115167618
  batch 350 loss: 0.46180607557296754
  batch 400 loss: 0.49773717105388643
  batch 450 loss: 0.49342155277729033
  batch 500 loss: 0.4731040769815445
  batch 550 loss: 0.5242334628105163
  batch 600 loss: 0.4810653918981552
  batch 650 loss: 0.4916851991415024
  batch 700 loss: 0.5033533674478531
  batch 750 loss: 0.48620024740695955
  batch 800 loss: 0.5228387326002121
  batch 850 loss: 0.4995879822969437
  batch 900 loss: 0.49393706500530243
running loss: 11.986707627773285
LOSS train 0.49394 valid 0.90759, valid PER 25.64%
Training finished in 8.0 minutes.
Model saved to checkpoints/20231210_043329/model_15
Loading model from checkpoints/20231210_043329/model_15
SUB: 14.56%, DEL: 11.58%, INS: 2.15%, COR: 73.86%, PER: 28.29%
