Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.01, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='adam', grad_clip=1.5)
Total number of model parameters is 166952
EPOCH 1, Learning Rate: 0.01
  batch 50 loss: 4.275942311286927
  batch 100 loss: 2.7436024808883666
  batch 150 loss: 2.4111971998214723
  batch 200 loss: 2.2292094302177428
  batch 250 loss: 2.0969964551925657
  batch 300 loss: 1.9751974582672118
  batch 350 loss: 1.8570631098747254
  batch 400 loss: 1.8655668020248413
  batch 450 loss: 1.8217387104034424
  batch 500 loss: 1.740560212135315
  batch 550 loss: 1.7236651468276978
  batch 600 loss: 1.7146604418754579
  batch 650 loss: 1.6468044233322143
  batch 700 loss: 1.6970712161064148
  batch 750 loss: 1.6172943091392518
  batch 800 loss: 1.6530636024475098
  batch 850 loss: 1.5920508313179016
  batch 900 loss: 1.6116357374191284
LOSS train 1.61164 valid 1.52268, valid PER 51.41%
EPOCH 2, Learning Rate: 0.01
  batch 50 loss: 1.5403441548347474
  batch 100 loss: 1.5695782995224
  batch 150 loss: 1.5375762939453126
  batch 200 loss: 1.6118900394439697
  batch 250 loss: 1.592455163002014
  batch 300 loss: 1.541943073272705
  batch 350 loss: 1.4632958221435546
  batch 400 loss: 1.4827613592147828
  batch 450 loss: 1.4581729102134704
  batch 500 loss: 1.4957122254371642
  batch 550 loss: 1.4975160908699037
  batch 600 loss: 1.4804147291183472
  batch 650 loss: 1.455712161064148
  batch 700 loss: 1.5001838064193727
  batch 750 loss: 1.4880896639823913
  batch 800 loss: 1.442407112121582
  batch 850 loss: 1.4642685079574584
  batch 900 loss: 1.51215993642807
LOSS train 1.51216 valid 1.39173, valid PER 45.28%
EPOCH 3, Learning Rate: 0.01
  batch 50 loss: 1.450474145412445
  batch 100 loss: 1.4479978013038635
  batch 150 loss: 1.4505750799179078
  batch 200 loss: 1.4204674172401428
  batch 250 loss: 1.4307158517837524
  batch 300 loss: 1.4563675546646118
  batch 350 loss: 1.5063481330871582
  batch 400 loss: 1.4954379034042358
  batch 450 loss: 1.445785210132599
  batch 500 loss: 1.464325966835022
  batch 550 loss: 1.4662786436080932
  batch 600 loss: 1.4757326674461364
  batch 650 loss: 1.43045725107193
  batch 700 loss: 1.4787115859985351
  batch 750 loss: 1.509092252254486
  batch 800 loss: 1.3851708936691285
  batch 850 loss: 1.442159161567688
  batch 900 loss: 1.3846995782852174
LOSS train 1.38470 valid 1.36968, valid PER 43.87%
EPOCH 4, Learning Rate: 0.01
  batch 50 loss: 1.3744746327400208
  batch 100 loss: 1.4343672442436217
  batch 150 loss: 1.4048846197128295
  batch 200 loss: 1.4454971218109132
  batch 250 loss: 1.4663867521286011
  batch 300 loss: 1.4549097800254822
  batch 350 loss: 1.3584061551094055
  batch 400 loss: 1.380399353504181
  batch 450 loss: 1.363614637851715
  batch 500 loss: 1.3719440877437592
  batch 550 loss: 1.411086962223053
  batch 600 loss: 1.431433231830597
  batch 650 loss: 1.4451140546798706
  batch 700 loss: 1.3868562412261962
  batch 750 loss: 1.3546189403533935
  batch 800 loss: 1.3625697994232178
  batch 850 loss: 1.3852123832702636
  batch 900 loss: 1.4249452197551726
LOSS train 1.42495 valid 1.34090, valid PER 42.39%
EPOCH 5, Learning Rate: 0.01
  batch 50 loss: 1.3409596753120423
  batch 100 loss: 1.3350691556930543
  batch 150 loss: 1.398254337310791
  batch 200 loss: 1.3127666640281677
  batch 250 loss: 1.3374333119392394
  batch 300 loss: 1.3756139945983887
  batch 350 loss: 1.3556317234039306
  batch 400 loss: 1.3684555625915527
  batch 450 loss: 1.3363386821746825
  batch 500 loss: 1.3832968068122864
  batch 550 loss: 1.3208193349838258
  batch 600 loss: 1.3839900302886963
  batch 650 loss: 1.3209618544578552
  batch 700 loss: 1.375884871482849
  batch 750 loss: 1.3342912077903748
  batch 800 loss: 1.3354035592079163
  batch 850 loss: 1.3772893285751342
  batch 900 loss: 1.3672646808624267
LOSS train 1.36726 valid 1.24978, valid PER 39.33%
EPOCH 6, Learning Rate: 0.01
  batch 50 loss: 1.3657466220855712
  batch 100 loss: 1.2809031116962433
  batch 150 loss: 1.2989833664894104
  batch 200 loss: 1.3279300022125244
  batch 250 loss: 1.3745663857460022
  batch 300 loss: 1.3188074207305909
  batch 350 loss: 1.3174193012714386
  batch 400 loss: 1.338674201965332
  batch 450 loss: 1.3769812512397765
  batch 500 loss: 1.3233354187011719
  batch 550 loss: 1.3467746686935425
  batch 600 loss: 1.3107859563827515
  batch 650 loss: 1.3526624751091003
  batch 700 loss: 1.3445618414878846
  batch 750 loss: 1.336911083459854
  batch 800 loss: 1.3370603156089782
  batch 850 loss: 1.31136869430542
  batch 900 loss: 1.3204080891609191
LOSS train 1.32041 valid 1.29804, valid PER 41.89%
EPOCH 7, Learning Rate: 0.01
  batch 50 loss: 1.3082661175727843
  batch 100 loss: 1.350574448108673
  batch 150 loss: 1.3282968497276306
  batch 200 loss: 1.3159027123451232
  batch 250 loss: 1.3072634959220886
  batch 300 loss: 1.3137480592727662
  batch 350 loss: 1.2867702150344849
  batch 400 loss: 1.3222898769378661
  batch 450 loss: 1.3456866025924683
  batch 500 loss: 1.290900502204895
  batch 550 loss: 1.28316140294075
  batch 600 loss: 1.4020343589782716
  batch 650 loss: 1.3493098425865173
  batch 700 loss: 1.364229828119278
  batch 750 loss: 1.3023784518241883
  batch 800 loss: 1.308156793117523
  batch 850 loss: 1.3409134030342102
  batch 900 loss: 1.3717954897880553
LOSS train 1.37180 valid 1.29405, valid PER 40.53%
EPOCH 8, Learning Rate: 0.01
  batch 50 loss: 1.3137018013000488
  batch 100 loss: 1.2955403900146485
  batch 150 loss: 1.3068665051460266
  batch 200 loss: 1.2936295902729034
  batch 250 loss: 1.3122094368934631
  batch 300 loss: 1.2737969982624053
  batch 350 loss: 1.3792477750778198
  batch 400 loss: 1.3289255428314208
  batch 450 loss: 1.3382056522369385
  batch 500 loss: 1.3397163558006286
  batch 550 loss: 1.3003706860542297
  batch 600 loss: 1.35516934633255
  batch 650 loss: 1.3427708673477172
  batch 700 loss: 1.290931612253189
  batch 750 loss: 1.3125986003875731
  batch 800 loss: 1.3377202153205872
  batch 850 loss: 1.3832245087623596
  batch 900 loss: 1.325837653875351
LOSS train 1.32584 valid 1.26493, valid PER 39.95%
EPOCH 9, Learning Rate: 0.01
  batch 50 loss: 1.2477313375473023
  batch 100 loss: 1.2872581493854522
  batch 150 loss: 1.3089537942409515
  batch 200 loss: 1.2433233380317688
  batch 250 loss: 1.2762855768203736
  batch 300 loss: 1.2856845736503602
  batch 350 loss: 1.3077436733245849
  batch 400 loss: 1.2847306513786316
  batch 450 loss: 1.3074014115333557
  batch 500 loss: 1.265700364112854
  batch 550 loss: 1.3160614490509033
  batch 600 loss: 1.3091670727729798
  batch 650 loss: 1.2902644062042237
  batch 700 loss: 1.330844750404358
  batch 750 loss: 1.2986378931999207
  batch 800 loss: 1.3155313205718995
  batch 850 loss: 1.316613299846649
  batch 900 loss: 1.2995483207702636
LOSS train 1.29955 valid 1.27153, valid PER 40.29%
EPOCH 10, Learning Rate: 0.01
  batch 50 loss: 1.2539122104644775
  batch 100 loss: 1.3588070821762086
  batch 150 loss: 1.3241610932350159
  batch 200 loss: 1.32136190533638
  batch 250 loss: 1.3098714685440063
  batch 300 loss: 1.340603804588318
  batch 350 loss: 1.4078366804122924
  batch 400 loss: 1.3604782295227051
  batch 450 loss: 1.2923911380767823
  batch 500 loss: 1.3636153888702394
  batch 550 loss: 1.3764526224136353
  batch 600 loss: 1.3947890818119049
  batch 650 loss: 1.3592299842834472
  batch 700 loss: 1.3275376439094544
  batch 750 loss: 1.315106408596039
  batch 800 loss: 1.343083004951477
  batch 850 loss: 1.3411872696876526
  batch 900 loss: 1.3576992177963256
LOSS train 1.35770 valid 1.29269, valid PER 40.85%
EPOCH 11, Learning Rate: 0.01
  batch 50 loss: 1.325321614742279
  batch 100 loss: 1.3308049082756042
  batch 150 loss: 1.335118634700775
  batch 200 loss: 1.3801261281967163
  batch 250 loss: 1.3389229822158812
  batch 300 loss: 1.329305077791214
  batch 350 loss: 1.3483472752571106
  batch 400 loss: 1.4380885982513427
  batch 450 loss: 1.3839257562160492
  batch 500 loss: 1.3739743304252625
  batch 550 loss: 1.3358289229869842
  batch 600 loss: 1.3444064569473266
  batch 650 loss: 1.3853782868385316
  batch 700 loss: 1.299984061717987
  batch 750 loss: 1.2981864643096923
  batch 800 loss: 1.349227020740509
  batch 850 loss: 1.3557497358322144
  batch 900 loss: 1.3669614362716676
LOSS train 1.36696 valid 1.30754, valid PER 40.57%
EPOCH 12, Learning Rate: 0.01
  batch 50 loss: 1.3106070685386657
  batch 100 loss: 1.3279254472255706
  batch 150 loss: 1.303201150894165
  batch 200 loss: 1.3163207626342774
  batch 250 loss: 1.383198187351227
  batch 300 loss: 1.324847662448883
  batch 350 loss: 1.3126030695438384
  batch 400 loss: 1.3764570951461792
  batch 450 loss: 1.3771716141700745
  batch 500 loss: 1.3666775369644164
  batch 550 loss: 1.2983853173255921
  batch 600 loss: 1.2728137803077697
  batch 650 loss: 1.3262488007545472
  batch 700 loss: 1.34438720703125
  batch 750 loss: 1.3030964183807372
  batch 800 loss: 1.2910867428779602
  batch 850 loss: 1.3326458406448365
  batch 900 loss: 1.3556106567382813
LOSS train 1.35561 valid 1.31456, valid PER 42.75%
EPOCH 13, Learning Rate: 0.01
  batch 50 loss: 1.2652085471153258
  batch 100 loss: 1.2742649281024934
  batch 150 loss: 1.2901323652267456
  batch 200 loss: 1.2788603150844573
  batch 250 loss: 1.3231335175037384
  batch 300 loss: 1.2871185290813445
  batch 350 loss: 1.293353419303894
  batch 400 loss: 1.3061283934116363
  batch 450 loss: 1.3151149320602418
  batch 500 loss: 1.2838658964633942
  batch 550 loss: 1.30654660820961
  batch 600 loss: 1.3186103808879852
  batch 650 loss: 1.3109468173980714
  batch 700 loss: 1.3261533999443054
  batch 750 loss: 1.3179734539985657
  batch 800 loss: 1.3079105758666991
  batch 850 loss: 1.3404863357543946
  batch 900 loss: 1.345279450416565
LOSS train 1.34528 valid 1.28627, valid PER 41.31%
EPOCH 14, Learning Rate: 0.01
  batch 50 loss: 1.2973747658729553
  batch 100 loss: 1.2713421928882598
  batch 150 loss: 1.2739164996147156
  batch 200 loss: 1.3057718360424042
  batch 250 loss: 1.2703002738952636
  batch 300 loss: 1.3185284006595612
  batch 350 loss: 1.253471040725708
  batch 400 loss: 1.2944809794425964
  batch 450 loss: 1.303178641796112
  batch 500 loss: 1.3135990965366364
  batch 550 loss: 1.3286467266082764
  batch 600 loss: 1.2589347088336944
  batch 650 loss: 1.3052115249633789
  batch 700 loss: 1.334049233198166
  batch 750 loss: 1.2635886359214783
  batch 800 loss: 1.2585823583602904
  batch 850 loss: 1.3282608914375305
  batch 900 loss: 1.346265254020691
LOSS train 1.34627 valid 1.33934, valid PER 42.30%
EPOCH 15, Learning Rate: 0.01
  batch 50 loss: 1.3096904528141022
  batch 100 loss: 1.3206660330295563
  batch 150 loss: 1.275677433013916
  batch 200 loss: 1.3254655861854554
  batch 250 loss: 1.3023570132255555
  batch 300 loss: 1.3222034692764282
  batch 350 loss: 1.3159579861164092
  batch 400 loss: 1.2966893315315247
  batch 450 loss: 1.2942088198661805
  batch 500 loss: 1.275382387638092
  batch 550 loss: 1.335021080970764
  batch 600 loss: 1.3582674288749694
  batch 650 loss: 1.346319422721863
  batch 700 loss: 1.3237372636795044
  batch 750 loss: 1.329902219772339
  batch 800 loss: 1.2806243872642518
  batch 850 loss: 1.2826470136642456
  batch 900 loss: 1.3203575718402862
LOSS train 1.32036 valid 1.30542, valid PER 41.51%
EPOCH 16, Learning Rate: 0.01
  batch 50 loss: 1.335941766500473
  batch 100 loss: 1.2625011086463929
  batch 150 loss: 1.2672452020645142
  batch 200 loss: 1.2814792895317078
  batch 250 loss: 1.3158625197410583
  batch 300 loss: 1.30459299325943
  batch 350 loss: 1.3080963730812072
  batch 400 loss: 1.3011765599250793
  batch 450 loss: 1.3105804920196533
  batch 500 loss: 1.2865656185150147
  batch 550 loss: 1.3159695756435394
  batch 600 loss: 1.3403809702396392
  batch 650 loss: 1.335258285999298
  batch 700 loss: 1.2892081642150879
  batch 750 loss: 1.307321813106537
  batch 800 loss: 1.316992495059967
  batch 850 loss: 1.2987694501876832
  batch 900 loss: 1.3133415794372558
LOSS train 1.31334 valid 1.30511, valid PER 41.67%
EPOCH 17, Learning Rate: 0.01
  batch 50 loss: 1.3084385812282562
  batch 100 loss: 1.3339996969699859
  batch 150 loss: 1.305715469121933
  batch 200 loss: 1.3154396796226502
  batch 250 loss: 1.3253906989097595
  batch 300 loss: 1.3429948365688325
  batch 350 loss: 1.3124922442436218
  batch 400 loss: 1.3356366086006164
  batch 450 loss: 1.3382503175735474
  batch 500 loss: 1.2857060384750367
  batch 550 loss: 1.3451384162902833
  batch 600 loss: 1.3945237517356872
  batch 650 loss: 1.3176933276653289
  batch 700 loss: 1.2973810243606567
  batch 750 loss: 1.3170209753513336
  batch 800 loss: 1.331536066532135
  batch 850 loss: 1.3257452249526978
  batch 900 loss: 1.3230282378196716
LOSS train 1.32303 valid 1.30766, valid PER 40.72%
EPOCH 18, Learning Rate: 0.01
  batch 50 loss: 1.339185152053833
  batch 100 loss: 1.3341051483154296
  batch 150 loss: 1.3229642915725708
  batch 200 loss: 1.3189755845069886
  batch 250 loss: 1.326485469341278
  batch 300 loss: 1.310921959877014
  batch 350 loss: 1.3542810320854186
  batch 400 loss: 1.3349383449554444
  batch 450 loss: 1.3759463286399842
  batch 500 loss: 1.366518292427063
  batch 550 loss: 1.3815854835510253
  batch 600 loss: 1.351260223388672
  batch 650 loss: 1.338741544485092
  batch 700 loss: 1.386215009689331
  batch 750 loss: 1.319653879404068
  batch 800 loss: 1.3699929332733154
  batch 850 loss: 1.3650611865520477
  batch 900 loss: 1.3851793098449707
LOSS train 1.38518 valid 1.33590, valid PER 40.70%
EPOCH 19, Learning Rate: 0.01
  batch 50 loss: 1.2634577798843383
  batch 100 loss: 1.3034202146530152
  batch 150 loss: 1.3360488247871398
  batch 200 loss: 1.3510976719856262
  batch 250 loss: 1.3737622666358948
  batch 300 loss: 1.3650244784355163
  batch 350 loss: 1.3229154992103576
  batch 400 loss: 1.355907609462738
  batch 450 loss: 1.317187716960907
  batch 500 loss: 1.3346437311172485
  batch 550 loss: 1.352041895389557
  batch 600 loss: 1.3031516540050507
  batch 650 loss: 1.3778141331672669
  batch 700 loss: 1.3330252492427825
  batch 750 loss: 1.2889108264446258
  batch 800 loss: 1.326401286125183
  batch 850 loss: 1.351809287071228
  batch 900 loss: 1.3189337468147277
LOSS train 1.31893 valid 1.28158, valid PER 40.83%
EPOCH 20, Learning Rate: 0.01
  batch 50 loss: 1.2933650422096252
  batch 100 loss: 1.2847259831428528
  batch 150 loss: 1.2870566630363465
  batch 200 loss: 1.2903707027435303
  batch 250 loss: 1.2450591540336609
  batch 300 loss: 1.332309024333954
  batch 350 loss: 1.266262115240097
  batch 400 loss: 1.2959106540679932
  batch 450 loss: 1.2998030138015748
  batch 500 loss: 1.2785003900527954
  batch 550 loss: 1.3665694355964662
  batch 600 loss: 1.278235046863556
  batch 650 loss: 1.3451941704750061
  batch 700 loss: 1.3141537141799926
  batch 750 loss: 1.3272451341152192
  batch 800 loss: 1.3716276502609253
  batch 850 loss: 1.3561865901947021
  batch 900 loss: 1.3791143321990966
LOSS train 1.37911 valid 1.31441, valid PER 41.32%
Training finished in 4.0 minutes.
Model saved to checkpoints/20231210_004706/model_5
Loading model from checkpoints/20231210_004706/model_5
SUB: 19.69%, DEL: 18.95%, INS: 1.11%, COR: 61.37%, PER: 39.75%
