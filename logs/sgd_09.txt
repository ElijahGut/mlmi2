Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.9, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 166952
EPOCH 1, Learning Rate: 0.9
  batch 50 loss: 4.31068772315979
  batch 100 loss: 3.061180205345154
  batch 150 loss: 2.8588394594192503
  batch 200 loss: 2.7300219202041625
  batch 250 loss: 2.6908050680160525
  batch 300 loss: 2.5466306447982787
  batch 350 loss: 2.3932254791259764
  batch 400 loss: 2.325211181640625
  batch 450 loss: 2.268136694431305
  batch 500 loss: 2.154429759979248
  batch 550 loss: 2.2038637948036195
  batch 600 loss: 2.077292070388794
  batch 650 loss: 1.9852485537528992
  batch 700 loss: 1.921171145439148
  batch 750 loss: 1.8618014645576477
  batch 800 loss: 1.8200903415679932
  batch 850 loss: 1.7786845302581786
  batch 900 loss: 1.7667873311042785
LOSS train 1.76679 valid 1.71725, valid PER 66.99%
EPOCH 2, Learning Rate: 0.9
  batch 50 loss: 1.727568666934967
  batch 100 loss: 1.6576194953918457
  batch 150 loss: 1.6431572675704955
  batch 200 loss: 1.6809243559837341
  batch 250 loss: 1.6595301818847656
  batch 300 loss: 1.6266571497917175
  batch 350 loss: 1.526930124759674
  batch 400 loss: 1.5666878294944764
  batch 450 loss: 1.5036888647079467
  batch 500 loss: 1.5421720147132874
  batch 550 loss: 1.516144654750824
  batch 600 loss: 1.465322070121765
  batch 650 loss: 1.503430483341217
  batch 700 loss: 1.4910426592826844
  batch 750 loss: 1.4486006164550782
  batch 800 loss: 1.4135705137252808
  batch 850 loss: 1.4551539063453673
  batch 900 loss: 1.4514251852035522
LOSS train 1.45143 valid 1.31314, valid PER 42.18%
EPOCH 3, Learning Rate: 0.9
  batch 50 loss: 1.4077083587646484
  batch 100 loss: 1.3575192177295685
  batch 150 loss: 1.3596318435668946
  batch 200 loss: 1.3724394345283508
  batch 250 loss: 1.349237402677536
  batch 300 loss: 1.3299604153633118
  batch 350 loss: 1.380513608455658
  batch 400 loss: 1.3437102484703063
  batch 450 loss: 1.311606662273407
  batch 500 loss: 1.3183559346199036
  batch 550 loss: 1.3334687197208404
  batch 600 loss: 1.3150142526626587
  batch 650 loss: 1.2817573320865632
  batch 700 loss: 1.3075209605693816
  batch 750 loss: 1.3767288899421692
  batch 800 loss: 1.284703518152237
  batch 850 loss: 1.308911988735199
  batch 900 loss: 1.2532323491573334
LOSS train 1.25323 valid 1.23260, valid PER 39.15%
EPOCH 4, Learning Rate: 0.9
  batch 50 loss: 1.2269265413284303
  batch 100 loss: 1.28474045753479
  batch 150 loss: 1.210217959880829
  batch 200 loss: 1.2575604271888734
  batch 250 loss: 1.296875514984131
  batch 300 loss: 1.2917327618598937
  batch 350 loss: 1.1918694603443145
  batch 400 loss: 1.2738456976413728
  batch 450 loss: 1.2468108654022216
  batch 500 loss: 1.2387670493125915
  batch 550 loss: 1.2647773599624634
  batch 600 loss: 1.2851227331161499
  batch 650 loss: 1.264533234834671
  batch 700 loss: 1.2687274432182312
  batch 750 loss: 1.223461114168167
  batch 800 loss: 1.1966356182098388
  batch 850 loss: 1.2315964782238007
  batch 900 loss: 1.2700943291187285
LOSS train 1.27009 valid 1.14826, valid PER 36.52%
EPOCH 5, Learning Rate: 0.9
  batch 50 loss: 1.159657553434372
  batch 100 loss: 1.1899956607818603
  batch 150 loss: 1.2398148846626282
  batch 200 loss: 1.1444685137271882
  batch 250 loss: 1.1838095319271087
  batch 300 loss: 1.1702699673175812
  batch 350 loss: 1.1831415736675261
  batch 400 loss: 1.1617222809791565
  batch 450 loss: 1.1931209397315978
  batch 500 loss: 1.193471486568451
  batch 550 loss: 1.1318180668354034
  batch 600 loss: 1.2372701930999757
  batch 650 loss: 1.1626905083656311
  batch 700 loss: 1.2033221960067748
  batch 750 loss: 1.1279886329174043
  batch 800 loss: 1.149741381406784
  batch 850 loss: 1.1893310403823854
  batch 900 loss: 1.1960045206546783
LOSS train 1.19600 valid 1.15932, valid PER 36.84%
EPOCH 6, Learning Rate: 0.9
  batch 50 loss: 1.1630993270874024
  batch 100 loss: 1.121763242483139
  batch 150 loss: 1.1067749381065368
  batch 200 loss: 1.1309050190448762
  batch 250 loss: 1.1677456152439118
  batch 300 loss: 1.1436157405376435
  batch 350 loss: 1.1324447560310364
  batch 400 loss: 1.149531421661377
  batch 450 loss: 1.1515099835395812
  batch 500 loss: 1.1398758101463318
  batch 550 loss: 1.1565927577018738
  batch 600 loss: 1.1176160264015198
  batch 650 loss: 1.1576421248912812
  batch 700 loss: 1.133981556892395
  batch 750 loss: 1.1227181100845336
  batch 800 loss: 1.1270666253566741
  batch 850 loss: 1.1204568779468536
  batch 900 loss: 1.1244533383846282
LOSS train 1.12445 valid 1.12594, valid PER 35.42%
EPOCH 7, Learning Rate: 0.9
  batch 50 loss: 1.1503749322891235
  batch 100 loss: 1.1494470703601838
  batch 150 loss: 1.1143600463867187
  batch 200 loss: 1.0901430392265319
  batch 250 loss: 1.1227353286743165
  batch 300 loss: 1.1032051253318786
  batch 350 loss: 1.1013987720012666
  batch 400 loss: 1.1260246813297272
  batch 450 loss: 1.1121623241901397
  batch 500 loss: 1.0967460453510285
  batch 550 loss: 1.0970907032489776
  batch 600 loss: 1.0856448912620544
  batch 650 loss: 1.102462613582611
  batch 700 loss: 1.1177751326560974
  batch 750 loss: 1.0898917162418365
  batch 800 loss: 1.0807405853271483
  batch 850 loss: 1.1067608404159546
  batch 900 loss: 1.1359645175933837
LOSS train 1.13596 valid 1.06882, valid PER 34.06%
EPOCH 8, Learning Rate: 0.9
  batch 50 loss: 1.068099431991577
  batch 100 loss: 1.0736156284809113
  batch 150 loss: 1.064633469581604
  batch 200 loss: 1.0458207035064697
  batch 250 loss: 1.0632854688167572
  batch 300 loss: 1.0048251175880432
  batch 350 loss: 1.0800707471370696
  batch 400 loss: 1.0516632068157197
  batch 450 loss: 1.083125079870224
  batch 500 loss: 1.101878763437271
  batch 550 loss: 1.0553876626491547
  batch 600 loss: 1.0804766988754273
  batch 650 loss: 1.0948366856575011
  batch 700 loss: 1.063427572250366
  batch 750 loss: 1.0729668879508971
  batch 800 loss: 1.0666902196407317
  batch 850 loss: 1.1041845560073853
  batch 900 loss: 1.067180448770523
LOSS train 1.06718 valid 1.03168, valid PER 33.49%
EPOCH 9, Learning Rate: 0.9
  batch 50 loss: 1.015290962457657
  batch 100 loss: 1.0559399616718292
  batch 150 loss: 1.0630912864208222
  batch 200 loss: 1.0212389874458312
  batch 250 loss: 1.0576912581920623
  batch 300 loss: 1.055295705795288
  batch 350 loss: 1.084842143058777
  batch 400 loss: 1.0688156294822693
  batch 450 loss: 1.056544953584671
  batch 500 loss: 1.017826588153839
  batch 550 loss: 1.0844992041587829
  batch 600 loss: 1.0959784877300263
  batch 650 loss: 1.053893814086914
  batch 700 loss: 1.081540471315384
  batch 750 loss: 1.0822795355319976
  batch 800 loss: 1.0828241086006165
  batch 850 loss: 1.0671750831604003
  batch 900 loss: 1.028592803478241
LOSS train 1.02859 valid 1.03322, valid PER 33.76%
EPOCH 10, Learning Rate: 0.9
  batch 50 loss: 0.9778171229362488
  batch 100 loss: 1.018321144580841
  batch 150 loss: 1.062430032491684
  batch 200 loss: 1.063986780643463
  batch 250 loss: 1.062223744392395
  batch 300 loss: 1.0053069627285003
  batch 350 loss: 1.0500339937210084
  batch 400 loss: 1.021543151140213
  batch 450 loss: 1.0167593145370484
  batch 500 loss: 1.055107741355896
  batch 550 loss: 1.0567398500442504
  batch 600 loss: 1.021129335165024
  batch 650 loss: 1.029748649597168
  batch 700 loss: 1.0375268590450286
  batch 750 loss: 1.0150393998622895
  batch 800 loss: 1.0268405294418335
  batch 850 loss: 1.0459475100040436
  batch 900 loss: 1.0392958116531372
LOSS train 1.03930 valid 1.05422, valid PER 34.23%
EPOCH 11, Learning Rate: 0.9
  batch 50 loss: 0.9843350231647492
  batch 100 loss: 0.9737340009212494
  batch 150 loss: 0.9731388545036316
  batch 200 loss: 1.0395943665504455
  batch 250 loss: 1.022973836660385
  batch 300 loss: 0.9832734549045563
  batch 350 loss: 1.022254648208618
  batch 400 loss: 1.0578060090541839
  batch 450 loss: 1.048000556230545
  batch 500 loss: 0.9759257280826569
  batch 550 loss: 1.0059141170978547
  batch 600 loss: 0.9791667675971985
  batch 650 loss: 1.032990117073059
  batch 700 loss: 0.9604977309703827
  batch 750 loss: 0.9765039253234863
  batch 800 loss: 1.0264879548549652
  batch 850 loss: 1.0429052102565766
  batch 900 loss: 1.0064021253585815
LOSS train 1.00640 valid 1.03115, valid PER 32.88%
EPOCH 12, Learning Rate: 0.9
  batch 50 loss: 0.9622831082344055
  batch 100 loss: 0.9584009087085724
  batch 150 loss: 0.9696048772335053
  batch 200 loss: 0.9667984640598297
  batch 250 loss: 0.9892889332771301
  batch 300 loss: 0.9650123465061188
  batch 350 loss: 0.9800897943973541
  batch 400 loss: 1.0007794320583343
  batch 450 loss: 0.9929788184165954
  batch 500 loss: 1.014994615316391
  batch 550 loss: 0.9476427090167999
  batch 600 loss: 0.9702087712287902
  batch 650 loss: 1.0281630325317384
  batch 700 loss: 0.9801340031623841
  batch 750 loss: 0.9615849792957306
  batch 800 loss: 0.9725984859466553
  batch 850 loss: 1.014285191297531
  batch 900 loss: 1.029823924303055
LOSS train 1.02982 valid 1.00420, valid PER 32.18%
EPOCH 13, Learning Rate: 0.9
  batch 50 loss: 0.9511302149295807
  batch 100 loss: 0.9916473996639251
  batch 150 loss: 0.9381116628646851
  batch 200 loss: 1.0027807986736297
  batch 250 loss: 1.0084482252597808
  batch 300 loss: 0.956775450706482
  batch 350 loss: 0.9981261837482452
  batch 400 loss: 1.0092768526077271
  batch 450 loss: 1.000247391462326
  batch 500 loss: 0.9578756725788117
  batch 550 loss: 0.9969431614875793
  batch 600 loss: 1.0019727194309234
  batch 650 loss: 1.0027856981754304
  batch 700 loss: 1.0122215270996093
  batch 750 loss: 0.9536419451236725
  batch 800 loss: 0.9595936739444733
  batch 850 loss: 1.0129337561130525
  batch 900 loss: 0.9932028019428253
LOSS train 0.99320 valid 1.00341, valid PER 31.77%
EPOCH 14, Learning Rate: 0.9
  batch 50 loss: 0.9402957475185394
  batch 100 loss: 0.9933482670783996
  batch 150 loss: 0.9739956223964691
  batch 200 loss: 0.9674133551120758
  batch 250 loss: 0.9755383789539337
  batch 300 loss: 1.0076398468017578
  batch 350 loss: 0.9421918201446533
  batch 400 loss: 0.9563027727603912
  batch 450 loss: 0.9545422673225403
  batch 500 loss: 0.9874216330051422
  batch 550 loss: 0.9959473586082459
  batch 600 loss: 0.942451866865158
  batch 650 loss: 0.9681981420516967
  batch 700 loss: 0.9874650251865387
  batch 750 loss: 0.9523822557926178
  batch 800 loss: 0.9296706247329712
  batch 850 loss: 1.0066873514652253
  batch 900 loss: 0.954700335264206
LOSS train 0.95470 valid 1.01396, valid PER 31.97%
EPOCH 15, Learning Rate: 0.9
  batch 50 loss: 0.9467756688594818
  batch 100 loss: 0.9165437364578247
  batch 150 loss: 0.959161548614502
  batch 200 loss: 1.0106636822223662
  batch 250 loss: 0.9607925939559937
  batch 300 loss: 0.9864043498039246
  batch 350 loss: 0.9633681797981262
  batch 400 loss: 0.9315159666538239
  batch 450 loss: 0.9470998859405517
  batch 500 loss: 0.9071137380599975
  batch 550 loss: 0.9621207416057587
  batch 600 loss: 0.9901547253131866
  batch 650 loss: 0.9890814423561096
  batch 700 loss: 0.9889526557922363
  batch 750 loss: 0.9853171885013581
  batch 800 loss: 0.9425927650928497
  batch 850 loss: 0.9446929764747619
  batch 900 loss: 0.9712873828411103
LOSS train 0.97129 valid 1.01951, valid PER 32.34%
EPOCH 16, Learning Rate: 0.9
  batch 50 loss: 0.9822128617763519
  batch 100 loss: 0.9438166487216949
  batch 150 loss: 0.9252007806301117
  batch 200 loss: 0.9368255412578583
  batch 250 loss: 0.9659054458141327
  batch 300 loss: 0.9404827249050141
  batch 350 loss: 0.9938658452033997
  batch 400 loss: 0.9714943766593933
  batch 450 loss: 0.9912999212741852
  batch 500 loss: 0.9145013260841369
  batch 550 loss: 0.9696469831466675
  batch 600 loss: 0.9165407288074493
  batch 650 loss: 0.970252137184143
  batch 700 loss: 0.9450191569328308
  batch 750 loss: 0.9524964129924774
  batch 800 loss: 0.9674740183353424
  batch 850 loss: 0.9542209911346435
  batch 900 loss: 0.9315597641468049
LOSS train 0.93156 valid 0.95744, valid PER 30.21%
EPOCH 17, Learning Rate: 0.9
  batch 50 loss: 0.9232670485973358
  batch 100 loss: 0.934938976764679
  batch 150 loss: 0.9135477209091186
  batch 200 loss: 0.9274532890319824
  batch 250 loss: 0.9382533740997314
  batch 300 loss: 0.9386192607879639
  batch 350 loss: 0.9027068066596985
  batch 400 loss: 0.9705899238586426
  batch 450 loss: 0.9709480893611908
  batch 500 loss: 0.9251540565490722
  batch 550 loss: 0.9297169315814972
  batch 600 loss: 1.0386113381385804
  batch 650 loss: 0.94170441031456
  batch 700 loss: 0.9590735399723053
  batch 750 loss: 0.9044351756572724
  batch 800 loss: 0.9433245778083801
  batch 850 loss: 0.946744532585144
  batch 900 loss: 0.9240642893314361
LOSS train 0.92406 valid 0.98893, valid PER 31.10%
EPOCH 18, Learning Rate: 0.9
  batch 50 loss: 0.9522785151004791
  batch 100 loss: 0.9273954856395722
  batch 150 loss: 0.9380768525600434
  batch 200 loss: 0.9196488976478576
  batch 250 loss: 0.9305427134037018
  batch 300 loss: 0.9267179358005524
  batch 350 loss: 0.942558411359787
  batch 400 loss: 0.9042319881916047
  batch 450 loss: 0.986075724363327
  batch 500 loss: 0.9527173781394959
  batch 550 loss: 0.9219691801071167
  batch 600 loss: 0.9283624505996704
  batch 650 loss: 0.9430778753757477
  batch 700 loss: 0.9799275279045105
  batch 750 loss: 0.9399737286567688
  batch 800 loss: 0.9303069269657135
  batch 850 loss: 0.9231084728240967
  batch 900 loss: 0.9703204596042633
LOSS train 0.97032 valid 1.00749, valid PER 32.08%
EPOCH 19, Learning Rate: 0.9
  batch 50 loss: 0.8894494068622589
  batch 100 loss: 0.8992672681808471
  batch 150 loss: 0.9293633687496186
  batch 200 loss: 0.9167247080802917
  batch 250 loss: 0.9423859524726867
  batch 300 loss: 0.9484186601638794
  batch 350 loss: 0.9300298082828522
  batch 400 loss: 0.9460654890537262
  batch 450 loss: 0.925866619348526
  batch 500 loss: 0.9367723143100739
  batch 550 loss: 0.9242149150371551
  batch 600 loss: 0.9277587366104126
  batch 650 loss: 0.9891512155532837
  batch 700 loss: 0.9426498436927795
  batch 750 loss: 0.9288745093345642
  batch 800 loss: 0.9582068812847138
  batch 850 loss: 0.9763056230545044
  batch 900 loss: 0.9352071940898895
LOSS train 0.93521 valid 1.00064, valid PER 32.23%
EPOCH 20, Learning Rate: 0.9
  batch 50 loss: 0.9412015175819397
  batch 100 loss: 0.9401549613475799
  batch 150 loss: 0.9278424680233002
  batch 200 loss: 0.9287639510631561
  batch 250 loss: 0.9354326570034027
  batch 300 loss: 0.950266215801239
  batch 350 loss: 0.8976404166221619
  batch 400 loss: 0.9284892654418946
  batch 450 loss: 0.9276620721817017
  batch 500 loss: 0.9181741189956665
  batch 550 loss: 0.9778538393974304
  batch 600 loss: 0.9433813536167145
  batch 650 loss: 0.9588316035270691
  batch 700 loss: 0.9647571909427642
  batch 750 loss: 0.9589550578594208
  batch 800 loss: 0.9630657279491425
  batch 850 loss: 0.9739805781841278
  batch 900 loss: 0.9556210577487946
LOSS train 0.95562 valid 1.00821, valid PER 31.62%
Training finished in 3.0 minutes.
Model saved to checkpoints/20231210_033508/model_16
Loading model from checkpoints/20231210_033508/model_16
SUB: 16.77%, DEL: 13.82%, INS: 1.68%, COR: 69.41%, PER: 32.27%
