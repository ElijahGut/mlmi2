Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=2, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=0.5, is_bidir=True)
Total number of model parameters is 562216
EPOCH 1, Learning Rate: 0.5
  batch 50 loss: 5.930881462097168
  batch 100 loss: 3.3477422857284544
  batch 150 loss: 3.33195960521698
  batch 200 loss: 3.292571153640747
  batch 250 loss: 3.222686138153076
  batch 300 loss: 3.126128940582275
  batch 350 loss: 3.008558783531189
  batch 400 loss: 2.8706393957138063
  batch 450 loss: 2.733881072998047
  batch 500 loss: 2.578261151313782
  batch 550 loss: 2.478421549797058
  batch 600 loss: 2.4097014617919923
  batch 650 loss: 2.3065595293045043
  batch 700 loss: 2.2691298866271974
  batch 750 loss: 2.2144824957847593
  batch 800 loss: 2.165417881011963
  batch 850 loss: 2.132751610279083
  batch 900 loss: 2.099637761116028
LOSS train 2.09964 valid 2.01645, valid PER 75.04%
EPOCH 2, Learning Rate: 0.5
  batch 50 loss: 2.0523774337768557
  batch 100 loss: 1.9947476530075072
  batch 150 loss: 1.9147738385200501
  batch 200 loss: 1.9054639673233031
  batch 250 loss: 1.8975668382644653
  batch 300 loss: 1.8503804659843446
  batch 350 loss: 1.8208350253105163
  batch 400 loss: 1.796259617805481
  batch 450 loss: 1.7948415184020996
  batch 500 loss: 1.759720139503479
  batch 550 loss: 1.7457485675811768
  batch 600 loss: 1.7212690925598144
  batch 650 loss: 1.6806093287467956
  batch 700 loss: 1.6921825766563416
  batch 750 loss: 1.664253294467926
  batch 800 loss: 1.6278272700309753
  batch 850 loss: 1.6242651653289795
  batch 900 loss: 1.609707612991333
LOSS train 1.60971 valid 1.53959, valid PER 57.52%
EPOCH 3, Learning Rate: 0.5
  batch 50 loss: 1.5588484978675843
  batch 100 loss: 1.6071862530708314
  batch 150 loss: 1.6018805027008056
  batch 200 loss: 1.5341333293914794
  batch 250 loss: 1.5379796504974366
  batch 300 loss: 1.5282829928398132
  batch 350 loss: 1.5410003805160521
  batch 400 loss: 1.5105356359481812
  batch 450 loss: 1.4908808088302612
  batch 500 loss: 1.4715264248847961
  batch 550 loss: 1.4650263237953185
  batch 600 loss: 1.410304787158966
  batch 650 loss: 1.4389120483398437
  batch 700 loss: 1.4285209631919862
  batch 750 loss: 1.4417585468292236
  batch 800 loss: 1.4471649146080017
  batch 850 loss: 1.4082689595222473
  batch 900 loss: 1.4057105422019958
LOSS train 1.40571 valid 1.31332, valid PER 43.57%
EPOCH 4, Learning Rate: 0.5
  batch 50 loss: 1.4215513420104982
  batch 100 loss: 1.341857682466507
  batch 150 loss: 1.3658902800083161
  batch 200 loss: 1.3493557286262512
  batch 250 loss: 1.3553736233711242
  batch 300 loss: 1.3585721063613891
  batch 350 loss: 1.3271638607978822
  batch 400 loss: 1.3000109958648682
  batch 450 loss: 1.2919006609916688
  batch 500 loss: 1.35236230134964
  batch 550 loss: 1.2781976354122162
  batch 600 loss: 1.2814040970802307
  batch 650 loss: 1.3241746616363526
  batch 700 loss: 1.332651435136795
  batch 750 loss: 1.2714193785190582
  batch 800 loss: 1.2753514158725738
  batch 850 loss: 1.2536763370037078
  batch 900 loss: 1.2441148412227632
LOSS train 1.24411 valid 1.19202, valid PER 36.94%
EPOCH 5, Learning Rate: 0.5
  batch 50 loss: 1.2512397575378418
  batch 100 loss: 1.2119134402275085
  batch 150 loss: 1.2492161011695861
  batch 200 loss: 1.2632319331169128
  batch 250 loss: 1.2040401339530944
  batch 300 loss: 1.2187841165065765
  batch 350 loss: 1.1909160387516022
  batch 400 loss: 1.1694433164596558
  batch 450 loss: 1.1614495265483855
  batch 500 loss: 1.165972945690155
  batch 550 loss: 1.2132415902614593
  batch 600 loss: 1.1896747255325317
  batch 650 loss: 1.189793895483017
  batch 700 loss: 1.1777339196205139
  batch 750 loss: 1.1776469051837921
  batch 800 loss: 1.207263207435608
  batch 850 loss: 1.1940718245506288
  batch 900 loss: 1.1594505071640016
LOSS train 1.15945 valid 1.10484, valid PER 34.51%
EPOCH 6, Learning Rate: 0.5
  batch 50 loss: 1.134306128025055
  batch 100 loss: 1.141739810705185
  batch 150 loss: 1.138190996646881
  batch 200 loss: 1.1005044376850128
  batch 250 loss: 1.1193271744251252
  batch 300 loss: 1.1475421786308289
  batch 350 loss: 1.1294079148769378
  batch 400 loss: 1.1243824970722198
  batch 450 loss: 1.144559415578842
  batch 500 loss: 1.0761997020244598
  batch 550 loss: 1.1391632914543153
  batch 600 loss: 1.1059365093708038
  batch 650 loss: 1.0930189609527587
  batch 700 loss: 1.0821989214420318
  batch 750 loss: 1.132085189819336
  batch 800 loss: 1.103409856557846
  batch 850 loss: 1.1423122358322144
  batch 900 loss: 1.1447394716739654
LOSS train 1.14474 valid 1.04770, valid PER 33.26%
EPOCH 7, Learning Rate: 0.5
  batch 50 loss: 1.0768000102043152
  batch 100 loss: 1.1302788865566253
  batch 150 loss: 1.064569991827011
  batch 200 loss: 1.0581967663764953
  batch 250 loss: 1.107470383644104
  batch 300 loss: 1.0902010214328766
  batch 350 loss: 1.1079885244369507
  batch 400 loss: 1.0569981622695923
  batch 450 loss: 1.0642261183261872
  batch 500 loss: 1.0552304577827454
  batch 550 loss: 1.03107484459877
  batch 600 loss: 1.0466447865962982
  batch 650 loss: 1.0310834980010986
  batch 700 loss: 1.0915401554107667
  batch 750 loss: 1.055263774394989
  batch 800 loss: 1.0463715386390686
  batch 850 loss: 1.0202410042285919
  batch 900 loss: 1.0338126754760741
LOSS train 1.03381 valid 1.02400, valid PER 31.76%
EPOCH 8, Learning Rate: 0.5
  batch 50 loss: 1.0459567058086394
  batch 100 loss: 1.019601148366928
  batch 150 loss: 1.0503784775733949
  batch 200 loss: 1.02285453915596
  batch 250 loss: 1.0061050868034362
  batch 300 loss: 0.9782888042926788
  batch 350 loss: 1.0171334326267243
  batch 400 loss: 0.9958111763000488
  batch 450 loss: 1.0734896969795227
  batch 500 loss: 1.0303569209575654
  batch 550 loss: 1.0185807824134827
  batch 600 loss: 1.0005871808528901
  batch 650 loss: 1.014569455385208
  batch 700 loss: 1.0289433646202086
  batch 750 loss: 1.031007891893387
  batch 800 loss: 1.0201007056236266
  batch 850 loss: 0.9984903872013092
  batch 900 loss: 0.9988971042633057
LOSS train 0.99890 valid 0.96182, valid PER 29.74%
EPOCH 9, Learning Rate: 0.5
  batch 50 loss: 0.9747094786167145
  batch 100 loss: 0.9634980094432831
  batch 150 loss: 0.9709699928760529
  batch 200 loss: 0.952571769952774
  batch 250 loss: 0.9462034249305725
  batch 300 loss: 0.9831001913547516
  batch 350 loss: 0.9499310040473938
  batch 400 loss: 0.9810712313652039
  batch 450 loss: 0.9815587592124939
  batch 500 loss: 0.9624023199081421
  batch 550 loss: 0.9901924824714661
  batch 600 loss: 0.9931301438808441
  batch 650 loss: 0.9863116943836212
  batch 700 loss: 0.9750479936599732
  batch 750 loss: 0.9658358168601989
  batch 800 loss: 0.9970140874385833
  batch 850 loss: 0.9887664079666137
  batch 900 loss: 0.9508744049072265
LOSS train 0.95087 valid 0.92847, valid PER 29.00%
EPOCH 10, Learning Rate: 0.5
  batch 50 loss: 0.9354234290122986
  batch 100 loss: 0.9587998473644257
  batch 150 loss: 0.9701123285293579
  batch 200 loss: 0.9278655695915222
  batch 250 loss: 0.9341350901126861
  batch 300 loss: 0.9401525342464447
  batch 350 loss: 0.9231677520275116
  batch 400 loss: 0.9328561162948609
  batch 450 loss: 0.9301251947879792
  batch 500 loss: 0.9303690278530121
  batch 550 loss: 0.931661376953125
  batch 600 loss: 0.9401522016525269
  batch 650 loss: 0.9553036153316498
  batch 700 loss: 0.9650582420825958
  batch 750 loss: 0.9540487349033355
  batch 800 loss: 0.9400568175315857
  batch 850 loss: 0.9332425022125244
  batch 900 loss: 0.9207775223255158
LOSS train 0.92078 valid 0.93098, valid PER 28.82%
EPOCH 11, Learning Rate: 0.5
  batch 50 loss: 0.937076667547226
  batch 100 loss: 0.8983947241306305
  batch 150 loss: 0.8989147293567658
  batch 200 loss: 0.8648005115985871
  batch 250 loss: 0.8916541302204132
  batch 300 loss: 0.8862678194046021
  batch 350 loss: 0.9324647223949433
  batch 400 loss: 0.9004525482654572
  batch 450 loss: 0.9139679515361786
  batch 500 loss: 0.8960754656791687
  batch 550 loss: 0.9167341446876526
  batch 600 loss: 0.9142894637584686
  batch 650 loss: 0.921930023431778
  batch 700 loss: 0.9876634395122528
  batch 750 loss: 0.9141866505146027
  batch 800 loss: 0.9113623332977295
  batch 850 loss: 0.9010576295852661
  batch 900 loss: 0.9178328430652618
LOSS train 0.91783 valid 0.90738, valid PER 27.94%
EPOCH 12, Learning Rate: 0.5
  batch 50 loss: 0.855452790260315
  batch 100 loss: 0.8494146358966828
  batch 150 loss: 0.8536850655078888
  batch 200 loss: 0.8955701947212219
  batch 250 loss: 0.8632297277450561
  batch 300 loss: 0.9083736562728881
  batch 350 loss: 0.8718266510963439
  batch 400 loss: 0.9044623172283173
  batch 450 loss: 0.8704468834400177
  batch 500 loss: 0.8973914968967438
  batch 550 loss: 0.8808906823396683
  batch 600 loss: 0.9011255073547363
  batch 650 loss: 0.9009279811382294
  batch 700 loss: 0.866430641412735
  batch 750 loss: 0.8919607973098755
  batch 800 loss: 0.8495302271842956
  batch 850 loss: 0.8778645396232605
  batch 900 loss: 0.8787194454669952
LOSS train 0.87872 valid 0.87509, valid PER 27.08%
EPOCH 13, Learning Rate: 0.5
  batch 50 loss: 0.8450294041633606
  batch 100 loss: 0.8473483669757843
  batch 150 loss: 0.8937057554721832
  batch 200 loss: 0.841799179315567
  batch 250 loss: 0.8379623651504516
  batch 300 loss: 0.8923023027181626
  batch 350 loss: 0.8401738703250885
  batch 400 loss: 0.8290431189537049
  batch 450 loss: 0.8681750047206879
  batch 500 loss: 0.859377670288086
  batch 550 loss: 0.9052115511894226
  batch 600 loss: 0.8793394780158996
  batch 650 loss: 0.8418857145309449
  batch 700 loss: 0.8623014998435974
  batch 750 loss: 0.8314759516716004
  batch 800 loss: 0.8620434939861298
  batch 850 loss: 0.8323010885715485
  batch 900 loss: 0.8607994508743286
LOSS train 0.86080 valid 0.86335, valid PER 26.77%
EPOCH 14, Learning Rate: 0.5
  batch 50 loss: 0.8185898053646088
  batch 100 loss: 0.7986994051933288
  batch 150 loss: 0.8268872618675231
  batch 200 loss: 0.8270903217792511
  batch 250 loss: 0.8346071124076844
  batch 300 loss: 0.8213499999046325
  batch 350 loss: 0.8084971702098847
  batch 400 loss: 0.8425098013877869
  batch 450 loss: 0.8120363640785218
  batch 500 loss: 0.8225215327739716
  batch 550 loss: 0.8441891515254974
  batch 600 loss: 0.8277719748020173
  batch 650 loss: 0.844754023551941
  batch 700 loss: 0.8545073854923249
  batch 750 loss: 0.8346866297721863
  batch 800 loss: 0.8186030960083008
  batch 850 loss: 0.871917655467987
  batch 900 loss: 0.8395196092128754
LOSS train 0.83952 valid 0.84403, valid PER 25.61%
EPOCH 15, Learning Rate: 0.5
  batch 50 loss: 0.7849236452579498
  batch 100 loss: 0.7981244516372681
  batch 150 loss: 0.8298843634128571
  batch 200 loss: 0.8235678827762604
  batch 250 loss: 0.8305988037586212
  batch 300 loss: 0.8261951422691345
  batch 350 loss: 0.8247694504261017
  batch 400 loss: 0.8051124119758606
  batch 450 loss: 0.8042569041252137
  batch 500 loss: 0.8072748959064484
  batch 550 loss: 0.8452456843852997
  batch 600 loss: 0.8410813653469086
  batch 650 loss: 0.8041408705711365
  batch 700 loss: 0.8023765993118286
  batch 750 loss: 0.8356691825389863
  batch 800 loss: 0.8076623058319092
  batch 850 loss: 0.8033968496322632
  batch 900 loss: 0.7839324599504471
LOSS train 0.78393 valid 0.84252, valid PER 26.71%
EPOCH 16, Learning Rate: 0.5
  batch 50 loss: 0.7992368340492249
  batch 100 loss: 0.7741516304016113
  batch 150 loss: 0.7940263217687606
  batch 200 loss: 0.8129713368415833
  batch 250 loss: 0.800940819978714
  batch 300 loss: 0.818210289478302
  batch 350 loss: 0.7978279584646225
  batch 400 loss: 0.8019119834899903
  batch 450 loss: 0.8181687653064728
  batch 500 loss: 0.8000859189033508
  batch 550 loss: 0.7711940693855286
  batch 600 loss: 0.8169531953334809
  batch 650 loss: 0.8080875766277313
  batch 700 loss: 0.7688392049074173
  batch 750 loss: 0.8003920936584472
  batch 800 loss: 0.7836641359329224
  batch 850 loss: 0.7753921055793762
  batch 900 loss: 0.7965553498268128
LOSS train 0.79656 valid 0.83893, valid PER 26.07%
EPOCH 17, Learning Rate: 0.5
  batch 50 loss: 0.7861803495883941
  batch 100 loss: 0.726009396314621
  batch 150 loss: 0.7892249715328217
  batch 200 loss: 0.7416406792402267
  batch 250 loss: 0.7685151696205139
  batch 300 loss: 0.7552827727794648
  batch 350 loss: 0.794595662355423
  batch 400 loss: 0.770330246090889
  batch 450 loss: 0.772802300453186
  batch 500 loss: 0.7830156433582306
  batch 550 loss: 0.7642258203029633
  batch 600 loss: 0.7858206415176392
  batch 650 loss: 0.7541685247421265
  batch 700 loss: 0.7887304675579071
  batch 750 loss: 0.7479517269134521
  batch 800 loss: 0.7924042248725891
  batch 850 loss: 0.7884909164905548
  batch 900 loss: 0.7872909498214722
LOSS train 0.78729 valid 0.81683, valid PER 25.00%
EPOCH 18, Learning Rate: 0.5
  batch 50 loss: 0.746759392619133
  batch 100 loss: 0.7435645544528962
  batch 150 loss: 0.7779047417640687
  batch 200 loss: 0.7674471271038056
  batch 250 loss: 0.7323714339733124
  batch 300 loss: 0.7309504687786103
  batch 350 loss: 0.7315179771184921
  batch 400 loss: 0.7550852942466736
  batch 450 loss: 0.7705114197731018
  batch 500 loss: 0.7631342470645904
  batch 550 loss: 0.8019651174545288
  batch 600 loss: 0.7733531606197357
  batch 650 loss: 0.7353148943185807
  batch 700 loss: 0.7442821007966995
  batch 750 loss: 0.753942312002182
  batch 800 loss: 0.783258969783783
  batch 850 loss: 0.7776437556743622
  batch 900 loss: 0.7569523173570633
LOSS train 0.75695 valid 0.82113, valid PER 25.34%
EPOCH 19, Learning Rate: 0.5
  batch 50 loss: 0.7378594613075257
  batch 100 loss: 0.7297437423467636
  batch 150 loss: 0.722188600897789
  batch 200 loss: 0.7081807291507721
  batch 250 loss: 0.7613978958129883
  batch 300 loss: 0.7541122698783874
  batch 350 loss: 0.7664546871185303
  batch 400 loss: 0.7399865281581879
  batch 450 loss: 0.7084412735700607
  batch 500 loss: 0.7246496093273163
  batch 550 loss: 0.7484118056297302
  batch 600 loss: 0.7659313476085663
  batch 650 loss: 0.7842032706737518
  batch 700 loss: 0.7696124422550201
  batch 750 loss: 0.7308403635025025
  batch 800 loss: 0.7101923102140426
  batch 850 loss: 0.7588747870922089
  batch 900 loss: 0.7509952330589295
LOSS train 0.75100 valid 0.82409, valid PER 25.22%
EPOCH 20, Learning Rate: 0.5
  batch 50 loss: 0.6923807102441788
  batch 100 loss: 0.7196265912055969
  batch 150 loss: 0.7264735579490662
  batch 200 loss: 0.7129766356945038
  batch 250 loss: 0.7698546802997589
  batch 300 loss: 0.7373698461055755
  batch 350 loss: 0.71157310962677
  batch 400 loss: 0.7174276012182236
  batch 450 loss: 0.7091776704788209
  batch 500 loss: 0.729012246131897
  batch 550 loss: 0.7353377664089202
  batch 600 loss: 0.7216351854801178
  batch 650 loss: 0.7306607353687287
  batch 700 loss: 0.7106917494535446
  batch 750 loss: 0.7010170686244964
  batch 800 loss: 0.7354603314399719
  batch 850 loss: 0.7486827063560486
  batch 900 loss: 0.7183422017097473
LOSS train 0.71834 valid 0.81091, valid PER 25.04%
Training finished in 6.0 minutes.
Model saved to checkpoints/20231207_190756/model_20
Loading model from checkpoints/20231207_190756/model_20
SUB: 15.90%, DEL: 8.51%, INS: 2.14%, COR: 75.59%, PER: 26.55%
