Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.3, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 166952
EPOCH 1, Learning Rate: 0.3
  batch 50 loss: 5.182102932929992
  batch 100 loss: 3.254398922920227
  batch 150 loss: 3.1476568508148195
  batch 200 loss: 2.9817841720581053
  batch 250 loss: 2.788852128982544
  batch 300 loss: 2.6166938591003417
  batch 350 loss: 2.531553726196289
  batch 400 loss: 2.466086826324463
  batch 450 loss: 2.3933301830291747
  batch 500 loss: 2.2956569147109986
  batch 550 loss: 2.2451282429695127
  batch 600 loss: 2.210271875858307
  batch 650 loss: 2.1234955644607543
  batch 700 loss: 2.133698811531067
  batch 750 loss: 2.067773506641388
  batch 800 loss: 2.043906817436218
  batch 850 loss: 2.0067279529571533
  batch 900 loss: 2.0062746024131775
LOSS train 2.00627 valid 1.94073, valid PER 75.18%
EPOCH 2, Learning Rate: 0.3
  batch 50 loss: 1.9512647104263305
  batch 100 loss: 1.8869011449813842
  batch 150 loss: 1.8540679550170898
  batch 200 loss: 1.8496385550498962
  batch 250 loss: 1.8526855397224427
  batch 300 loss: 1.8235711073875427
  batch 350 loss: 1.7397573566436768
  batch 400 loss: 1.7540873956680298
  batch 450 loss: 1.7238647627830506
  batch 500 loss: 1.7314445757865906
  batch 550 loss: 1.7451784014701843
  batch 600 loss: 1.6890802121162414
  batch 650 loss: 1.7207803511619568
  batch 700 loss: 1.6876728320121765
  batch 750 loss: 1.672711124420166
  batch 800 loss: 1.6192097067832947
  batch 850 loss: 1.6142369675636292
  batch 900 loss: 1.6455291533470153
LOSS train 1.64553 valid 1.55466, valid PER 60.26%
EPOCH 3, Learning Rate: 0.3
  batch 50 loss: 1.608319070339203
  batch 100 loss: 1.5714853501319885
  batch 150 loss: 1.5712499523162842
  batch 200 loss: 1.5534041476249696
  batch 250 loss: 1.5419618439674379
  batch 300 loss: 1.541454746723175
  batch 350 loss: 1.5737312293052674
  batch 400 loss: 1.5552953815460204
  batch 450 loss: 1.5042192316055298
  batch 500 loss: 1.5127231407165527
  batch 550 loss: 1.5054143905639648
  batch 600 loss: 1.483474154472351
  batch 650 loss: 1.4440352535247802
  batch 700 loss: 1.4805067586898804
  batch 750 loss: 1.5347759819030762
  batch 800 loss: 1.451652455329895
  batch 850 loss: 1.4898724007606505
  batch 900 loss: 1.41895898103714
LOSS train 1.41896 valid 1.39797, valid PER 49.97%
EPOCH 4, Learning Rate: 0.3
  batch 50 loss: 1.4192247605323791
  batch 100 loss: 1.4519149303436278
  batch 150 loss: 1.3987654042243958
  batch 200 loss: 1.4422270274162292
  batch 250 loss: 1.4267623567581176
  batch 300 loss: 1.4329527807235718
  batch 350 loss: 1.35389262676239
  batch 400 loss: 1.4207576847076415
  batch 450 loss: 1.3881786751747132
  batch 500 loss: 1.3525366878509522
  batch 550 loss: 1.3797949290275573
  batch 600 loss: 1.4133780479431153
  batch 650 loss: 1.3864207458496094
  batch 700 loss: 1.3470954942703246
  batch 750 loss: 1.3448304843902588
  batch 800 loss: 1.2903729200363159
  batch 850 loss: 1.3467076444625854
  batch 900 loss: 1.3809198069572448
LOSS train 1.38092 valid 1.30404, valid PER 46.39%
EPOCH 5, Learning Rate: 0.3
  batch 50 loss: 1.320586838722229
  batch 100 loss: 1.3072188258171082
  batch 150 loss: 1.3459920620918273
  batch 200 loss: 1.284089254140854
  batch 250 loss: 1.2993075847625732
  batch 300 loss: 1.3146509861946105
  batch 350 loss: 1.3017097115516663
  batch 400 loss: 1.2889142918586731
  batch 450 loss: 1.2744615840911866
  batch 500 loss: 1.2943263292312621
  batch 550 loss: 1.2429244124889374
  batch 600 loss: 1.3207339024543763
  batch 650 loss: 1.2856267035007476
  batch 700 loss: 1.3142948317527772
  batch 750 loss: 1.2309447848796844
  batch 800 loss: 1.2621018695831299
  batch 850 loss: 1.2735235118865966
  batch 900 loss: 1.2773345184326172
LOSS train 1.27733 valid 1.20010, valid PER 40.09%
EPOCH 6, Learning Rate: 0.3
  batch 50 loss: 1.273590214252472
  batch 100 loss: 1.2175059616565704
  batch 150 loss: 1.202834633588791
  batch 200 loss: 1.2260605156421662
  batch 250 loss: 1.2582460176944732
  batch 300 loss: 1.242816412448883
  batch 350 loss: 1.2232209181785583
  batch 400 loss: 1.2155678343772889
  batch 450 loss: 1.2182463026046753
  batch 500 loss: 1.2091215896606444
  batch 550 loss: 1.2308147180080413
  batch 600 loss: 1.202725956439972
  batch 650 loss: 1.2180262851715087
  batch 700 loss: 1.2014351534843444
  batch 750 loss: 1.1861775982379914
  batch 800 loss: 1.1916411054134368
  batch 850 loss: 1.1666846871376038
  batch 900 loss: 1.1943522953987122
LOSS train 1.19435 valid 1.17129, valid PER 39.40%
EPOCH 7, Learning Rate: 0.3
  batch 50 loss: 1.1856327605247499
  batch 100 loss: 1.1935085225105286
  batch 150 loss: 1.1679416525363921
  batch 200 loss: 1.1549531602859497
  batch 250 loss: 1.1643622958660125
  batch 300 loss: 1.1525472164154054
  batch 350 loss: 1.155125255584717
  batch 400 loss: 1.1655761504173279
  batch 450 loss: 1.1507131552696228
  batch 500 loss: 1.1408842742443084
  batch 550 loss: 1.1460280430316925
  batch 600 loss: 1.154229154586792
  batch 650 loss: 1.151884970664978
  batch 700 loss: 1.1554646003246307
  batch 750 loss: 1.1324394154548645
  batch 800 loss: 1.1294673490524292
  batch 850 loss: 1.1668846654891967
  batch 900 loss: 1.1896189117431641
LOSS train 1.18962 valid 1.11460, valid PER 37.06%
EPOCH 8, Learning Rate: 0.3
  batch 50 loss: 1.1254582118988037
  batch 100 loss: 1.1257457005977631
  batch 150 loss: 1.1145778346061705
  batch 200 loss: 1.0909017658233642
  batch 250 loss: 1.125409859418869
  batch 300 loss: 1.0546019864082337
  batch 350 loss: 1.138514814376831
  batch 400 loss: 1.1012926173210145
  batch 450 loss: 1.1141850101947783
  batch 500 loss: 1.167173674106598
  batch 550 loss: 1.0730033373832704
  batch 600 loss: 1.1133853471279145
  batch 650 loss: 1.1462175261974334
  batch 700 loss: 1.092528121471405
  batch 750 loss: 1.094898726940155
  batch 800 loss: 1.104950168132782
  batch 850 loss: 1.1197988629341125
  batch 900 loss: 1.107937079668045
LOSS train 1.10794 valid 1.07260, valid PER 35.07%
EPOCH 9, Learning Rate: 0.3
  batch 50 loss: 1.0384201419353485
  batch 100 loss: 1.0895798337459563
  batch 150 loss: 1.0774528634548188
  batch 200 loss: 1.0708271431922913
  batch 250 loss: 1.095931077003479
  batch 300 loss: 1.0842444968223572
  batch 350 loss: 1.1049526810646058
  batch 400 loss: 1.1059322488307952
  batch 450 loss: 1.0810945379734038
  batch 500 loss: 1.051272234916687
  batch 550 loss: 1.0885281622409821
  batch 600 loss: 1.1000930058956147
  batch 650 loss: 1.0650681149959564
  batch 700 loss: 1.0478373432159425
  batch 750 loss: 1.066636106967926
  batch 800 loss: 1.0765490984916688
  batch 850 loss: 1.0922862720489501
  batch 900 loss: 1.0382051908969878
LOSS train 1.03821 valid 1.06750, valid PER 34.19%
EPOCH 10, Learning Rate: 0.3
  batch 50 loss: 1.0096779096126556
  batch 100 loss: 1.0264662730693817
  batch 150 loss: 1.0732595527172089
  batch 200 loss: 1.0613503658771515
  batch 250 loss: 1.0570244443416597
  batch 300 loss: 1.0282219219207764
  batch 350 loss: 1.0576500952243806
  batch 400 loss: 1.0172883403301238
  batch 450 loss: 1.0238643193244934
  batch 500 loss: 1.0598644089698792
  batch 550 loss: 1.0697271239757538
  batch 600 loss: 1.0434076118469238
  batch 650 loss: 1.016588227748871
  batch 700 loss: 1.0425678038597106
  batch 750 loss: 1.0414244496822358
  batch 800 loss: 1.040624874830246
  batch 850 loss: 1.0510897374153136
  batch 900 loss: 1.0606425821781158
LOSS train 1.06064 valid 1.04289, valid PER 34.13%
EPOCH 11, Learning Rate: 0.3
  batch 50 loss: 1.0063109111785888
  batch 100 loss: 0.9909650957584382
  batch 150 loss: 0.9927611458301544
  batch 200 loss: 1.0519944763183593
  batch 250 loss: 1.0310669577121734
  batch 300 loss: 0.9942409336566925
  batch 350 loss: 1.0142058396339417
  batch 400 loss: 1.0205344688892364
  batch 450 loss: 1.0261411988735198
  batch 500 loss: 0.9924811089038849
  batch 550 loss: 1.0062420916557313
  batch 600 loss: 1.0010830605030059
  batch 650 loss: 1.0751696574687957
  batch 700 loss: 0.9833878695964813
  batch 750 loss: 0.9799321854114532
  batch 800 loss: 1.0449329483509064
  batch 850 loss: 1.0557545173168181
  batch 900 loss: 1.0231532680988311
LOSS train 1.02315 valid 1.01848, valid PER 33.12%
EPOCH 12, Learning Rate: 0.3
  batch 50 loss: 0.998047217130661
  batch 100 loss: 0.9938022887706757
  batch 150 loss: 0.960702452659607
  batch 200 loss: 0.9689337098598481
  batch 250 loss: 0.9986979413032532
  batch 300 loss: 0.9853293836116791
  batch 350 loss: 0.9775071775913239
  batch 400 loss: 0.9864346039295196
  batch 450 loss: 0.9963749265670776
  batch 500 loss: 1.0318904089927674
  batch 550 loss: 0.9519506311416626
  batch 600 loss: 0.9704854762554169
  batch 650 loss: 1.004536999464035
  batch 700 loss: 0.9826488661766052
  batch 750 loss: 0.9880864632129669
  batch 800 loss: 0.9511938428878784
  batch 850 loss: 1.0116623401641847
  batch 900 loss: 1.0090347349643707
LOSS train 1.00903 valid 1.00184, valid PER 32.18%
EPOCH 13, Learning Rate: 0.3
  batch 50 loss: 0.941520049571991
  batch 100 loss: 0.9804948651790619
  batch 150 loss: 0.9457488775253295
  batch 200 loss: 0.9601266312599183
  batch 250 loss: 0.9718665242195129
  batch 300 loss: 0.9518286311626434
  batch 350 loss: 0.9469024300575256
  batch 400 loss: 0.9928272008895874
  batch 450 loss: 0.9797203850746155
  batch 500 loss: 0.9507999563217163
  batch 550 loss: 0.9784375762939453
  batch 600 loss: 0.9773790073394776
  batch 650 loss: 0.977945339679718
  batch 700 loss: 0.9828365385532379
  batch 750 loss: 0.9238613843917847
  batch 800 loss: 0.9492588996887207
  batch 850 loss: 1.0022558569908142
  batch 900 loss: 0.9611190521717071
LOSS train 0.96112 valid 0.97900, valid PER 31.41%
EPOCH 14, Learning Rate: 0.3
  batch 50 loss: 0.9385241639614105
  batch 100 loss: 0.93706946849823
  batch 150 loss: 0.9384073495864869
  batch 200 loss: 0.9319526517391205
  batch 250 loss: 0.9463274133205414
  batch 300 loss: 0.9788605785369873
  batch 350 loss: 0.9232076108455658
  batch 400 loss: 0.9447758746147156
  batch 450 loss: 0.9416301429271698
  batch 500 loss: 0.9472194135189056
  batch 550 loss: 0.9683842647075653
  batch 600 loss: 0.9231141185760499
  batch 650 loss: 0.9571045482158661
  batch 700 loss: 0.9807145130634308
  batch 750 loss: 0.944550051689148
  batch 800 loss: 0.894300969839096
  batch 850 loss: 0.9598268735408783
  batch 900 loss: 0.92898512840271
LOSS train 0.92899 valid 0.97612, valid PER 30.98%
EPOCH 15, Learning Rate: 0.3
  batch 50 loss: 0.9469528234004975
  batch 100 loss: 0.9100058174133301
  batch 150 loss: 0.915396020412445
  batch 200 loss: 0.9657780122756958
  batch 250 loss: 0.9331266689300537
  batch 300 loss: 0.9278941321372985
  batch 350 loss: 0.9165746319293976
  batch 400 loss: 0.9057048177719116
  batch 450 loss: 0.9202487254142762
  batch 500 loss: 0.8864596319198609
  batch 550 loss: 0.930810101032257
  batch 600 loss: 0.9513812601566315
  batch 650 loss: 0.9516639149188996
  batch 700 loss: 0.9383804595470429
  batch 750 loss: 0.9370675790309906
  batch 800 loss: 0.9115017831325531
  batch 850 loss: 0.8958700215816497
  batch 900 loss: 0.9127753353118897
LOSS train 0.91278 valid 0.97997, valid PER 31.20%
EPOCH 16, Learning Rate: 0.3
  batch 50 loss: 0.9303058695793152
  batch 100 loss: 0.883851660490036
  batch 150 loss: 0.875001620054245
  batch 200 loss: 0.9078042352199555
  batch 250 loss: 0.9193636953830719
  batch 300 loss: 0.9007784712314606
  batch 350 loss: 0.9252448666095734
  batch 400 loss: 0.9166499316692353
  batch 450 loss: 0.9246617293357849
  batch 500 loss: 0.8732045078277588
  batch 550 loss: 0.931996432542801
  batch 600 loss: 0.8999823665618897
  batch 650 loss: 0.9191848158836364
  batch 700 loss: 0.8950075697898865
  batch 750 loss: 0.8979942905902862
  batch 800 loss: 0.936197440624237
  batch 850 loss: 0.916204024553299
  batch 900 loss: 0.8918400692939759
LOSS train 0.89184 valid 0.94306, valid PER 29.80%
EPOCH 17, Learning Rate: 0.3
  batch 50 loss: 0.8908958601951599
  batch 100 loss: 0.900814437866211
  batch 150 loss: 0.8879497277736664
  batch 200 loss: 0.8714952075481415
  batch 250 loss: 0.8847139525413513
  batch 300 loss: 0.8992659687995911
  batch 350 loss: 0.8670970857143402
  batch 400 loss: 0.9144951224327087
  batch 450 loss: 0.9184724509716033
  batch 500 loss: 0.8793248653411865
  batch 550 loss: 0.8933248102664948
  batch 600 loss: 0.9503791809082032
  batch 650 loss: 0.8812611043453217
  batch 700 loss: 0.8728192079067231
  batch 750 loss: 0.8688265025615692
  batch 800 loss: 0.8774566757678985
  batch 850 loss: 0.89755495429039
  batch 900 loss: 0.8830001878738404
LOSS train 0.88300 valid 0.94858, valid PER 29.72%
EPOCH 18, Learning Rate: 0.3
  batch 50 loss: 0.885119469165802
  batch 100 loss: 0.8830952894687653
  batch 150 loss: 0.8854960107803345
  batch 200 loss: 0.8891408050060272
  batch 250 loss: 0.8736746263504028
  batch 300 loss: 0.8710165882110595
  batch 350 loss: 0.8883926069736481
  batch 400 loss: 0.845376158952713
  batch 450 loss: 0.9017306637763977
  batch 500 loss: 0.8755460774898529
  batch 550 loss: 0.9040490770339966
  batch 600 loss: 0.8639043033123016
  batch 650 loss: 0.875479142665863
  batch 700 loss: 0.9082406330108642
  batch 750 loss: 0.8709036713838577
  batch 800 loss: 0.8725264060497284
  batch 850 loss: 0.8654762148857117
  batch 900 loss: 0.9172594344615936
LOSS train 0.91726 valid 0.95802, valid PER 30.47%
EPOCH 19, Learning Rate: 0.3
  batch 50 loss: 0.8255089664459229
  batch 100 loss: 0.8241178441047669
  batch 150 loss: 0.8407588148117066
  batch 200 loss: 0.8551019525527954
  batch 250 loss: 0.8665592455863953
  batch 300 loss: 0.8496370303630829
  batch 350 loss: 0.8606267178058624
  batch 400 loss: 0.872319153547287
  batch 450 loss: 0.8694911241531372
  batch 500 loss: 0.8667221701145172
  batch 550 loss: 0.8498717772960663
  batch 600 loss: 0.8519769179821014
  batch 650 loss: 0.9309488368034363
  batch 700 loss: 0.8498098158836365
  batch 750 loss: 0.8367069208621979
  batch 800 loss: 0.8749869155883789
  batch 850 loss: 0.8932436859607696
  batch 900 loss: 0.8733917438983917
LOSS train 0.87339 valid 0.94955, valid PER 29.84%
EPOCH 20, Learning Rate: 0.3
  batch 50 loss: 0.8493557226657867
  batch 100 loss: 0.8189255499839783
  batch 150 loss: 0.8301171153783798
  batch 200 loss: 0.8731326091289521
  batch 250 loss: 0.8462677383422852
  batch 300 loss: 0.8609173035621643
  batch 350 loss: 0.8298509740829467
  batch 400 loss: 0.8459409987926483
  batch 450 loss: 0.8613253450393676
  batch 500 loss: 0.8028040516376496
  batch 550 loss: 0.9026931059360505
  batch 600 loss: 0.8148846590518951
  batch 650 loss: 0.8581880962848664
  batch 700 loss: 0.8591412651538849
  batch 750 loss: 0.8249479103088379
  batch 800 loss: 0.865404372215271
  batch 850 loss: 0.8713497519493103
  batch 900 loss: 0.8702033638954163
LOSS train 0.87020 valid 0.93028, valid PER 28.70%
Training finished in 4.0 minutes.
Model saved to checkpoints/20231210_011330/model_20
Loading model from checkpoints/20231210_011330/model_20
SUB: 15.92%, DEL: 12.71%, INS: 1.88%, COR: 71.37%, PER: 30.50%
