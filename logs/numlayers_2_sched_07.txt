Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=2, fbank_dims=23, model_dims=128, concat=1, lr=0.7, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 562216
EPOCH 1, Learning Rate: 0.7
  batch 50 loss: 4.5895408296585085
  batch 100 loss: 3.2839416694641113
  batch 150 loss: 3.1860474824905394
  batch 200 loss: 2.9863959074020388
  batch 250 loss: 2.761398959159851
  batch 300 loss: 2.5492514514923097
  batch 350 loss: 2.4638746786117554
  batch 400 loss: 2.3682154989242554
  batch 450 loss: 2.288534142971039
  batch 500 loss: 2.161188769340515
  batch 550 loss: 2.1059482169151305
  batch 600 loss: 2.018505346775055
  batch 650 loss: 1.9298525142669678
  batch 700 loss: 1.9345029044151305
  batch 750 loss: 1.853232102394104
  batch 800 loss: 1.8187991166114807
  batch 850 loss: 1.769704372882843
  batch 900 loss: 1.7373870372772218
avg val loss: 1.6300567388534546
LOSS train 1.73739 valid 1.63006, valid PER 62.00%
EPOCH 2, Learning Rate: 0.7
  batch 50 loss: 1.6944277858734131
  batch 100 loss: 1.673301522731781
  batch 150 loss: 1.5476856303215027
  batch 200 loss: 1.5768489623069764
  batch 250 loss: 1.5580596446990966
  batch 300 loss: 1.5439996933937072
  batch 350 loss: 1.5250591540336609
  batch 400 loss: 1.4833918070793153
  batch 450 loss: 1.4606105923652648
  batch 500 loss: 1.450299482345581
  batch 550 loss: 1.4294008326530456
  batch 600 loss: 1.398590452671051
  batch 650 loss: 1.3576480889320373
  batch 700 loss: 1.3732319593429565
  batch 750 loss: 1.3583173060417175
  batch 800 loss: 1.297262089252472
  batch 850 loss: 1.3190526103973388
  batch 900 loss: 1.2723871743679047
avg val loss: 1.190972089767456
LOSS train 1.27239 valid 1.19097, valid PER 37.08%
EPOCH 3, Learning Rate: 0.7
  batch 50 loss: 1.2183642315864562
  batch 100 loss: 1.2916616117954254
  batch 150 loss: 1.2999258923530579
  batch 200 loss: 1.2276381504535676
  batch 250 loss: 1.241431369781494
  batch 300 loss: 1.23398819565773
  batch 350 loss: 1.25478670835495
  batch 400 loss: 1.2238780081272125
  batch 450 loss: 1.191226122379303
  batch 500 loss: 1.1752022564411164
  batch 550 loss: 1.2069806945323944
  batch 600 loss: 1.1141882193088533
  batch 650 loss: 1.1805602264404298
  batch 700 loss: 1.159484896659851
  batch 750 loss: 1.1685982322692872
  batch 800 loss: 1.1826274847984315
  batch 850 loss: 1.1452988243103028
  batch 900 loss: 1.1445276713371277
avg val loss: 1.0625638961791992
LOSS train 1.14453 valid 1.06256, valid PER 33.38%
EPOCH 4, Learning Rate: 0.7
  batch 50 loss: 1.1543106865882873
  batch 100 loss: 1.0865560960769653
  batch 150 loss: 1.120848958492279
  batch 200 loss: 1.105066546201706
  batch 250 loss: 1.0730210769176483
  batch 300 loss: 1.1077315711975098
  batch 350 loss: 1.0832265830039978
  batch 400 loss: 1.024420839548111
  batch 450 loss: 1.0587102842330933
  batch 500 loss: 1.1097415208816528
  batch 550 loss: 1.0421943533420563
  batch 600 loss: 1.05193910241127
  batch 650 loss: 1.0984764528274535
  batch 700 loss: 1.0986419141292572
  batch 750 loss: 1.0689270520210266
  batch 800 loss: 1.0371213567256927
  batch 850 loss: 1.0344694781303405
  batch 900 loss: 1.0579785668849946
avg val loss: 0.9945129752159119
LOSS train 1.05798 valid 0.99451, valid PER 30.80%
EPOCH 5, Learning Rate: 0.7
  batch 50 loss: 1.024972414970398
  batch 100 loss: 1.0104958140850067
  batch 150 loss: 1.009740252494812
  batch 200 loss: 1.0608554887771606
  batch 250 loss: 0.9810117387771606
  batch 300 loss: 1.01852881193161
  batch 350 loss: 0.9938433969020843
  batch 400 loss: 0.9811671698093414
  batch 450 loss: 0.9848925995826722
  batch 500 loss: 0.9687598621845246
  batch 550 loss: 1.0271898138523101
  batch 600 loss: 1.0095719194412232
  batch 650 loss: 1.0146261656284332
  batch 700 loss: 0.9670354425907135
  batch 750 loss: 1.0107951390743255
  batch 800 loss: 1.0297148430347443
  batch 850 loss: 1.0128479719161987
  batch 900 loss: 0.9789093160629272
avg val loss: 0.9378564357757568
LOSS train 0.97891 valid 0.93786, valid PER 28.70%
EPOCH 6, Learning Rate: 0.7
  batch 50 loss: 0.9509776473045349
  batch 100 loss: 0.9730971074104309
  batch 150 loss: 0.9580993235111237
  batch 200 loss: 0.937048876285553
  batch 250 loss: 0.9581596338748932
  batch 300 loss: 0.9747538077831268
  batch 350 loss: 0.9758219742774963
  batch 400 loss: 0.9436168467998505
  batch 450 loss: 0.9749467372894287
  batch 500 loss: 0.9005958545207977
  batch 550 loss: 0.9556476402282715
  batch 600 loss: 0.9280532610416412
  batch 650 loss: 0.9140231537818909
  batch 700 loss: 0.9256495428085327
  batch 750 loss: 0.9590661072731018
  batch 800 loss: 0.9338291454315185
  batch 850 loss: 0.9637712121009827
  batch 900 loss: 0.9555378377437591
avg val loss: 0.8860138654708862
LOSS train 0.95554 valid 0.88601, valid PER 27.45%
EPOCH 7, Learning Rate: 0.7
  batch 50 loss: 0.8871347880363465
  batch 100 loss: 0.9395964884757996
  batch 150 loss: 0.8886187183856964
  batch 200 loss: 0.9029807937145233
  batch 250 loss: 0.9395211791992187
  batch 300 loss: 0.9051977157592773
  batch 350 loss: 0.934476054906845
  batch 400 loss: 0.8768209993839264
  batch 450 loss: 0.9084224909543991
  batch 500 loss: 0.8836358273029328
  batch 550 loss: 0.8940446734428406
  batch 600 loss: 0.9215485990047455
  batch 650 loss: 0.8828115618228912
  batch 700 loss: 0.9598953664302826
  batch 750 loss: 0.9078380930423736
  batch 800 loss: 0.900425306558609
  batch 850 loss: 0.8898045456409455
  batch 900 loss: 0.8827981865406036
avg val loss: 0.889205276966095
LOSS train 0.88280 valid 0.88921, valid PER 27.88%
EPOCH 8, Learning Rate: 0.35
  batch 50 loss: 0.847537888288498
  batch 100 loss: 0.7970674711465836
  batch 150 loss: 0.8069371628761292
  batch 200 loss: 0.7964338886737824
  batch 250 loss: 0.7909528338909149
  batch 300 loss: 0.7630720543861389
  batch 350 loss: 0.7816537845134736
  batch 400 loss: 0.775204348564148
  batch 450 loss: 0.8387338149547577
  batch 500 loss: 0.7967145359516143
  batch 550 loss: 0.8014286518096924
  batch 600 loss: 0.7568459814786911
  batch 650 loss: 0.7711113256216049
  batch 700 loss: 0.799747964143753
  batch 750 loss: 0.7949913465976715
  batch 800 loss: 0.7937828350067139
  batch 850 loss: 0.7697318851947784
  batch 900 loss: 0.7870852398872376
avg val loss: 0.8102686405181885
LOSS train 0.78709 valid 0.81027, valid PER 25.22%
EPOCH 9, Learning Rate: 0.35
  batch 50 loss: 0.7330505132675171
  batch 100 loss: 0.7361393284797668
  batch 150 loss: 0.7559935247898102
  batch 200 loss: 0.7380488330125808
  batch 250 loss: 0.7095488375425338
  batch 300 loss: 0.7550762283802033
  batch 350 loss: 0.7330682146549224
  batch 400 loss: 0.7512633216381073
  batch 450 loss: 0.771965274810791
  batch 500 loss: 0.7417451906204223
  batch 550 loss: 0.7531719481945038
  batch 600 loss: 0.7723677706718445
  batch 650 loss: 0.7578195071220398
  batch 700 loss: 0.7409259951114655
  batch 750 loss: 0.7602092635631561
  batch 800 loss: 0.7821148943901062
  batch 850 loss: 0.7807421100139618
  batch 900 loss: 0.7291900944709778
avg val loss: 0.7894280552864075
LOSS train 0.72919 valid 0.78943, valid PER 24.19%
EPOCH 10, Learning Rate: 0.35
  batch 50 loss: 0.7145439577102661
  batch 100 loss: 0.7278108406066894
  batch 150 loss: 0.742015153169632
  batch 200 loss: 0.7213266885280609
  batch 250 loss: 0.7141704440116883
  batch 300 loss: 0.7266748344898224
  batch 350 loss: 0.7198687839508057
  batch 400 loss: 0.6992112004756927
  batch 450 loss: 0.7053575176000595
  batch 500 loss: 0.7181273931264878
  batch 550 loss: 0.7349207425117492
  batch 600 loss: 0.7366479623317719
  batch 650 loss: 0.7443868362903595
  batch 700 loss: 0.739234380722046
  batch 750 loss: 0.7376694065332413
  batch 800 loss: 0.7530249333381653
  batch 850 loss: 0.7290131998062134
  batch 900 loss: 0.73337304353714
avg val loss: 0.7924903035163879
LOSS train 0.73337 valid 0.79249, valid PER 24.59%
EPOCH 11, Learning Rate: 0.175
  batch 50 loss: 0.707445387840271
  batch 100 loss: 0.6740412086248397
  batch 150 loss: 0.6670636129379273
  batch 200 loss: 0.6218112742900849
  batch 250 loss: 0.6572982931137085
  batch 300 loss: 0.6329480820894241
  batch 350 loss: 0.6944955855607986
  batch 400 loss: 0.6620942604541779
  batch 450 loss: 0.6578329300880432
  batch 500 loss: 0.6705939221382141
  batch 550 loss: 0.6857854741811752
  batch 600 loss: 0.6744022160768509
  batch 650 loss: 0.6676524770259857
  batch 700 loss: 0.7120839518308639
  batch 750 loss: 0.6612144649028778
  batch 800 loss: 0.670547035932541
  batch 850 loss: 0.6630016559362412
  batch 900 loss: 0.6940650564432144
avg val loss: 0.7439092993736267
LOSS train 0.69407 valid 0.74391, valid PER 22.91%
EPOCH 12, Learning Rate: 0.175
  batch 50 loss: 0.6257501047849655
  batch 100 loss: 0.6296086049079895
  batch 150 loss: 0.6552827894687653
  batch 200 loss: 0.6406937682628632
  batch 250 loss: 0.6373764598369598
  batch 300 loss: 0.666417915225029
  batch 350 loss: 0.6447741013765335
  batch 400 loss: 0.6771988070011139
  batch 450 loss: 0.6330756974220276
  batch 500 loss: 0.6563740855455399
  batch 550 loss: 0.6449158126115799
  batch 600 loss: 0.67062912940979
  batch 650 loss: 0.6525788480043411
  batch 700 loss: 0.6448395150899887
  batch 750 loss: 0.6509340447187424
  batch 800 loss: 0.6098245120048523
  batch 850 loss: 0.6354760330915451
  batch 900 loss: 0.6677290642261505
avg val loss: 0.7469022274017334
LOSS train 0.66773 valid 0.74690, valid PER 22.66%
EPOCH 13, Learning Rate: 0.0875
  batch 50 loss: 0.6130775398015976
  batch 100 loss: 0.6222994434833526
  batch 150 loss: 0.640401661992073
  batch 200 loss: 0.583791207075119
  batch 250 loss: 0.6097030574083329
  batch 300 loss: 0.6412057191133499
  batch 350 loss: 0.6052444034814835
  batch 400 loss: 0.601500597000122
  batch 450 loss: 0.6079902142286301
  batch 500 loss: 0.6098525530099869
  batch 550 loss: 0.6528039389848709
  batch 600 loss: 0.6200145411491395
  batch 650 loss: 0.6093512660264969
  batch 700 loss: 0.6273739218711853
  batch 750 loss: 0.5828770399093628
  batch 800 loss: 0.6060679560899734
  batch 850 loss: 0.5871808069944382
  batch 900 loss: 0.6265737253427506
avg val loss: 0.7283930778503418
LOSS train 0.62657 valid 0.72839, valid PER 22.30%
EPOCH 14, Learning Rate: 0.0875
  batch 50 loss: 0.6011765193939209
  batch 100 loss: 0.579581065773964
  batch 150 loss: 0.606003428697586
  batch 200 loss: 0.6042475432157517
  batch 250 loss: 0.5848476469516755
  batch 300 loss: 0.581304339170456
  batch 350 loss: 0.5866711342334747
  batch 400 loss: 0.6183097112178803
  batch 450 loss: 0.5833016091585159
  batch 500 loss: 0.6043018758296966
  batch 550 loss: 0.6028032737970352
  batch 600 loss: 0.5778815710544586
  batch 650 loss: 0.6099258869886398
  batch 700 loss: 0.617261216044426
  batch 750 loss: 0.6038233405351638
  batch 800 loss: 0.5923185682296753
  batch 850 loss: 0.629293829202652
  batch 900 loss: 0.6142477488517761
avg val loss: 0.7331587672233582
LOSS train 0.61425 valid 0.73316, valid PER 22.26%
EPOCH 15, Learning Rate: 0.04375
  batch 50 loss: 0.567148910164833
  batch 100 loss: 0.5864137285947799
  batch 150 loss: 0.5908838301897049
  batch 200 loss: 0.6122295379638671
  batch 250 loss: 0.5995027005672455
  batch 300 loss: 0.5851782214641571
  batch 350 loss: 0.5760857409238815
  batch 400 loss: 0.5765659964084625
  batch 450 loss: 0.5720099365711212
  batch 500 loss: 0.5768112510442733
  batch 550 loss: 0.6085120642185211
  batch 600 loss: 0.6189761930704116
  batch 650 loss: 0.5812674206495285
  batch 700 loss: 0.575724042057991
  batch 750 loss: 0.6003510838747025
  batch 800 loss: 0.5737682455778121
  batch 850 loss: 0.5589393615722656
  batch 900 loss: 0.5528292983770371
avg val loss: 0.7266201376914978
LOSS train 0.55283 valid 0.72662, valid PER 21.98%
EPOCH 16, Learning Rate: 0.04375
  batch 50 loss: 0.5881793695688248
  batch 100 loss: 0.5583004814386368
  batch 150 loss: 0.5756206852197647
  batch 200 loss: 0.6047617197036743
  batch 250 loss: 0.5612303745746613
  batch 300 loss: 0.5693368262052536
  batch 350 loss: 0.5829484337568283
  batch 400 loss: 0.5909960407018662
  batch 450 loss: 0.6003893685340881
  batch 500 loss: 0.5648290371894836
  batch 550 loss: 0.5572032731771469
  batch 600 loss: 0.6090827828645706
  batch 650 loss: 0.6002282059192657
  batch 700 loss: 0.5398534446954727
  batch 750 loss: 0.5818884670734406
  batch 800 loss: 0.5631195735931397
  batch 850 loss: 0.5699597680568695
  batch 900 loss: 0.5787308031320572
avg val loss: 0.7265754342079163
LOSS train 0.57873 valid 0.72658, valid PER 22.04%
EPOCH 17, Learning Rate: 0.021875
  batch 50 loss: 0.5830631214380264
  batch 100 loss: 0.5380797904729843
  batch 150 loss: 0.6000964635610581
  batch 200 loss: 0.5485251194238663
  batch 250 loss: 0.56451609313488
  batch 300 loss: 0.5503883528709411
  batch 350 loss: 0.5793223601579666
  batch 400 loss: 0.5742551046609878
  batch 450 loss: 0.5559535849094391
  batch 500 loss: 0.5659726488590241
  batch 550 loss: 0.5777493607997894
  batch 600 loss: 0.6013680189847946
  batch 650 loss: 0.5473621970415116
  batch 700 loss: 0.5823826676607132
  batch 750 loss: 0.5365851992368698
  batch 800 loss: 0.5638176590204239
  batch 850 loss: 0.5715257877111435
  batch 900 loss: 0.5656622189283371
avg val loss: 0.7259120941162109
LOSS train 0.56566 valid 0.72591, valid PER 21.96%
EPOCH 18, Learning Rate: 0.021875
  batch 50 loss: 0.5430700635910034
  batch 100 loss: 0.5391058117151261
  batch 150 loss: 0.5963189387321473
  batch 200 loss: 0.569015599489212
  batch 250 loss: 0.5508531039953232
  batch 300 loss: 0.5533865213394165
  batch 350 loss: 0.5477503263950347
  batch 400 loss: 0.5536891347169877
  batch 450 loss: 0.5529449737071991
  batch 500 loss: 0.5502506393194199
  batch 550 loss: 0.6019652175903321
  batch 600 loss: 0.5777001339197159
  batch 650 loss: 0.5520914351940155
  batch 700 loss: 0.550170664191246
  batch 750 loss: 0.5566604900360107
  batch 800 loss: 0.5874812984466553
  batch 850 loss: 0.5833625257015228
  batch 900 loss: 0.5602611017227173
avg val loss: 0.7240807414054871
LOSS train 0.56026 valid 0.72408, valid PER 21.89%
EPOCH 19, Learning Rate: 0.021875
  batch 50 loss: 0.5761465615034104
  batch 100 loss: 0.5606899780035018
  batch 150 loss: 0.5557926565408706
  batch 200 loss: 0.5418266475200653
  batch 250 loss: 0.5748576527833938
  batch 300 loss: 0.5956382095813751
  batch 350 loss: 0.5775514465570449
  batch 400 loss: 0.5570367270708084
  batch 450 loss: 0.5256747317314148
  batch 500 loss: 0.5410346484184265
  batch 550 loss: 0.5642526930570603
  batch 600 loss: 0.5755775785446167
  batch 650 loss: 0.5815076971054077
  batch 700 loss: 0.5776345080137253
  batch 750 loss: 0.5511765354871749
  batch 800 loss: 0.5280597066879272
  batch 850 loss: 0.5506785774230957
  batch 900 loss: 0.5456323534250259
avg val loss: 0.7233050465583801
LOSS train 0.54563 valid 0.72331, valid PER 22.02%
EPOCH 20, Learning Rate: 0.021875
  batch 50 loss: 0.5338483536243439
  batch 100 loss: 0.5640381401777268
  batch 150 loss: 0.5642896592617035
  batch 200 loss: 0.5631303662061691
  batch 250 loss: 0.5941401261091233
  batch 300 loss: 0.5617707079648971
  batch 350 loss: 0.5450873440504074
  batch 400 loss: 0.5466795110702515
  batch 450 loss: 0.5359276485443115
  batch 500 loss: 0.5793783241510391
  batch 550 loss: 0.569926210641861
  batch 600 loss: 0.5624597692489623
  batch 650 loss: 0.582395910024643
  batch 700 loss: 0.5383715051412582
  batch 750 loss: 0.5416550725698471
  batch 800 loss: 0.564524695277214
  batch 850 loss: 0.5722019481658935
  batch 900 loss: 0.5341919451951981
avg val loss: 0.7252840995788574
LOSS train 0.53419 valid 0.72528, valid PER 21.76%
Training finished in 5.0 minutes.
Model saved to checkpoints/20231210_135158/model_19
Loading model from checkpoints/20231210_135158/model_19
SUB: 14.65%, DEL: 7.47%, INS: 2.10%, COR: 77.88%, PER: 24.22%
