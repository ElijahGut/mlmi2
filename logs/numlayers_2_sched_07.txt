Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=2, fbank_dims=23, model_dims=128, concat=1, lr=0.7, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 562216
EPOCH 1, Learning Rate: 0.7
  batch 50 loss: 4.58954626083374
  batch 100 loss: 3.283941960334778
  batch 150 loss: 3.1860483837127687
  batch 200 loss: 2.9863974761962893
  batch 250 loss: 2.7613970756530763
  batch 300 loss: 2.5492537879943846
  batch 350 loss: 2.463858528137207
  batch 400 loss: 2.3682156372070313
  batch 450 loss: 2.2885352206230163
  batch 500 loss: 2.161194095611572
  batch 550 loss: 2.105943238735199
  batch 600 loss: 2.0185029554367064
  batch 650 loss: 1.9298613476753235
  batch 700 loss: 1.9345753622055053
  batch 750 loss: 1.8554886031150817
  batch 800 loss: 1.8176780867576598
  batch 850 loss: 1.7689054608345032
  batch 900 loss: 1.7387147665023803
running loss: 40.131714820861816
LOSS train 1.73871 valid 1.63081, valid PER 61.87%
EPOCH 2, Learning Rate: 0.7
  batch 50 loss: 1.6945169162750244
  batch 100 loss: 1.6763849210739137
  batch 150 loss: 1.5474864435195923
  batch 200 loss: 1.575589725971222
  batch 250 loss: 1.5610295176506042
  batch 300 loss: 1.5382872867584227
  batch 350 loss: 1.5233945298194884
  batch 400 loss: 1.4864169573783874
  batch 450 loss: 1.4630030488967896
  batch 500 loss: 1.4485248470306396
  batch 550 loss: 1.4461983513832093
  batch 600 loss: 1.4012397193908692
  batch 650 loss: 1.3572061491012573
  batch 700 loss: 1.383687334060669
  batch 750 loss: 1.3527090001106261
  batch 800 loss: 1.3062032675743103
  batch 850 loss: 1.3068898272514344
  batch 900 loss: 1.2794268608093262
running loss: 31.101360321044922
LOSS train 1.27943 valid 1.20042, valid PER 37.42%
EPOCH 3, Learning Rate: 0.7
  batch 50 loss: 1.2126539409160615
  batch 100 loss: 1.2918118607997895
  batch 150 loss: 1.3034954237937928
  batch 200 loss: 1.22688960313797
  batch 250 loss: 1.2356903433799744
  batch 300 loss: 1.227391220331192
  batch 350 loss: 1.247880721092224
  batch 400 loss: 1.228649971485138
  batch 450 loss: 1.2028956806659699
  batch 500 loss: 1.1822620737552643
  batch 550 loss: 1.1945996069908142
  batch 600 loss: 1.1278508496284485
  batch 650 loss: 1.1782629871368409
  batch 700 loss: 1.1635171461105347
  batch 750 loss: 1.1796803724765779
  batch 800 loss: 1.175924084186554
  batch 850 loss: 1.1510373163223266
  batch 900 loss: 1.1578205251693725
running loss: 28.760711550712585
LOSS train 1.15782 valid 1.08287, valid PER 33.92%
EPOCH 4, Learning Rate: 0.7
  batch 50 loss: 1.151330099105835
  batch 100 loss: 1.0836559915542603
  batch 150 loss: 1.1057465469837189
  batch 200 loss: 1.1035276007652284
  batch 250 loss: 1.086746472120285
  batch 300 loss: 1.0981035733222961
  batch 350 loss: 1.0867442560195923
  batch 400 loss: 1.0307070231437683
  batch 450 loss: 1.057721096277237
  batch 500 loss: 1.1142986404895783
  batch 550 loss: 1.0560307097434998
  batch 600 loss: 1.0349542486667633
  batch 650 loss: 1.1051106429100037
  batch 700 loss: 1.1056980073451996
  batch 750 loss: 1.0694445765018463
  batch 800 loss: 1.0560029256343841
  batch 850 loss: 1.037707849740982
  batch 900 loss: 1.0557534098625183
running loss: 25.925401270389557
LOSS train 1.05575 valid 1.00666, valid PER 31.22%
EPOCH 5, Learning Rate: 0.7
  batch 50 loss: 1.0144083034992217
  batch 100 loss: 0.9895344591140747
  batch 150 loss: 1.0065856432914735
  batch 200 loss: 1.0456466221809386
  batch 250 loss: 0.9842606723308563
  batch 300 loss: 1.032715735435486
  batch 350 loss: 0.9864003872871399
  batch 400 loss: 0.9650271725654602
  batch 450 loss: 0.9634536480903626
  batch 500 loss: 0.9738115835189819
  batch 550 loss: 1.0273818063735962
  batch 600 loss: 1.0139188468456268
  batch 650 loss: 0.9899952161312103
  batch 700 loss: 0.96526398062706
  batch 750 loss: 0.9694098818302155
  batch 800 loss: 1.0279877877235413
  batch 850 loss: 0.9958797860145568
  batch 900 loss: 0.9725956523418426
running loss: 23.637527346611023
LOSS train 0.97260 valid 0.92305, valid PER 28.82%
EPOCH 6, Learning Rate: 0.7
  batch 50 loss: 0.9308862340450287
  batch 100 loss: 0.9464892935752869
  batch 150 loss: 0.9327895474433899
  batch 200 loss: 0.9041174626350403
  batch 250 loss: 0.9128952884674072
  batch 300 loss: 0.958706910610199
  batch 350 loss: 0.9692638170719147
  batch 400 loss: 0.9332396924495697
  batch 450 loss: 0.965107125043869
  batch 500 loss: 0.8989847528934479
  batch 550 loss: 0.920501903295517
  batch 600 loss: 0.9262175810337067
  batch 650 loss: 0.9247191083431244
  batch 700 loss: 0.9217298293113708
  batch 750 loss: 0.9576148104667663
  batch 800 loss: 0.9291385281085968
  batch 850 loss: 0.9728816902637482
  batch 900 loss: 0.9641073226928711
running loss: 21.667645633220673
LOSS train 0.96411 valid 0.88949, valid PER 28.01%
EPOCH 7, Learning Rate: 0.7
  batch 50 loss: 0.8677925193309783
  batch 100 loss: 0.9213622915744781
  batch 150 loss: 0.8746697497367859
  batch 200 loss: 0.88134605884552
  batch 250 loss: 0.922410523891449
  batch 300 loss: 0.9096483325958252
  batch 350 loss: 0.934781757593155
  batch 400 loss: 0.8645953345298767
  batch 450 loss: 0.8765156829357147
  batch 500 loss: 0.8670041704177857
  batch 550 loss: 0.861597615480423
  batch 600 loss: 0.8930674529075623
  batch 650 loss: 0.8899147248268128
  batch 700 loss: 0.9351015436649323
  batch 750 loss: 0.8718391788005829
  batch 800 loss: 0.8752289032936096
  batch 850 loss: 0.8614607203006744
  batch 900 loss: 0.8760444140434265
running loss: 20.232979774475098
LOSS train 0.87604 valid 0.87288, valid PER 27.91%
EPOCH 8, Learning Rate: 0.7
  batch 50 loss: 0.8658500134944915
  batch 100 loss: 0.8503384292125702
  batch 150 loss: 0.8606772828102112
  batch 200 loss: 0.8603380489349365
  batch 250 loss: 0.8589320659637452
  batch 300 loss: 0.8258966481685639
  batch 350 loss: 0.8543007433414459
  batch 400 loss: 0.833390554189682
  batch 450 loss: 0.8853510737419128
  batch 500 loss: 0.8538532292842865
  batch 550 loss: 0.8672239089012146
  batch 600 loss: 0.8278960013389587
  batch 650 loss: 0.8495564734935761
  batch 700 loss: 0.8682544088363647
  batch 750 loss: 0.8509124135971069
  batch 800 loss: 0.8580951023101807
  batch 850 loss: 0.8137307798862458
  batch 900 loss: 0.8453742194175721
running loss: 19.928069472312927
LOSS train 0.84537 valid 0.85166, valid PER 26.27%
EPOCH 9, Learning Rate: 0.7
  batch 50 loss: 0.7834920382499695
  batch 100 loss: 0.780021060705185
  batch 150 loss: 0.8240156590938568
  batch 200 loss: 0.8137778270244599
  batch 250 loss: 0.7862126135826111
  batch 300 loss: 0.8231882047653198
  batch 350 loss: 0.8030156135559082
  batch 400 loss: 0.8321647489070892
  batch 450 loss: 0.84554447889328
  batch 500 loss: 0.816915158033371
  batch 550 loss: 0.8134089934825898
  batch 600 loss: 0.8490146088600159
  batch 650 loss: 0.8341712534427643
  batch 700 loss: 0.8112756156921387
  batch 750 loss: 0.830436166524887
  batch 800 loss: 0.8363644766807556
  batch 850 loss: 0.8423694741725921
  batch 900 loss: 0.7938238513469696
running loss: 20.400208234786987
LOSS train 0.79382 valid 0.83507, valid PER 25.82%
EPOCH 10, Learning Rate: 0.35
  batch 50 loss: 0.7519140803813934
  batch 100 loss: 0.737603400349617
  batch 150 loss: 0.755686160326004
  batch 200 loss: 0.7031237655878066
  batch 250 loss: 0.7068345177173615
  batch 300 loss: 0.7139976960420609
  batch 350 loss: 0.7015792059898377
  batch 400 loss: 0.6753258860111236
  batch 450 loss: 0.6859509474039078
  batch 500 loss: 0.6904741668701172
  batch 550 loss: 0.722914302945137
  batch 600 loss: 0.7179109215736389
  batch 650 loss: 0.7327375888824463
  batch 700 loss: 0.7079942882061004
  batch 750 loss: 0.727392497062683
  batch 800 loss: 0.7228788959980011
  batch 850 loss: 0.7019581252336502
  batch 900 loss: 0.7083254849910736
running loss: 17.810015559196472
LOSS train 0.70833 valid 0.77258, valid PER 23.90%
EPOCH 11, Learning Rate: 0.35
  batch 50 loss: 0.685565282702446
  batch 100 loss: 0.6692933267354966
  batch 150 loss: 0.665164223909378
  batch 200 loss: 0.6254093384742737
  batch 250 loss: 0.6583756560087204
  batch 300 loss: 0.6469842302799225
  batch 350 loss: 0.7154204851388931
  batch 400 loss: 0.6652578473091125
  batch 450 loss: 0.6789599770307541
  batch 500 loss: 0.6818498522043228
  batch 550 loss: 0.7023269319534302
  batch 600 loss: 0.6814520138502121
  batch 650 loss: 0.6908534443378449
  batch 700 loss: 0.7361432093381882
  batch 750 loss: 0.6711198276281357
  batch 800 loss: 0.6833164495229721
  batch 850 loss: 0.6841565322875977
  batch 900 loss: 0.6989647436141968
running loss: 15.04655933380127
LOSS train 0.69896 valid 0.76089, valid PER 23.10%
EPOCH 12, Learning Rate: 0.35
  batch 50 loss: 0.619549595117569
  batch 100 loss: 0.6291398763656616
  batch 150 loss: 0.6509920412302017
  batch 200 loss: 0.6408330684900284
  batch 250 loss: 0.6564120000600815
  batch 300 loss: 0.6869775509834289
  batch 350 loss: 0.6586338222026825
  batch 400 loss: 0.686015899181366
  batch 450 loss: 0.6376457566022873
  batch 500 loss: 0.675976168513298
  batch 550 loss: 0.6624239313602448
  batch 600 loss: 0.6755845296382904
  batch 650 loss: 0.6637452185153961
  batch 700 loss: 0.6574173188209533
  batch 750 loss: 0.6694190871715545
  batch 800 loss: 0.6273708283901215
  batch 850 loss: 0.6541361844539643
  batch 900 loss: 0.6786520445346832
running loss: 16.098136395215988
LOSS train 0.67865 valid 0.75377, valid PER 22.87%
EPOCH 13, Learning Rate: 0.175
  batch 50 loss: 0.6173526388406754
  batch 100 loss: 0.615269449353218
  batch 150 loss: 0.6176624250411987
  batch 200 loss: 0.563961055278778
  batch 250 loss: 0.5866323924064636
  batch 300 loss: 0.6197132039070129
  batch 350 loss: 0.5849133348464965
  batch 400 loss: 0.5839703583717346
  batch 450 loss: 0.5980125743150712
  batch 500 loss: 0.600441821217537
  batch 550 loss: 0.6327218526601791
  batch 600 loss: 0.6143207693099976
  batch 650 loss: 0.6086880952119827
  batch 700 loss: 0.6149589502811432
  batch 750 loss: 0.5881091189384461
  batch 800 loss: 0.5893127912282944
  batch 850 loss: 0.5750039452314377
  batch 900 loss: 0.5977873122692108
running loss: 14.638198882341385
LOSS train 0.59779 valid 0.73249, valid PER 22.19%
EPOCH 14, Learning Rate: 0.175
  batch 50 loss: 0.5722344690561294
  batch 100 loss: 0.5543294465541839
  batch 150 loss: 0.5941113412380219
  batch 200 loss: 0.590768119096756
  batch 250 loss: 0.5692152225971222
  batch 300 loss: 0.575707402229309
  batch 350 loss: 0.5709475773572922
  batch 400 loss: 0.5991706830263138
  batch 450 loss: 0.5700570750236511
  batch 500 loss: 0.601411092877388
  batch 550 loss: 0.6003249651193618
  batch 600 loss: 0.5691796886920929
  batch 650 loss: 0.5719262558221817
  batch 700 loss: 0.5954341202974319
  batch 750 loss: 0.5877898311614991
  batch 800 loss: 0.5783368158340454
  batch 850 loss: 0.6065363550186157
  batch 900 loss: 0.6019922965764999
running loss: 13.745224416255951
LOSS train 0.60199 valid 0.72074, valid PER 21.97%
EPOCH 15, Learning Rate: 0.175
  batch 50 loss: 0.5472348177433014
  batch 100 loss: 0.5639576888084412
  batch 150 loss: 0.5708223837614059
  batch 200 loss: 0.5898777461051941
  batch 250 loss: 0.5907170712947846
  batch 300 loss: 0.580493061542511
  batch 350 loss: 0.5659735596179962
  batch 400 loss: 0.5625892895460128
  batch 450 loss: 0.5724302452802658
  batch 500 loss: 0.5666052067279815
  batch 550 loss: 0.6022314536571503
  batch 600 loss: 0.6077842104434967
  batch 650 loss: 0.5745788359642029
  batch 700 loss: 0.5653081464767457
  batch 750 loss: 0.5903905045986175
  batch 800 loss: 0.5540241402387619
  batch 850 loss: 0.5545693498849869
  batch 900 loss: 0.5476378107070923
running loss: 13.93674710392952
LOSS train 0.54764 valid 0.71721, valid PER 22.09%
EPOCH 16, Learning Rate: 0.0875
  batch 50 loss: 0.5466195154190063
  batch 100 loss: 0.52912193775177
  batch 150 loss: 0.5426935249567032
  batch 200 loss: 0.5575737529993057
  batch 250 loss: 0.530465049147606
  batch 300 loss: 0.5293467569351197
  batch 350 loss: 0.5642636197805405
  batch 400 loss: 0.544747314453125
  batch 450 loss: 0.5755511021614075
  batch 500 loss: 0.5239338201284408
  batch 550 loss: 0.5218620586395264
  batch 600 loss: 0.5733782094717026
  batch 650 loss: 0.5641103440523147
  batch 700 loss: 0.5077172696590424
  batch 750 loss: 0.5427268862724304
  batch 800 loss: 0.5226403921842575
  batch 850 loss: 0.5284648990631103
  batch 900 loss: 0.5356056326627732
running loss: 13.022347748279572
LOSS train 0.53561 valid 0.71973, valid PER 21.68%
EPOCH 17, Learning Rate: 0.0875
  batch 50 loss: 0.5396773874759674
  batch 100 loss: 0.5065059638023377
  batch 150 loss: 0.5639721828699112
  batch 200 loss: 0.5131257635354995
  batch 250 loss: 0.5222290790081024
  batch 300 loss: 0.5150372993946075
  batch 350 loss: 0.5464643514156342
  batch 400 loss: 0.527267553806305
  batch 450 loss: 0.5164995843172073
  batch 500 loss: 0.5313635456562043
  batch 550 loss: 0.5289703607559204
  batch 600 loss: 0.5595509052276612
  batch 650 loss: 0.5132867801189422
  batch 700 loss: 0.5498522984981536
  batch 750 loss: 0.503832032084465
  batch 800 loss: 0.5417430859804153
  batch 850 loss: 0.5408008134365082
  batch 900 loss: 0.5301076310873032
running loss: 12.484498679637909
LOSS train 0.53011 valid 0.72381, valid PER 21.54%
EPOCH 18, Learning Rate: 0.0875
  batch 50 loss: 0.4880241990089417
  batch 100 loss: 0.5113192129135132
  batch 150 loss: 0.5560911786556244
  batch 200 loss: 0.5199148315191269
  batch 250 loss: 0.503540461063385
  batch 300 loss: 0.5146153783798217
  batch 350 loss: 0.49837815642356875
  batch 400 loss: 0.5081450152397156
  batch 450 loss: 0.5180752235651016
  batch 500 loss: 0.5175634634494781
  batch 550 loss: 0.5486872041225433
  batch 600 loss: 0.5264254605770111
  batch 650 loss: 0.5189064437150955
  batch 700 loss: 0.5215315514802933
  batch 750 loss: 0.5214446127414704
  batch 800 loss: 0.555518142580986
  batch 850 loss: 0.5396722275018692
  batch 900 loss: 0.525204302072525
running loss: 12.640685051679611
LOSS train 0.52520 valid 0.72066, valid PER 21.53%
EPOCH 19, Learning Rate: 0.04375
  batch 50 loss: 0.5103548181056976
  batch 100 loss: 0.5036108377575874
  batch 150 loss: 0.4998404312133789
  batch 200 loss: 0.47974303245544436
  batch 250 loss: 0.5150726395845413
  batch 300 loss: 0.5368147313594818
  batch 350 loss: 0.5233620536327362
  batch 400 loss: 0.5085591465234757
  batch 450 loss: 0.4810697615146637
  batch 500 loss: 0.499687659740448
  batch 550 loss: 0.5183355543017387
  batch 600 loss: 0.5252449935674668
  batch 650 loss: 0.5270620799064636
  batch 700 loss: 0.5312125158309936
  batch 750 loss: 0.5012833440303802
  batch 800 loss: 0.4800754988193512
  batch 850 loss: 0.49774671912193297
  batch 900 loss: 0.49211227595806123
running loss: 11.582285046577454
LOSS train 0.49211 valid 0.71968, valid PER 21.53%
EPOCH 20, Learning Rate: 0.04375
  batch 50 loss: 0.48485624849796294
  batch 100 loss: 0.5082676929235458
  batch 150 loss: 0.49977334022521974
  batch 200 loss: 0.5066013652086258
  batch 250 loss: 0.513234430551529
  batch 300 loss: 0.5093369042873382
  batch 350 loss: 0.4877552750706673
  batch 400 loss: 0.4886473435163498
  batch 450 loss: 0.47207269072532654
  batch 500 loss: 0.5160327357053757
  batch 550 loss: 0.5213318252563477
  batch 600 loss: 0.49732928216457367
  batch 650 loss: 0.5248968458175659
  batch 700 loss: 0.47639971256256103
  batch 750 loss: 0.48487880766391755
  batch 800 loss: 0.508310005068779
  batch 850 loss: 0.5056348180770874
  batch 900 loss: 0.48489474415779116
running loss: 12.859361559152603
LOSS train 0.48489 valid 0.72118, valid PER 21.52%
Training finished in 5.0 minutes.
Model saved to checkpoints/20231210_043017/model_15
Loading model from checkpoints/20231210_043017/model_15
SUB: 16.49%, DEL: 13.74%, INS: 1.54%, COR: 69.78%, PER: 31.77%
