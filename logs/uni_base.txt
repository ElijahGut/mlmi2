Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0, optimiser='sgd', grad_clip=None, is_bidir=True)
Total number of model parameters is 83496
EPOCH 1, Learning Rate: 0.5
  batch 50 loss: 4.061703839302063
  batch 100 loss: 3.0928565645217896
  batch 150 loss: 3.2453551149368285
  batch 200 loss: 2.9319540452957153
  batch 250 loss: 2.8014694690704345
  batch 300 loss: 2.6246673393249513
  batch 350 loss: 2.5282610845565796
  batch 400 loss: 2.471819143295288
  batch 450 loss: 2.3886134672164916
  batch 500 loss: 2.321262912750244
  batch 550 loss: 2.2168284606933595
  batch 600 loss: 2.1396750617027283
  batch 650 loss: 2.060019335746765
  batch 700 loss: 2.0488401508331298
  batch 750 loss: 2.0310867404937745
  batch 800 loss: 1.9813883638381957
  batch 850 loss: 2.0730767321586607
  batch 900 loss: 1.9970760560035705
LOSS train 1.99708 valid 1.90031, valid PER 72.33%
EPOCH 2, Learning Rate: 0.5
  batch 50 loss: 1.9192699074745179
  batch 100 loss: 1.8393159127235412
  batch 150 loss: 1.8278790259361266
  batch 200 loss: 1.8135626268386842
  batch 250 loss: 1.819486858844757
  batch 300 loss: 1.764568908214569
  batch 350 loss: 1.6831658577919006
  batch 400 loss: 1.691600842475891
  batch 450 loss: 1.6391508030891417
  batch 500 loss: 1.6538261198997497
  batch 550 loss: 1.6707143354415894
  batch 600 loss: 1.597211904525757
  batch 650 loss: 1.6236087989807129
  batch 700 loss: 1.5993969678878783
  batch 750 loss: 1.565853147506714
  batch 800 loss: 1.5150585985183715
  batch 850 loss: 1.5157185363769532
  batch 900 loss: 1.5168426895141602
LOSS train 1.51684 valid 1.54868, valid PER 48.81%
EPOCH 3, Learning Rate: 0.5
  batch 50 loss: 1.4840404987335205
  batch 100 loss: 1.471971151828766
  batch 150 loss: 1.4571519446372987
  batch 200 loss: 1.4569552969932555
  batch 250 loss: 1.4453324723243712
  batch 300 loss: 1.4480334043502807
  batch 350 loss: 1.47757803440094
  batch 400 loss: 1.4403692388534546
  batch 450 loss: 1.418328170776367
  batch 500 loss: 1.4344289350509642
  batch 550 loss: 1.4309473133087158
  batch 600 loss: 1.4063259530067445
  batch 650 loss: 1.4060887265205384
  batch 700 loss: 1.4284052753448486
  batch 750 loss: 1.4647823286056518
  batch 800 loss: 1.389485969543457
  batch 850 loss: 1.398690299987793
  batch 900 loss: 1.348988606929779
LOSS train 1.34899 valid 1.43111, valid PER 42.67%
EPOCH 4, Learning Rate: 0.5
  batch 50 loss: 1.3532223200798035
  batch 100 loss: 1.3484043407440185
  batch 150 loss: 1.3018941712379455
  batch 200 loss: 1.3458992075920104
  batch 250 loss: 1.3528066682815552
  batch 300 loss: 1.3751039981842041
  batch 350 loss: 1.2870963168144227
  batch 400 loss: 1.3282928228378297
  batch 450 loss: 1.329887454509735
  batch 500 loss: 1.3276138496398926
  batch 550 loss: 1.414471399784088
  batch 600 loss: 1.3641408705711364
  batch 650 loss: 1.3379762494564056
  batch 700 loss: 1.342028133869171
  batch 750 loss: 1.3264753770828248
  batch 800 loss: 1.278315668106079
  batch 850 loss: 1.3191385555267334
  batch 900 loss: 1.3960663056373597
LOSS train 1.39607 valid 1.43862, valid PER 43.73%
EPOCH 5, Learning Rate: 0.5
  batch 50 loss: 1.3508572268486023
  batch 100 loss: 1.343999763727188
  batch 150 loss: 1.3618553519248962
  batch 200 loss: 1.2913952231407166
  batch 250 loss: 1.286805989742279
  batch 300 loss: 1.28129123210907
  batch 350 loss: 1.2886017096042632
  batch 400 loss: 1.2819415831565857
  batch 450 loss: 1.2500999426841737
  batch 500 loss: 1.2845132517814637
  batch 550 loss: 1.2369196355342864
  batch 600 loss: 1.2847209453582764
  batch 650 loss: 1.3203648900985718
  batch 700 loss: 1.3160719347000123
  batch 750 loss: 1.2378516912460327
  batch 800 loss: 1.2643810856342315
  batch 850 loss: 1.2894686782360076
  batch 900 loss: 1.2789415645599365
LOSS train 1.27894 valid 1.30778, valid PER 40.71%
EPOCH 6, Learning Rate: 0.5
  batch 50 loss: 1.2480188417434692
  batch 100 loss: 1.2157153916358947
  batch 150 loss: 1.1889539635181428
  batch 200 loss: 1.2314437401294709
  batch 250 loss: 1.2526643037796021
  batch 300 loss: 1.2271564459800721
  batch 350 loss: 1.2558723342418672
  batch 400 loss: 1.2177024328708648
  batch 450 loss: 1.2691377818584442
  batch 500 loss: 1.2303626585006713
  batch 550 loss: 1.2818932986259461
  batch 600 loss: 1.2286978769302368
  batch 650 loss: 1.3122354245185852
  batch 700 loss: 1.333270425796509
  batch 750 loss: 1.2026586246490478
  batch 800 loss: 1.2437385416030884
  batch 850 loss: 1.2433390653133392
  batch 900 loss: 1.3393131256103517
LOSS train 1.33931 valid 1.30266, valid PER 41.43%
EPOCH 7, Learning Rate: 0.5
  batch 50 loss: 1.2699873423576356
  batch 100 loss: 1.2659117698669433
  batch 150 loss: 1.3143773865699768
  batch 200 loss: 1.2240479958057404
  batch 250 loss: 1.2528415608406067
  batch 300 loss: 1.2133895134925843
  batch 350 loss: 1.2676736557483672
  batch 400 loss: 1.2674867272377015
  batch 450 loss: 1.21885635137558
  batch 500 loss: 1.2268104112148286
  batch 550 loss: 1.230817527770996
  batch 600 loss: 1.2362573754787445
  batch 650 loss: 1.211908402442932
  batch 700 loss: 1.2532980728149414
  batch 750 loss: 1.1904230916500091
  batch 800 loss: 1.1784670317173005
  batch 850 loss: 1.2329129827022554
  batch 900 loss: 1.2703329086303712
LOSS train 1.27033 valid 1.23367, valid PER 39.37%
EPOCH 8, Learning Rate: 0.5
  batch 50 loss: 1.2337397849559784
  batch 100 loss: 1.1781458580493926
  batch 150 loss: 1.1952666914463044
  batch 200 loss: 1.1712192273139954
  batch 250 loss: 1.2097727298736571
  batch 300 loss: 1.1598782169818878
  batch 350 loss: 1.2341060245037079
  batch 400 loss: 1.1870623922348023
  batch 450 loss: 1.2071036314964294
  batch 500 loss: 1.2741766679286957
  batch 550 loss: 1.2109193801879883
  batch 600 loss: 1.2312431514263154
  batch 650 loss: 1.3433618211746217
  batch 700 loss: 1.2807193899154663
  batch 750 loss: 1.2384456384181977
  batch 800 loss: 1.2740079283714294
  batch 850 loss: 1.3098356544971466
  batch 900 loss: 1.2382336378097534
LOSS train 1.23823 valid 1.31577, valid PER 41.58%
EPOCH 9, Learning Rate: 0.5
  batch 50 loss: 1.2383354532718658
  batch 100 loss: 1.282837462425232
  batch 150 loss: 1.2544173717498779
  batch 200 loss: 1.2129589760303496
  batch 250 loss: 1.2433810901641846
  batch 300 loss: 1.2349647772312164
  batch 350 loss: 1.2212228405475616
  batch 400 loss: 1.2291806221008301
  batch 450 loss: 1.2358022737503052
  batch 500 loss: 1.2523625826835632
  batch 550 loss: 1.374793657064438
  batch 600 loss: 1.332577464580536
  batch 650 loss: 1.3677510237693786
  batch 700 loss: 1.370996572971344
  batch 750 loss: 1.3087149238586426
  batch 800 loss: 1.306187903881073
  batch 850 loss: 1.315317780971527
  batch 900 loss: 1.263019654750824
LOSS train 1.26302 valid 1.27395, valid PER 39.81%
EPOCH 10, Learning Rate: 0.5
  batch 50 loss: 1.2199014377593995
  batch 100 loss: 1.2601602637767793
  batch 150 loss: 1.2766507589817047
  batch 200 loss: 1.2896074283123016
  batch 250 loss: 1.2778729581832886
  batch 300 loss: 1.2325370502471924
  batch 350 loss: 1.236929030418396
  batch 400 loss: 1.196666053533554
  batch 450 loss: 1.2196123445034026
  batch 500 loss: 1.259622914791107
  batch 550 loss: 1.2220611131191255
  batch 600 loss: 1.1910329818725587
  batch 650 loss: 1.2121015119552612
  batch 700 loss: 1.2201230335235596
  batch 750 loss: 1.222310037612915
  batch 800 loss: 1.2389275074005126
  batch 850 loss: 1.3044139456748962
  batch 900 loss: 1.2670256400108337
LOSS train 1.26703 valid 1.28316, valid PER 40.88%
EPOCH 11, Learning Rate: 0.5
  batch 50 loss: 1.1928221929073333
  batch 100 loss: 1.223760256767273
  batch 150 loss: 1.1773192632198333
  batch 200 loss: 1.2968333399295806
  batch 250 loss: 1.2108930373191833
  batch 300 loss: 1.1507239270210265
  batch 350 loss: 1.2381343376636504
  batch 400 loss: 1.2561453926563262
  batch 450 loss: 1.2478271126747131
  batch 500 loss: 1.2065198445320129
  batch 550 loss: 1.3212424540519714
  batch 600 loss: 1.3761798071861266
  batch 650 loss: 1.3092831921577455
  batch 700 loss: 1.2088146913051605
  batch 750 loss: 1.228142156600952
  batch 800 loss: 1.2781314635276795
  batch 850 loss: 1.3136353015899658
  batch 900 loss: 1.3324694752693176
LOSS train 1.33247 valid 1.28433, valid PER 40.99%
EPOCH 12, Learning Rate: 0.5
  batch 50 loss: 1.2575999736785888
  batch 100 loss: 1.2419076597690581
  batch 150 loss: 1.3340953850746156
  batch 200 loss: 1.3111460483074189
  batch 250 loss: 1.4373279547691344
  batch 300 loss: 1.2834206366539
  batch 350 loss: 1.302493177652359
  batch 400 loss: 1.3073135483264924
  batch 450 loss: 1.2799094581604005
  batch 500 loss: 1.280111062526703
  batch 550 loss: 1.175908454656601
  batch 600 loss: 1.2797757828235625
  batch 650 loss: 1.324292666912079
  batch 700 loss: 1.2576331543922423
  batch 750 loss: 1.235707801580429
  batch 800 loss: 1.2397847938537598
  batch 850 loss: 1.3361210870742797
  batch 900 loss: 1.3235082197189332
LOSS train 1.32351 valid 1.29827, valid PER 40.79%
EPOCH 13, Learning Rate: 0.5
  batch 50 loss: 1.2502287888526917
  batch 100 loss: 1.3124275279045106
  batch 150 loss: 1.2697373449802398
  batch 200 loss: 1.5711938166618347
  batch 250 loss: 1.3207903599739075
  batch 300 loss: 1.3463886976242065
  batch 350 loss: 1.3050945377349854
  batch 400 loss: 1.3483295583724975
  batch 450 loss: 1.4774805951118468
  batch 500 loss: 1.3578788447380066
  batch 550 loss: 1.2947871196269989
  batch 600 loss: 1.2773937201499939
  batch 650 loss: 1.2609152352809907
  batch 700 loss: 1.2457577443122865
  batch 750 loss: 1.3699050450325012
  batch 800 loss: 1.3883314156532287
  batch 850 loss: 1.3492339253425598
  batch 900 loss: 1.350316367149353
LOSS train 1.35032 valid 1.33023, valid PER 42.12%
EPOCH 14, Learning Rate: 0.5
  batch 50 loss: 1.3055405616760254
  batch 100 loss: 1.3805290842056275
  batch 150 loss: 1.335367077589035
  batch 200 loss: 1.3296451950073243
  batch 250 loss: 1.3495185041427613
  batch 300 loss: 1.4115955519676209
  batch 350 loss: 1.4489499402046204
  batch 400 loss: 1.4279205203056335
  batch 450 loss: 1.3312935853004455
  batch 500 loss: 1.3430325293540955
  batch 550 loss: 1.3498201167583466
  batch 600 loss: 1.2911469435691834
  batch 650 loss: 1.2798994481563568
  batch 700 loss: 1.307317440509796
  batch 750 loss: 1.2649457550048828
  batch 800 loss: 1.2040515947341919
  batch 850 loss: 1.263488371372223
  batch 900 loss: 1.2428754687309265
LOSS train 1.24288 valid 1.31848, valid PER 41.49%
EPOCH 15, Learning Rate: 0.5
  batch 50 loss: 1.2773654019832612
  batch 100 loss: 1.2535088634490967
  batch 150 loss: 1.3058454251289369
  batch 200 loss: 1.3870986819267273
  batch 250 loss: 1.3494229197502137
  batch 300 loss: 1.3622458100318908
  batch 350 loss: 1.3560666990280152
  batch 400 loss: 1.3975706005096435
  batch 450 loss: 1.3858521533012391
  batch 500 loss: 1.3566723585128784
  batch 550 loss: 1.3921589422225953
  batch 600 loss: 1.3113338184356689
  batch 650 loss: 1.3045755696296693
  batch 700 loss: 1.3004586267471314
  batch 750 loss: 1.3907269930839539
  batch 800 loss: 1.3336745643615722
  batch 850 loss: 1.324778642654419
  batch 900 loss: 1.3425377225875854
LOSS train 1.34254 valid 1.81333, valid PER 51.17%
EPOCH 16, Learning Rate: 0.5
  batch 50 loss: 1.4715836930274964
  batch 100 loss: 1.297416000366211
  batch 150 loss: 1.2624928867816925
  batch 200 loss: 1.244482569694519
  batch 250 loss: 1.2961677145957946
  batch 300 loss: 1.2600570333003998
  batch 350 loss: 1.2687064278125764
  batch 400 loss: 1.2418814706802368
  batch 450 loss: 1.253022961616516
  batch 500 loss: 1.2543314170837403
  batch 550 loss: 1.6291172885894776
  batch 600 loss: 1.556708073616028
  batch 650 loss: 1.5548990917205812
  batch 700 loss: 1.51537269115448
  batch 750 loss: 1.4717659449577332
  batch 800 loss: 1.3990094995498656
  batch 850 loss: 1.3844833421707152
  batch 900 loss: 1.49460955619812
LOSS train 1.49461 valid 1.55657, valid PER 47.35%
EPOCH 17, Learning Rate: 0.5
  batch 50 loss: 1.47249502658844
  batch 100 loss: 1.4011609768867492
  batch 150 loss: 1.4542138004302978
  batch 200 loss: 1.4352347779273986
  batch 250 loss: 1.4247338795661926
  batch 300 loss: 1.4264106607437135
  batch 350 loss: 1.371522641181946
  batch 400 loss: 1.3920569396018982
  batch 450 loss: 1.4214093351364137
  batch 500 loss: 1.3492070436477661
  batch 550 loss: 1.3630101907253265
  batch 600 loss: 1.404196448326111
  batch 650 loss: 1.355246593952179
  batch 700 loss: 1.3802427864074707
  batch 750 loss: 1.3479922485351563
  batch 800 loss: 1.422325668334961
  batch 850 loss: 1.3939037227630615
  batch 900 loss: 1.3485038352012635
LOSS train 1.34850 valid 1.46123, valid PER 44.84%
EPOCH 18, Learning Rate: 0.5
  batch 50 loss: 1.409684567451477
  batch 100 loss: 1.4033110761642456
  batch 150 loss: 1.3261916947364807
  batch 200 loss: 1.3213934481143952
  batch 250 loss: 1.2987797737121582
  batch 300 loss: 1.2753108286857604
  batch 350 loss: 1.4391265153884887
  batch 400 loss: 1.4062308716773986
  batch 450 loss: 1.455503375530243
  batch 500 loss: 1.524070339202881
  batch 550 loss: 1.4763258624076843
  batch 600 loss: 1.5298112726211548
  batch 650 loss: 1.795898756980896
  batch 700 loss: 1.9595728635787963
  batch 750 loss: 1.8857860350608826
  batch 800 loss: 1.7452142357826232
  batch 850 loss: 1.6507526326179505
  batch 900 loss: 1.6164016437530517
LOSS train 1.61640 valid 1.62773, valid PER 50.29%
EPOCH 19, Learning Rate: 0.5
  batch 50 loss: 1.5176583695411683
  batch 100 loss: 1.6211637187004089
  batch 150 loss: 1.6345885920524597
  batch 200 loss: 1.559580113887787
  batch 250 loss: 1.5713568758964538
  batch 300 loss: 1.5344152641296387
  batch 350 loss: 1.4752960562705995
  batch 400 loss: 1.5441633725166322
  batch 450 loss: 1.5114548850059508
  batch 500 loss: 1.5639289975166322
  batch 550 loss: 1.9626786661148072
  batch 600 loss: 1.873205235004425
  batch 650 loss: 2.000500202178955
  batch 700 loss: 1.7925943374633788
  batch 750 loss: 1.7055635309219361
  batch 800 loss: 1.671379063129425
  batch 850 loss: 1.6175211429595948
  batch 900 loss: 1.644244623184204
LOSS train 1.64424 valid 1.71858, valid PER 53.57%
EPOCH 20, Learning Rate: 0.5
  batch 50 loss: 1.6273653817176819
  batch 100 loss: 1.6226019191741943
  batch 150 loss: 1.5624703097343444
  batch 200 loss: 1.5423391580581665
  batch 250 loss: 1.522476818561554
  batch 300 loss: 1.5317707133293152
  batch 350 loss: 1.484845097064972
  batch 400 loss: 1.5470965361595155
  batch 450 loss: 1.5519882559776306
  batch 500 loss: 1.4939353346824646
  batch 550 loss: 1.5271525168418885
  batch 600 loss: 1.7058945727348327
  batch 650 loss: 1.9521105289459229
  batch 700 loss: 1.7902031207084657
  batch 750 loss: 1.677857050895691
  batch 800 loss: 1.7068251633644105
  batch 850 loss: 1.6427772879600524
  batch 900 loss: 1.6215575909614564
LOSS train 1.62156 valid 1.61057, valid PER 49.36%
Training finished in 4.0 minutes.
Model saved to checkpoints/20231209_102632/model_7
Loading model from checkpoints/20231209_102632/model_7
SUB: 22.27%, DEL: 16.17%, INS: 1.86%, COR: 61.56%, PER: 40.30%
