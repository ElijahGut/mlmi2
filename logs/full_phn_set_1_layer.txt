Namespace(seed=123, train_json='train_fbank61.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.9, vocab='vocab_61.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 172863
EPOCH 1, Learning Rate: 0.9
  batch 50 loss: 5.334942350387573
  batch 100 loss: 3.8140073347091676
  batch 150 loss: 3.6520943355560305
  batch 200 loss: 3.4963033246994017
  batch 250 loss: 3.314673275947571
  batch 300 loss: 3.105841007232666
  batch 350 loss: 2.9262663173675536
  batch 400 loss: 2.9090108919143676
  batch 450 loss: 2.7355991220474243
  batch 500 loss: 2.6602522277832032
  batch 550 loss: 2.528142714500427
  batch 600 loss: 2.4202244019508363
  batch 650 loss: 2.278483459949493
  batch 700 loss: 2.297561106681824
  batch 750 loss: 2.2367277002334593
  batch 800 loss: 2.1790129590034484
  batch 850 loss: 2.1065095710754393
  batch 900 loss: 2.0800007724761964
running loss: 49.3260201215744
LOSS train 2.08000 valid 3.17122, valid PER 67.98%
EPOCH 2, Learning Rate: 0.9
  batch 50 loss: 2.006151547431946
  batch 100 loss: 1.9312102842330932
  batch 150 loss: 1.9704111409187317
  batch 200 loss: 1.94775732755661
  batch 250 loss: 1.9651061010360718
  batch 300 loss: 1.8986057019233704
  batch 350 loss: 1.8133967447280883
  batch 400 loss: 1.8403115773200989
  batch 450 loss: 1.7658126139640808
  batch 500 loss: 1.840386188030243
  batch 550 loss: 1.8184419870376587
  batch 600 loss: 1.774774444103241
  batch 650 loss: 1.8030574107170105
  batch 700 loss: 1.7476780438423156
  batch 750 loss: 1.7472014451026916
  batch 800 loss: 1.6763832688331604
  batch 850 loss: 1.6955733752250672
  batch 900 loss: 1.7220647811889649
running loss: 39.60587453842163
LOSS train 1.72206 valid 2.94894, valid PER 45.28%
EPOCH 3, Learning Rate: 0.9
  batch 50 loss: 1.6676497411727906
  batch 100 loss: 1.6250830483436585
  batch 150 loss: 1.6227364039421082
  batch 200 loss: 1.6030452418327332
  batch 250 loss: 1.5983716106414796
  batch 300 loss: 1.5659565806388855
  batch 350 loss: 1.6122434544563293
  batch 400 loss: 1.5617917156219483
  batch 450 loss: 1.5961396026611328
  batch 500 loss: 1.5414681816101075
  batch 550 loss: 1.5559661173820496
  batch 600 loss: 1.5268109798431397
  batch 650 loss: 1.5287798643112183
  batch 700 loss: 1.5306178331375122
  batch 750 loss: 1.590275673866272
  batch 800 loss: 1.5132514643669128
  batch 850 loss: 1.5635423588752746
  batch 900 loss: 1.4706378388404846
running loss: 34.91225612163544
LOSS train 1.47064 valid 2.94041, valid PER 39.04%
EPOCH 4, Learning Rate: 0.9
  batch 50 loss: 1.444061300754547
  batch 100 loss: 1.4999129939079285
  batch 150 loss: 1.4249390435218812
  batch 200 loss: 1.4723442888259888
  batch 250 loss: 1.4487708473205567
  batch 300 loss: 1.4835986113548278
  batch 350 loss: 1.3870996391773225
  batch 400 loss: 1.4560460376739501
  batch 450 loss: 1.4182058274745941
  batch 500 loss: 1.4361849689483643
  batch 550 loss: 1.4336259865760803
  batch 600 loss: 1.438032205104828
  batch 650 loss: 1.4413665461540222
  batch 700 loss: 1.3864556288719176
  batch 750 loss: 1.404499762058258
  batch 800 loss: 1.3770674896240234
  batch 850 loss: 1.3932629585266114
  batch 900 loss: 1.4381075048446654
running loss: 34.099560141563416
LOSS train 1.43811 valid 2.99127, valid PER 37.89%
EPOCH 5, Learning Rate: 0.9
  batch 50 loss: 1.381076853275299
  batch 100 loss: 1.3695168805122375
  batch 150 loss: 1.386474883556366
  batch 200 loss: 1.3319094157218934
  batch 250 loss: 1.3459226989746094
  batch 300 loss: 1.328780677318573
  batch 350 loss: 1.3702473521232605
  batch 400 loss: 1.3398334336280824
  batch 450 loss: 1.3592283844947814
  batch 500 loss: 1.3492496585845948
  batch 550 loss: 1.3052004301548004
  batch 600 loss: 1.3855293250083924
  batch 650 loss: 1.363657729625702
  batch 700 loss: 1.378822422027588
  batch 750 loss: 1.3139883470535278
  batch 800 loss: 1.3273338794708252
  batch 850 loss: 1.3414272165298462
  batch 900 loss: 1.351905701160431
running loss: 30.780797123908997
LOSS train 1.35191 valid 2.95818, valid PER 35.05%
EPOCH 6, Learning Rate: 0.9
  batch 50 loss: 1.3424936175346374
  batch 100 loss: 1.248951197862625
  batch 150 loss: 1.2894527518749237
  batch 200 loss: 1.2663188576698303
  batch 250 loss: 1.3320016503334045
  batch 300 loss: 1.3117605113983155
  batch 350 loss: 1.2967521500587464
  batch 400 loss: 1.2926272010803224
  batch 450 loss: 1.3117118072509766
  batch 500 loss: 1.2811005592346192
  batch 550 loss: 1.3359446382522584
  batch 600 loss: 1.2858754515647888
  batch 650 loss: 1.3053572833538056
  batch 700 loss: 1.2931468343734742
  batch 750 loss: 1.2849054384231566
  batch 800 loss: 1.2620441365242003
  batch 850 loss: 1.2322800505161284
  batch 900 loss: 1.289878339767456
running loss: 30.472639977931976
LOSS train 1.28988 valid 2.99250, valid PER 34.99%
EPOCH 7, Learning Rate: 0.9
  batch 50 loss: 1.2552589797973632
  batch 100 loss: 1.2565058553218842
  batch 150 loss: 1.2443327367305757
  batch 200 loss: 1.251860123872757
  batch 250 loss: 1.2545826530456543
  batch 300 loss: 1.2402634406089783
  batch 350 loss: 1.2475141596794128
  batch 400 loss: 1.2412550556659698
  batch 450 loss: 1.2473035049438477
  batch 500 loss: 1.2390701067447663
  batch 550 loss: 1.2277757632732391
  batch 600 loss: 1.2487806832790376
  batch 650 loss: 1.2298936140537262
  batch 700 loss: 1.2695641469955445
  batch 750 loss: 1.2211738884449006
  batch 800 loss: 1.2249180567264557
  batch 850 loss: 1.2500523662567138
  batch 900 loss: 1.2917197382450103
running loss: 29.396798253059387
LOSS train 1.29172 valid 2.96523, valid PER 34.69%
EPOCH 8, Learning Rate: 0.9
  batch 50 loss: 1.220162205696106
  batch 100 loss: 1.1918562746047974
  batch 150 loss: 1.2201676666736603
  batch 200 loss: 1.1787063670158386
  batch 250 loss: 1.1990264403820037
  batch 300 loss: 1.1487687337398529
  batch 350 loss: 1.2507387685775757
  batch 400 loss: 1.1978153455257416
  batch 450 loss: 1.2206646120548248
  batch 500 loss: 1.2328548073768615
  batch 550 loss: 1.169732939004898
  batch 600 loss: 1.2576983547210694
  batch 650 loss: 1.2546420741081237
  batch 700 loss: 1.1633429527282715
  batch 750 loss: 1.2043363666534423
  batch 800 loss: 1.246898444890976
  batch 850 loss: 1.2186704444885255
  batch 900 loss: 1.2083884572982788
running loss: 28.64966368675232
LOSS train 1.20839 valid 2.98115, valid PER 31.84%
EPOCH 9, Learning Rate: 0.9
  batch 50 loss: 1.0994396770000459
  batch 100 loss: 1.1598084211349486
  batch 150 loss: 1.1845551145076751
  batch 200 loss: 1.1310452270507811
  batch 250 loss: 1.1882500123977662
  batch 300 loss: 1.19865238904953
  batch 350 loss: 1.2152830255031586
  batch 400 loss: 1.1894579899311066
  batch 450 loss: 1.192544277906418
  batch 500 loss: 1.1501884853839874
  batch 550 loss: 1.2031678998470305
  batch 600 loss: 1.215742416381836
  batch 650 loss: 1.1636746263504028
  batch 700 loss: 1.1298226153850555
  batch 750 loss: 1.1976074731349946
  batch 800 loss: 1.189502890110016
  batch 850 loss: 1.1872939085960388
  batch 900 loss: 1.1600839364528657
running loss: 28.751951813697815
LOSS train 1.16008 valid 2.96835, valid PER 31.62%
EPOCH 10, Learning Rate: 0.45
  batch 50 loss: 1.0671022582054137
  batch 100 loss: 1.0685598683357238
  batch 150 loss: 1.0699730598926545
  batch 200 loss: 1.0661104035377502
  batch 250 loss: 1.1162032759189606
  batch 300 loss: 1.0454284465312957
  batch 350 loss: 1.0698934626579284
  batch 400 loss: 1.0207078504562377
  batch 450 loss: 1.0282721364498137
  batch 500 loss: 1.0730802261829375
  batch 550 loss: 1.0836627030372619
  batch 600 loss: 1.0495632565021515
  batch 650 loss: 1.0418935859203338
  batch 700 loss: 1.0649655652046204
  batch 750 loss: 1.0413759768009185
  batch 800 loss: 1.0563839745521546
  batch 850 loss: 1.0652492094039916
  batch 900 loss: 1.0728656148910523
running loss: 24.669214487075806
LOSS train 1.07287 valid 3.07025, valid PER 30.92%
EPOCH 11, Learning Rate: 0.45
  batch 50 loss: 1.0094686186313628
  batch 100 loss: 0.9918453919887543
  batch 150 loss: 0.9974695658683777
  batch 200 loss: 1.0493573594093322
  batch 250 loss: 1.0710026919841766
  batch 300 loss: 1.0082301926612853
  batch 350 loss: 1.0307602512836456
  batch 400 loss: 1.0536630797386168
  batch 450 loss: 1.0409365665912629
  batch 500 loss: 1.0243228876590729
  batch 550 loss: 1.0483914721012115
  batch 600 loss: 1.0205826079845428
  batch 650 loss: 1.0837127339839936
  batch 700 loss: 0.9964260184764862
  batch 750 loss: 1.018408064842224
  batch 800 loss: 1.0417725658416748
  batch 850 loss: 1.0496527028083802
  batch 900 loss: 1.0644014620780944
running loss: 24.277431428432465
LOSS train 1.06440 valid 3.05606, valid PER 30.30%
EPOCH 12, Learning Rate: 0.45
  batch 50 loss: 1.0054007434844972
  batch 100 loss: 0.9918408095836639
  batch 150 loss: 0.9522591590881347
  batch 200 loss: 0.99107297539711
  batch 250 loss: 1.002938051223755
  batch 300 loss: 1.0024632954597472
  batch 350 loss: 1.0016950809955596
  batch 400 loss: 1.031353267431259
  batch 450 loss: 1.0385471725463866
  batch 500 loss: 1.0455257141590117
  batch 550 loss: 0.9582956719398499
  batch 600 loss: 0.992812911272049
  batch 650 loss: 1.0507111406326295
  batch 700 loss: 1.0339849054813386
  batch 750 loss: 1.0001156282424928
  batch 800 loss: 1.009557968378067
  batch 850 loss: 1.0507102847099303
  batch 900 loss: 1.044793735742569
running loss: 24.07845151424408
LOSS train 1.04479 valid 3.07631, valid PER 29.99%
EPOCH 13, Learning Rate: 0.45
  batch 50 loss: 0.9844240975379944
  batch 100 loss: 0.9925329756736755
  batch 150 loss: 0.9630712866783142
  batch 200 loss: 1.004104002714157
  batch 250 loss: 0.9825127339363098
  batch 300 loss: 0.9684363770484924
  batch 350 loss: 0.989337922334671
  batch 400 loss: 1.0078311705589293
  batch 450 loss: 1.0140021872520446
  batch 500 loss: 0.9918123316764832
  batch 550 loss: 1.0218771004676819
  batch 600 loss: 0.982569900751114
  batch 650 loss: 1.0132897233963012
  batch 700 loss: 1.026568431854248
  batch 750 loss: 0.9583979904651642
  batch 800 loss: 0.9851203751564026
  batch 850 loss: 1.0371753823757173
  batch 900 loss: 1.0068691873550415
running loss: 24.931713461875916
LOSS train 1.00687 valid 3.09571, valid PER 29.73%
EPOCH 14, Learning Rate: 0.225
  batch 50 loss: 0.9429795026779175
  batch 100 loss: 0.9525411391258239
  batch 150 loss: 0.9204977691173554
  batch 200 loss: 0.9301418805122376
  batch 250 loss: 0.9297266519069671
  batch 300 loss: 0.9498581576347351
  batch 350 loss: 0.9018716275691986
  batch 400 loss: 0.9090452110767364
  batch 450 loss: 0.9258970046043395
  batch 500 loss: 0.9215468978881836
  batch 550 loss: 0.9470149981975555
  batch 600 loss: 0.9076965224742889
  batch 650 loss: 0.947699967622757
  batch 700 loss: 0.9806914520263672
  batch 750 loss: 0.9252565729618073
  batch 800 loss: 0.8865031313896179
  batch 850 loss: 0.9455292189121246
  batch 900 loss: 0.9349194598197937
running loss: 23.07011640071869
LOSS train 0.93492 valid 3.13567, valid PER 28.88%
EPOCH 15, Learning Rate: 0.225
  batch 50 loss: 0.9027652287483215
  batch 100 loss: 0.9120748913288117
  batch 150 loss: 0.8864182496070862
  batch 200 loss: 0.9163256156444549
  batch 250 loss: 0.925038549900055
  batch 300 loss: 0.8929878771305084
  batch 350 loss: 0.9157473719120026
  batch 400 loss: 0.9114020121097565
  batch 450 loss: 0.9081964004039764
  batch 500 loss: 0.8696761286258697
  batch 550 loss: 0.9070152354240417
  batch 600 loss: 0.921343914270401
  batch 650 loss: 0.9453620648384095
  batch 700 loss: 0.9220947384834289
  batch 750 loss: 0.9281290340423584
  batch 800 loss: 0.9222436571121215
  batch 850 loss: 0.8772054970264435
  batch 900 loss: 0.9491830205917359
running loss: 22.026481866836548
LOSS train 0.94918 valid 3.14800, valid PER 28.76%
EPOCH 16, Learning Rate: 0.225
  batch 50 loss: 0.9190142631530762
  batch 100 loss: 0.8727888810634613
  batch 150 loss: 0.8937711226940155
  batch 200 loss: 0.895768187046051
  batch 250 loss: 0.9138988733291626
  batch 300 loss: 0.91834596991539
  batch 350 loss: 0.9087662887573242
  batch 400 loss: 0.929858500957489
  batch 450 loss: 0.9224857842922211
  batch 500 loss: 0.8841566395759582
  batch 550 loss: 0.8876997673511505
  batch 600 loss: 0.8929987108707428
  batch 650 loss: 0.9160229432582855
  batch 700 loss: 0.889704977273941
  batch 750 loss: 0.8938778245449066
  batch 800 loss: 0.9146729230880737
  batch 850 loss: 0.9112420356273652
  batch 900 loss: 0.9133442664146423
running loss: 21.55802607536316
LOSS train 0.91334 valid 3.20612, valid PER 28.42%
EPOCH 17, Learning Rate: 0.225
  batch 50 loss: 0.9001218163967133
  batch 100 loss: 0.8962516331672669
  batch 150 loss: 0.8783775663375855
  batch 200 loss: 0.8834887981414795
  batch 250 loss: 0.8858358144760132
  batch 300 loss: 0.9037667977809906
  batch 350 loss: 0.8660500502586365
  batch 400 loss: 0.9307412147521973
  batch 450 loss: 0.9039751374721527
  batch 500 loss: 0.8920486938953399
  batch 550 loss: 0.8971187508106232
  batch 600 loss: 0.9215000200271607
  batch 650 loss: 0.8664640307426452
  batch 700 loss: 0.8824666941165924
  batch 750 loss: 0.8905362761020661
  batch 800 loss: 0.8751381850242614
  batch 850 loss: 0.9075383532047272
  batch 900 loss: 0.876806925535202
running loss: 21.854098081588745
LOSS train 0.87681 valid 3.21852, valid PER 28.83%
EPOCH 18, Learning Rate: 0.1125
  batch 50 loss: 0.8516221976280213
  batch 100 loss: 0.8700522744655609
  batch 150 loss: 0.8737574362754822
  batch 200 loss: 0.8629261946678162
  batch 250 loss: 0.8797166192531586
  batch 300 loss: 0.8264136219024658
  batch 350 loss: 0.8451167821884156
  batch 400 loss: 0.8413157200813294
  batch 450 loss: 0.8712752795219422
  batch 500 loss: 0.8429058635234833
  batch 550 loss: 0.883334321975708
  batch 600 loss: 0.8313770711421966
  batch 650 loss: 0.8286288154125213
  batch 700 loss: 0.8783438789844513
  batch 750 loss: 0.8511395359039307
  batch 800 loss: 0.8483593952655792
  batch 850 loss: 0.8360206937789917
  batch 900 loss: 0.8725531816482544
running loss: 21.14718222618103
LOSS train 0.87255 valid 3.23234, valid PER 27.95%
EPOCH 19, Learning Rate: 0.1125
  batch 50 loss: 0.8179592311382293
  batch 100 loss: 0.8323422574996948
  batch 150 loss: 0.8383943164348602
  batch 200 loss: 0.8421634018421174
  batch 250 loss: 0.8468296205997468
  batch 300 loss: 0.8419266152381897
  batch 350 loss: 0.8108845806121826
  batch 400 loss: 0.8272754526138306
  batch 450 loss: 0.8717673301696778
  batch 500 loss: 0.8494986259937286
  batch 550 loss: 0.8224896693229675
  batch 600 loss: 0.8420583629608154
  batch 650 loss: 0.890611584186554
  batch 700 loss: 0.8425099384784699
  batch 750 loss: 0.8215038084983826
  batch 800 loss: 0.8685108256340027
  batch 850 loss: 0.8528157389163971
  batch 900 loss: 0.8399806439876556
running loss: 19.81071048974991
LOSS train 0.83998 valid 3.18761, valid PER 27.81%
EPOCH 20, Learning Rate: 0.1125
  batch 50 loss: 0.8381732153892517
  batch 100 loss: 0.8108331513404846
  batch 150 loss: 0.8110156464576721
  batch 200 loss: 0.8426495814323425
  batch 250 loss: 0.8317705309391021
  batch 300 loss: 0.8498465144634246
  batch 350 loss: 0.7990841245651246
  batch 400 loss: 0.8204698121547699
  batch 450 loss: 0.8489155411720276
  batch 500 loss: 0.8072722065448761
  batch 550 loss: 0.8785747516155243
  batch 600 loss: 0.7866536176204681
  batch 650 loss: 0.8504385960102081
  batch 700 loss: 0.8366275894641876
  batch 750 loss: 0.8262712347507477
  batch 800 loss: 0.8529943108558655
  batch 850 loss: 0.8448147666454315
  batch 900 loss: 0.8544479739665986
running loss: 20.688835322856903
LOSS train 0.85445 valid 3.24253, valid PER 27.58%
Training finished in 3.0 minutes.
Model saved to checkpoints/20231210_052405/model_3
Loading model from checkpoints/20231210_052405/model_3
SUB: 18.14%, DEL: 21.19%, INS: 1.49%, COR: 60.67%, PER: 40.82%
