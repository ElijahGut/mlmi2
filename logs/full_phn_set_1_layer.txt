Namespace(seed=123, train_json='train_fbank61.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.9, vocab='vocab_61.txt', report_interval=50, num_epochs=20, dropout=0.3, optimiser='sgd', grad_clip=1.5)
Total number of model parameters is 172863
EPOCH 1, Learning Rate: 0.9
  batch 50 loss: 5.334935793876648
  batch 100 loss: 3.8140018224716186
  batch 150 loss: 3.656104474067688
  batch 200 loss: 3.470327525138855
  batch 250 loss: 3.311368761062622
  batch 300 loss: 3.139402298927307
  batch 350 loss: 2.9343983507156373
  batch 400 loss: 2.8894298219680787
  batch 450 loss: 2.7288620615005494
  batch 500 loss: 2.6625618171691894
  batch 550 loss: 2.5803051567077637
  batch 600 loss: 2.472192196846008
  batch 650 loss: 2.3871112632751466
  batch 700 loss: 2.3476463317871095
  batch 750 loss: 2.313668954372406
  batch 800 loss: 2.2108331060409547
  batch 850 loss: 2.1545549964904787
  batch 900 loss: 2.1009678053855896
avg val loss: 3.25187087059021
LOSS train 2.10097 valid 3.25187, valid PER 70.11%
EPOCH 2, Learning Rate: 0.9
  batch 50 loss: 2.040934262275696
  batch 100 loss: 1.9602516770362854
  batch 150 loss: 1.9734153747558594
  batch 200 loss: 1.9632181668281554
  batch 250 loss: 1.9856889843940735
  batch 300 loss: 1.9035951018333435
  batch 350 loss: 1.8252273941040038
  batch 400 loss: 1.8394264030456542
  batch 450 loss: 1.7787130379676819
  batch 500 loss: 1.8477729201316833
  batch 550 loss: 1.8269463229179381
  batch 600 loss: 1.7878854036331178
  batch 650 loss: 1.8188597607612609
  batch 700 loss: 1.760302288532257
  batch 750 loss: 1.761890549659729
  batch 800 loss: 1.6785329031944274
  batch 850 loss: 1.711759696006775
  batch 900 loss: 1.7270857882499695
avg val loss: 3.023210048675537
LOSS train 1.72709 valid 3.02321, valid PER 45.20%
EPOCH 3, Learning Rate: 0.9
  batch 50 loss: 1.6737338185310364
  batch 100 loss: 1.6484777998924256
  batch 150 loss: 1.6286660385131837
  batch 200 loss: 1.603763084411621
  batch 250 loss: 1.6106276488304139
  batch 300 loss: 1.5862206792831421
  batch 350 loss: 1.6248538184165955
  batch 400 loss: 1.5711412811279297
  batch 450 loss: 1.5907089114189148
  batch 500 loss: 1.5434037470817565
  batch 550 loss: 1.551799120903015
  batch 600 loss: 1.536683030128479
  batch 650 loss: 1.515103838443756
  batch 700 loss: 1.542882239818573
  batch 750 loss: 1.5647103905677795
  batch 800 loss: 1.5080197930336
  batch 850 loss: 1.5454176378250122
  batch 900 loss: 1.4751731634140015
avg val loss: 3.0425167083740234
LOSS train 1.47517 valid 3.04252, valid PER 38.83%
EPOCH 4, Learning Rate: 0.45
  batch 50 loss: 1.3899569630622863
  batch 100 loss: 1.417665672302246
  batch 150 loss: 1.3597074270248413
  batch 200 loss: 1.3849430108070373
  batch 250 loss: 1.3712268900871276
  batch 300 loss: 1.3907622218132019
  batch 350 loss: 1.312730768918991
  batch 400 loss: 1.3645517921447754
  batch 450 loss: 1.3538312029838562
  batch 500 loss: 1.3388518381118775
  batch 550 loss: 1.3607093906402588
  batch 600 loss: 1.373869616985321
  batch 650 loss: 1.356281774044037
  batch 700 loss: 1.3175510311126708
  batch 750 loss: 1.344522420167923
  batch 800 loss: 1.3052344024181366
  batch 850 loss: 1.3361303114891052
  batch 900 loss: 1.3716444396972656
avg val loss: 2.9570724964141846
LOSS train 1.37164 valid 2.95707, valid PER 35.88%
EPOCH 5, Learning Rate: 0.45
  batch 50 loss: 1.3157699728012084
  batch 100 loss: 1.3056447505950928
  batch 150 loss: 1.3316426944732667
  batch 200 loss: 1.2831086492538453
  batch 250 loss: 1.2887679076194762
  batch 300 loss: 1.275605936050415
  batch 350 loss: 1.3017000818252564
  batch 400 loss: 1.2913519096374513
  batch 450 loss: 1.3003272700309754
  batch 500 loss: 1.3104127478599548
  batch 550 loss: 1.2642528343200683
  batch 600 loss: 1.331636025905609
  batch 650 loss: 1.2865391767024994
  batch 700 loss: 1.334173767566681
  batch 750 loss: 1.2726519465446473
  batch 800 loss: 1.286577777862549
  batch 850 loss: 1.2873349857330323
  batch 900 loss: 1.280093777179718
avg val loss: 3.010639190673828
LOSS train 1.28009 valid 3.01064, valid PER 34.41%
EPOCH 6, Learning Rate: 0.225
  batch 50 loss: 1.2706208038330078
  batch 100 loss: 1.1633293223381043
  batch 150 loss: 1.1824578154087066
  batch 200 loss: 1.1735893452167512
  batch 250 loss: 1.2232085871696472
  batch 300 loss: 1.2129330134391785
  batch 350 loss: 1.2109908866882324
  batch 400 loss: 1.184735780954361
  batch 450 loss: 1.2162980771064758
  batch 500 loss: 1.1873053884506226
  batch 550 loss: 1.2304623556137084
  batch 600 loss: 1.1893002498149872
  batch 650 loss: 1.2224111294746398
  batch 700 loss: 1.2070737552642823
  batch 750 loss: 1.180133295059204
  batch 800 loss: 1.1868435549736023
  batch 850 loss: 1.166929235458374
  batch 900 loss: 1.2110224521160127
avg val loss: 2.9922192096710205
LOSS train 1.21102 valid 2.99222, valid PER 33.55%
EPOCH 7, Learning Rate: 0.1125
  batch 50 loss: 1.165763682126999
  batch 100 loss: 1.1756442368030549
  batch 150 loss: 1.1454992640018462
  batch 200 loss: 1.1419646430015564
  batch 250 loss: 1.1392585349082947
  batch 300 loss: 1.1278188729286194
  batch 350 loss: 1.1514647126197814
  batch 400 loss: 1.1430620276927947
  batch 450 loss: 1.1324045467376709
  batch 500 loss: 1.1355066037178039
  batch 550 loss: 1.1293715620040894
  batch 600 loss: 1.148343207836151
  batch 650 loss: 1.1365732586383819
  batch 700 loss: 1.1660401451587676
  batch 750 loss: 1.1248215663433074
  batch 800 loss: 1.1290801668167114
  batch 850 loss: 1.1523248851299286
  batch 900 loss: 1.1730741560459137
avg val loss: 2.966348171234131
LOSS train 1.17307 valid 2.96635, valid PER 32.39%
EPOCH 8, Learning Rate: 0.05625
  batch 50 loss: 1.1386346220970154
  batch 100 loss: 1.1355382192134857
  batch 150 loss: 1.1021309566497803
  batch 200 loss: 1.0877215206623077
  batch 250 loss: 1.119996303319931
  batch 300 loss: 1.0754194045066834
  batch 350 loss: 1.1535175454616546
  batch 400 loss: 1.1004818677902222
  batch 450 loss: 1.1304481494426728
  batch 500 loss: 1.1351942610740662
  batch 550 loss: 1.0868505120277405
  batch 600 loss: 1.1378272211551665
  batch 650 loss: 1.1357007813453674
  batch 700 loss: 1.0775077843666077
  batch 750 loss: 1.1115598630905152
  batch 800 loss: 1.132721860408783
  batch 850 loss: 1.1173168981075288
  batch 900 loss: 1.1183224046230316
avg val loss: 2.9684338569641113
LOSS train 1.11832 valid 2.96843, valid PER 32.06%
EPOCH 9, Learning Rate: 0.028125
  batch 50 loss: 1.068345514535904
  batch 100 loss: 1.1064887189865111
  batch 150 loss: 1.1100446391105652
  batch 200 loss: 1.062484347820282
  batch 250 loss: 1.1045983779430388
  batch 300 loss: 1.1086066234111787
  batch 350 loss: 1.1583369696140289
  batch 400 loss: 1.0954264688491822
  batch 450 loss: 1.101280791759491
  batch 500 loss: 1.0778638756275176
  batch 550 loss: 1.1201347529888153
  batch 600 loss: 1.1176737368106842
  batch 650 loss: 1.0900788331031799
  batch 700 loss: 1.0553220653533935
  batch 750 loss: 1.108040965795517
  batch 800 loss: 1.1089699161052704
  batch 850 loss: 1.1046714675426483
  batch 900 loss: 1.0642138266563415
avg val loss: 2.9970130920410156
LOSS train 1.06421 valid 2.99701, valid PER 31.74%
EPOCH 10, Learning Rate: 0.0140625
  batch 50 loss: 1.0682110691070557
  batch 100 loss: 1.0994917011260987
  batch 150 loss: 1.09583855509758
  batch 200 loss: 1.0886333453655244
  batch 250 loss: 1.1362157607078551
  batch 300 loss: 1.080397251844406
  batch 350 loss: 1.1007676279544831
  batch 400 loss: 1.0572965335845947
  batch 450 loss: 1.054192646741867
  batch 500 loss: 1.0915409111976624
  batch 550 loss: 1.1002597784996033
  batch 600 loss: 1.0775833773612975
  batch 650 loss: 1.0781988763809205
  batch 700 loss: 1.0968965256214143
  batch 750 loss: 1.0842642760276795
  batch 800 loss: 1.0915913939476014
  batch 850 loss: 1.103627508878708
  batch 900 loss: 1.1055996930599212
avg val loss: 3.0051803588867188
LOSS train 1.10560 valid 3.00518, valid PER 31.80%
EPOCH 11, Learning Rate: 0.00703125
  batch 50 loss: 1.0697784888744355
  batch 100 loss: 1.061853210926056
  batch 150 loss: 1.066186980009079
  batch 200 loss: 1.1168891775608063
  batch 250 loss: 1.109508022069931
  batch 300 loss: 1.0494341087341308
  batch 350 loss: 1.0910417652130127
  batch 400 loss: 1.0988557708263398
  batch 450 loss: 1.0853467655181885
  batch 500 loss: 1.0515051913261413
  batch 550 loss: 1.079332377910614
  batch 600 loss: 1.0683124959468842
  batch 650 loss: 1.1383753252029418
  batch 700 loss: 1.047704668045044
  batch 750 loss: 1.0553287065029144
  batch 800 loss: 1.108048713207245
  batch 850 loss: 1.0970844376087188
  batch 900 loss: 1.1037857556343078
avg val loss: 3.0057735443115234
LOSS train 1.10379 valid 3.00577, valid PER 31.69%
EPOCH 12, Learning Rate: 0.003515625
  batch 50 loss: 1.0982693135738373
  batch 100 loss: 1.0859844875335694
  batch 150 loss: 1.0433501505851746
  batch 200 loss: 1.068714473247528
  batch 250 loss: 1.097694420814514
  batch 300 loss: 1.0788255321979523
  batch 350 loss: 1.0671183621883393
  batch 400 loss: 1.0874654984474181
  batch 450 loss: 1.1023318195343017
  batch 500 loss: 1.1066811966896057
  batch 550 loss: 1.0277131307125091
  batch 600 loss: 1.0632854223251342
  batch 650 loss: 1.1137908053398133
  batch 700 loss: 1.0799022507667542
  batch 750 loss: 1.0702598512172699
  batch 800 loss: 1.055340085029602
  batch 850 loss: 1.1120535957813262
  batch 900 loss: 1.121933456659317
avg val loss: 3.009664297103882
LOSS train 1.12193 valid 3.00966, valid PER 31.70%
EPOCH 13, Learning Rate: 0.0017578125
  batch 50 loss: 1.0814033961296081
  batch 100 loss: 1.0908442449569702
  batch 150 loss: 1.063547990322113
  batch 200 loss: 1.0873400938510895
  batch 250 loss: 1.0717815208435058
  batch 300 loss: 1.080015391111374
  batch 350 loss: 1.0595455181598663
  batch 400 loss: 1.1008261740207672
  batch 450 loss: 1.0912690711021424
  batch 500 loss: 1.0604374253749846
  batch 550 loss: 1.085369622707367
  batch 600 loss: 1.0554061245918274
  batch 650 loss: 1.0941285133361816
  batch 700 loss: 1.1049457466602326
  batch 750 loss: 1.0516817605495452
  batch 800 loss: 1.0760448861122132
  batch 850 loss: 1.0957251167297364
  batch 900 loss: 1.0906222546100617
avg val loss: 3.007554292678833
LOSS train 1.09062 valid 3.00755, valid PER 31.71%
EPOCH 14, Learning Rate: 0.00087890625
  batch 50 loss: 1.0772086489200592
  batch 100 loss: 1.0861104583740235
  batch 150 loss: 1.0786020827293397
  batch 200 loss: 1.0739100921154021
  batch 250 loss: 1.081000975370407
  batch 300 loss: 1.10443009018898
  batch 350 loss: 1.0545002174377442
  batch 400 loss: 1.0782138752937316
  batch 450 loss: 1.0864472842216493
  batch 500 loss: 1.0697295677661895
  batch 550 loss: 1.0843291676044464
  batch 600 loss: 1.0713043534755706
  batch 650 loss: 1.086286609172821
  batch 700 loss: 1.118244037628174
  batch 750 loss: 1.0648271358013153
  batch 800 loss: 1.0312016415596008
  batch 850 loss: 1.0750868225097656
  batch 900 loss: 1.0722414648532868
avg val loss: 3.008060932159424
LOSS train 1.07224 valid 3.00806, valid PER 31.71%
EPOCH 15, Learning Rate: 0.000439453125
  batch 50 loss: 1.1048468744754791
  batch 100 loss: 1.0927485346794128
  batch 150 loss: 1.070120724439621
  batch 200 loss: 1.0885160303115844
  batch 250 loss: 1.0869374132156373
  batch 300 loss: 1.0578177797794341
  batch 350 loss: 1.0693666517734528
  batch 400 loss: 1.0729691445827485
  batch 450 loss: 1.069338412284851
  batch 500 loss: 1.0463359427452088
  batch 550 loss: 1.0751639580726624
  batch 600 loss: 1.097648923397064
  batch 650 loss: 1.1047578322887421
  batch 700 loss: 1.0845626270771027
  batch 750 loss: 1.0851445972919465
  batch 800 loss: 1.0829563856124877
  batch 850 loss: 1.0414202845096587
  batch 900 loss: 1.0931315886974335
avg val loss: 3.007704019546509
LOSS train 1.09313 valid 3.00770, valid PER 31.70%
EPOCH 16, Learning Rate: 0.0002197265625
  batch 50 loss: 1.1061672532558442
  batch 100 loss: 1.0545355224609374
  batch 150 loss: 1.0819864308834075
  batch 200 loss: 1.0735065519809723
  batch 250 loss: 1.0961734390258788
  batch 300 loss: 1.0851333248615265
  batch 350 loss: 1.088633371591568
  batch 400 loss: 1.1081040596961975
  batch 450 loss: 1.106384425163269
  batch 500 loss: 1.048098281621933
  batch 550 loss: 1.068962494134903
  batch 600 loss: 1.0876874423027039
  batch 650 loss: 1.0622151064872742
  batch 700 loss: 1.0549383270740509
  batch 750 loss: 1.066009155511856
  batch 800 loss: 1.101925904750824
  batch 850 loss: 1.0850079107284545
  batch 900 loss: 1.0801753902435303
avg val loss: 3.0075759887695312
LOSS train 1.08018 valid 3.00758, valid PER 31.68%
EPOCH 17, Learning Rate: 0.00010986328125
  batch 50 loss: 1.113275396823883
  batch 100 loss: 1.1056382989883422
  batch 150 loss: 1.0694237387180328
  batch 200 loss: 1.061937620639801
  batch 250 loss: 1.0771031868457794
  batch 300 loss: 1.0851558172702789
  batch 350 loss: 1.0568766582012177
  batch 400 loss: 1.1160834991931916
  batch 450 loss: 1.090940169095993
  batch 500 loss: 1.0693393802642823
  batch 550 loss: 1.0758326292037963
  batch 600 loss: 1.1161657190322876
  batch 650 loss: 1.0674240243434907
  batch 700 loss: 1.0567895841598511
  batch 750 loss: 1.062304517030716
  batch 800 loss: 1.0497978711128235
  batch 850 loss: 1.0846265745162964
  batch 900 loss: 1.0534973466396331
avg val loss: 3.0074877738952637
LOSS train 1.05350 valid 3.00749, valid PER 31.68%
EPOCH 18, Learning Rate: 5.4931640625e-05
  batch 50 loss: 1.098585238456726
  batch 100 loss: 1.0834366357326508
  batch 150 loss: 1.0780231034755707
  batch 200 loss: 1.082507072687149
  batch 250 loss: 1.0847145438194274
  batch 300 loss: 1.0581722033023835
  batch 350 loss: 1.081389501094818
  batch 400 loss: 1.0626002371311187
  batch 450 loss: 1.1089533746242524
  batch 500 loss: 1.091723040342331
  batch 550 loss: 1.1039068472385407
  batch 600 loss: 1.0599234461784364
  batch 650 loss: 1.0543917059898376
  batch 700 loss: 1.1132946169376374
  batch 750 loss: 1.0723377776145935
  batch 800 loss: 1.0683312606811524
  batch 850 loss: 1.0316062915325164
  batch 900 loss: 1.094249336719513
avg val loss: 3.007439136505127
LOSS train 1.09425 valid 3.00744, valid PER 31.69%
EPOCH 19, Learning Rate: 2.74658203125e-05
  batch 50 loss: 1.0586341857910155
  batch 100 loss: 1.065269935131073
  batch 150 loss: 1.0837410759925843
  batch 200 loss: 1.0842679464817047
  batch 250 loss: 1.0941674566268922
  batch 300 loss: 1.0754559493064881
  batch 350 loss: 1.0592362678050995
  batch 400 loss: 1.0777670228481293
  batch 450 loss: 1.0900447952747345
  batch 500 loss: 1.0873589789867402
  batch 550 loss: 1.056848063468933
  batch 600 loss: 1.084323731660843
  batch 650 loss: 1.119572056531906
  batch 700 loss: 1.0676704573631286
  batch 750 loss: 1.0755773818492889
  batch 800 loss: 1.08755455493927
  batch 850 loss: 1.0808791077136994
  batch 900 loss: 1.0639908826351165
avg val loss: 3.0075008869171143
LOSS train 1.06399 valid 3.00750, valid PER 31.69%
EPOCH 20, Learning Rate: 1.373291015625e-05
  batch 50 loss: 1.0768291807174684
  batch 100 loss: 1.0707149004936218
  batch 150 loss: 1.0741345477104187
  batch 200 loss: 1.0979007184505463
  batch 250 loss: 1.062652269601822
  batch 300 loss: 1.0887085747718812
  batch 350 loss: 1.0317195796966552
  batch 400 loss: 1.0759741318225862
  batch 450 loss: 1.0828085958957672
  batch 500 loss: 1.0593904399871825
  batch 550 loss: 1.131488859653473
  batch 600 loss: 1.0238284730911256
  batch 650 loss: 1.084088990688324
  batch 700 loss: 1.0875296783447266
  batch 750 loss: 1.0651726055145263
  batch 800 loss: 1.1019557452201842
  batch 850 loss: 1.0883035171031952
  batch 900 loss: 1.0978133690357208
avg val loss: 3.00748348236084
LOSS train 1.09781 valid 3.00748, valid PER 31.69%
Training finished in 3.0 minutes.
Model saved to checkpoints/20231210_144610/model_4
Loading model from checkpoints/20231210_144610/model_4
SUB: 16.52%, DEL: 19.41%, INS: 1.40%, COR: 64.08%, PER: 37.33%
